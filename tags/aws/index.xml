<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Aws on Kubermates</title><link>https://kubermates.org/tags/aws/</link><description>Recent content in Aws on Kubermates</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Fri, 10 Oct 2025 21:13:56 +0000</lastBuildDate><atom:link href="https://kubermates.org/tags/aws/index.xml" rel="self" type="application/rss+xml"/><item><title>SaaS deployment architectures with Amazon EKS</title><link>https://kubermates.org/docs/2025-10-10-saas-deployment-architectures-with-amazon-eks/</link><pubDate>Fri, 10 Oct 2025 21:13:56 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-10-saas-deployment-architectures-with-amazon-eks/</guid><description>SaaS deployment architectures with Amazon EKS Patterns for managing remote environment with Amazon EKS in its core Shared responsibility Building distributed SaaS on Kubernetes Application packaging and deployment SaaS Provider Hosted Remote Application Plane Customer Hosted Data Plane with EKS Hybrid Nodes Conclusion About the authors As companies scale their software as a service (SaaS) offerings, they’re expanding their market reach by offering flexible deployment options directly within their customers’ environments. This versatile deployment model enables organizations to maintain data sovereignty, meet compliance standards, achieve optimal performance through reduced latency, and maximize efficiency by running applications close to existing customer datasets. SaaS providers can embrace this approach to serve a broader range of industries and unlock new business opportunities, particularly in highly regulated sectors and performance-sensitive markets. This method of extending the deployment environments into the tenant’s owned environments has been labelled “SaaS Anywhere” in this post. Although SaaS Anywhere solves important challenges related to managing remote customers’ environments, they introduce significant complexity in management and operation. SaaS providers must develop robust systems for provisioning and maintaining their application stack across numerous customer environments, implement cross-account monitoring solutions, manage distributed lifecycle updates, and provide consistent security controls. All of this is done while maintaining operational excellence at scale. In this post we explore patterns and practices for building and operating these distributed Amazon Elastic Kubernetes Service (Amazon EKS )-based applications effectively. When designing SaaS solutions, organizations typically employ one of three deployment models, each offering distinct advantages for specific use cases: SaaS Provider Hosted : Both data and control planes reside in the provider’s Amazon Web Services (AWS) account, optimizing for operational efficiency and rapid customer onboarding. Although lightweight agents may exist in customer environments for telemetry, all core processing remains provider hosted. Remote Application Plane : The data plane runs in the customer’s environment while the control plane stays in the provider’s account. This model balances compliance needs with operational efficiency, allowing customers to maintain data sovereignty while using AWS services.</description></item><item><title>Implementing granular failover in multi-Region Amazon EKS</title><link>https://kubermates.org/docs/2025-09-18-implementing-granular-failover-in-multi-region-amazon-eks/</link><pubDate>Thu, 18 Sep 2025 14:13:01 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-18-implementing-granular-failover-in-multi-region-amazon-eks/</guid><description>Implementing granular failover in multi-Region Amazon EKS Typical architecture The all-or-nothing health check problem Solution overview Prerequisites Walkthrough Configure environment variables Create EKS clusters in Region 1 and Region 2 Create IngressClass configuration Deploy app1 and app2 on the EKS clusters Configure Route 53 health checks for app1 and app2 Configure Route 53 alias records for app1 and app2 Test application failure Things to know Cleaning up Conclusion About the authors Enterprises across industries operate tens of millions of Amazon Elastic Kubernetes Service (Amazon EKS) clusters annually, supporting use cases from web/mobile applications to data processing, machine learning (ML), generative AI, gaming, and Internet of Things (IoT). As organizations increasingly adopt multi-tenant platform models where multiple application teams share an EKS cluster, they need more granular control over application availability in their multi-Region architectures—particularly for serving global users and meeting strict regulatory requirements. Although multi-tenant models optimize resource usage and reduce operational overhead, they present challenges when applications need individualized Recovery Point Objective (RPO) and Recovery Time Objective (RTO) targets across multiple AWS Regions. Traditional approaches force uniform failover policies across all applications where a single application’s failure creates an unnecessary compromise between operational efficiency and application-specific resilience. In this post, we demonstrate how to configure Amazon Route 53 to enable unique failover behavior for each application within your multi-tenant Amazon EKS environment across AWS Regions, which allows you to maintain the cost benefits of shared infrastructure while meeting diverse availability requirements. Before diving into the solution, we demonstrate how a typical multi-Region multi-tenant Amazon EKS architecture is structured, and the key components that enable Regional traffic routing. Figure 1 consists of the following key components: Route 53 Routes traffic to the Application Load Balancer (ALB) in the respective Region Makes sure of high availability and resiliency during failure events Routes traffic to the Application Load Balancer (ALB) in the respective Region Makes sure of high availability and resiliency during failure events AWS Load Balancer Controller Exposes applications running on the EKS cluster through ALB Uses IP target type to register application Pods to the ALB Exposes applications running on the EKS cluster through ALB Uses IP target type to register application Pods to the ALB The Route 53 configuration details are as follows: Regional Routing Each AWS Region uses a Route 53 alias record to route traffic to its Region-specific ALB. Example: app1. example. com points to ALB in Region 1 Example: app2. example. com points to ALB in Region 2 Each AWS Region uses a Route 53 alias record to route traffic to its Region-specific ALB.</description></item><item><title>Use Raspberry Pi 5 as Amazon EKS Hybrid Nodes for edge workloads</title><link>https://kubermates.org/docs/2025-09-17-use-raspberry-pi-5-as-amazon-eks-hybrid-nodes-for-edge-workloads/</link><pubDate>Wed, 17 Sep 2025 15:08:59 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-17-use-raspberry-pi-5-as-amazon-eks-hybrid-nodes-for-edge-workloads/</guid><description>Use Raspberry Pi 5 as Amazon EKS Hybrid Nodes for edge workloads Why Raspberry Pi 5? Architectural overview Getting started Step 1: Create the EKS cluster Step 2: Set up the VPN server Add the Raspberry Pi to the cluster as a remote node Setting up the Container Network Interface Step 1: Install Cilium Deploying a sample application on Amazon EKS Hybrid Nodes with edge integration Step 1: Hardware requirements and setup Step 2: Deploy the DynamoDB table Step 3: Deploy the sensor application Step 4: Deploy the frontend dashboard Conclusion About the authors Since its launch, Amazon Elastic Kubernetes Service (Amazon EKS) has powered tens of millions of clusters so that users can accelerate application deployment, optimize costs, and use the flexibility of Amazon Web Services (AWS) for hosting containerized applications. Amazon EKS eliminates the operational complexities of maintaining Kubernetes control plane infrastructure, while offering seamless integration with AWS resources and infrastructure. However, some workloads need to be run at the edge with real-time processing, such as latency-sensitive applications that generate large volumes of data. In these scenarios, when there is consistent internet connectivity available, users often seek the benefits of cloud integrations while continuing to use their on-premises hardware. That’s why we introduced Amazon EKS Hybrid Nodes at AWS re:Invent 2024, so that users can extend their Kubernetes data plane to the edge while continuing to run the Kubernetes control plane in an AWS Region. Amazon EKS Hybrid Nodes unifies Kubernetes management across cloud, on-premises, and edge environments by enabling users to use their on-premises infrastructure as nodes in EKS clusters, alongside Amazon Elastic Compute Cloud (Amazon EC2). To demonstrate the use of Amazon EKS Hybrid Nodes, we explored a practical use case from the manufacturing sector. These environments often rely on real-time data from digital sensors that must be processed locally due to latency and reliability, while still using the cloud for analytics and long-term storage. Our use case involves reading distance values from an ultrasonic sensor, processing them on a local edge device running as a Hybrid Node, and storing them in Amazon DynamoDB on AWS. In this post, we demonstrate how to implement Amazon EKS Hybrid Nodes using the Raspberry Pi 5 , a popular edge computing platform. We cover the following: Setting up an EKS cluster that seamlessly connects cloud and edge infrastructure Securing connectivity using the WireGuard VPN for site-to-site communication Enabling container networking with Cilium for hybrid node deployments Demonstrating a real-world Internet of Things (IoT) application that demonstrates the power of edge-cloud integration The Raspberry Pi 5 is compact and can be deployed at the edge so that you can process data before it is transmitted to the cloud. Building on this strength, we created a microservices-based application partly running on the edge on a Raspberry Pi 5 and partly on AWS in the cloud.</description></item><item><title>Kubernetes right-sizing with metrics-driven GitOps automation</title><link>https://kubermates.org/docs/2025-09-11-kubernetes-right-sizing-with-metrics-driven-gitops-automation/</link><pubDate>Thu, 11 Sep 2025 15:17:23 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-11-kubernetes-right-sizing-with-metrics-driven-gitops-automation/</guid><description>Kubernetes right-sizing with metrics-driven GitOps automation Understanding the challenges of resource management in Amazon EKS The impact of inefficient resource management Existing solutions for Kubernetes resource management How the proposed solution addresses the challenges Solution overview Workflow overview Key architectural considerations Walkthrough Prerequisites Implementing a GitOps-driven automation for resource optimization GitOps principle Setting up the recommendation generator Environment and metrics source Local or External Installation Automating the GitOps workflow Cleaning up Conclusion About the authors Efficient resource allocation in Kubernetes is essential for optimizing application performance and controlling costs. In Amazon Elastic Kubernetes Service (Amazon EKS) , managing resource requests and limits manually can be challenging and error-prone. This post introduces an automated, and GitOps-driven approach to resource optimization using Amazon Web Services (AWS) services such as Amazon Managed Service for Prometheus and Amazon Bedrock. This approach is particularly beneficial for users who prefer non-intrusive methods for resource optimization. Understanding resource management in Kubernetes is crucial for optimal cluster performance. When deploying pods, the Kubernetes scheduler evaluates resource requests to find suitable nodes that can accommodate the specified CPU and memory requirements. These requests act as the minimum guaranteed resources for the pod, while limits serve as upper bounds to prevent any single pod from monopolizing node resources. Over-provisioning and under-provisioning of resources in Kubernetes can lead to increased costs and performance issues. Striking the right balance is essential for optimal resource usage. In a shared environment, one pod consuming excessive resources can degrade the performance of others on the same node. Applications with fluctuating resource demands can be challenging to manage. Without adaptive resource allocation strategies, these workloads may experience performance degradation or resource waste.</description></item><item><title>How to build highly available Kubernetes applications with Amazon EKS Auto Mode</title><link>https://kubermates.org/docs/2025-09-09-how-to-build-highly-available-kubernetes-applications-with-amazon-eks-auto-mode/</link><pubDate>Tue, 09 Sep 2025 17:49:16 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-09-how-to-build-highly-available-kubernetes-applications-with-amazon-eks-auto-mode/</guid><description>How to build highly available Kubernetes applications with Amazon EKS Auto Mode Solution overview Test scenarios Pod fail scenario Node fail scenario AZ fail scenario Cluster version upgrade scenario Karpenter node disruption Conclusion About the authors As organizations scale their Kubernetes deployments, many find themselves facing critical operational challenges. Consider how DevOps teams spend countless hours planning and executing cluster upgrades, managing add-ons, and making sure that security patches are applied consistently. There is a clear need for reliable, automated cluster lifecycle management with teams struggling to maintain consistent cluster configurations and security postures across environments. Amazon Elastic Kubernetes Service (Amazon EKS) Auto Mode addresses these challenges by automating control plane updates, streamlining add-on management, and making sure that clusters maintain current best practices. This post explores the capabilities of EKS Auto Mode in depth, subjecting it to a series of challenging scenarios such as failure simulations, node recycling, and cluster upgrades—all while maintaining uninterrupted service traffic. This guide delves into strategies for achieving high availability in the face of the dynamic nature of EKS Auto Mode by using a range of Kubernetes features to maximize uptime. The goal is to provide a comprehensive guide that shows how to harness the potential of EKS Auto Mode, thus making sure that your services remain robust and resilient in the demanding environments. Although there is a wealth of comprehensive literature on the broader subject of reliability in container ecosystems, this post specifically narrows its scope to the nuanced considerations of operating reliable workloads within EKS Auto Mode environments. Before delving into the specifics of Amazon EKS and its Auto Mode feature , you must understand key Kubernetes concepts that are instrumental in maximizing service uptime during different cluster events. These foundational elements form the bedrock of resilient application architectures in Kubernetes environments, regardless of the specific cloud provider or management mode. Mastering these concepts enables you to use EKS Auto Mode effectively and build highly available systems that withstand various operational challenges. In the rest of this section we explore these essential Kubernetes features that play a pivotal role in maintaining service continuity during both planned and unplanned events.</description></item><item><title>How to run AI model inference with GPUs on Amazon EKS Auto Mode</title><link>https://kubermates.org/docs/2025-09-04-how-to-run-ai-model-inference-with-gpus-on-amazon-eks-auto-mode/</link><pubDate>Thu, 04 Sep 2025 16:12:23 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-04-how-to-run-ai-model-inference-with-gpus-on-amazon-eks-auto-mode/</guid><description>How to run AI model inference with GPUs on Amazon EKS Auto Mode Key features that make EKS Auto Mode ideal for AI/ML workloads Walkthrough Prerequisites Set up environment variables Set up EKS Auto Mode cluster and run a model Reducing model cold start time in AI inference workloads Conclusion About the authors AI model inference using GPUs is becoming a core part of modern applications, powering real-time recommendations, intelligent assistants, content generation, and other latency-sensitive AI features. Kubernetes has become the orchestrator of choice for running inference workloads, and organizations want to use its capabilities while still maintaining a strong focus on rapid innovation and time-to-market. But here’s the challenge: while teams see the value of Kubernetes for its dynamic scaling and efficient resource management, they often get slowed down by the need to learn Kubernetes concepts, manage cluster configurations, and handle security updates. This shifts focus away from what matters most: deploying and optimizing AI models. That is where Amazon Elastic Kubernetes Service (Amazon EKS) Auto Mode comes in. EKS Auto Mode Automates node creation, manages core capabilities , and handles upgrades and security patching. In turn, this enables to run your inference workloads without the operational overhead. In this post, we show you how to swiftly deploy inference workloads on EKS Auto Mode. We also demonstrate key features that streamline GPU management, show best practices for model deployment, and walk through a practical example by deploying open weight models from OpenAI using vLLM. Whether you’re building a new AI/machine learning (ML) platform or optimizing existing workflows, these patterns help you accelerate development while maintaining operational efficiency. In this section, we take a closer look at the GPU-specific features that come pre-configured and ready to use with an EKS Auto Mode cluster. These capabilities are also available in self-managed Amazon EKS environments, but they typically need manual setup and tuning.</description></item><item><title>Dynamic Kubernetes request right sizing with Kubecost</title><link>https://kubermates.org/docs/2025-09-03-dynamic-kubernetes-request-right-sizing-with-kubecost/</link><pubDate>Wed, 03 Sep 2025 18:52:27 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-03-dynamic-kubernetes-request-right-sizing-with-kubecost/</guid><description>Dynamic Kubernetes request right sizing with Kubecost What are container requests? Kubecost savings insights Customizing recommendations Acting on Kubecost recommendations One-time resizing Scheduled right sizing Automating resizing with Helm Conclusion About the authors This post was co-written with Kai Wombacher, Founding Product Manager at Kubecost. In this post we show you how to use the Kubecost Amazon Elastic Kubernetes Service (Amazon EKS) add-on to lower infrastructure costs and boost Kubernetes efficiency. The Container Request Right Sizing feature allows you to find how container requests are configured, look for inefficiencies, and fix them either manually or through automated remediation. Specifically, we cover how to review Kubecost’s right sizing recommendations and take action on them using one-time updates or scheduled, automated resizing within your Amazon EKS environment to continuously optimize resource usage. Over-requested containers are one of the most common sources of cloud resource waste in Kubernetes environments. Without visibility and automation, development teams can request far more resources than their applications use, which leads to overprovisioned nodes and higher costs. In Kubernetes, a container request is a declared amount of CPU and memory that a workload needs. It plays a crucial role in how workloads are scheduled and how nodes are used. When a container specifies a CPU or memory request, the scheduler looks for a node that has at least that amount of unallocated capacity. When a pod is placed on a node, the requested resources are essentially reserved, regardless of whether the container uses them in practice. Although this reservation behavior makes sure that workloads have access to the resources they need, it can also lead to inefficient resource usage if requests are set too high. For example, if a container requests 1 CPU but only uses 200 millicores (0.2 CPU), then that added 0.8 CPU goes unused, yet the node capacity is still reserved and charged for.</description></item><item><title>Unlocking next-generation AI performance with Dynamic Resource Allocation on Amazon EKS and Amazon EC2 P6e-GB200</title><link>https://kubermates.org/docs/2025-09-02-unlocking-next-generation-ai-performance-with-dynamic-resource-allocation-on-ama/</link><pubDate>Tue, 02 Sep 2025 23:33:59 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-02-unlocking-next-generation-ai-performance-with-dynamic-resource-allocation-on-ama/</guid><description>Unlocking next-generation AI performance with Dynamic Resource Allocation on Amazon EKS and Amazon EC2 P6e-GB200 The power behind P6e-GB200: NVIDIA GB200 Grace Blackwell architecture Understanding EC2 P6e-GB200 UltraServer architecture Integrating P6e-GB200 UltraServers with Amazon EKS The challenge: running distributed AI workloads on Kubernetes The solution: Kubernetes DRA and IMEX How DRA solves traditional GPU allocation problems Topology-aware scheduling and memory coherence Workload scheduling flow with DRA How to use p6e-GB200 with Kubernetes DRA with Amazon EKS Prerequisites Step 1: Reserve P6e-GB200 UltraServer capacity Step 2: Create the EKS cluster configuration file Step 3: Deploy the EKS cluster Step 4: Deploy the NVIDIA GPU Operator Step 5: Install the NVIDIA DRA Driver Step 6: Verify DRA resources Validating IMEX channel allocation Apply and validate Multi-node IMEX communication in action Conclusion About the authors The rapid evolution of agentic AI and large language models (LLMs), particularly reasoning models, has created unprecedented demand for computational resources. Today’s most advanced AI models span hundreds of billions to trillions of parameters and necessitate massive computational power, extensive memory footprints, and ultra-fast interconnects to function efficiently. Organizations developing applications for natural language processing, scientific simulations, 3D content generation, and multimodal inference need infrastructure that can scale from today’s billion-parameter models to tomorrow’s trillion-parameter frontiers while maintaining performance. In this post, we explore how the new Amazon Elastic Compute Cloud (Amazon EC2) P6e-GB200 UltraServers are transforming distributed AI workload through seamless Kubernetes integration. Amazon Web Services (AWS) introduced the EC2 P6e-GB200 UltraServers to meet the growing demand for large-scale AI model training and inference. They represent a significant architectural breakthrough for distributed AI workloads. Furthermore, the EC2 P6e-GB200 UltraServer launch includes support for Amazon Elastic Kubernetes Service (Amazon EKS) , providing a Kubernetes-native environment for deploying and scaling from hundreds-of-billions to trillion-parameter models as the AI landscape continues to evolve. At the heart of EC2 P6e-GB200 UltraServers is the NVIDIA GB200 Grace Blackwell Superchip , which integrates two NVIDIA Blackwell GPUs with a NVIDIA Grace CPU. Furthermore, it provides NVLink-Chip-to-Chip (C2C) connection between these components, delivering 900 GB/s of bidirectional bandwidth, which is substantially faster than traditional PCIe interfaces. When deployed at rack scale, EC2 P6e-GB200 UltraServers participate in NVIDIA’s GB200 NVL72 architecture , creating memory-coherent domains of up to 72 GPUs. Fifth-generation NVLink technology enables GPU-to-GPU communication across discrete servers within the same domain at up to 1.8 TB/s per GPU. Critical to this performance is Elastic Fabric Adapter (EFAv4) networking, which delivers up to 28.8 Tbps of total network bandwidth per UltraServer.</description></item><item><title>Introducing Seekable OCI Parallel Pull mode for Amazon EKS</title><link>https://kubermates.org/docs/2025-08-27-introducing-seekable-oci-parallel-pull-mode-for-amazon-eks/</link><pubDate>Wed, 27 Aug 2025 19:13:44 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-27-introducing-seekable-oci-parallel-pull-mode-for-amazon-eks/</guid><description>Introducing Seekable OCI Parallel Pull mode for Amazon EKS Introducing Parallel Pull mode for the SOCI snapshotter Understanding image pulls SOCI Parallel Pull details Performance consideration SOCI Parallel Pull in action Tuning configuration Benchmark Getting started with SOCI Parallel Pull Mode About the authors Containerization has transformed how customers build and deploy modern cloud native applications, offering unparalleled benefits in portability, scalability, and operational efficiency. Containers provide integrated dependency management and enable a standard distribution and deployment model for any workload. With Amazon Elastic Kubernetes Service (Amazon EKS), Kubernetes has emerged as a go-to solution for customers running large-scale containerized workloads that need to efficiently scale to meet evolving needs. However, one persistent challenge continues to impact specific deployment and scaling aspects of Kubernetes workload operations. Container image pulls, particularly when working with large and complex container images, can directly impact the responsiveness and agility of your systems. With the growth of AI/ML workloads, where we see particularly large images, this directly impacts operations as images may take several minutes to pull and prepare. In our recent Under the Hood post for EKS Ultra Scale Clusters, we briefly touched on our evolving solution for this problem, Seekable OCI (SOCI) Parallel Pull. In this post, we’ll explain how container image pulls work and how they impact deployment and scaling operations, we’ll dive deeper into how SOCI parallel pull works, and finally show how it can help you improve image pull performance with your workloads on Amazon EKS. As average container image sizes have grown in recent years, container startup performance has become a critical element of modern cloud native system performance. Image pull and preparation can account for more than 75% of total startup time for new and scaling workloads. This challenge is particularly acute with the rise of AI/ML workloads on Amazon EKS. These workloads have driven significant growth in container image sizes, where images are commonly tens of gigabytes in size.</description></item><item><title>Migrate to Amazon EKS: Data plane cost modeling with Karpenter and KWOK</title><link>https://kubermates.org/docs/2025-08-21-migrate-to-amazon-eks-data-plane-cost-modeling-with-karpenter-and-kwok/</link><pubDate>Thu, 21 Aug 2025 18:18:42 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-21-migrate-to-amazon-eks-data-plane-cost-modeling-with-karpenter-and-kwok/</guid><description>Migrate to Amazon EKS: Data plane cost modeling with Karpenter and KWOK Solution overview Solution walkthrough Prerequisites Step 1: Create the source EKS cluster Step 2: Deploy an example workload Step 3: Extract cluster configuration with Velero Step 4: Create the destination EKS cluster Step 5: Deploy Karpenter with the KWOK provider Step 6: Restore the backup Clean up Conclusion About the authors When migrating Kubernetes clusters to Amazon Elastic Kubernetes Service (Amazon EKS) , organizations typically follow three phases: assessment, mobilize, and migrate and modernize. The assessment phase involves evaluating technical feasibility for Amazon EKS workloads, analyzing current Kubernetes environments, identifying compatibility issues, estimating costs, and determining timelines with business impact considerations. During the mobilize phase, organizations create detailed migration plans, establish EKS environments with proper networking and security, train teams, and develop testing procedures. The final migrate and modernize phase involves transferring applications and data, validating functionality, implementing cloud-centered features, optimizing resources and costs, and enhancing observability to fully use AWS capabilities. One of the most significant challenges organizations face during the process is cost estimation, which happens in the assessment phase. Karpenter is an open source Kubernetes node autoscaler that efficiently provisions just-in-time compute resources to match workload demands. Unlike traditional autoscalers, Karpenter directly integrates with cloud providers to make intelligent, real-time decisions about instance types, availability zones, and capacity options. It evaluates pod requirements and constraints to select optimal instances, considering factors such as CPU, memory, price, and availability. Karpenter can consolidate workloads for cost efficiency and rapidly scale from zero to handle sudden demand spikes. It supports both spot and on-demand instances, and automatically terminates nodes when they’re no longer needed, optimizing cluster resource utilization and reducing cloud costs. Karpenter uses the concept of Providers to interact with different infrastructure platforms for provisioning and managing compute resources. KWOK (Kubernetes WithOut Kubelet) is a toolkit that simulates data plane nodes without allocating actual infrastructure, and can be used as a provider to create lightweight testing environments that enable developers to validate provisioning decisions, try various (virtual) instance types, and debug scaling behaviors.</description></item><item><title>Canary delivery with Argo Rollout and Amazon VPC Lattice for Amazon EKS</title><link>https://kubermates.org/docs/2025-08-12-canary-delivery-with-argo-rollout-and-amazon-vpc-lattice-for-amazon-eks/</link><pubDate>Tue, 12 Aug 2025 23:55:07 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-12-canary-delivery-with-argo-rollout-and-amazon-vpc-lattice-for-amazon-eks/</guid><description>Canary delivery with Argo Rollout and Amazon VPC Lattice for Amazon EKS Solution overview Walkthrough: progressive delivery with VPC Lattice and Argo Rollouts Integration highlights Conclusion About the authors Modern application delivery demands agility and reliability, where updates are rolled out progressively while making sure of the minimal impact on end users. Progressive delivery strategies, such as canary deployments, allow organizations to release new features by shifting traffic incrementally between old and new versions of a service. This allows organizations to first release features to a small subset of users, monitor system behavior and performance in real time, and automatically roll back if anomalies are detected. This is particularly valuable in modern microservices environments running on platforms such as Amazon Elastic Kubernetes Service (Amazon EKS) , where service meshes and traffic routers provide the necessary infrastructure for fine-grained control over traffic routing. This post explores an architectural approach to implementing progressive delivery using Amazon VPC Lattice, Amazon CloudWatch Synthetics , and Argo Rollouts. The solution uses VPC Lattice for enhanced traffic control across microservices, CloudWatch Synthetics for real-time health and validation monitoring, and Argo Rollouts for orchestrating canary updates. The content in this post addresses readers who are already familiar with networking constructs on Amazon Web Services (AWS), such as Amazon Virtual Private Cloud (Amazon VPC) , CloudWatch Synthetics and Amazon EKS. Instead of defining these services, we focus on their capabilities and integration with VPC Lattice. We also build upon your existing understanding of VPC Lattice concepts and Argo Rollouts. For more background on Amazon VPC Lattice, we recommend that you review the post, Build secure multi-account multi-VPC connectivity for your applications with Amazon VPC Lattice , and the collection of resources in the VPC Lattice Getting started guide. The architecture integrates multiple AWS services and Kubernetes-native components, providing a comprehensive solution for progressive delivery: Amazon EKS : A fully managed Kubernetes service to host microservices. VPC Lattice : A service networking layer that enables consistent traffic routing, authentication, and observability across services.</description></item><item><title>Simplify network connectivity using Tailscale with Amazon EKS Hybrid Nodes</title><link>https://kubermates.org/docs/2025-08-06-simplify-network-connectivity-using-tailscale-with-amazon-eks-hybrid-nodes/</link><pubDate>Wed, 06 Aug 2025 22:12:21 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-06-simplify-network-connectivity-using-tailscale-with-amazon-eks-hybrid-nodes/</guid><description>Simplify network connectivity using Tailscale with Amazon EKS Hybrid Nodes Prerequisites Cost and security Architecture overview Step 1: Define a remote pod, and node network Step 2: Install Tailscale on your EKS Hybrid Node Step 3: Add a Tailscale subnet router inside your Amazon VPC Step 4: Update subnet routes in the Amazon VPC Step 5: Verify connectivity between the two Tailscale devices AWS CloudShell connectivity test: Step 6: Instructions for EKS Hybrid Nodes Cleaning up Conclusion Next steps About the authors This post was co-authored with Lee Briggs, Director of Solutions Engineering at Tailscale. In this post, we guide you through integrating Tailscale with your Amazon Elastic Kubernetes Service (EKS) Hybrid Nodes environment. Amazon EKS Hybrid Nodes is a feature of Amazon EKS that enables you to streamline your Kubernetes management by connecting on-premises and edge infrastructure to an EKS cluster running in Amazon Web Services (AWS). This unified approach allows AWS to manage the Kubernetes control plane in the cloud while you maintain your hybrid nodes in on-premises or edge locations. We demonstrate how to configure a remote pod network and node address space. Install Tailscale on your hybrid nodes, set up a subnet router within your Amazon Virtual Private Cloud (Amazon VPC) , and update your AWS routes accordingly. This integration provides direct, encrypted connections that streamline the network architecture needed for EKS Hybrid Nodes. Although EKS Hybrid Nodes streamlines the Kubernetes management challenge, network connectivity between your on-premises infrastructure and AWS remains a critical requirement. Tailscale can help streamline this network connectivity between your EKS Hybrid Nodes data plane and Amazon EKS Kubernetes control plane. Unlike traditional VPNs, which tunnel all network traffic through a central gateway server, Tailscale creates a peer-to-peer mesh network (known as a tailnet ). It enables encrypted point-to-point connections using the open source WireGuard protocol, connecting devices and services across different networks with enhanced security features. However, you can still use Tailscale like a traditional VPN.</description></item><item><title>Scaling beyond IPv4: integrating IPv6 Amazon EKS clusters into existing Istio Service Mesh</title><link>https://kubermates.org/docs/2025-07-22-scaling-beyond-ipv4-integrating-ipv6-amazon-eks-clusters-into-existing-istio-ser/</link><pubDate>Tue, 22 Jul 2025 18:54:25 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-22-scaling-beyond-ipv4-integrating-ipv6-amazon-eks-clusters-into-existing-istio-ser/</guid><description>Scaling beyond IPv4: integrating IPv6 Amazon EKS clusters into existing Istio Service Mesh Amazon EKS IPv6 interoperability with IPv4 in Istio Service Mesh Solution overview Istio Multi-Primary Multicluster deployment model on a single network Istio Multi-Primary Multicluster deployment model on multi-network Walkthrough Initial setup Conclusion About the authors Organizations are increasingly adopting IPv6 for their Amazon Elastic Kubernetes Service (Amazon EKS) deployments, driven by three key factors: depletion of private IPv4 addresses, the need to streamline or eliminate overlay networks, and improved network security requirements on Amazon Web Services (AWS). In IPv6-enabled EKS clusters, each pod receives a unique IPv6 address from the Amazon Virtual Private Cloud (Amazon VPC) IPv6 range, with seamless compatibility facilitated by the Amazon EKS VPC Container Network Interface (CNI). This solution effectively addresses two major IPv4 limitations: the scarcity of private addresses and the security vulnerabilities created by overlapping IPv4 spaces that need Network Address Translation (NAT) at the node level. When transitioning to IPv6, you likely need to run both IPv4 and IPv6 EKS clusters simultaneously. This is particularly important for organizations using Istio Service Mesh with Amazon EKS, because IPv6 clusters must integrate with the existing Service Mesh and work smoothly alongside IPv4 clusters. To streamline this transition, you can configure your Istio Service Mesh to support both your current IPv4 EKS clusters and your new IPv6 EKS clusters. If Istio Service Mesh isn’t part of your infrastructure, then we suggest exploring Amazon VPC Lattice as an alternative solution to speed up your IPv6 implementation on AWS. This post provides a step-by-step guide for combining IPv6-enabled EKS clusters with your existing Istio Service Mesh and IPv4 workloads, enabling a graceful transition to IPv6 on AWS. This guide covers detailed instructions for enabling communication between IPv6 and IPv4 EKS clusters, along with recommended practices for implementing IPv6 across both single and multiple VPC configurations. The functionality of Amazon EKS IPv6 builds on the native dual-stack capabilities of VPC. When you enable IPv6 in your VPC, it receives both IPv4 prefixes and a /56 IPv6 prefix. This IPv6 prefix can come from three sources: Amazon’s Global Unicast Address (GUA) space, your own IPv6 range (BYOIPv6), or a Unique Local Address (ULA) space.</description></item><item><title>Deep dive into cluster networking for Amazon EKS Hybrid Nodes</title><link>https://kubermates.org/docs/2025-07-21-deep-dive-into-cluster-networking-for-amazon-eks-hybrid-nodes/</link><pubDate>Mon, 21 Jul 2025 22:22:52 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-21-deep-dive-into-cluster-networking-for-amazon-eks-hybrid-nodes/</guid><description>Deep dive into cluster networking for Amazon EKS Hybrid Nodes Architecture overview CNI considerations Load balancing considerations Prerequisites Walkthrough BGP routing (Cilium example) Static routing (Calico example) On-premises load balancer (MetalLB example) External load balancer (AWS Load Balancer Controller example) Cleaning up Conclusion About the author Amazon Elastic Kubernetes Service ( Amazon EKS ) Hybrid Nodes enables organizations to integrate their existing on-premises and edge computing infrastructure into EKS clusters as remote nodes. EKS Hybrid Nodes provides you with the flexibility to run your containerized applications wherever needed, while maintaining standardized Kubernetes management practices and addressing latency, compliance, and data residency needs. EKS Hybrid Nodes accelerates infrastructure modernization by repurposing existing hardware investments. Organizations can harness the elastic scalability, high availability, and fully managed advantages of Amazon EKS, while making sure of operational consistency through unified workflows and toolsets across hybrid environments. One of the key aspects of the EKS Hybrid Nodes solution is the hybrid network architecture between the cloud-based Amazon EKS control plane and your on-premises nodes. This post dives deep into the cluster networking configurations, guiding you through the process of integrating an EKS cluster with hybrid nodes in your existing infrastructure. In this walkthrough, we set up different Container Network Interface (CNI) options and load balancing solutions on EKS Hybrid Nodes to meet your networking requirements. EKS Hybrid Nodes needs private network connectivity between the cloud-hosted Amazon EKS control plane and the hybrid nodes running in your on-premises environment. This connectivity can be established using either Amazon Web Services (AWS) Direct Connect or AWS Site-to-Site VPN , through an AWS Transit Gateway or the Virtual Private Gateway into your Amazon Virtual Private Cloud (Amazon VPC). For an optimal experience, AWS recommends reliable network connectivity with at least 100 Mbps bandwidth, and a maximum of 200ms round-trip latency, for hybrid nodes connecting to the AWS Region. This is general guidance rather than a strict requirement, and specific bandwidth and latency requirements may differ based on the quantity of hybrid nodes and your application’s unique characteristics. The node and pod Classless Inter-Domain Routing (CIDR) blocks for your hybrid nodes and container workloads must be within the IPv4 RFC-1918 ranges.</description></item><item><title>Under the hood: Amazon EKS ultra scale clusters</title><link>https://kubermates.org/docs/2025-07-16-under-the-hood-amazon-eks-ultra-scale-clusters/</link><pubDate>Wed, 16 Jul 2025 00:14:37 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-16-under-the-hood-amazon-eks-ultra-scale-clusters/</guid><description>This post was co-authored by Shyam Jeedigunta, Principal Engineer, Amazon EKS; Apoorva Kulkarni, Sr. Specialist Solutions Architect, Containers and Raghav Tripathi, Sr. Software Dev Manager, Amazon EKS. Today, Amazon Elastic Kubernetes Service (Amazon EKS) announced support for clusters with up to 100,000 nodes. With Amazon EC2’s new generation accelerated computing instance types, this translates to 1. 6 million AWS Trainium chips or 800,000 NVIDIA GPUs in a single Kubernetes cluster. This unlocks ultra scale artificial intelligence (AI) and machine leaning (ML) workloads such as state-of-the-art model training, fine-tuning and agentic inference. Besides customers directly consuming Amazon EKS today, these improvements also extend to other AI/ML services like Amazon SageMaker HyperPod with EKS that leverage EKS as their compute layer, advancing AWS’s overall ultra scale computing capabilities. Our customers have made it clear that containerization of training jobs and operators such as Kubeflow, the ability to streamline resource provisioning and lifecycle through projects like Karpenter, support for pluggable scheduling strategies, and access to a vast ecosystem of cloud-native tools is critical for their success in the AI/ML domain. Kubernetes has emerged as a key enabler here due to its powerful and extensible API model along with robust container orchestration capabilities, allowing accelerated workloads to scale quickly and run reliably. Through multiple technical innovations, architectural improvements and open-source collaboration, Amazon EKS has built the next generation of its cluster control plane and data plane for ultra scale, with full Kubernetes conformance. At AWS, we recommend customers running general-purpose applications with low coupling and horizontal scalability to follow a cell-based architecture as the strategy to sustain growth.</description></item><item><title>Amazon EKS enables ultra scale AI/ML workloads with support for 100K nodes per cluster</title><link>https://kubermates.org/docs/2025-07-16-amazon-eks-enables-ultra-scale-ai-ml-workloads-with-support-for-100k-nodes-per-c/</link><pubDate>Wed, 16 Jul 2025 00:14:26 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-16-amazon-eks-enables-ultra-scale-ai-ml-workloads-with-support-for-100k-nodes-per-c/</guid><description>&lt;p&gt;Open the original post ↗ &lt;a href="https://aws.amazon.com/blogs/containers/amazon-eks-enables-ultra-scale-ai-ml-workloads-with-support-for-100k-nodes-per-cluster/"&gt;https://aws.amazon.com/blogs/containers/amazon-eks-enables-ultra-scale-ai-ml-workloads-with-support-for-100k-nodes-per-cluster/&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Autoscaling in Kubernetes: KEDA, Karpenter, and Native Autoscalers</title><link>https://kubermates.org/blog/autoscaling-in-kubernetes-keda-karpenter-and-native-autoscalers-1gpo/</link><pubDate>Mon, 02 Sep 2024 11:41:41 +0000</pubDate><guid>https://kubermates.org/blog/autoscaling-in-kubernetes-keda-karpenter-and-native-autoscalers-1gpo/</guid><description>&lt;p&gt;Autoscaling is a critical component of any robust Kubernetes environment, ensuring your applications and infrastructure can dynamically adjust to meet demand. In this guide, we&amp;rsquo;ll explore three powerful autoscaling tools: &lt;strong&gt;KEDA&lt;/strong&gt; for event-driven pod autoscaling, &lt;strong&gt;Karpenter&lt;/strong&gt; for dynamic node scaling, and Kubernetes&amp;rsquo; native autoscalers (HPA and VPA). We&amp;rsquo;ll dive into how to use them effectively, with plenty of examples to get you started. 🚀&lt;/p&gt;
&lt;h2 id="introduction-to-keda-"&gt;Introduction to KEDA 🚀&lt;/h2&gt;
&lt;p&gt;KEDA (Kubernetes-based Event Driven Autoscaling) allows you to scale applications based on custom event metrics, not just CPU or memory usage. It’s ideal for scenarios where workloads are triggered by external events, such as message queues, databases, or HTTP requests. Whether you&amp;rsquo;re processing incoming orders, reacting to sensor data, or scaling based on custom Prometheus metrics, KEDA has you covered! 💥&lt;/p&gt;</description></item></channel></rss>