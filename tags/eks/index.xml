<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Eks on Kubermates</title><link>https://kubermates.org/tags/eks/</link><description>Recent content in Eks on Kubermates</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Tue, 02 Sep 2025 23:33:59 +0000</lastBuildDate><atom:link href="https://kubermates.org/tags/eks/index.xml" rel="self" type="application/rss+xml"/><item><title>Unlocking next-generation AI performance with Dynamic Resource Allocation on Amazon EKS and Amazon EC2 P6e-GB200</title><link>https://kubermates.org/docs/2025-09-02-unlocking-next-generation-ai-performance-with-dynamic-resource-allocation-on-ama/</link><pubDate>Tue, 02 Sep 2025 23:33:59 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-02-unlocking-next-generation-ai-performance-with-dynamic-resource-allocation-on-ama/</guid><description>Unlocking next-generation AI performance with Dynamic Resource Allocation on Amazon EKS and Amazon EC2 P6e-GB200 The power behind P6e-GB200: NVIDIA GB200 Grace Blackwell architecture Understanding EC2 P6e-GB200 UltraServer architecture Integrating P6e-GB200 UltraServers with Amazon EKS The challenge: running distributed AI workloads on Kubernetes The solution: Kubernetes DRA and IMEX How DRA solves traditional GPU allocation problems Topology-aware scheduling and memory coherence Workload scheduling flow with DRA How to use p6e-GB200 with Kubernetes DRA with Amazon EKS Prerequisites Step 1: Reserve P6e-GB200 UltraServer capacity Step 2: Create the EKS cluster configuration file Step 3: Deploy the EKS cluster Step 4: Deploy the NVIDIA GPU Operator Step 5: Install the NVIDIA DRA Driver Step 6: Verify DRA resources Validating IMEX channel allocation Apply and validate Multi-node IMEX communication in action Conclusion About the authors The rapid evolution of agentic AI and large language models (LLMs), particularly reasoning models, has created unprecedented demand for computational resources. Today’s most advanced AI models span hundreds of billions to trillions of parameters and necessitate massive computational power, extensive memory footprints, and ultra-fast interconnects to function efficiently. Organizations developing applications for natural language processing, scientific simulations, 3D content generation, and multimodal inference need infrastructure that can scale from today’s billion-parameter models to tomorrow’s trillion-parameter frontiers while maintaining performance. In this post, we explore how the new Amazon Elastic Compute Cloud (Amazon EC2) P6e-GB200 UltraServers are transforming distributed AI workload through seamless Kubernetes integration. Amazon Web Services (AWS) introduced the EC2 P6e-GB200 UltraServers to meet the growing demand for large-scale AI model training and inference. They represent a significant architectural breakthrough for distributed AI workloads. Furthermore, the EC2 P6e-GB200 UltraServer launch includes support for Amazon Elastic Kubernetes Service (Amazon EKS) , providing a Kubernetes-native environment for deploying and scaling from hundreds-of-billions to trillion-parameter models as the AI landscape continues to evolve. At the heart of EC2 P6e-GB200 UltraServers is the NVIDIA GB200 Grace Blackwell Superchip , which integrates two NVIDIA Blackwell GPUs with a NVIDIA Grace CPU. Furthermore, it provides NVLink-Chip-to-Chip (C2C) connection between these components, delivering 900 GB/s of bidirectional bandwidth, which is substantially faster than traditional PCIe interfaces. When deployed at rack scale, EC2 P6e-GB200 UltraServers participate in NVIDIA’s GB200 NVL72 architecture , creating memory-coherent domains of up to 72 GPUs. Fifth-generation NVLink technology enables GPU-to-GPU communication across discrete servers within the same domain at up to 1.8 TB/s per GPU. Critical to this performance is Elastic Fabric Adapter (EFAv4) networking, which delivers up to 28.8 Tbps of total network bandwidth per UltraServer.</description></item><item><title>Introducing Seekable OCI Parallel Pull mode for Amazon EKS</title><link>https://kubermates.org/docs/2025-08-27-introducing-seekable-oci-parallel-pull-mode-for-amazon-eks/</link><pubDate>Wed, 27 Aug 2025 19:13:44 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-27-introducing-seekable-oci-parallel-pull-mode-for-amazon-eks/</guid><description>Introducing Seekable OCI Parallel Pull mode for Amazon EKS Introducing Parallel Pull mode for the SOCI snapshotter Understanding image pulls SOCI Parallel Pull details Performance consideration SOCI Parallel Pull in action Tuning configuration Benchmark Getting started with SOCI Parallel Pull Mode About the authors Containerization has transformed how customers build and deploy modern cloud native applications, offering unparalleled benefits in portability, scalability, and operational efficiency. Containers provide integrated dependency management and enable a standard distribution and deployment model for any workload. With Amazon Elastic Kubernetes Service (Amazon EKS), Kubernetes has emerged as a go-to solution for customers running large-scale containerized workloads that need to efficiently scale to meet evolving needs. However, one persistent challenge continues to impact specific deployment and scaling aspects of Kubernetes workload operations. Container image pulls, particularly when working with large and complex container images, can directly impact the responsiveness and agility of your systems. With the growth of AI/ML workloads, where we see particularly large images, this directly impacts operations as images may take several minutes to pull and prepare. In our recent Under the Hood post for EKS Ultra Scale Clusters, we briefly touched on our evolving solution for this problem, Seekable OCI (SOCI) Parallel Pull. In this post, we’ll explain how container image pulls work and how they impact deployment and scaling operations, we’ll dive deeper into how SOCI parallel pull works, and finally show how it can help you improve image pull performance with your workloads on Amazon EKS. As average container image sizes have grown in recent years, container startup performance has become a critical element of modern cloud native system performance. Image pull and preparation can account for more than 75% of total startup time for new and scaling workloads. This challenge is particularly acute with the rise of AI/ML workloads on Amazon EKS. These workloads have driven significant growth in container image sizes, where images are commonly tens of gigabytes in size.</description></item><item><title>Refresh cluster insights</title><link>https://kubermates.org/releases/2025-08-27-refresh-cluster-insights/</link><pubDate>Wed, 27 Aug 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-27-refresh-cluster-insights/</guid><description>View cluster insights View configuration insights (Console) View upgrade insights (Console) View cluster insights (AWS CLI) Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Amazon EKS provides two types of insights: Configuration insights and Upgrade insights. Configuration insights identify misconfigurations in your EKS Hybrid Nodes setup that could impair functionality of your cluster or workloads. Upgrade insights identify issues that could impact your ability to upgrade to new versions of Kubernetes. To see the list of insight checks performed and any relevant issues that Amazon EKS has identified, you can call the look in the AWS Management Console, the AWS CLI, AWS SDKs, and Amazon EKS ListInsights API operation. ListInsights Open the Amazon EKS console. Open the Amazon EKS console. From the cluster list, choose the name of the Amazon EKS cluster for which you want to see the insights. Choose Monitor cluster. Choose the Cluster health tab. In the Configuration insights table, you will see the following columns: Name â The check that was performed by Amazon EKS against the cluster. Insight status â An insight with a status of Error means that there is a misconfiguration that is likely impacting cluster functionality.</description></item><item><title>AWS managed policy updates</title><link>https://kubermates.org/releases/2025-08-26-aws-managed-policy-updates/</link><pubDate>Tue, 26 Aug 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-26-aws-managed-policy-updates/</guid><description>AWS managed policies for Amazon Elastic Kubernetes Service AWS managed policy: AmazonEKS_CNI_Policy AWS managed policy: AmazonEKSClusterPolicy AWS managed policy: AmazonEKSDashboardConsoleReadOnly AWS managed policy: AmazonEKSFargatePodExecutionRolePolicy AWS managed policy: AmazonEKSForFargateServiceRolePolicy AWS managed policy: AmazonEKSComputePolicy Permissions details AWS managed policy: AmazonEKSNetworkingPolicy Permissions details AWS managed policy: AmazonEKSBlockStoragePolicy Permissions details AWS managed policy: AmazonEKSLoadBalancingPolicy Permissions details AWS managed policy: AmazonEKSServicePolicy AWS managed policy: AmazonEKSServiceRolePolicy AWS managed policy: AmazonEKSVPCResourceController AWS managed policy: AmazonEKSWorkerNodePolicy AWS managed policy: AmazonEKSWorkerNodeMinimalPolicy AWS managed policy: AWSServiceRoleForAmazonEKSNodegroup AWS managed policy: AmazonEKSDashboardServiceRolePolicy AWS managed policy: AmazonEBSCSIDriverPolicy AWS managed policy: AmazonEFSCSIDriverPolicy AWS managed policy: AmazonEKSLocalOutpostClusterPolicy AWS managed policy: AmazonEKSLocalOutpostServiceRolePolicy Amazon EKS updates to AWS managed policies Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. An AWS managed policy is a standalone policy that is created and administered by AWS. AWS managed policies are designed to provide permissions for many common use cases so that you can start assigning permissions to users, groups, and roles. Keep in mind that AWS managed policies might not grant least-privilege permissions for your specific use cases because theyâre available for all AWS customers to use. We recommend that you reduce permissions further by defining customer managed policies that are specific to your use cases. You cannot change the permissions defined in AWS managed policies. If AWS updates the permissions defined in an AWS managed policy, the update affects all principal identities (users, groups, and roles) that the policy is attached to. AWS is most likely to update an AWS managed policy when a new AWS service is launched or new API operations become available for existing services. For more information, see AWS managed policies in the IAM User Guide. You can attach the AmazonEKS_CNI_Policy to your IAM entities. Before you create an Amazon EC2 node group, this policy must be attached to either the node IAM role , or to an IAM role thatâs used specifically by the Amazon VPC CNI plugin for Kubernetes. This is so that it can perform actions on your behalf.</description></item><item><title>Migrate to Amazon EKS: Data plane cost modeling with Karpenter and KWOK</title><link>https://kubermates.org/docs/2025-08-21-migrate-to-amazon-eks-data-plane-cost-modeling-with-karpenter-and-kwok/</link><pubDate>Thu, 21 Aug 2025 18:18:42 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-21-migrate-to-amazon-eks-data-plane-cost-modeling-with-karpenter-and-kwok/</guid><description>Migrate to Amazon EKS: Data plane cost modeling with Karpenter and KWOK Solution overview Solution walkthrough Prerequisites Step 1: Create the source EKS cluster Step 2: Deploy an example workload Step 3: Extract cluster configuration with Velero Step 4: Create the destination EKS cluster Step 5: Deploy Karpenter with the KWOK provider Step 6: Restore the backup Clean up Conclusion About the authors When migrating Kubernetes clusters to Amazon Elastic Kubernetes Service (Amazon EKS) , organizations typically follow three phases: assessment, mobilize, and migrate and modernize. The assessment phase involves evaluating technical feasibility for Amazon EKS workloads, analyzing current Kubernetes environments, identifying compatibility issues, estimating costs, and determining timelines with business impact considerations. During the mobilize phase, organizations create detailed migration plans, establish EKS environments with proper networking and security, train teams, and develop testing procedures. The final migrate and modernize phase involves transferring applications and data, validating functionality, implementing cloud-centered features, optimizing resources and costs, and enhancing observability to fully use AWS capabilities. One of the most significant challenges organizations face during the process is cost estimation, which happens in the assessment phase. Karpenter is an open source Kubernetes node autoscaler that efficiently provisions just-in-time compute resources to match workload demands. Unlike traditional autoscalers, Karpenter directly integrates with cloud providers to make intelligent, real-time decisions about instance types, availability zones, and capacity options. It evaluates pod requirements and constraints to select optimal instances, considering factors such as CPU, memory, price, and availability. Karpenter can consolidate workloads for cost efficiency and rapidly scale from zero to handle sudden demand spikes. It supports both spot and on-demand instances, and automatically terminates nodes when they’re no longer needed, optimizing cluster resource utilization and reducing cloud costs. Karpenter uses the concept of Providers to interact with different infrastructure platforms for provisioning and managing compute resources. KWOK (Kubernetes WithOut Kubelet) is a toolkit that simulates data plane nodes without allocating actual infrastructure, and can be used as a provider to create lightweight testing environments that enable developers to validate provisioning decisions, try various (virtual) instance types, and debug scaling behaviors.</description></item><item><title>Cross-service confused deputy prevention</title><link>https://kubermates.org/releases/2025-08-19-cross-service-confused-deputy-prevention/</link><pubDate>Tue, 19 Aug 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-19-cross-service-confused-deputy-prevention/</guid><description>Cross-service confused deputy prevention in Amazon EKS Amazon EKS cluster role cross-service confused deputy prevention Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. The confused deputy problem is a security issue where an entity that doesnât have permission to perform an action can coerce a more-privileged entity to perform the action. In AWS, cross-service impersonation can result in the confused deputy problem. Cross-service impersonation can occur when one service (the calling service ) calls another service (the called service ). The calling service can be manipulated to use its permissions to act on another customerâs resources in a way it should not otherwise have permission to access. To prevent this, AWS provides tools that help you protect your data for all services with service principals that have been given access to resources in your account. We recommend using the aws:SourceArn , aws:SourceAccount global condition context keys in resource policies to limit the permissions that Amazon Elastic Kubernetes Service (Amazon EKS) gives another service to the resource. aws:SourceArn aws:SourceAccount aws:SourceArn Use aws:SourceArn to associate only one resource with cross-service access. aws:SourceArn aws:SourceAccount Use aws:SourceAccount to let any resource in that account be associated with the cross-service use. aws:SourceAccount The most effective way to protect against the confused deputy problem is to use the aws:SourceArn global condition context key with the full ARN of the resource. If you donât know the full ARN of the resource or if you are specifying multiple resources, use the aws:SourceArn global context condition key with wildcard characters (&lt;em&gt;) for the unknown portions of the ARN. For example, arn:aws:&lt;servicename&gt;:&lt;/em&gt;:&amp;lt;123456789012&amp;gt;:*.</description></item><item><title>Canary delivery with Argo Rollout and Amazon VPC Lattice for Amazon EKS</title><link>https://kubermates.org/docs/2025-08-12-canary-delivery-with-argo-rollout-and-amazon-vpc-lattice-for-amazon-eks/</link><pubDate>Tue, 12 Aug 2025 23:55:07 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-12-canary-delivery-with-argo-rollout-and-amazon-vpc-lattice-for-amazon-eks/</guid><description>Canary delivery with Argo Rollout and Amazon VPC Lattice for Amazon EKS Solution overview Walkthrough: progressive delivery with VPC Lattice and Argo Rollouts Integration highlights Conclusion About the authors Modern application delivery demands agility and reliability, where updates are rolled out progressively while making sure of the minimal impact on end users. Progressive delivery strategies, such as canary deployments, allow organizations to release new features by shifting traffic incrementally between old and new versions of a service. This allows organizations to first release features to a small subset of users, monitor system behavior and performance in real time, and automatically roll back if anomalies are detected. This is particularly valuable in modern microservices environments running on platforms such as Amazon Elastic Kubernetes Service (Amazon EKS) , where service meshes and traffic routers provide the necessary infrastructure for fine-grained control over traffic routing. This post explores an architectural approach to implementing progressive delivery using Amazon VPC Lattice, Amazon CloudWatch Synthetics , and Argo Rollouts. The solution uses VPC Lattice for enhanced traffic control across microservices, CloudWatch Synthetics for real-time health and validation monitoring, and Argo Rollouts for orchestrating canary updates. The content in this post addresses readers who are already familiar with networking constructs on Amazon Web Services (AWS), such as Amazon Virtual Private Cloud (Amazon VPC) , CloudWatch Synthetics and Amazon EKS. Instead of defining these services, we focus on their capabilities and integration with VPC Lattice. We also build upon your existing understanding of VPC Lattice concepts and Argo Rollouts. For more background on Amazon VPC Lattice, we recommend that you review the post, Build secure multi-account multi-VPC connectivity for your applications with Amazon VPC Lattice , and the collection of resources in the VPC Lattice Getting started guide. The architecture integrates multiple AWS services and Kubernetes-native components, providing a comprehensive solution for progressive delivery: Amazon EKS : A fully managed Kubernetes service to host microservices. VPC Lattice : A service networking layer that enables consistent traffic routing, authentication, and observability across services.</description></item><item><title>Simplify network connectivity using Tailscale with Amazon EKS Hybrid Nodes</title><link>https://kubermates.org/docs/2025-08-06-simplify-network-connectivity-using-tailscale-with-amazon-eks-hybrid-nodes/</link><pubDate>Wed, 06 Aug 2025 22:12:21 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-06-simplify-network-connectivity-using-tailscale-with-amazon-eks-hybrid-nodes/</guid><description>Simplify network connectivity using Tailscale with Amazon EKS Hybrid Nodes Prerequisites Cost and security Architecture overview Step 1: Define a remote pod, and node network Step 2: Install Tailscale on your EKS Hybrid Node Step 3: Add a Tailscale subnet router inside your Amazon VPC Step 4: Update subnet routes in the Amazon VPC Step 5: Verify connectivity between the two Tailscale devices AWS CloudShell connectivity test: Step 6: Instructions for EKS Hybrid Nodes Cleaning up Conclusion Next steps About the authors This post was co-authored with Lee Briggs, Director of Solutions Engineering at Tailscale. In this post, we guide you through integrating Tailscale with your Amazon Elastic Kubernetes Service (EKS) Hybrid Nodes environment. Amazon EKS Hybrid Nodes is a feature of Amazon EKS that enables you to streamline your Kubernetes management by connecting on-premises and edge infrastructure to an EKS cluster running in Amazon Web Services (AWS). This unified approach allows AWS to manage the Kubernetes control plane in the cloud while you maintain your hybrid nodes in on-premises or edge locations. We demonstrate how to configure a remote pod network and node address space. Install Tailscale on your hybrid nodes, set up a subnet router within your Amazon Virtual Private Cloud (Amazon VPC) , and update your AWS routes accordingly. This integration provides direct, encrypted connections that streamline the network architecture needed for EKS Hybrid Nodes. Although EKS Hybrid Nodes streamlines the Kubernetes management challenge, network connectivity between your on-premises infrastructure and AWS remains a critical requirement. Tailscale can help streamline this network connectivity between your EKS Hybrid Nodes data plane and Amazon EKS Kubernetes control plane. Unlike traditional VPNs, which tunnel all network traffic through a central gateway server, Tailscale creates a peer-to-peer mesh network (known as a tailnet ). It enables encrypted point-to-point connections using the open source WireGuard protocol, connecting devices and services across different networks with enhanced security features. However, you can still use Tailscale like a traditional VPN.</description></item><item><title>Amazon EKS platform version update</title><link>https://kubermates.org/releases/2025-07-30-amazon-eks-platform-version-update/</link><pubDate>Wed, 30 Jul 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-30-amazon-eks-platform-version-update/</guid><description>View Amazon EKS platform versions for each Kubernetes version Kubernetes version 1.33 Kubernetes version 1.32 Kubernetes version 1.31 Kubernetes version 1.30 Kubernetes version 1.29 Kubernetes version 1.28 Get current platform version Change platform version Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Amazon EKS platform versions represent the capabilities of the Amazon EKS cluster control plane, such as which Kubernetes API server flags are enabled, as well as the current Kubernetes patch version. Each Kubernetes minor version has one or more associated Amazon EKS platform versions. The platform versions for different Kubernetes minor versions are independent. You can retrieve your clusterâs current platform version using the AWS CLI or AWS Management Console. If you have a local cluster on AWS Outposts, see Learn Kubernetes and Amazon EKS platform versions for AWS Outposts instead of this topic. When a new Kubernetes minor version is available in Amazon EKS, such as 1.33, the initial Amazon EKS platform version for that Kubernetes minor version starts at eks. 1. However, Amazon EKS releases new platform versions periodically to enable new Kubernetes control plane settings and to provide security fixes. eks. 1 When new Amazon EKS platform versions become available for a minor version: The Amazon EKS platform version number is incremented ( eks. &amp;lt;n+1&amp;gt; ).</description></item><item><title>Scaling beyond IPv4: integrating IPv6 Amazon EKS clusters into existing Istio Service Mesh</title><link>https://kubermates.org/docs/2025-07-22-scaling-beyond-ipv4-integrating-ipv6-amazon-eks-clusters-into-existing-istio-ser/</link><pubDate>Tue, 22 Jul 2025 18:54:25 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-22-scaling-beyond-ipv4-integrating-ipv6-amazon-eks-clusters-into-existing-istio-ser/</guid><description>Scaling beyond IPv4: integrating IPv6 Amazon EKS clusters into existing Istio Service Mesh Amazon EKS IPv6 interoperability with IPv4 in Istio Service Mesh Solution overview Istio Multi-Primary Multicluster deployment model on a single network Istio Multi-Primary Multicluster deployment model on multi-network Walkthrough Initial setup Conclusion About the authors Organizations are increasingly adopting IPv6 for their Amazon Elastic Kubernetes Service (Amazon EKS) deployments, driven by three key factors: depletion of private IPv4 addresses, the need to streamline or eliminate overlay networks, and improved network security requirements on Amazon Web Services (AWS). In IPv6-enabled EKS clusters, each pod receives a unique IPv6 address from the Amazon Virtual Private Cloud (Amazon VPC) IPv6 range, with seamless compatibility facilitated by the Amazon EKS VPC Container Network Interface (CNI). This solution effectively addresses two major IPv4 limitations: the scarcity of private addresses and the security vulnerabilities created by overlapping IPv4 spaces that need Network Address Translation (NAT) at the node level. When transitioning to IPv6, you likely need to run both IPv4 and IPv6 EKS clusters simultaneously. This is particularly important for organizations using Istio Service Mesh with Amazon EKS, because IPv6 clusters must integrate with the existing Service Mesh and work smoothly alongside IPv4 clusters. To streamline this transition, you can configure your Istio Service Mesh to support both your current IPv4 EKS clusters and your new IPv6 EKS clusters. If Istio Service Mesh isn’t part of your infrastructure, then we suggest exploring Amazon VPC Lattice as an alternative solution to speed up your IPv6 implementation on AWS. This post provides a step-by-step guide for combining IPv6-enabled EKS clusters with your existing Istio Service Mesh and IPv4 workloads, enabling a graceful transition to IPv6 on AWS. This guide covers detailed instructions for enabling communication between IPv6 and IPv4 EKS clusters, along with recommended practices for implementing IPv6 across both single and multiple VPC configurations. The functionality of Amazon EKS IPv6 builds on the native dual-stack capabilities of VPC. When you enable IPv6 in your VPC, it receives both IPv4 prefixes and a /56 IPv6 prefix. This IPv6 prefix can come from three sources: Amazon’s Global Unicast Address (GUA) space, your own IPv6 range (BYOIPv6), or a Unique Local Address (ULA) space.</description></item><item><title>Deep dive into cluster networking for Amazon EKS Hybrid Nodes</title><link>https://kubermates.org/docs/2025-07-21-deep-dive-into-cluster-networking-for-amazon-eks-hybrid-nodes/</link><pubDate>Mon, 21 Jul 2025 22:22:52 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-21-deep-dive-into-cluster-networking-for-amazon-eks-hybrid-nodes/</guid><description>Deep dive into cluster networking for Amazon EKS Hybrid Nodes Architecture overview CNI considerations Load balancing considerations Prerequisites Walkthrough BGP routing (Cilium example) Static routing (Calico example) On-premises load balancer (MetalLB example) External load balancer (AWS Load Balancer Controller example) Cleaning up Conclusion About the author Amazon Elastic Kubernetes Service ( Amazon EKS ) Hybrid Nodes enables organizations to integrate their existing on-premises and edge computing infrastructure into EKS clusters as remote nodes. EKS Hybrid Nodes provides you with the flexibility to run your containerized applications wherever needed, while maintaining standardized Kubernetes management practices and addressing latency, compliance, and data residency needs. EKS Hybrid Nodes accelerates infrastructure modernization by repurposing existing hardware investments. Organizations can harness the elastic scalability, high availability, and fully managed advantages of Amazon EKS, while making sure of operational consistency through unified workflows and toolsets across hybrid environments. One of the key aspects of the EKS Hybrid Nodes solution is the hybrid network architecture between the cloud-based Amazon EKS control plane and your on-premises nodes. This post dives deep into the cluster networking configurations, guiding you through the process of integrating an EKS cluster with hybrid nodes in your existing infrastructure. In this walkthrough, we set up different Container Network Interface (CNI) options and load balancing solutions on EKS Hybrid Nodes to meet your networking requirements. EKS Hybrid Nodes needs private network connectivity between the cloud-hosted Amazon EKS control plane and the hybrid nodes running in your on-premises environment. This connectivity can be established using either Amazon Web Services (AWS) Direct Connect or AWS Site-to-Site VPN , through an AWS Transit Gateway or the Virtual Private Gateway into your Amazon Virtual Private Cloud (Amazon VPC). For an optimal experience, AWS recommends reliable network connectivity with at least 100 Mbps bandwidth, and a maximum of 200ms round-trip latency, for hybrid nodes connecting to the AWS Region. This is general guidance rather than a strict requirement, and specific bandwidth and latency requirements may differ based on the quantity of hybrid nodes and your application’s unique characteristics. The node and pod Classless Inter-Domain Routing (CIDR) blocks for your hybrid nodes and container workloads must be within the IPv4 RFC-1918 ranges.</description></item><item><title>Under the hood: Amazon EKS ultra scale clusters</title><link>https://kubermates.org/docs/2025-07-16-under-the-hood-amazon-eks-ultra-scale-clusters/</link><pubDate>Wed, 16 Jul 2025 00:14:37 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-16-under-the-hood-amazon-eks-ultra-scale-clusters/</guid><description>This post was co-authored by Shyam Jeedigunta, Principal Engineer, Amazon EKS; Apoorva Kulkarni, Sr. Specialist Solutions Architect, Containers and Raghav Tripathi, Sr. Software Dev Manager, Amazon EKS. Today, Amazon Elastic Kubernetes Service (Amazon EKS) announced support for clusters with up to 100,000 nodes. With Amazon EC2’s new generation accelerated computing instance types, this translates to 1. 6 million AWS Trainium chips or 800,000 NVIDIA GPUs in a single Kubernetes cluster. This unlocks ultra scale artificial intelligence (AI) and machine leaning (ML) workloads such as state-of-the-art model training, fine-tuning and agentic inference. Besides customers directly consuming Amazon EKS today, these improvements also extend to other AI/ML services like Amazon SageMaker HyperPod with EKS that leverage EKS as their compute layer, advancing AWS’s overall ultra scale computing capabilities. Our customers have made it clear that containerization of training jobs and operators such as Kubeflow, the ability to streamline resource provisioning and lifecycle through projects like Karpenter, support for pluggable scheduling strategies, and access to a vast ecosystem of cloud-native tools is critical for their success in the AI/ML domain. Kubernetes has emerged as a key enabler here due to its powerful and extensible API model along with robust container orchestration capabilities, allowing accelerated workloads to scale quickly and run reliably. Through multiple technical innovations, architectural improvements and open-source collaboration, Amazon EKS has built the next generation of its cluster control plane and data plane for ultra scale, with full Kubernetes conformance. At AWS, we recommend customers running general-purpose applications with low coupling and horizontal scalability to follow a cell-based architecture as the strategy to sustain growth.</description></item><item><title>Amazon EKS enables ultra scale AI/ML workloads with support for 100K nodes per cluster</title><link>https://kubermates.org/docs/2025-07-16-amazon-eks-enables-ultra-scale-ai-ml-workloads-with-support-for-100k-nodes-per-c/</link><pubDate>Wed, 16 Jul 2025 00:14:26 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-16-amazon-eks-enables-ultra-scale-ai-ml-workloads-with-support-for-100k-nodes-per-c/</guid><description>&lt;p&gt;Open the original post ↗ &lt;a href="https://aws.amazon.com/blogs/containers/amazon-eks-enables-ultra-scale-ai-ml-workloads-with-support-for-100k-nodes-per-cluster/"&gt;https://aws.amazon.com/blogs/containers/amazon-eks-enables-ultra-scale-ai-ml-workloads-with-support-for-100k-nodes-per-cluster/&lt;/a&gt;&lt;/p&gt;</description></item><item><title>VPC CNI Multi-NIC feature for multi-homed pods</title><link>https://kubermates.org/releases/2025-07-15-vpc-cni-multi-nic-feature-for-multi-homed-pods/</link><pubDate>Tue, 15 Jul 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-15-vpc-cni-multi-nic-feature-for-multi-homed-pods/</guid><description>Attach multiple network interfaces to Pods Background Considerations IPv6 Considerations Usage Frequently Asked Questions 1. What is a network interface card (NIC)? 2. What is a multi-homed pod? 3. Why should I use this feature? 4. How do I use this feature? 5. How do I configure my workloads to use multiple NICs on a supported worker node? 6. What network interface adapters are supported with this feature? 7. Can I see if a node in my cluster has ENA support? 8. Can I see the different IP addresses associated with a pod? 9. Can I control the number of network interfaces for my pods? 10. Can I configure my pods to use a specific NIC? 11. Does this feature work with the other VPC CNI networking features? 12.</description></item><item><title>VPC CNI troubleshooting content update</title><link>https://kubermates.org/releases/2025-06-30-vpc-cni-troubleshooting-content-update/</link><pubDate>Mon, 30 Jun 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-06-30-vpc-cni-troubleshooting-content-update/</guid><description>Troubleshooting Kubernetes network policies For Amazon EKS New policyendpoints CRD and permissions Network policy logs Amazon EKS add-on Self-managed add-on Send network policy logs to Amazon CloudWatch Logs Included eBPF SDK Known issues and solutions Network policy logs generated despite enable-policy-event-logs set to false Network policy map cleanup issues Network policies arenât applied Pods donât return to default deny state after policy deletion in strict mode Security Groups for Pods startup latency FailedScheduling due to insufficient vpc. amazonaws. com/pod-eni IPAM connectivity issues and segmentation faults Failed to find device by name error CVE vulnerabilities in Multus CNI image Flow Info DENY verdicts in logs Pod-to-pod communication issues after migrating from Calico Network policy agent doesnât support standalone pods Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. This is the troubleshooting guide for network policy feature of the Amazon VPC CNI. This guide covers: Install information, CRD and RBAC permissions New policyendpoints CRD and permissions Install information, CRD and RBAC permissions New policyendpoints CRD and permissions Logs to examine when diagnosing network policy problems Network policy logs Logs to examine when diagnosing network policy problems Network policy logs Running the eBPF SDK collection of tools to troubleshoot Running the eBPF SDK collection of tools to troubleshoot Known issues and solutions Known issues and solutions Known issues and solutions Known issues and solutions Note that network policies are only applied to pods that are made by Kubernetes Deployments. For more limitations of the network policies in the VPC CNI, see Considerations. You can troubleshoot and investigate network connections that use network policies by reading the Network policy logs and by running tools from the eBPF SDK. policyendpoints CRD: policyendpoints. networking. k8s. aws CRD: policyendpoints. networking.</description></item><item><title>AWS managed policy updates</title><link>https://kubermates.org/releases/2025-06-26-aws-managed-policy-updates/</link><pubDate>Thu, 26 Jun 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-06-26-aws-managed-policy-updates/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. An AWS managed policy is a standalone policy that is created and administered by AWS. AWS managed policies are designed to provide permissions for many common use cases so that you can start assigning permissions to users, groups, and roles. Keep in mind that AWS managed policies might not grant least-privilege permissions for your specific use cases because theyâre available for all AWS customers to use. We recommend that you reduce permissions further by defining customer managed policies that are specific to your use cases. You cannot change the permissions defined in AWS managed policies. If AWS updates the permissions defined in an AWS managed policy, the update affects all principal identities (users, groups, and roles) that the policy is attached to. AWS is most likely to update an AWS managed policy when a new AWS service is launched or new API operations become available for existing services. For more information, see AWS managed policies in the IAM User Guide. You can attach the AmazonEKS_CNI_Policy to your IAM entities. Before you create an Amazon EC2 node group, this policy must be attached to either the node IAM role , or to an IAM role thatâs used specifically by the Amazon VPC CNI plugin for Kubernetes. This is so that it can perform actions on your behalf.</description></item><item><title>Amazon EKS Auto Mode update to NodeClass</title><link>https://kubermates.org/releases/2025-06-13-amazon-eks-auto-mode-update-to-nodeclass/</link><pubDate>Fri, 13 Jun 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-06-13-amazon-eks-auto-mode-update-to-nodeclass/</guid><description>Create a Node Class for Amazon EKS Create a Node Class Basic Node Class Example Create node class access entry Create access entry with CLI Create access entry with CloudFormation Node Class Specification Considerations Subnet selection for Pods Use cases Example configuration Considerations for subnet selectors for Pods Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Amazon EKS Node Classes are templates that offer granular control over the configuration of your EKS Auto Mode managed nodes. A Node Class defines infrastructure-level settings that apply to groups of nodes in your EKS cluster, including network configuration, storage settings, and resource tagging. This topic explains how to create and configure a Node Class to meet your specific operational requirements. When you need to customize how EKS Auto Mode provisions and configures EC2 instances beyond the default settings, creating a Node Class gives you precise control over critical infrastructure parameters. For example, you can specify private subnet placement for enhanced security, configure instance ephemeral storage for performance-sensitive workloads, or apply custom tagging for cost allocation. To create a NodeClass , follow these steps: NodeClass Create a YAML file (for example, nodeclass. yaml ) with your Node Class configuration Create a YAML file (for example, nodeclass. yaml ) with your Node Class configuration nodeclass. yaml Apply the configuration to your cluster using kubectl Apply the configuration to your cluster using kubectl kubectl Reference the Node Class in your Node Pool configuration. For more information, see Create a Node Pool for EKS Auto Mode. Reference the Node Class in your Node Pool configuration.</description></item><item><title>Target secondary and cross-account roles with EKS Pod Identities</title><link>https://kubermates.org/releases/2025-06-11-target-secondary-and-cross-account-roles-with-eks-pod-identities/</link><pubDate>Wed, 11 Jun 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-06-11-target-secondary-and-cross-account-roles-with-eks-pod-identities/</guid><description>Access AWS Resources using EKS Pod Identity Target IAM Roles Prerequisites How It Works Caching considerations Step 1: Create and associate a Target IAM Role Create the Target IAM Role Update the Target IAM Role trust policy Update the permission policy for EKS Pod Identity role Step 2: Associate the Target IAM Role to a Kubernetes service account Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. When running applications on Amazon Elastic Kubernetes Service (Amazon EKS), you might need to access AWS resources that exist in different AWS accounts. This guide shows you how to set up cross account access using EKS Pod Identity, which enables your Kubernetes pods to access other AWS resources using target roles. Before you begin, ensure you have completed the following steps: Set up the Amazon EKS Pod Identity Agent Set up the Amazon EKS Pod Identity Agent Create an EKS Pod Identity role Create an EKS Pod Identity role Pod Identity enables applications in your EKS cluster to access AWS resources across accounts through a process called role chaining. When creating a Pod Identity association, you can provide two IAM roles: an EKS Pod Identity role in the same account as your EKS cluster and a Target IAM Role from the account containing your AWS resources you wish to access, (like S3 buckets or RDS Databases). The EKS Pod Identity role must be in your EKS clusterâs account due to IAM PassRole requirements, while the Target IAM Role can be in any AWS account. PassRole enables an AWS entity to delegate role assumption to another service. EKS Pod Identity uses PassRole to connect a role to a Kubernetes service account, requiring both the role and the identity passing it to be in the same AWS account as the EKS cluster. When your application pod needs to access AWS resources, it requests credentials from Pod Identity. Pod Identity then automatically performs two role assumptions in sequence: first assuming the EKS Pod Identity role , then using those credentials to assume the Target IAM Role. This process provides your pod with temporary credentials that have the permissions defined in the target role, allowing secure access to resources in other AWS accounts. Due to caching mechanisms, updates to an IAM role in an existing Pod Identity association may not take effect immediately in the pods running on your EKS cluster.</description></item><item><title>Amazon EKS AWS Region expansion</title><link>https://kubermates.org/releases/2025-06-06-amazon-eks-aws-region-expansion/</link><pubDate>Fri, 06 Jun 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-06-06-amazon-eks-aws-region-expansion/</guid><description>Document history Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. The following table describes some of the major updates and new features for the Amazon EKS User Guide. August 27, 2025 AWS managed policy updates Added permission to AmazonEKSServiceRolePolicy. This role can attach new access policy AmazonEKSEventPolicy. Restricted permissions for ec2:DeleteLaunchTemplate and ec2:TerminateInstances. AmazonEKSServiceRolePolicy AmazonEKSEventPolicy ec2:DeleteLaunchTemplate ec2:TerminateInstances August 26, 2025 Cross-service confused deputy prevention Added a topic with an example trust policy that you can apply for Cross-service confused deputy prevention. Amazon EKS accepts the aws:SourceArn and aws:SourceAccount conditions in the trust policy of an EKS cluster role. aws:SourceArn aws:SourceAccount August 19, 2025 Amazon EKS platform version update This is a new platform version with security fixes and enhancements. This includes new patch versions of Kubernetes 1.33.2 , 1.32.6 , 1.31.10 , and 1.30.14. 1.33.2 1.32.6 1.31.10 1.30.14 July 30, 2025 VPC CNI Multi-NIC feature for multi-homed pods Amazon EKS adds multi-homed pods to the VPC CNI. Now you can configure a workload and the VPC CNI assigned IP addresses from every NIC on the EC2 instance to each pod. The application can make concurrent connections to use the bandwidth from each NIC.</description></item><item><title>IPv6 access control for dual-stack public endpoints for new IPv6 clusters</title><link>https://kubermates.org/releases/2025-06-05-ipv6-access-control-for-dual-stack-public-endpoints-for-new-ipv6-clusters/</link><pubDate>Thu, 05 Jun 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-06-05-ipv6-access-control-for-dual-stack-public-endpoints-for-new-ipv6-clusters/</guid><description>Cluster API server endpoint IPv6 cluster endpoint format IPv4 cluster endpoint format Cluster private endpoint Modifying cluster endpoint access Accessing a private only API server Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. This topic helps you to enable private access for your Amazon EKS clusterâs Kubernetes API server endpoint and limit, or completely disable, public access from the internet. When you create a new cluster, Amazon EKS creates an endpoint for the managed Kubernetes API server that you use to communicate with your cluster (using Kubernetes management tools such as kubectl ). By default, this API server endpoint is public to the internet, and access to the API server is secured using a combination of AWS Identity and Access Management (IAM) and native Kubernetes Role Based Access Control (RBAC). This endpoint is known as the cluster public endpoint. Also there is a cluster private endpoint. For more information about the cluster private endpoint, see the following section Cluster private endpoint. kubectl IPv6 EKS creates a unique dual-stack endpoint in the following format for new IPv6 clusters that are made after October 2024. An IPv6 cluster is a cluster that you select IPv6 in the IP family ( ipFamily ) setting of the cluster. IPv6 IPv6 ipFamily EKS cluster public/private endpoint: eks-cluster. region. api.</description></item><item><title>Kubernetes version 1.33</title><link>https://kubermates.org/releases/2025-05-29-kubernetes-version-1-33/</link><pubDate>Thu, 29 May 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-05-29-kubernetes-version-1-33/</guid><description>Review release notes for Kubernetes versions on standard support Kubernetes 1.33 Kubernetes 1.32 Anonymous authentication changes Amazon Linux 2 AMI deprecation Kubernetes 1.31 Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. This topic gives important changes to be aware of for each Kubernetes version in standard support. When upgrading, carefully review the changes that have occurred between the old and new versions for your cluster. Kubernetes 1.33 is now available in Amazon EKS. For more information about Kubernetes 1.33 , see the official release announcement. 1.33 1.33 The Dynamic Resource Allocation beta Kubernetes API is enabled. This beta API improves the experience of scheduling and monitoring workloads that require resources such as GPUs. The beta API is defined by the Kubernetes community, and might change in future versions of Kubernetes. Carefully review Feature stages in the Kubernetes documentation to understand the implications of using beta APIs. The Dynamic Resource Allocation beta Kubernetes API is enabled. This beta API improves the experience of scheduling and monitoring workloads that require resources such as GPUs. The beta API is defined by the Kubernetes community, and might change in future versions of Kubernetes.</description></item><item><title>New cluster insights for EKS Hybrid Nodes</title><link>https://kubermates.org/releases/2025-05-29-new-cluster-insights-for-eks-hybrid-nodes/</link><pubDate>Thu, 29 May 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-05-29-new-cluster-insights-for-eks-hybrid-nodes/</guid><description>Prepare for Kubernetes version upgrades and troubleshoot misconfigurations with cluster insights Cluster insight types Considerations Use cases Upgrade insights Configuration insights Get started Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Amazon EKS cluster insights provide detection of issues and recommendations to resolve them to help you manage your cluster. Every Amazon EKS cluster undergoes automatic, recurring checks against an Amazon EKS curated list of insights. These insight checks are fully managed by Amazon EKS and offer recommendations on how to address any findings. Configuration insights : Identifies misconfigurations in your EKS Hybrid Nodes setup that could impair functionality of your cluster or workloads. Upgrade insights : Identifies issues that could impact your ability to upgrade to new versions of Kubernetes. Frequency : Amazon EKS refreshes cluster insights every 24 hours, or you can manually refresh them to see the latest status. For example, you can manually refresh cluster insights after addressing an issue to see if the issue was resolved. Frequency : Amazon EKS refreshes cluster insights every 24 hours, or you can manually refresh them to see the latest status. For example, you can manually refresh cluster insights after addressing an issue to see if the issue was resolved. Permissions : Amazon EKS automatically creates a cluster access entry for cluster insights in every EKS cluster. This entry gives EKS permission to view information about your cluster.</description></item><item><title>Add-on support for Amazon FSx CSI driver</title><link>https://kubermates.org/releases/2025-05-23-add-on-support-for-amazon-fsx-csi-driver/</link><pubDate>Fri, 23 May 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-05-23-add-on-support-for-amazon-fsx-csi-driver/</guid><description>Use high-performance app storage with Amazon FSx for Lustre Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. The Amazon FSx for Lustre Container Storage Interface (CSI) driver provides a CSI interface that allows Amazon EKS clusters to manage the lifecycle of Amazon FSx for Lustre file systems. For more information, see the Amazon FSx for Lustre User Guide. For details on how to deploy the Amazon FSx for Lustre CSI driver to your Amazon EKS cluster and verify that it works, see Deploy the FSx for Lustre driver. Thanks for letting us know we&amp;rsquo;re doing a good job! If you&amp;rsquo;ve got a moment, please tell us what we did right so we can do more of it. Thanks for letting us know this page needs work. We&amp;rsquo;re sorry we let you down. If you&amp;rsquo;ve got a moment, please tell us how we can make the documentation better.</description></item><item><title>Edit Prometheus scrapers in the console</title><link>https://kubermates.org/releases/2025-05-22-edit-prometheus-scrapers-in-the-console/</link><pubDate>Thu, 22 May 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-05-22-edit-prometheus-scrapers-in-the-console/</guid><description>Monitor your cluster metrics with Prometheus Step 1: Turn on Prometheus metrics Step 2: Use the Prometheus metrics Step 3: Manage Prometheus scrapers Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Prometheus is a monitoring and time series database that scrapes endpoints. It provides the ability to query, aggregate, and store collected data. You can also use it for alerting and alert aggregation. This topic explains how to set up Prometheus as either a managed or open source option. Monitoring Amazon EKS control plane metrics is a common use case. Amazon Managed Service for Prometheus is a Prometheus-compatible monitoring and alerting service that makes it easy to monitor containerized applications and infrastructure at scale. It is a fully-managed service that automatically scales the ingestion, storage, querying, and alerting of your metrics. It also integrates with AWS security services to enable fast and secure access to your data. You can use the open-source PromQL query language to query your metrics and alert on them. Also, you can use alert manager in Amazon Managed Service for Prometheus to set up alerting rules for critical alerts. You can then send these critical alerts as notifications to an Amazon SNS topic.</description></item><item><title>Bottlerocket for hybrid nodes</title><link>https://kubermates.org/releases/2025-04-29-bottlerocket-for-hybrid-nodes/</link><pubDate>Tue, 29 Apr 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-04-29-bottlerocket-for-hybrid-nodes/</guid><description>Connect hybrid nodes with Bottlerocket Prerequisites Step 1: Create the Bottlerocket settings TOML file SSM IAM Roles Anywhere Step 2: Provision the Bottlerocket vSphere VM with user data Creating VM for the first time Updating user data for an existing VM Step 3: Verify the hybrid node connection Step 4: Configure a CNI for hybrid nodes Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. This topic describes how to connect hybrid nodes running Bottlerocket to an Amazon EKS cluster. Bottlerocket is an open source Linux distribution that is sponsored and supported by AWS. Bottlerocket is purpose-built for hosting container workloads. With Bottlerocket, you can improve the availability of containerized deployments and reduce operational costs by automating updates to your container infrastructure. Bottlerocket includes only the essential software to run containers, which improves resource usage, reduces security threats, and lowers management overhead. Only VMware variants of Bottlerocket version v1.37.0 and above are supported with EKS Hybrid Nodes. VMware variants of Bottlerocket are available for Kubernetes versions v1.28 and above. The OS images for these variants include the kubelet, containerd, aws-iam-authenticator and other software prerequisites for EKS Hybrid Nodes. You can configure these components using a Bottlerocket settings file that includes base64 encoded user-data for the Bottlerocket bootstrap and admin containers. Configuring these settings enables Bottlerocket to use your hybrid nodes credentials provider to authenticate hybrid nodes to your cluster. After your hybrid nodes join the cluster, they will appear with status Not Ready in the Amazon EKS console and in Kubernetes-compatible tooling such as kubectl.</description></item><item><title>New concepts pages for hybrid networking</title><link>https://kubermates.org/releases/2025-04-18-new-concepts-pages-for-hybrid-networking/</link><pubDate>Fri, 18 Apr 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-04-18-new-concepts-pages-for-hybrid-networking/</guid><description>Concepts for hybrid nodes Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. With Amazon EKS Hybrid Nodes , you join physical or virtual machines running in on-premises or edge environments to Amazon EKS clusters running in the AWS Cloud. This approach brings many benefits, but also introduces new networking concepts and architectures for those familiar with running Kubernetes clusters in a single network environment. The following sections dive deep into the Kubernetes and networking concepts for EKS Hybrid Nodes and details how traffic flows through the hybrid architecture. These sections require that you are familiar with basic Kubernetes networking knowledge, such as the concepts of pods, nodes, services, Kubernetes control plane, kubelet and kube-proxy. We recommend reading these pages in order, starting with the Networking concepts for hybrid nodes , then the Kubernetes concepts for hybrid nodes , and finally the Network traffic flows for hybrid nodes. Networking concepts for hybrid nodes Networking concepts for hybrid nodes Kubernetes concepts for hybrid nodes Kubernetes concepts for hybrid nodes Network traffic flows for hybrid nodes Network traffic flows for hybrid nodes Thanks for letting us know we&amp;rsquo;re doing a good job! If you&amp;rsquo;ve got a moment, please tell us what we did right so we can do more of it. Thanks for letting us know this page needs work. We&amp;rsquo;re sorry we let you down. If you&amp;rsquo;ve got a moment, please tell us how we can make the documentation better.</description></item><item><title>AWS managed policy updates</title><link>https://kubermates.org/releases/2025-04-14-aws-managed-policy-updates/</link><pubDate>Mon, 14 Apr 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-04-14-aws-managed-policy-updates/</guid><description>AWS managed policies for Amazon Elastic Kubernetes Service AWS managed policy: AmazonEKS_CNI_Policy AWS managed policy: AmazonEKSClusterPolicy AWS managed policy: AmazonEKSDashboardConsoleReadOnly AWS managed policy: AmazonEKSFargatePodExecutionRolePolicy AWS managed policy: AmazonEKSForFargateServiceRolePolicy AWS managed policy: AmazonEKSComputePolicy Permissions details AWS managed policy: AmazonEKSNetworkingPolicy Permissions details AWS managed policy: AmazonEKSBlockStoragePolicy Permissions details AWS managed policy: AmazonEKSLoadBalancingPolicy Permissions details AWS managed policy: AmazonEKSServicePolicy AWS managed policy: AmazonEKSServiceRolePolicy AWS managed policy: AmazonEKSVPCResourceController AWS managed policy: AmazonEKSWorkerNodePolicy AWS managed policy: AmazonEKSWorkerNodeMinimalPolicy AWS managed policy: AWSServiceRoleForAmazonEKSNodegroup AWS managed policy: AmazonEKSDashboardServiceRolePolicy AWS managed policy: AmazonEBSCSIDriverPolicy AWS managed policy: AmazonEFSCSIDriverPolicy AWS managed policy: AmazonEKSLocalOutpostClusterPolicy AWS managed policy: AmazonEKSLocalOutpostServiceRolePolicy Amazon EKS updates to AWS managed policies Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. An AWS managed policy is a standalone policy that is created and administered by AWS. AWS managed policies are designed to provide permissions for many common use cases so that you can start assigning permissions to users, groups, and roles. Keep in mind that AWS managed policies might not grant least-privilege permissions for your specific use cases because theyâre available for all AWS customers to use. We recommend that you reduce permissions further by defining customer managed policies that are specific to your use cases. You cannot change the permissions defined in AWS managed policies. If AWS updates the permissions defined in an AWS managed policy, the update affects all principal identities (users, groups, and roles) that the policy is attached to. AWS is most likely to update an AWS managed policy when a new AWS service is launched or new API operations become available for existing services. For more information, see AWS managed policies in the IAM User Guide. You can attach the AmazonEKS_CNI_Policy to your IAM entities. Before you create an Amazon EC2 node group, this policy must be attached to either the node IAM role , or to an IAM role thatâs used specifically by the Amazon VPC CNI plugin for Kubernetes. This is so that it can perform actions on your behalf.</description></item><item><title>EKS Hybrid Nodes for existing clusters</title><link>https://kubermates.org/releases/2025-03-31-eks-hybrid-nodes-for-existing-clusters/</link><pubDate>Mon, 31 Mar 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-03-31-eks-hybrid-nodes-for-existing-clusters/</guid><description>Enable hybrid nodes on an existing Amazon EKS cluster or modify configuration Prerequisites Considerations Enable hybrid nodes on an existing cluster Enable EKS Hybrid Nodes in an existing cluster - AWS CloudFormation Enable EKS Hybrid Nodes in an existing cluster - AWS CLI Enable EKS Hybrid Nodes in an existing cluster - AWS Management Console Update hybrid nodes configuration in an existing cluster Update hybrid configuration in an existing cluster - AWS CloudFormation Update hybrid configuration in an existing cluster - AWS CLI Update hybrid configuration in an existing cluster - AWS Management Console Disable Hybrid nodes in an existing cluster Disable EKS Hybrid Nodes in an existing cluster - AWS CloudFormation Disable EKS Hybrid Nodes in an existing cluster - AWS CLI Disable EKS Hybrid Nodes in an existing cluster - AWS Management Console Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. This topic provides an overview of the available options and describes what to consider when you add, change, or remove the hybrid nodes configuration for an Amazon EKS cluster. To enable an Amazon EKS cluster to use hybrid nodes, add the IP address CIDR ranges of your on-premises node and optionally pod network in the RemoteNetworkConfig configuration. EKS uses this list of CIDRs to enable connectivity between the cluster and your on-premises networks. For a full list of options when updating your cluster configuration, see the UpdateClusterConfig in the Amazon EKS API Reference. RemoteNetworkConfig You can do any of the following actions to the EKS Hybrid Nodes networking configuration in a cluster: Add remote network configuration to enable EKS Hybrid Nodes in an existing cluster. Add remote network configuration to enable EKS Hybrid Nodes in an existing cluster. Add, change, or remove the remote node networks or the remote pod networks in an existing cluster. Remove all remote node network CIDR ranges to disable EKS Hybrid Nodes in an existing cluster. Before enabling your Amazon EKS cluster for hybrid nodes, ensure your environment meets the requirements outlined at Prerequisite setup for hybrid nodes , and detailed at Prepare networking for hybrid nodes , Prepare operating system for hybrid nodes , and Prepare credentials for hybrid nodes. Your cluster must use IPv4 address family. Your cluster must use either API or API_AND_CONFIG_MAP for the cluster authentication mode.</description></item><item><title>Node health for EKS Hybrid Nodes</title><link>https://kubermates.org/releases/2025-03-31-node-health-for-eks-hybrid-nodes/</link><pubDate>Mon, 31 Mar 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-03-31-node-health-for-eks-hybrid-nodes/</guid><description>Enable node auto repair and investigate node health issues Node monitoring agent Node auto repair Node health issues Kernel node health issues Networking node health issues Neuron node health issues NVIDIA node health issues Runtime node health issues Storage node health issues Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Node health refers to the operational status and capability of a node to effectively run workloads. A healthy node maintains expected connectivity, has sufficient resources, and can successfully run Pods without disruption. For information on getting details about your nodes, see View the health status of your nodes and Retrieve node logs for a managed node using kubectl and S3. To help with maintaining healthy nodes, Amazon EKS offers the node monitoring agent and node auto repair. The node monitoring agent and node auto repair are only available on Linux. These features arenât available on Windows. The node monitoring agent automatically reads node logs to detect certain health issues. It parses through node logs to detect failures and surfaces various status information about worker nodes. A dedicated NodeCondition is applied on the worker nodes for each category of issues detected, such as storage and networking issues. Descriptions of detected health issues are made available in the observability dashboard. For more information, see Node health issues.</description></item><item><title>Rollback: Prevent accidental upgrades with cluster insights</title><link>https://kubermates.org/releases/2025-03-28-rollback-prevent-accidental-upgrades-with-cluster-insights/</link><pubDate>Fri, 28 Mar 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-03-28-rollback-prevent-accidental-upgrades-with-cluster-insights/</guid><description>Update existing cluster to new Kubernetes version Considerations for Amazon EKS Auto Mode Summary Step 1: Prepare for upgrade Step 2: Review upgrade considerations Review upgrade insights Detailed considerations Step 3: Update cluster control plane Update cluster - eksctl Update cluster - AWS console Update cluster - AWS CLI Step 4: Update cluster components Downgrade the Kubernetes version for an Amazon EKS cluster Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. When a new Kubernetes version is available in Amazon EKS, you can update your Amazon EKS cluster to the latest version. Once you upgrade a cluster, you canât downgrade to a previous version. Before you update to a new Kubernetes version, we recommend that you review the information in Understand the Kubernetes version lifecycle on EKS and the update steps in this topic. New Kubernetes versions sometimes introduce significant changes. Therefore, we recommend that you test the behavior of your applications against a new Kubernetes version before you update your production clusters. You can do this by building a continuous integration workflow to test your application behavior before moving to a new Kubernetes version. The update process consists of Amazon EKS launching new API server nodes with the updated Kubernetes version to replace the existing ones. Amazon EKS performs standard infrastructure and readiness health checks for network traffic on these new nodes to verify that theyâre working as expected. However, once youâve started the cluster upgrade, you canât pause or stop it. If any of these checks fail, Amazon EKS reverts the infrastructure deployment, and your cluster remains on the prior Kubernetes version. Running applications arenât affected, and your cluster is never left in a non-deterministic or unrecoverable state.</description></item><item><title>Bottlerocket FIPS AMIs</title><link>https://kubermates.org/releases/2025-03-27-bottlerocket-fips-amis/</link><pubDate>Thu, 27 Mar 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-03-27-bottlerocket-fips-amis/</guid><description>Make your worker nodes FIPS ready with Bottlerocket FIPS AMIs Considerations Create a managed node group with a Bottlerocket FIPS AMI Disable the FIPS endpoint for non-supported AWS Regions Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. The Federal Information Processing Standard (FIPS) Publication 140-3 is a United States and Canadian government standard that specifies the security requirements for cryptographic modules that protect sensitive information. Bottlerocket makes it easier to adhere to FIPS by offering AMIs with a FIPS kernel. These AMIs are preconfigured to use FIPS 140-3 validated cryptographic modules. This includes the Amazon Linux 2023 Kernel Crypto API Cryptographic Module and the AWS-LC Cryptographic Module. Using Bottlerocket FIPS AMIs makes your worker nodes &amp;ldquo;FIPS ready&amp;rdquo; but not automatically &amp;ldquo;FIPS-compliant&amp;rdquo;. For more information, see Federal Information Processing Standard (FIPS) 140-3. If your cluster uses isolated subnets, the Amazon ECR FIPS endpoint may not be accessible. This can cause the node bootstrap to fail. Make sure that your network configuration allows access to the necessary FIPS endpoints. For more information, see Access a resource through a resource VPC endpoint in the AWS PrivateLink Guide. If your cluster uses isolated subnets, the Amazon ECR FIPS endpoint may not be accessible.</description></item><item><title>Update strategies for managed node groups</title><link>https://kubermates.org/releases/2025-01-27-update-strategies-for-managed-node-groups/</link><pubDate>Mon, 27 Jan 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-01-27-update-strategies-for-managed-node-groups/</guid><description>Understand each phase of node updates Setup phase Scale up phase Upgrade phase PodEvictionFailure errors during the upgrade phase Scale down phase Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. The Amazon EKS managed worker node upgrade strategy has four different phases described in the following sections. The setup phase has these steps: It creates a new Amazon EC2 launch template version for the Auto Scaling Group thatâs associated with your node group. The new launch template version uses the target AMI or a custom launch template version for the update. It creates a new Amazon EC2 launch template version for the Auto Scaling Group thatâs associated with your node group. The new launch template version uses the target AMI or a custom launch template version for the update. It updates the Auto Scaling Group to use the latest launch template version. It determines the maximum quantity of nodes to upgrade in parallel using the updateConfig property for the node group. The maximum unavailable has a quota of 100 nodes. The default value is one node. For more information, see the updateConfig property in the Amazon EKS API Reference. It determines the maximum quantity of nodes to upgrade in parallel using the updateConfig property for the node group.</description></item><item><title>Kubernetes version 1.32</title><link>https://kubermates.org/releases/2025-01-23-kubernetes-version-1-32/</link><pubDate>Thu, 23 Jan 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-01-23-kubernetes-version-1-32/</guid><description>Understand the Kubernetes version lifecycle on EKS Available versions on standard support Available versions on extended support Amazon EKS Kubernetes release calendar Get version information with AWS CLI To retrieve information about available Kubernetes versions on EKS using the AWS CLI Amazon EKS version FAQs Amazon EKS extended support FAQs Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Kubernetes rapidly evolves with new features, design updates, and bug fixes. The community releases new Kubernetes minor versions (such as 1.33 ) on average once every four months. Amazon EKS follows the upstream release and deprecation cycle for minor versions. As new Kubernetes versions become available in Amazon EKS, we recommend that you proactively update your clusters to use the latest available version. 1.33 A minor version is under standard support in Amazon EKS for the first 14 months after itâs released. Once a version is past the end of standard support date, it enters extended support for the next 12 months. Extended support allows you to stay at a specific Kubernetes version for longer at an additional cost per cluster hour. If you havenât updated your cluster before the extended support period ends, your cluster is auto-upgraded to the oldest currently supported extended version. Extended support is enabled by default. To disable, see Disable EKS extended support. We recommend that you create your cluster with the latest available Kubernetes version supported by Amazon EKS.</description></item><item><title>Amazon EKS Auto Mode</title><link>https://kubermates.org/releases/2024-12-01-amazon-eks-auto-mode/</link><pubDate>Sun, 01 Dec 2024 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2024-12-01-amazon-eks-auto-mode/</guid><description>Automate cluster infrastructure with EKS Auto Mode Features Automated Components Configuration Shared responsibility model Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. EKS Auto Mode extends AWS management of Kubernetes clusters beyond the cluster itself, to allow AWS to also set up and manage the infrastructure that enables the smooth operation of your workloads. You can delegate key infrastructure decisions and leverage the expertise of AWS for day-to-day operations. Cluster infrastructure managed by AWS includes many Kubernetes capabilities as core components, as opposed to add-ons, such as compute autoscaling, pod and service networking, application load balancing, cluster DNS, block storage, and GPU support. To get started, you can deploy a new EKS Auto Mode cluster or enable EKS Auto Mode on an existing cluster. You can deploy, upgrade, or modify your EKS Auto Mode clusters using eksctl, the AWS CLI, the AWS Management Console, EKS APIs, or your preferred infrastructure-as-code tools. With EKS Auto Mode, you can continue using your preferred Kubernetes-compatible tools. EKS Auto Mode integrates with AWS services like Amazon EC2, Amazon EBS, and ELB, leveraging AWS cloud resources that follow best practices. These resources are automatically scaled, cost-optimized, and regularly updated to help minimize operational costs and overhead. EKS Auto Mode provides the following high-level features: Streamline Kubernetes Cluster Management : EKS Auto Mode streamlines EKS management by providing production-ready clusters with minimal operational overhead. With EKS Auto Mode, you can run demanding, dynamic workloads confidently, without requiring deep EKS expertise. Application Availability : EKS Auto Mode dynamically adds or removes nodes in your EKS cluster based on the demands of your Kubernetes applications.</description></item><item><title>Amazon EKS Hybrid Nodes</title><link>https://kubermates.org/releases/2024-12-01-amazon-eks-hybrid-nodes/</link><pubDate>Sun, 01 Dec 2024 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2024-12-01-amazon-eks-hybrid-nodes/</guid><description>Amazon EKS Hybrid Nodes overview Features Limits Considerations Additional resources Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. With Amazon EKS Hybrid Nodes , you can use your on-premises and edge infrastructure as nodes in Amazon EKS clusters. AWS manages the AWS-hosted Kubernetes control plane of the Amazon EKS cluster, and you manage the hybrid nodes that run in your on-premises or edge environments. This unifies Kubernetes management across your environments and offloads Kubernetes control plane management to AWS for your on-premises and edge applications. Amazon EKS Hybrid Nodes works with any on-premises hardware or virtual machines, bringing the efficiency, scalability, and availability of Amazon EKS to wherever your applications need to run. You can use a wide range of Amazon EKS features with Amazon EKS Hybrid Nodes including Amazon EKS add-ons, Amazon EKS Pod Identity, cluster access entries, cluster insights, and extended Kubernetes version support. Amazon EKS Hybrid Nodes natively integrates with AWS services including AWS Systems Manager, AWS IAM Roles Anywhere, Amazon Managed Service for Prometheus, and Amazon CloudWatch for centralized monitoring, logging, and identity management. With Amazon EKS Hybrid Nodes, there are no upfront commitments or minimum fees, and you are charged per hour for the vCPU resources of your hybrid nodes when they are attached to your Amazon EKS clusters. For more pricing information, see Amazon EKS Pricing. EKS Hybrid Nodes has the following high-level features: Managed Kubernetes control plane : AWS manages the AWS-hosted Kubernetes control plane of the EKS cluster, and you manage the hybrid nodes that run in your on-premises or edge environments. This unifies Kubernetes management across your environments and offloads Kubernetes control plane management to AWS for your on-premises and edge applications. By moving the Kubernetes control plane to AWS, you can conserve on-premises capacity for your applications and trust that the Kubernetes control plane scales with your workloads.</description></item><item><title>Kubernetes version 1.30 is now available for local clusters on AWS Outposts</title><link>https://kubermates.org/releases/2024-11-21-kubernetes-version-1-30-is-now-available-for-local-clusters-on-aws-outposts/</link><pubDate>Thu, 21 Nov 2024 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2024-11-21-kubernetes-version-1-30-is-now-available-for-local-clusters-on-aws-outposts/</guid><description>Learn Kubernetes and Amazon EKS platform versions for AWS Outposts Kubernetes version 1.31 Kubernetes version 1.30 Kubernetes version 1.29 Kubernetes version 1.28 Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Local cluster platform versions represent the capabilities of the Amazon EKS cluster on AWS Outposts. The versions include the components that run on the Kubernetes control plane, which Kubernetes API server flags are enabled. They also include the current Kubernetes patch version. Each Kubernetes minor version has one or more associated platform versions. The platform versions for different Kubernetes minor versions are independent. The platform versions for local clusters and Amazon EKS clusters in the cloud are independent. When a new Kubernetes minor version is available for local clusters, such as 1.31 , the initial platform version for that Kubernetes minor version starts at eks-local-outposts. 1. However, Amazon EKS releases new platform versions periodically to enable new Kubernetes control plane settings and to provide security fixes. 1.31 eks-local-outposts. 1 When new local cluster platform versions become available for a minor version: The platform version number is incremented ( eks-local-outposts.</description></item><item><title>Bottlerocket AMIs that use FIPS 140-3</title><link>https://kubermates.org/releases/2024-11-20-bottlerocket-amis-that-use-fips-140-3/</link><pubDate>Wed, 20 Nov 2024 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2024-11-20-bottlerocket-amis-that-use-fips-140-3/</guid><description>Retrieve recommended Bottlerocket AMI IDs Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. When deploying nodes, you can specify an ID for a pre-built Amazon EKS optimized Amazon Machine Image (AMI). To retrieve an AMI ID that fits your desired configuration, query the AWS Systems Manager Parameter Store API. Using this API eliminates the need to manually look up Amazon EKS optimized AMI IDs. For more information, see GetParameter. The IAM principal that you use must have the ssm:GetParameter IAM permission to retrieve the Amazon EKS optimized AMI metadata. ssm:GetParameter You can retrieve the image ID of the latest recommended Amazon EKS optimized Bottlerocket AMI with the following AWS CLI command, which uses the sub-parameter image_id. Make the following modifications to the command as needed and then run the modified command: image_id Replace kubernetes-version with a supported platform-version. Replace kubernetes-version with a supported platform-version. kubernetes-version Replace -flavor with one of the following options. Remove -flavor for variants without a GPU. Use -nvidia for GPU-enabled variants.</description></item><item><title>Observability dashboard</title><link>https://kubermates.org/releases/2024-11-18-observability-dashboard/</link><pubDate>Mon, 18 Nov 2024 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2024-11-18-observability-dashboard/</guid><description>Monitor your cluster with the observability dashboard Summary Cluster health Control plane monitoring Metrics CloudWatch Log Insights View control plane logs in CloudWatch Cluster insights Node health issues Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. The Amazon EKS console includes an observability dashboard that gives visibility into the performance of your cluster. The information it provides helps you to quickly detect, troubleshoot, and remediate issues. You can open the applicable section of the observability dashboard by choosing an item in the Health and performance summary. This summary is included in several places, including the Observability tab. The observability dashboard is split into several tabs. The Health and performance summary lists the quantity of items in various categories. Each number acts as a hyperlink to a location in the observability dashboard with a list for that category. Cluster health provides important notifications to be aware of, some of which you may need to take action on as soon as possible. With this list, you can see descriptions and the affected resources. Cluster health includes two tables: Health issues and Configuration insights. To refresh the status of Health issues , choose the refresh button ( â» ).</description></item><item><title>New role creation in console for add-ons that support EKS Pod Identities</title><link>https://kubermates.org/releases/2024-11-15-new-role-creation-in-console-for-add-ons-that-support-eks-pod-identities/</link><pubDate>Fri, 15 Nov 2024 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2024-11-15-new-role-creation-in-console-for-add-ons-that-support-eks-pod-identities/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Amazon EKS add-ons are add-on software for Amazon EKS clusters. All Amazon EKS add-ons: Include the latest security patches and bug fixes. Are validated by AWS to work with Amazon EKS. Reduce the amount of work required to manage the add-on software. You can create an Amazon EKS add-on using eksctl , the AWS Management Console, or the AWS CLI. If the add-on requires an IAM role, see the details for the specific add-on in Amazon EKS add-ons for details about creating the role. Complete the following before you create an add-on: The cluster must exist before you create an add-on for it. For more information, see Create an Amazon EKS cluster. Check if your add-on requires an IAM role. For more information, see Verify Amazon EKS add-on version compatibility with a cluster. Verify that the Amazon EKS add-on version is compatabile with your cluster.</description></item></channel></rss>