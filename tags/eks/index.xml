<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Eks on Kubermates</title><link>https://kubermates.org/tags/eks/</link><description>Recent content in Eks on Kubermates</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 18 Dec 2025 17:27:31 +0000</lastBuildDate><atom:link href="https://kubermates.org/tags/eks/index.xml" rel="self" type="application/rss+xml"/><item><title>Deep dive: Streamlining GitOps with Amazon EKS capability for Argo CD</title><link>https://kubermates.org/docs/2025-12-18-deep-dive-streamlining-gitops-with-amazon-eks-capability-for-argo-cd/</link><pubDate>Thu, 18 Dec 2025 17:27:31 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-18-deep-dive-streamlining-gitops-with-amazon-eks-capability-for-argo-cd/</guid><description>Deep dive: Streamlining GitOps with Amazon EKS capability for Argo CD Architecture overview: Hub-and-Spoke topology Prerequisites Solution Walkthrough Configure AWS IAM Identity Center Create hub cluster with Argo CD Capability Create spoke clusters Register clusters with Argo CD Configure Git sources Native ECR integration Implement multi-tenancy with projects Scale deployments with ApplicationSets CI/CD pipeline integration Operational visibility Clean up Conclusion About the authors Organizations use GitOps as the standard for managing Kubernetes deployments at scale. Running Argo CD in production means managing high availability, upgrades, Single Sign-On (SSO) configuration, and cross-cluster connectivity. This operational scope grows with each additional cluster across regions or AWS accounts. Amazon Elastic Kubernetes Service (EKS) Capability for Argo CD – referred to as “Argo CD Capability” in the remainder of this blog post – is part of the newly launched Amazon EKS Capabilities feature. It provides a fully managed GitOps continuous deployment solution that eliminates the operational overhead of running Argo CD on your clusters. The capability runs in AWS managed service accounts outside of your clusters, with AWS handling scaling, upgrades, inter-cluster communications, and offering native integrations with other AWS services such as AWS Secret Manager , Amazon Elastic Container Registry (ECR) , AWS CodeCommit and AWS CodeConnections. In this deep dive, we explore advanced scenarios with Argo CD including hub-and-spoke multi-cluster deployments, native AWS service integrations, multi-tenancy implementation, scaling with advanced Argo CD configurations and integration with CI/CD pipeline. For a detailed comparison with self-managed solutions, see Comparing EKS Capability for Argo CD to self-managed Argo CD. In a hub-and-spoke architecture, the Argo CD Capability is created on a dedicated central EKS cluster (the hub) that serves as the control plane for GitOps operations, and it is not created on the spoke clusters. Although Argo CD in the central hub cluster can technically manage and deploy applications to both the hub and the spoke clusters, in this blog the hub cluster is designed exclusively for management tasks and does not host any business workloads. “Figure 1: Sample topology of hub-and-spoke model for Argo CD Capability” This topology provides platform teams with a single pane of glass to orchestrate deployments across an entire fleet of clusters—whether they’re in different regions, accounts, or have private Kubernetes API endpoints. Before you begin, verify that you have the following tools and configurations in place.</description></item><item><title>Amazon EKS introduces enhanced network policy capabilities</title><link>https://kubermates.org/docs/2025-12-15-amazon-eks-introduces-enhanced-network-policy-capabilities/</link><pubDate>Mon, 15 Dec 2025 22:25:17 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-15-amazon-eks-introduces-enhanced-network-policy-capabilities/</guid><description>Amazon EKS introduces enhanced network policy capabilities What are Admin Network Policies? Admin Policy examples What are Application Network Policies? How are Application Network Policies different from regular Network Policies? Application Network Policy example Conclusion About the authors Today, we are excited to announce the expansion of native network policy support in Amazon EKS to include both Admin Policies and Application Network Policies. With these additional policies, Cluster Administrators (e. g. platform or security teams) can set cluster-wide security rules for their clusters to enhance the overall network security for their Kubernetes workloads. In addition, Namespace Administrators (e. g. application teams) can now control pod traffic to external resources using domain names as filters. This approach replaces the need to maintain lists of specific IP addresses (which frequently change) or broad CIDR ranges (which often conflict with corporate security policies), instead enabling the creation of a trusted list of external website and services that pods are allowed to access. You can think of this as a “permitted destinations” list for your cluster’s outbound traffic. Standard Kubernetes Network Policies in a cluster allow you to implement a virtual firewall, segmenting network traffic inside a cluster. These policies let you create rules that govern both incoming (ingress) and outgoing (egress) traffic. You can restrict communication based on several parameters, including pod labels, namespaces, IP ranges (CIDR), and specific ports.</description></item><item><title>Automate java performance troubleshooting with AI-Powered thread dump analysis on Amazon ECS and EKS</title><link>https://kubermates.org/docs/2025-12-15-automate-java-performance-troubleshooting-with-ai-powered-thread-dump-analysis-o/</link><pubDate>Mon, 15 Dec 2025 18:57:47 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-15-automate-java-performance-troubleshooting-with-ai-powered-thread-dump-analysis-o/</guid><description>Automate java performance troubleshooting with AI-Powered thread dump analysis on Amazon ECS and EKS Overview of the solution Prerequisites Walkthrough Step 1: Deploying the base infrastructure Step 2: Setting up container environment and deploy the monitoring and analysis stack Implementation details Design principles JMX metrics with Spring Boot Automated thread dump collection via Grafana webhook AI-powered analysis with Amazon Bedrock Triggering a thread dump analysis Example Analysis Output Cleaning up Conclusion About the authors Picture this: your containerized Java application that was running smoothly yesterday is now consuming 90% CPU and barely responding to user requests. Now your customers are experiencing timeouts, and your ops team is under pressure to resolve the issue quickly. When debugging unresponsive applications or excessive CPU consumption, one of the most valuable diagnostic tools available is the thread dump. A thread dump provides a snapshot of the threads in a Java Virtual Machine (JVM) at a specific moment, revealing thread states, stack traces, and lock information. While thread dumps are essential for diagnosing complex performance issues, traditional thread dump analysis presents several challenges. Analyzing thread dumps requires deep JVM expertise. In many teams, only a handful of engineers know how to interpret them, which slows down troubleshooting. Manual analysis can also be time‑consuming, taking hours, if not days to sift through, especially with hundreds of threads. Furthermore, this reactive approach only identifies issues after degradation has already occurred, potentially impacting your customers. Generative AI can make proactive, expert-level troubleshooting faster and more straightforward for developers and operations teams. In this blog, we’ll walk through how to build an automated thread dump analysis pipeline that uses Prometheus for monitoring, Grafana for alerting, AWS Lambda for orchestration, and Amazon Bedrock for AI‑powered analysis. The solution works on both Amazon Elastic Container Servics (Amazon ECS ) and Amazon Elastic Kubernetes Service (Amazon EKS ), helping teams go from raw thread dumps to actionable insights within seconds of detecting an issue.</description></item><item><title>Amazon EKS Capabilities</title><link>https://kubermates.org/releases/2025-11-30-amazon-eks-capabilities/</link><pubDate>Sun, 30 Nov 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-11-30-amazon-eks-capabilities/</guid><description>EKS Capabilities Available Capabilities AWS Controllers for Kubernetes (ACK) Argo CD kro (Kube Resource Orchestrator) Benefits of EKS Capabilities Pricing How EKS Capabilities Work Common Use Cases Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Get started: Create ACK capability | Create Argo CD capability | Create kro capability Amazon EKS Capabilities is a layered set of fully managed cluster features that help accelerate developer velocity and offload the complexity of building and scaling with Kubernetes. EKS Capabilities are Kubernetes-native features for declarative continuous deployment, AWS resource management, and Kubernetes resource authoring and orchestration, all fully managed by AWS. With EKS Capabilities, you can focus more on building and scaling your workloads, offloading the operational burden of these foundational platform services to AWS. These capabilities run within EKS rather than in your clusters, eliminating the need to install, maintain, and scale critical platform components on your worker nodes. To get started, you can create one or more EKS Capabilities on a new or existing EKS cluster. To do this, you can use the AWS CLI, the AWS Management Console, EKS APIs, eksctl, or your preferred infrastructure-as-code tools. While EKS Capabilities are designed to work together, they are independent cloud resources that you can pick and choose based on your use case and requirements. All Kubernetes versions supported by EKS are supported for EKS Capabilities. EKS Capabilities are available in all AWS commercial Regions where Amazon EKS is available. For a list of supported Regions, see Amazon EKS endpoints and quotas in the AWS General Reference. ACK enables the management of AWS resources using Kubernetes APIs, allowing you to create and manage S3 buckets, RDS databases, IAM roles, and other AWS resources using Kubernetes custom resources.</description></item><item><title>Amazon EKS introduces Provisioned Control Plane</title><link>https://kubermates.org/docs/2025-11-27-amazon-eks-introduces-provisioned-control-plane/</link><pubDate>Thu, 27 Nov 2025 00:32:32 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-27-amazon-eks-introduces-provisioned-control-plane/</guid><description>Amazon EKS introduces Provisioned Control Plane Delivering predictable and high performance at scale How did we unlock this? Getting started with Provisioned Control Plane Creating a cluster with Provisioned Control Plane Updating control plane scaling tier Monitoring control plane scaling tier utilization Benchmarking with Provisioned Control Plane Conclusion About the authors Amazon Elastic Kubernetes Service (Amazon EKS) powers tens of millions of clusters annually, with an architecture refined by years of real-world insights from thousands of customers running diverse workloads. EKS automatically scales your cluster’s control plane to meet your workload demands. This dynamic, intelligent scaling powers most use cases, handling everything from startup applications to enterprise platforms to mission critical workloads. Building on this proven architecture, we’re introducing capabilities designed for next-generation workloads with specialized requirements. When you’re running AI training or inference workloads at ultra scale, running multi-tenant SaaS platforms, or running mission-critical web applications where every second matters, you need absolute predictability – the ability to guarantee control plane responsiveness before peak demand arrives. To meet these advanced requirements, we’re introducing EKS Provisioned Control Plane, an enhanced option that complements the Standard Control Plane capabilities. Amazon EKS Provisioned Control Plane gives you the ability to pre-allocate control plane capacity from a set of new scaling tiers, ensuring predictable and high performance for your most demanding workloads. By provisioning capacity ahead of time, your cluster handles instantaneous traffic bursts without the need for control plane scaling, which is crucial for serving traffic during high-demand events or sudden workload spikes. These new scaling tiers unlock significantly higher control plane performance and scalability required for emerging workload patterns like ultra scale AI training/inference, high performance computing, or large-scale data processing. Provisioned Control Plane allows you to choose from multiple control plane scaling tiers (XL, 2XL, 4XL) with each tier offering well defined performance on these Kubernetes attributes: API request concurrency – the volume of requests processed by the Kubernetes API servers concurrently Pod scheduling rate – the throughput at which the Kubernetes default scheduler assigns pods to nodes Cluster database size – the storage space allocated to etcd, the database that holds the cluster state/metadata Once you designate a scaling tier to your cluster, EKS ensures sufficient capacity is always available to the control plane to meet the attribute values for that tier. You also get comprehensive visibility into your tier utilization through granular metrics for each attribute, available through both the EKS Prometheus metrics endpoint and Amazon CloudWatch metrics vended to your account. As your workload demands evolve, you can switch between scaling tiers or go back to the standard control plane at any time.</description></item><item><title>Amazon EKS Blueprints for CDK: Now supporting Amazon EKS Auto Mode</title><link>https://kubermates.org/docs/2025-11-26-amazon-eks-blueprints-for-cdk-now-supporting-amazon-eks-auto-mode/</link><pubDate>Wed, 26 Nov 2025 22:28:12 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-26-amazon-eks-blueprints-for-cdk-now-supporting-amazon-eks-auto-mode/</guid><description>Amazon EKS Blueprints for CDK: Now supporting Amazon EKS Auto Mode What is EKS Blueprints for CDK? What is EKS Auto Mode? Prerequisites Implementing EKS Auto Mode with EKS Blueprints for CDK Pattern 1: Basic EKS Auto Mode cluster Pattern 2: EKS Auto Mode cluster with custom ARM NodePool for workloads Pattern 3: EKS Auto Mode cluster with custom AI Accelerator NodePool for AI/ML workloads Cleaning up Benefits of using EKS Auto Mode with EKS Blueprints Conclusion About the authors Amazon EKS Blueprints for CDK has recently added support for EKS Auto Mode , a significant enhancement that streamlines Kubernetes management by automatically provisioning infrastructure, choosing optimal compute instances, dynamically scaling resources, continuously optimizing costs, managing core add-ons, patching operating systems, and integrating with Amazon Web Services (AWS) security services. EKS Blueprints for CDK is an open source framework that helps AWS customers bootstrap and configure production-ready Amazon Elastic Kubernetes Service (Amazon EKS) clusters with the AWS Cloud Development Kit (AWS CDK). Customers can describe the desired state of their Amazon EKS environment with worker nodes, auto scaling, networking, and Kubernetes add-ons as an infrastructure as code (IaC) blueprint. These blueprints can be used in pipelines to set up consistent environments across AWS accounts and AWS Regions. EKS Blueprints is part of the broader initiative by AWS launched in 2022: Bootstrapping clusters with EKS Blueprints | Amazon Web Services. EKS Blueprints can bootstrap your clusters with Amazon EKS add-ons, and many popular open source add-ons, such as ArgoCD, Nginx, Keda, Fluent Bit, FluxCD, and more. The framework automatically chooses compatible versions for core Amazon EKS add-ons based on your Kubernetes version, eliminating the guesswork of which add-on versions work together. When you upgrade your cluster, the add-on versions automatically update to maintain compatibility, preventing version mismatch errors. EKS Blueprints comes with built-in compatibility handling for your add-ons, so that each add-on you deploy is compatible with your cluster’s configuration. Feedback and support for this framework is available through GitHub issues. EKS Blueprints provides specialized cluster builders that come pre-configured with the right add-ons and best practices for specific workloads. Whether you’re building observability stacks with Prometheus and Grafana, GPU clusters for machine learning (ML), Windows environments for.</description></item><item><title>Enhancing and monitoring network performance when running ML Inference on Amazon EKS</title><link>https://kubermates.org/docs/2025-11-26-enhancing-and-monitoring-network-performance-when-running-ml-inference-on-amazon/</link><pubDate>Wed, 26 Nov 2025 20:55:13 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-26-enhancing-and-monitoring-network-performance-when-running-ml-inference-on-amazon/</guid><description>Enhancing and monitoring network performance when running ML Inference on Amazon EKS Current challenges with network observability for ML inference workloads Deep-dive into Container Network Observability in Amazon EKS ML inference workload scenario Setting up Container Network Observability use cases for ML inference workload Visualize and confirm intercommunication between services for troubleshooting Analyze Availability Zone (AZ) traffic pattern between deployments Network Health Indicator Investigating ML inference Latency using performance metrics in Amazon Manged Grafana Cleaning up Conclusion About the authors Amazon Elastic Kubernetes Service (Amazon EKS) has become a popular choice for customers looking to run their workloads in the Amazon Web Services (AWS) Cloud with customers increasingly choosing to run their AI and Machine Learning (AI/ML) workloads on Amazon EKS. Customers can use Amazon EKS to customize configuration to match their workload requirements. Furthermore, Platform teams can use it to transfer their existing container orchestration model and expertise when deploying new workloads and standardize on Amazon EKS. Kubernetes also provides access to a rich environment of popular open source AI/ML frameworks, tools, and inference engines such as Ray, vLLM, Triton, PyTorch. Lastly, they can use Kubernetes’ tested capability to auto-scale, deploy and manage containerized workloads at scale, and implement the full cluster automation capabilities of EKS. Some use cases of AI/ML workloads deployed on Amazon EKS include generative AI Model training for Large Language Models (LLMs), real-time and batch ML inference, and Retrieval Augmented Generation (RAG) Pipelines. ML inference is the process where a trained model generates predictions on a user’s input prompt or query. Inference has become an important part of modern applications and powers applications such as content generation, intelligent assistants, recommendation engines. Over the years AWS has released a suite of resources and artifacts to accelerate and streamline customers’ usage of Amazon EKS as their service of choice for running AI/ML workloads. These include AI on EKS , Best Practices for Running AI/ML workloads , Amazon EKS-optimized accelerated AMIs for GPU Instances , and AWS Deep Learning Containers. Recently AWS announced Container Network Observability in Amazon EKS , a set of Amazon EKS network observability features that customers can use to observe, visualize, and enhance their Amazon EKS network environment. In this post we explore the feature sets, deep dive into how it works, and explore an ML inference workload scenario where we use it to monitor and enhance its network performance.</description></item><item><title>Data-driven Amazon EKS cost optimization: A practical guide to workload analysis</title><link>https://kubermates.org/docs/2025-11-26-data-driven-amazon-eks-cost-optimization-a-practical-guide-to-workload-analysis/</link><pubDate>Wed, 26 Nov 2025 17:32:47 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-26-data-driven-amazon-eks-cost-optimization-a-practical-guide-to-workload-analysis/</guid><description>Data-driven Amazon EKS cost optimization: A practical guide to workload analysis Common pattern of resource waste The greedy workload caused oversized pod resources Problem: Impact: Resolution: Recommendations: Tools to help with this: The pet workload causes excessive replica counts Problem: Impact: Overly strict topology spread constraints: Recommendation: Overly strict Pod Distribution Budget (PDB): Recommendations: The isolated workloads configured with fragmented node pools Why the savings occur: Recommendations: Conclusion About the authors This post introduces some of the key considerations for optimizing Amazon Elastic Kubernetes Service (Amazon EKS) costs in production environments. Through detailed workload analysis and comprehensive monitoring, we demonstrate a proven best practice to maximize cost savings while maintaining performance and resilience supported by real-world examples and practical implementation guidelines. In pursuing optimal performance and resilience, organizations often struggle to balance cost efficiency, as shown in the following figure. Figure 1: Strategic balance triangle showing trade-offs Through collaboration with application owners and developers, a clear pattern emerges: the primary driver of cloud cost waste is overprovisioned resources, justified by performance and resilience considerations that may no longer reflect actual needs. In this post we discuss three critical areas of waste: Greedy workload : oversized pod resources for performance Pet workload : excessive replica counts for resilience Isolated workload: fragmented node pools with stranded capacity for performance Each decision, made with good intentions, accumulates into unnecessary spending over time. The challenge is finding the optimal balance through data-driven rightsizing and architectural optimization. We can call them greedy workloads if a Pod’s requests are higher than what the application actually tries to use. resources: requests: memory: 3Gi cpu: 800m limits: memory: 3Gi resources: requests: memory: 3Gi cpu: 800m limits: memory: 3Gi The application tries to actually use ~0. 2vCPU (200 m) and ~1 Gi of memory. Figure 2: Request and actual usage We can see that in total we’re actually only using ~400 m/1930 m (~21%) vCPU, and ~2 Gi/6.8 Gi (~29%) memory, as shown in the preceding figure. Despite having plenty of actual resources for more copies of this Pod, Kubernetes doesn’t place any more replicas on this node because the allocatable resources have been almost completely allocated. We adjust the requests to be 200 m CPU and 1 Gi of memory to match the application’s workload.</description></item><item><title>Introducing the fully managed Amazon EKS MCP Server (preview)</title><link>https://kubermates.org/docs/2025-11-21-introducing-the-fully-managed-amazon-eks-mcp-server-preview/</link><pubDate>Fri, 21 Nov 2025 21:43:33 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-21-introducing-the-fully-managed-amazon-eks-mcp-server-preview/</guid><description>Introducing the fully managed Amazon EKS MCP Server (preview) Amazon EKS MCP Server tools Getting started with Amazon EKS MCP Server Prerequisites Configuration Tool access levels Scenario 1: Upgrading an EKS cluster with conversational AI Checking upgrade readiness Upgrade readiness report Key benefits of using EKS MCP for upgrades Scenario 2: Deploying applications through natural language Key EKS MCP tools in action Deployment summary Scenario 3: Troubleshooting infrastructure issues Key EKS MCP tools in action Troubleshooting summary Enhanced EKS console experience with Amazon Q Integrated AI assistance Contextual intelligence Conclusion About the authors Learn how to manage your Amazon Elastic Kubernetes Service (Amazon EKS) clusters through simple conversations instead of complex kubectl commands or deep Kubernetes expertise. This post shows you how to use the new fully managed EKS Model Context Protocol (MCP) Server in Preview to deploy applications, troubleshoot issues, and upgrade clusters using natural language with no deep Kubernetes expertise required. We’ll walk through real scenarios showing how conversational AI turns multi-step manual tasks into simple natural language requests. Teams managing Kubernetes workloads require expertise across container orchestration, infrastructure, networking, and security. While Large Language Models (LLMs) help developers write code and manage workloads, they’re limited without real-time cluster access. Generic recommendations based on outdated training data don’t meet real-world needs. Model Context Protocol (MCP) solves this by giving AI models secure access to live cluster data. MCP is an open-source standard that lets AI models securely access external tools and data sources for better context. It provides a standardized interface that enriches AI applications with real-time, contextual knowledge of EKS clusters, enabling more accurate and tailored guidance throughout the application lifecycle, from development through operations. Earlier this year, AWS was one of the first managed Kubernetes service providers to announce an MCP server, within a few months of the release of MCP protocol. Customers could install this EKS MCP Server on their machines for EKS and Kubernetes resource management. This initial, locally installable version of EKS MCP Server enabled us to rapidly validate our approach and gather valuable customer feedback, which has directly shaped today’s announcement.</description></item><item><title>Guide to Amazon EKS and Kubernetes sessions at AWS re:Invent 2025</title><link>https://kubermates.org/docs/2025-11-21-guide-to-amazon-eks-and-kubernetes-sessions-at-aws-re-invent-2025/</link><pubDate>Fri, 21 Nov 2025 00:26:18 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-21-guide-to-amazon-eks-and-kubernetes-sessions-at-aws-re-invent-2025/</guid><description>Guide to Amazon EKS and Kubernetes sessions at AWS re:Invent 2025 Key breakout sessions Simplified Kubernetes management Generative AI &amp;amp; Agentic AI Deploying AI/ML workloads on Amazon EKS AI-powered Kubernetes development Platform engineering Performance and cost optimization Migration and modernization Security and observability Beyond the sessions Virtual participation Planning your week AWS re:Invent 2025 is back in Las Vegas from December 1-5 with the most comprehensive Kubernetes programming we’ve ever assembled. This year’s event features 48 dedicated sessions covering the full spectrum of cloud-native technologies and variety of use cases ranging from foundational cluster management to advanced AI/ML workload orchestration and deployment on Amazon Elastic Kubernetes Service (Amazon EKS). Amazon EKS and Kubernetes sessions are part of the Containers and Serverless (CNS) track this year, featuring the latest innovations that help customers build, run, and scale production-ready Kubernetes applications easily across any environment. The Amazon EKS and Amazon Elastic Container Registry (Amazon ECR) teams have curated an extensive catalog of track sessions that address all aspects of the real challenges teams face when running Kubernetes in production. From automation capabilities that reduce operational burden to architectural patterns for AI workloads and strategies for optimizing performance and costs, these sessions deliver practical guidance you can apply immediately to your Kubernetes environments. Be sure not to miss Amazon EKS Auto Mode which fully automates cluster management for compute, storage, and networking on AWS with a single click and Amazon EKS Hybrid Nodes which extends Kubernetes management to on-premises and edge environments for unified operations across distributed infrastructure. This year we also have a special session focused on the engineering that powers the scale of Amazon EKS to support ultra-scale clusters of up to 100K nodes and enable cutting-edge AI-powered developer experiences. These include Model Context Protocol (MCP) integration with Amazon EKS for context-aware Kubernetes workflows and support for deploying sophisticated multi-agent AI systems with secure agent-to-agent communication. With these innovations, customers from small startups to sophisticated enterprises have everything they need to get started with and run production-grade Kubernetes applications at scale on AWS. Whether you’re evaluating Kubernetes for the first time or architecting multi-region platforms, you’ll find sessions tailored to your needs. For a detailed explanation of the different session formats, check out the event page. Don’t miss our key breakout sessions featuring new launches and innovations: CNS205: The future of Kubernetes on AWS , featuring the strategic direction for Kubernetes on AWS and key innovations for running Kubernetes across cloud, on-premises, and edge.</description></item><item><title>Monitoring network performance on Amazon EKS using AWS Managed Open-Source Services</title><link>https://kubermates.org/docs/2025-11-20-monitoring-network-performance-on-amazon-eks-using-aws-managed-open-source-servi/</link><pubDate>Thu, 20 Nov 2025 18:48:46 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-20-monitoring-network-performance-on-amazon-eks-using-aws-managed-open-source-servi/</guid><description>Monitoring network performance on Amazon EKS using AWS Managed Open-Source Services Architecture diagram Walkthrough Prerequisites Setup Network performance testing and visualization: Key considerations Managing dashboards and data sources with Grafana Operator Using ADOT for telemetry collection Cleanup Conclusion About the authors As organizations scale their microservices architectures on Amazon Elastic Kubernetes Service (Amazon EKS) , platform and development teams face mounting challenges in monitoring network performance across distributed workloads. While VPC Flow Logs provide visibility into IP traffic, they lack the Kubernetes context needed to correlate network flows to specific pods, services, and namespaces. This makes it difficult to diagnose connectivity issues, identify packet drops, or investigate security incidents. Platform teams struggle to answer critical questions such as: “ Which services are communicating with each other? Where are the latency bottlenecks? Are there any security policy violations?” Without Kubernetes-enriched network telemetry integrated into their observability and Security information and event management (SIEM) systems, teams spend excessive time troubleshooting network-related issues and lack the real-time visibility needed to ensure optimal performance and security across their EKS environments. In this post, we will discuss how to monitor the network performance of your workloads running in Amazon EKS clusters using new advanced network observability features which are a part of Container Network Observability in EKS. This includes capturing network performance metrics and exporting them to AWS Managed Open-Source services such as Amazon Managed Service for Prometheus , Amazon Managed Grafana , etc. You can leverage the same approach to integrate with third-party observability solutions such as Datadog, New Relic, etc. , or self-managed open-source tools like Prometheus. Amazon EKS introduced new advanced network observability features that give you the ability to dynamically visualize and quickly understand the landscape, performance and security of the network environment of your Kubernetes clusters. At a cluster level, it provides you with a Service Map that depicts end-to-end visibility of network traffic flows for workloads in the cluster (east ↔ west traffic). Alongside this, you can access Network Flow Analysis which provide more granular information around application network activity and the network security posture for your workloads. Additionally, you can export Kubernetes-enriched network performance metrics from all your EKS clusters to be analyzed in your preferred monitoring solution or SIEM.</description></item><item><title>Network observability</title><link>https://kubermates.org/releases/2025-11-19-network-observability/</link><pubDate>Wed, 19 Nov 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-11-19-network-observability/</guid><description>Monitor Kubernetes workload traffic with Container Network Observability Use cases Measure network performance to detect anomalies Leverage console visualizations for more precise troubleshooting Track top-talkers in your Amazon EKS environment Features Get started Prerequisites and important notes Required IAM permissions Using Infrastructure as Code (IaC) Terraform How does it work? Performance metrics Service map and flow table Considerations and limitations Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Amazon EKS provides enhanced network observability features that provide deeper insights into your container networking environment. These capabilities help you better understand, monitor, and troubleshoot your Kubernetes network landscape in AWS. With enhanced container network observability, you can leverage granular, network-related metrics for better proactive anomaly detection across cluster traffic, cross-AZ flows, and AWS services. Using these metrics, you can measure system performance and visualize the underlying metrics using your preferred observability stack. In addition, Amazon EKS now provides network monitoring visualizations in the AWS console that accelerate and enhance precise troubleshooting for faster root cause analysis. You can also leverage these visual capabilities to pinpoint top-talkers and network flows causing retransmissions and retransmission timeouts, eliminating blind spots during incidents. These capabilities are enabled by Amazon CloudWatch Network Flow Monitor. Several teams standardize on an observability stack that allows them to measure their systemâs performance, visualize system metrics and be alarmed in the event that a specific threshold is breached. Container network observability in EKS aligns with this by exposing key system metrics that you can scrape to broaden observability of your systemâs network performance at the pod and worker node level. In the event of an alarm from your monitoring system, you may want to hone in on the cluster and workload where an issue originated from. To support this, you can leverage visualizations in the EKS console that narrow the scope of investigation at a cluster level, and accelerate the disclosure of the network flows responsible for the most retransmissions, retransmission timeouts, and the volume of data transferred.</description></item><item><title>Extending GPU Fractionalization and Orchestration to the edge with NVIDIA Run:ai and Amazon EKS</title><link>https://kubermates.org/docs/2025-10-28-extending-gpu-fractionalization-and-orchestration-to-the-edge-with-nvidia-run-ai/</link><pubDate>Tue, 28 Oct 2025 18:05:27 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-28-extending-gpu-fractionalization-and-orchestration-to-the-edge-with-nvidia-run-ai/</guid><description>Extending GPU Fractionalization and Orchestration to the edge with NVIDIA Run:ai and Amazon EKS Extensive and performant global cloud infrastructure Challenges in bringing AI workloads to the edge Training Inference Run:ai support for AWS Hybrid and Edge services Best practices for Run:ai at the edge Conclusion About the authors As organizations of all sizes have rapidly embraced the opportunity to pair foundation models (FMs) with AI agents to streamline complex workflows and processes, the demand for artificial intelligence and machine learning capabilities across distributed locations has never been stronger. For example, some organizations need to run custom, in-house language models within a specific geographic boundary to meet data residency requirements, while others require processing data locally to serve latency-sensitive edge inference requests. These distributed processing needs often require running AI/ML workloads in local metro Points of Presence (PoPs), customer premises, and beyond – especially when an AWS Region is not close enough to meet performance or compliance requirements. Managing these distributed workloads introduces another challenge: the need for efficient and topology-aware GPU resource management becomes critical, particularly at the distributed edge, where capacity is often limited and requires optimal allocation. Building on these emerging needs for distributed AI/ML capabilities and efficient GPU resource management, Amazon Web Services (AWS) and NVIDIA have been working together to explore solutions native to the environments that customers most frequently use for model training and inference, such as Amazon Elastic Kubernetes Service. In a previous blog post , we showcased how NVIDIA Run:ai addresses key challenges in GPU resource management, including static allocation limitations, resource competition, and inefficient sharing in GPU clusters. The blog post detailed the implementation of NVIDIA Run:ai on Amazon EKS, which featured dynamic GPU fractions, node-level scheduling, and priority-based sharing. Since then, we have released NVIDIA Run:ai in AWS Marketplace , allowing customers to deploy the Run:ai control plane to their Amazon EKS clusters without having to manage the deployment of individual Helm Charts. Building on this collaboration, today we are extending this flexibility to the entire AWS cloud continuum, enabling you to optimize GPU resources wherever your workloads need to run – in an AWS Region , on-premises , or at the edge. That is why we are excited to announce native Run:ai support for Amazon EKS in AWS Local Zones (including Dedicated Local Zones), Amazon EKS on AWS Outposts , and Amazon EKS Hybrid Nodes. As part of this launch, you can now extend Run:ai environments to support a cluster of GPUs separated by hundreds (if not thousands) of miles across these AWS Hybrid and Edge services. This architectural pattern enables you to create powerful high availability and disaster recovery strategies while maximizing cost efficiency and complying with local data residency requirements.</description></item><item><title>Enhancing container security in Amazon EKS Auto Mode with KubeArmor</title><link>https://kubermates.org/docs/2025-10-21-enhancing-container-security-in-amazon-eks-auto-mode-with-kubearmor/</link><pubDate>Tue, 21 Oct 2025 19:25:41 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-21-enhancing-container-security-in-amazon-eks-auto-mode-with-kubearmor/</guid><description>Enhancing container security in Amazon EKS Auto Mode with KubeArmor The runtime security challenge in Kubernetes Architecture Key components Prerequisites How does KubeArmor improve EKS Auto Mode security? 1. System call-level protection 2. Granular process and file access control 3. Supply chain security enhancement 4. Zero-day vulnerability mitigation 5. Compliance support Real world use case Deployment steps Step 1: Create an EKS Auto Mode cluster Step 2: Install KubeArmor using Helm Step 3: Verify KubeArmor installation Step 4: Apply file integrity monitoring/protection policy Implementing security policies with KubeArmor Use case 1: Web application hardening Use case 2: Crypto mining prevention Progressive policy implementation Integrating with AWS security services Benefits of integration Amazon CloudWatch integration Enhanced threat response with AWS GuardDuty integration Unified security dashboard Cleaning up Step 1: Remove KubeArmor policies Step 2: Uninstall KubeArmor components Step 3: Remove CloudWatch integration resources Step 4: Delete the EKS Auto Mode cluster Step 5: Remove IAM resources (if created) Step 6: Remove local configuration Security Best Practices to consider Conclusion Resources About the authors This post was written with Rahul Jadhav from Accuknox. As organizations adopt Amazon Elastic Kubernetes Service (Amazon EKS) Auto Mode for its streamlined Kubernetes cluster management, security remains a shared responsibility. Although EKS Auto Mode automates control plane operations, container runtime security needs more attention. In this post, we explore how KubeArmor , an open source container-aware security enforcement system, enhances the security posture of containerized workloads running on EKS Auto Mode clusters. Although EKS Auto Mode significantly streamlines cluster management by automating control plane and node operations, securing the workloads running within the cluster remains a critical user responsibility. Traditional security tools often struggle to provide granular visibility and control at the container runtime level, leaving potential gaps for sophisticated attacks. Container runtime security presents unique challenges that traditional security approaches don’t fully address.</description></item><item><title>Extending EKS with Hybrid Nodes: IAM Roles Anywhere and HashiCorp Vault</title><link>https://kubermates.org/docs/2025-10-17-extending-eks-with-hybrid-nodes-iam-roles-anywhere-and-hashicorp-vault/</link><pubDate>Fri, 17 Oct 2025 20:30:49 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-17-extending-eks-with-hybrid-nodes-iam-roles-anywhere-and-hashicorp-vault/</guid><description>Extending EKS with Hybrid Nodes: IAM Roles Anywhere and HashiCorp Vault Solution overview Prerequisites PKI architecture IAM Roles Anywhere configuration Vault certificate management EKS Hybrid Nodes configuration Results Cleanup Conclusion About the author Amazon EKS Hybrid Nodes allows businesses to flexibly make use of compute resources outside of AWS by extending an Amazon Elastic Kubernetes Service (Amazon EKS) data plane beyond the AWS Cloud boundary. Use cases for EKS Hybrid Nodes include businesses who have goals or requirements focusing on data sovereignty, low latency communication, and government or industry regulations. In this blog post, we’ll explore how to use AWS Identity and Access Management (IAM) Roles Anywhere , supported by HashiCorp Vault PKI , to facilitate joining EKS Hybrid Nodes to an Amazon EKS Cluster. When a node joins an EKS cluster, it uses metadata from the cluster – such as the cluster certificate bundle – to authenticate. Permission to retrieve this metadata is granted by IAM via the eks:DescribeCluster operation, which can be attached to an IAM Role via an IAM Policy. eks:DescribeCluster Since EKS Hybrid Nodes reside outside of AWS, they cannot inherit IAM Policies directly, and a different mechanism is required to retrieve the cluster certificate bundle. One recommended option is to use AWS Systems Manager (SSM) to provide nodes with temporary IAM credentials and permissions, including eks:DescribeCluster. Another option to accomplish the same outcome is to make use of an existing Public Key Infrastructure (PKI) and AWS IAM Roles Anywhere , which will be the focus of this blog post. eks:DescribeCluster IAM Roles Anywhere supports temporary credential validity periods from the default value of one hour up to a maximum of twelve hours. For this solution, you should have the following prerequisites: An AWS account An Amazon EKS Cluster ( prerequisites ) One or more Linux servers that will become EKS Hybrid Nodes ( compatibility ) A HashiCorp Vault Server or Cluster Figure 1 – Vault PKI Architecture Diagram If you’re already using HashiCorp Vault to manage secrets and protect sensitive data, then you already have a PKI! Vault natively supports PKI as part of its secrets engine. All you must do is to enable it. vault secrets enable pki vault secrets enable pki Because you’ll establish a Trust between IAM Roles Anywhere (IAM-RA) and the Vault Certificate Authority (CA), it is important to be aware that the default Time To Live (TTL) for the Vault CA is 30 days.</description></item><item><title>New Amazon EKS Auto Mode features for enhanced security, network control, and performance</title><link>https://kubermates.org/docs/2025-10-16-new-amazon-eks-auto-mode-features-for-enhanced-security-network-control-and-perf/</link><pubDate>Thu, 16 Oct 2025 20:36:39 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-16-new-amazon-eks-auto-mode-features-for-enhanced-security-network-control-and-perf/</guid><description>New Amazon EKS Auto Mode features for enhanced security, network control, and performance Recent feature highlights Performance and capacity management Advanced networking capabilities Security and compliance Why these features matter Getting started with EKS Auto Mode Conclusion About the authors When we launched Amazon Elastic Kubernetes Service (Amazon EKS) Auto Mode at Amazon Web Services (AWS) re:Invent 2024, we introduced a transformative approach to Kubernetes cluster management on AWS. EKS Auto Mode fully automates Kubernetes cluster management for compute, storage, and networking, so that you can focus on building applications that drive innovation rather than managing cluster infrastructure. EKS Auto Mode addresses a fundamental challenge: although users choose Amazon EKS for the open standards of Kubernetes combined with the capabilities of AWS, they often struggle with the operational complexity of production-grade infrastructure. Managing compute provisioning, maintaining essential plugins for autoscaling, storage, and networking, handling OS patches, and optimizing resource utilization all require significant ongoing effort that takes focus away from the kind of innovation that their users want. EKS Auto Mode handles the complete Kubernetes cluster infrastructure lifecycle through five key capabilities: Automated compute management : Intelligent node provisioning, scaling, and lifecycle management that adapts to workload demands without manual intervention. Integrated networking and storage : Pre-configured Amazon Virtual Private Cloud (Amazon VPC) CNI, Amazon Elastic Block Store (Amazon EBS ) CSI drivers, and load balancer controllers that work together seamlessly from day one Secure and optimized compute runtime: Enforced encryption, automated patching, and secure access management that maintains compliance without operational overhead. Built-in cost optimization : Automatic instance type selection, Spot integration, and right-sizing that continuously optimizes spend based on actual usage patterns. Simplified cluster lifecycle : One-click cluster creation and updates that handle the complexity of Kubernetes version management and component compatibility This foundation has enabled many customers, from large enterprises to scaling startups, to shift cluster operations to AWS, freeing up valuable time and energy on their teams while retaining full Kubernetes interoperability. Teams can deploy production workloads faster, achieve better performance outcomes, and reduce infrastructure costs without needing specialized Kubernetes expertise. Since launch, we’ve been expanding Auto Mode’s capabilities based on our customers’ feedback and real-world usage patterns. This post covers the features and improvements we’ve delivered to address specific operational challenges and extend the functionality of EKS Auto Mode for diverse workloads. Over the past 10 months, we’ve delivered a steady stream of enhancements that expand the capabilities of EKS Auto Mode and address diverse customer requirements: Optimized node lifecycle management : EKS Auto Mode has significantly improved cluster scaling performance through several key enhancements.</description></item><item><title>Service linked role update</title><link>https://kubermates.org/releases/2025-10-15-service-linked-role-update/</link><pubDate>Wed, 15 Oct 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-10-15-service-linked-role-update/</guid><description>Using roles to connect a Kubernetes cluster to Amazon EKS Service-linked role permissions for Amazon EKS Creating a service-linked role for Amazon EKS Editing a service-linked role for Amazon EKS Deleting a service-linked role for Amazon EKS Cleaning up a service-linked role Manually delete the service-linked role Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Amazon EKS uses AWS Identity and Access Management (IAM) service-linked roles. A service-linked role is a unique type of IAM role that is linked directly to Amazon EKS. Service-linked roles are predefined by Amazon EKS and include all the permissions that the service requires to call other AWS services on your behalf. A service-linked role makes setting up Amazon EKS easier because you donât have to manually add the necessary permissions. Amazon EKS defines the permissions of its service-linked roles, and unless defined otherwise, only Amazon EKS can assume its roles. The defined permissions include the trust policy and the permissions policy, and that permissions policy cannot be attached to any other IAM entity. You can delete a service-linked role only after first deleting their related resources. This protects your Amazon EKS resources because you canât inadvertently remove permission to access the resources. For information about other services that support service-linked roles, see AWS services that work with IAM and look for the services that have Yes in the Service-linked role column. Choose a Yes with a link to view the service-linked role documentation for that service. Amazon EKS uses the service-linked role named AWSServiceRoleForAmazonEKSConnector.</description></item><item><title>Running Slurm on Amazon EKS with Slinky</title><link>https://kubermates.org/docs/2025-10-14-running-slurm-on-amazon-eks-with-slinky/</link><pubDate>Tue, 14 Oct 2025 16:58:24 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-14-running-slurm-on-amazon-eks-with-slinky/</guid><description>Running Slurm on Amazon EKS with Slinky A primer on Slurm The Slinky Project The use case for Slinky Architecture overview of Slurm on EKS with Slinky Slinky Slurm cluster components Benefits of running Slurm on EKS with Slinky Alternatives for running Slurm on AWS AWS ParallelCluster AWS Parallel Computing Service Amazon SageMaker HyperPod Alternative Kubernetes native job schedulers Volcano Apache YuniKorn Kueue When Slurm on EKS is right for you About the authors When building an AI infrastructure stack for pre-training, fine-tuning, or inference workloads, both Slurm and Kubernetes can be used as compute orchestration platforms to meet the needs of different teams and address different stages of the AI development lifecycle. However, traditionally this would result in managing disparate clusters of accelerated compute capacity, potentially duplicating operational overhead and risking resource underuse. But what if you could deploy a Slurm cluster as a Kubernetes service to get the best of both worlds? Think of Kubernetes as a large, modern office building providing shared resources (for example, electricity, internet, security, HVAC) for its tenants. When a specialized lab moves in, needing dedicated resources such as specific power and temperature control, you don’t build a new building. Instead, you integrate the lab into the existing building infrastructure, allowing it to use shared services while maintaining its own precise controls for high-performance work. In the same way, Slurm can be ran inside a Kubernetes environment such as Amazon Elastic Kubernetes Service (Amazon EKS) using the open source Slinky Project. In this post, we introduce the Slinky Project, discuss its benefits, explore some alternatives, and leave you with a bit of homework to go deploy the Slurm on EKS blueprint , which uses the Slinky Slurm operator. Slurm is an open source, highly scalable workload manager and job scheduler designed for managing compute resources on compute clusters of all sizes. It provides three core functions: allocating access to compute resources, providing a framework for launching and monitoring parallel computing jobs, and managing queues of pending work to resolve resource contention. Slurm is widely used in traditional High-Performance Computing (HPC) environments and in AI training to manage and schedule large-scale accelerated compute workloads across multi-node clusters. Slurm allows researchers and engineers to efficiently allocate CPU, GPU, and memory resources for distributed training jobs with fine-grained control over resource types and job priorities. Slurm’s reliability, advanced scheduling features, and integration with both on-premises and cloud environments make it a preferred choice for handling the scale, throughput, and reproducibility that modern AI research and industry demand.</description></item><item><title>How to manage EKS Pod Identities at scale using Argo CD and AWS ACK</title><link>https://kubermates.org/docs/2025-10-13-how-to-manage-eks-pod-identities-at-scale-using-argo-cd-and-aws-ack/</link><pubDate>Mon, 13 Oct 2025 21:16:16 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-13-how-to-manage-eks-pod-identities-at-scale-using-argo-cd-and-aws-ack/</guid><description>How to manage EKS Pod Identities at scale using Argo CD and AWS ACK What is GitOps? EKS Pod Identity: Simplifying IAM Permissions for Kubernetes Applications Overview of Amazon Controllers for Kubernetes (ACK) Prerequisites High-level overview of the steps Install Amazon EKS Pod Identity Agent Install the AWS Controllers for Kubernetes (ACK) 1. Creation of a role for our ACK controller and the EKS Pod Identity association 2. Install the ACK controller for EKS with Helm, edit region as needed Accessing Argo CD Deploy the sample application highlighting the problem 1. Clone the code repository for the sample application project 2. Create the IAM role to be used by the application 3. Open application. yaml file, the following parameters can be replaced with your own 4. Create the Argo CD application 5. Confirm the role from inside our container Validate the correct role is passed via a job Alternative Solution: Changing ARGOCD_SYNC_WAVE_DELAY Cleanup Conclusion About the authors In today’s blog post, we’ll explore how to manage at scale the association of Kubernetes service accounts with IAM roles through the Amazon Elastic Kubernetes Service (EKS) Pod Identity association. We will be using Argo CD , a popular GitOps delivery tool, and the AWS Controllers for Kubernetes (ACK) to automate the association, but as we will see this sometimes causes a critical challenge: the EKS Pod Identity API is eventually consistent. We need to verify the association is available for the credentials before the application pods are deployed. Our focus will be on correct deployment, thus addressing that challenge, without causing side effects.</description></item><item><title>SaaS deployment architectures with Amazon EKS</title><link>https://kubermates.org/docs/2025-10-10-saas-deployment-architectures-with-amazon-eks/</link><pubDate>Fri, 10 Oct 2025 21:13:56 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-10-saas-deployment-architectures-with-amazon-eks/</guid><description>SaaS deployment architectures with Amazon EKS Patterns for managing remote environment with Amazon EKS in its core Shared responsibility Building distributed SaaS on Kubernetes Application packaging and deployment SaaS Provider Hosted Remote Application Plane Customer Hosted Data Plane with EKS Hybrid Nodes Conclusion About the authors As companies scale their software as a service (SaaS) offerings, they’re expanding their market reach by offering flexible deployment options directly within their customers’ environments. This versatile deployment model enables organizations to maintain data sovereignty, meet compliance standards, achieve optimal performance through reduced latency, and maximize efficiency by running applications close to existing customer datasets. SaaS providers can embrace this approach to serve a broader range of industries and unlock new business opportunities, particularly in highly regulated sectors and performance-sensitive markets. This method of extending the deployment environments into the tenant’s owned environments has been labelled “SaaS Anywhere” in this post. Although SaaS Anywhere solves important challenges related to managing remote customers’ environments, they introduce significant complexity in management and operation. SaaS providers must develop robust systems for provisioning and maintaining their application stack across numerous customer environments, implement cross-account monitoring solutions, manage distributed lifecycle updates, and provide consistent security controls. All of this is done while maintaining operational excellence at scale. In this post we explore patterns and practices for building and operating these distributed Amazon Elastic Kubernetes Service (Amazon EKS )-based applications effectively. When designing SaaS solutions, organizations typically employ one of three deployment models, each offering distinct advantages for specific use cases: SaaS Provider Hosted : Both data and control planes reside in the provider’s Amazon Web Services (AWS) account, optimizing for operational efficiency and rapid customer onboarding. Although lightweight agents may exist in customer environments for telemetry, all core processing remains provider hosted. Remote Application Plane : The data plane runs in the customer’s environment while the control plane stays in the provider’s account. This model balances compliance needs with operational efficiency, allowing customers to maintain data sovereignty while using AWS services.</description></item><item><title>Kubernetes version 1.34</title><link>https://kubermates.org/releases/2025-10-02-kubernetes-version-1-34/</link><pubDate>Thu, 02 Oct 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-10-02-kubernetes-version-1-34/</guid><description>Understand the Kubernetes version lifecycle on EKS Available versions on standard support Available versions on extended support Amazon EKS Kubernetes release calendar Get version information with AWS CLI To retrieve information about available Kubernetes versions on EKS using the AWS CLI Amazon EKS version FAQs Amazon EKS extended support FAQs Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Kubernetes rapidly evolves with new features, design updates, and bug fixes. The community releases new Kubernetes minor versions (such as 1.33 ) on average once every four months. Amazon EKS follows the upstream release and deprecation cycle for minor versions. As new Kubernetes versions become available in Amazon EKS, we recommend that you proactively update your clusters to use the latest available version. 1.33 A minor version is under standard support in Amazon EKS for the first 14 months after itâs released. Once a version is past the end of standard support date, it enters extended support for the next 12 months. Extended support allows you to stay at a specific Kubernetes version for longer at an additional cost per cluster hour. If you havenât updated your cluster before the extended support period ends, your cluster is auto-upgraded to the oldest currently supported extended version. Extended support is enabled by default. To disable, see Disable EKS extended support. We recommend that you create your cluster with the latest available Kubernetes version supported by Amazon EKS.</description></item><item><title>Implementing granular failover in multi-Region Amazon EKS</title><link>https://kubermates.org/docs/2025-09-18-implementing-granular-failover-in-multi-region-amazon-eks/</link><pubDate>Thu, 18 Sep 2025 14:13:01 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-18-implementing-granular-failover-in-multi-region-amazon-eks/</guid><description>Implementing granular failover in multi-Region Amazon EKS Typical architecture The all-or-nothing health check problem Solution overview Prerequisites Walkthrough Configure environment variables Create EKS clusters in Region 1 and Region 2 Create IngressClass configuration Deploy app1 and app2 on the EKS clusters Configure Route 53 health checks for app1 and app2 Configure Route 53 alias records for app1 and app2 Test application failure Things to know Cleaning up Conclusion About the authors Enterprises across industries operate tens of millions of Amazon Elastic Kubernetes Service (Amazon EKS) clusters annually, supporting use cases from web/mobile applications to data processing, machine learning (ML), generative AI, gaming, and Internet of Things (IoT). As organizations increasingly adopt multi-tenant platform models where multiple application teams share an EKS cluster, they need more granular control over application availability in their multi-Region architectures—particularly for serving global users and meeting strict regulatory requirements. Although multi-tenant models optimize resource usage and reduce operational overhead, they present challenges when applications need individualized Recovery Point Objective (RPO) and Recovery Time Objective (RTO) targets across multiple AWS Regions. Traditional approaches force uniform failover policies across all applications where a single application’s failure creates an unnecessary compromise between operational efficiency and application-specific resilience. In this post, we demonstrate how to configure Amazon Route 53 to enable unique failover behavior for each application within your multi-tenant Amazon EKS environment across AWS Regions, which allows you to maintain the cost benefits of shared infrastructure while meeting diverse availability requirements. Before diving into the solution, we demonstrate how a typical multi-Region multi-tenant Amazon EKS architecture is structured, and the key components that enable Regional traffic routing. Figure 1 consists of the following key components: Route 53 Routes traffic to the Application Load Balancer (ALB) in the respective Region Makes sure of high availability and resiliency during failure events Routes traffic to the Application Load Balancer (ALB) in the respective Region Makes sure of high availability and resiliency during failure events AWS Load Balancer Controller Exposes applications running on the EKS cluster through ALB Uses IP target type to register application Pods to the ALB Exposes applications running on the EKS cluster through ALB Uses IP target type to register application Pods to the ALB The Route 53 configuration details are as follows: Regional Routing Each AWS Region uses a Route 53 alias record to route traffic to its Region-specific ALB. Example: app1. example. com points to ALB in Region 1 Example: app2. example. com points to ALB in Region 2 Each AWS Region uses a Route 53 alias record to route traffic to its Region-specific ALB.</description></item><item><title>Use Raspberry Pi 5 as Amazon EKS Hybrid Nodes for edge workloads</title><link>https://kubermates.org/docs/2025-09-17-use-raspberry-pi-5-as-amazon-eks-hybrid-nodes-for-edge-workloads/</link><pubDate>Wed, 17 Sep 2025 15:08:59 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-17-use-raspberry-pi-5-as-amazon-eks-hybrid-nodes-for-edge-workloads/</guid><description>Use Raspberry Pi 5 as Amazon EKS Hybrid Nodes for edge workloads Why Raspberry Pi 5? Architectural overview Getting started Step 1: Create the EKS cluster Step 2: Set up the VPN server Add the Raspberry Pi to the cluster as a remote node Setting up the Container Network Interface Step 1: Install Cilium Deploying a sample application on Amazon EKS Hybrid Nodes with edge integration Step 1: Hardware requirements and setup Step 2: Deploy the DynamoDB table Step 3: Deploy the sensor application Step 4: Deploy the frontend dashboard Conclusion About the authors Since its launch, Amazon Elastic Kubernetes Service (Amazon EKS) has powered tens of millions of clusters so that users can accelerate application deployment, optimize costs, and use the flexibility of Amazon Web Services (AWS) for hosting containerized applications. Amazon EKS eliminates the operational complexities of maintaining Kubernetes control plane infrastructure, while offering seamless integration with AWS resources and infrastructure. However, some workloads need to be run at the edge with real-time processing, such as latency-sensitive applications that generate large volumes of data. In these scenarios, when there is consistent internet connectivity available, users often seek the benefits of cloud integrations while continuing to use their on-premises hardware. That’s why we introduced Amazon EKS Hybrid Nodes at AWS re:Invent 2024, so that users can extend their Kubernetes data plane to the edge while continuing to run the Kubernetes control plane in an AWS Region. Amazon EKS Hybrid Nodes unifies Kubernetes management across cloud, on-premises, and edge environments by enabling users to use their on-premises infrastructure as nodes in EKS clusters, alongside Amazon Elastic Compute Cloud (Amazon EC2). To demonstrate the use of Amazon EKS Hybrid Nodes, we explored a practical use case from the manufacturing sector. These environments often rely on real-time data from digital sensors that must be processed locally due to latency and reliability, while still using the cloud for analytics and long-term storage. Our use case involves reading distance values from an ultrasonic sensor, processing them on a local edge device running as a Hybrid Node, and storing them in Amazon DynamoDB on AWS. In this post, we demonstrate how to implement Amazon EKS Hybrid Nodes using the Raspberry Pi 5 , a popular edge computing platform. We cover the following: Setting up an EKS cluster that seamlessly connects cloud and edge infrastructure Securing connectivity using the WireGuard VPN for site-to-site communication Enabling container networking with Cilium for hybrid node deployments Demonstrating a real-world Internet of Things (IoT) application that demonstrates the power of edge-cloud integration The Raspberry Pi 5 is compact and can be deployed at the edge so that you can process data before it is transmitted to the cloud. Building on this strength, we created a microservices-based application partly running on the edge on a Raspberry Pi 5 and partly on AWS in the cloud.</description></item><item><title>Kubernetes right-sizing with metrics-driven GitOps automation</title><link>https://kubermates.org/docs/2025-09-11-kubernetes-right-sizing-with-metrics-driven-gitops-automation/</link><pubDate>Thu, 11 Sep 2025 15:17:23 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-11-kubernetes-right-sizing-with-metrics-driven-gitops-automation/</guid><description>Kubernetes right-sizing with metrics-driven GitOps automation Understanding the challenges of resource management in Amazon EKS The impact of inefficient resource management Existing solutions for Kubernetes resource management How the proposed solution addresses the challenges Solution overview Workflow overview Key architectural considerations Walkthrough Prerequisites Implementing a GitOps-driven automation for resource optimization GitOps principle Setting up the recommendation generator Environment and metrics source Local or External Installation Automating the GitOps workflow Cleaning up Conclusion About the authors Efficient resource allocation in Kubernetes is essential for optimizing application performance and controlling costs. In Amazon Elastic Kubernetes Service (Amazon EKS) , managing resource requests and limits manually can be challenging and error-prone. This post introduces an automated, and GitOps-driven approach to resource optimization using Amazon Web Services (AWS) services such as Amazon Managed Service for Prometheus and Amazon Bedrock. This approach is particularly beneficial for users who prefer non-intrusive methods for resource optimization. Understanding resource management in Kubernetes is crucial for optimal cluster performance. When deploying pods, the Kubernetes scheduler evaluates resource requests to find suitable nodes that can accommodate the specified CPU and memory requirements. These requests act as the minimum guaranteed resources for the pod, while limits serve as upper bounds to prevent any single pod from monopolizing node resources. Over-provisioning and under-provisioning of resources in Kubernetes can lead to increased costs and performance issues. Striking the right balance is essential for optimal resource usage. In a shared environment, one pod consuming excessive resources can degrade the performance of others on the same node. Applications with fluctuating resource demands can be challenging to manage. Without adaptive resource allocation strategies, these workloads may experience performance degradation or resource waste.</description></item><item><title>How to build highly available Kubernetes applications with Amazon EKS Auto Mode</title><link>https://kubermates.org/docs/2025-09-09-how-to-build-highly-available-kubernetes-applications-with-amazon-eks-auto-mode/</link><pubDate>Tue, 09 Sep 2025 17:49:16 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-09-how-to-build-highly-available-kubernetes-applications-with-amazon-eks-auto-mode/</guid><description>How to build highly available Kubernetes applications with Amazon EKS Auto Mode Solution overview Test scenarios Pod fail scenario Node fail scenario AZ fail scenario Cluster version upgrade scenario Karpenter node disruption Conclusion About the authors As organizations scale their Kubernetes deployments, many find themselves facing critical operational challenges. Consider how DevOps teams spend countless hours planning and executing cluster upgrades, managing add-ons, and making sure that security patches are applied consistently. There is a clear need for reliable, automated cluster lifecycle management with teams struggling to maintain consistent cluster configurations and security postures across environments. Amazon Elastic Kubernetes Service (Amazon EKS) Auto Mode addresses these challenges by automating control plane updates, streamlining add-on management, and making sure that clusters maintain current best practices. This post explores the capabilities of EKS Auto Mode in depth, subjecting it to a series of challenging scenarios such as failure simulations, node recycling, and cluster upgrades—all while maintaining uninterrupted service traffic. This guide delves into strategies for achieving high availability in the face of the dynamic nature of EKS Auto Mode by using a range of Kubernetes features to maximize uptime. The goal is to provide a comprehensive guide that shows how to harness the potential of EKS Auto Mode, thus making sure that your services remain robust and resilient in the demanding environments. Although there is a wealth of comprehensive literature on the broader subject of reliability in container ecosystems, this post specifically narrows its scope to the nuanced considerations of operating reliable workloads within EKS Auto Mode environments. Before delving into the specifics of Amazon EKS and its Auto Mode feature , you must understand key Kubernetes concepts that are instrumental in maximizing service uptime during different cluster events. These foundational elements form the bedrock of resilient application architectures in Kubernetes environments, regardless of the specific cloud provider or management mode. Mastering these concepts enables you to use EKS Auto Mode effectively and build highly available systems that withstand various operational challenges. In the rest of this section we explore these essential Kubernetes features that play a pivotal role in maintaining service continuity during both planned and unplanned events.</description></item><item><title>How to run AI model inference with GPUs on Amazon EKS Auto Mode</title><link>https://kubermates.org/docs/2025-09-04-how-to-run-ai-model-inference-with-gpus-on-amazon-eks-auto-mode/</link><pubDate>Thu, 04 Sep 2025 16:12:23 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-04-how-to-run-ai-model-inference-with-gpus-on-amazon-eks-auto-mode/</guid><description>How to run AI model inference with GPUs on Amazon EKS Auto Mode Key features that make EKS Auto Mode ideal for AI/ML workloads Walkthrough Prerequisites Set up environment variables Set up EKS Auto Mode cluster and run a model Reducing model cold start time in AI inference workloads Conclusion About the authors AI model inference using GPUs is becoming a core part of modern applications, powering real-time recommendations, intelligent assistants, content generation, and other latency-sensitive AI features. Kubernetes has become the orchestrator of choice for running inference workloads, and organizations want to use its capabilities while still maintaining a strong focus on rapid innovation and time-to-market. But here’s the challenge: while teams see the value of Kubernetes for its dynamic scaling and efficient resource management, they often get slowed down by the need to learn Kubernetes concepts, manage cluster configurations, and handle security updates. This shifts focus away from what matters most: deploying and optimizing AI models. That is where Amazon Elastic Kubernetes Service (Amazon EKS) Auto Mode comes in. EKS Auto Mode Automates node creation, manages core capabilities , and handles upgrades and security patching. In turn, this enables to run your inference workloads without the operational overhead. In this post, we show you how to swiftly deploy inference workloads on EKS Auto Mode. We also demonstrate key features that streamline GPU management, show best practices for model deployment, and walk through a practical example by deploying open weight models from OpenAI using vLLM. Whether you’re building a new AI/machine learning (ML) platform or optimizing existing workflows, these patterns help you accelerate development while maintaining operational efficiency. In this section, we take a closer look at the GPU-specific features that come pre-configured and ready to use with an EKS Auto Mode cluster. These capabilities are also available in self-managed Amazon EKS environments, but they typically need manual setup and tuning.</description></item><item><title>Dynamic Kubernetes request right sizing with Kubecost</title><link>https://kubermates.org/docs/2025-09-03-dynamic-kubernetes-request-right-sizing-with-kubecost/</link><pubDate>Wed, 03 Sep 2025 18:52:27 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-03-dynamic-kubernetes-request-right-sizing-with-kubecost/</guid><description>Dynamic Kubernetes request right sizing with Kubecost What are container requests? Kubecost savings insights Customizing recommendations Acting on Kubecost recommendations One-time resizing Scheduled right sizing Automating resizing with Helm Conclusion About the authors This post was co-written with Kai Wombacher, Founding Product Manager at Kubecost. In this post we show you how to use the Kubecost Amazon Elastic Kubernetes Service (Amazon EKS) add-on to lower infrastructure costs and boost Kubernetes efficiency. The Container Request Right Sizing feature allows you to find how container requests are configured, look for inefficiencies, and fix them either manually or through automated remediation. Specifically, we cover how to review Kubecost’s right sizing recommendations and take action on them using one-time updates or scheduled, automated resizing within your Amazon EKS environment to continuously optimize resource usage. Over-requested containers are one of the most common sources of cloud resource waste in Kubernetes environments. Without visibility and automation, development teams can request far more resources than their applications use, which leads to overprovisioned nodes and higher costs. In Kubernetes, a container request is a declared amount of CPU and memory that a workload needs. It plays a crucial role in how workloads are scheduled and how nodes are used. When a container specifies a CPU or memory request, the scheduler looks for a node that has at least that amount of unallocated capacity. When a pod is placed on a node, the requested resources are essentially reserved, regardless of whether the container uses them in practice. Although this reservation behavior makes sure that workloads have access to the resources they need, it can also lead to inefficient resource usage if requests are set too high. For example, if a container requests 1 CPU but only uses 200 millicores (0.2 CPU), then that added 0.8 CPU goes unused, yet the node capacity is still reserved and charged for.</description></item><item><title>Unlocking next-generation AI performance with Dynamic Resource Allocation on Amazon EKS and Amazon EC2 P6e-GB200</title><link>https://kubermates.org/docs/2025-09-02-unlocking-next-generation-ai-performance-with-dynamic-resource-allocation-on-ama/</link><pubDate>Tue, 02 Sep 2025 23:33:59 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-02-unlocking-next-generation-ai-performance-with-dynamic-resource-allocation-on-ama/</guid><description>Unlocking next-generation AI performance with Dynamic Resource Allocation on Amazon EKS and Amazon EC2 P6e-GB200 The power behind P6e-GB200: NVIDIA GB200 Grace Blackwell architecture Understanding EC2 P6e-GB200 UltraServer architecture Integrating P6e-GB200 UltraServers with Amazon EKS The challenge: running distributed AI workloads on Kubernetes The solution: Kubernetes DRA and IMEX How DRA solves traditional GPU allocation problems Topology-aware scheduling and memory coherence Workload scheduling flow with DRA How to use p6e-GB200 with Kubernetes DRA with Amazon EKS Prerequisites Step 1: Reserve P6e-GB200 UltraServer capacity Step 2: Create the EKS cluster configuration file Step 3: Deploy the EKS cluster Step 4: Deploy the NVIDIA GPU Operator Step 5: Install the NVIDIA DRA Driver Step 6: Verify DRA resources Validating IMEX channel allocation Apply and validate Multi-node IMEX communication in action Conclusion About the authors The rapid evolution of agentic AI and large language models (LLMs), particularly reasoning models, has created unprecedented demand for computational resources. Today’s most advanced AI models span hundreds of billions to trillions of parameters and necessitate massive computational power, extensive memory footprints, and ultra-fast interconnects to function efficiently. Organizations developing applications for natural language processing, scientific simulations, 3D content generation, and multimodal inference need infrastructure that can scale from today’s billion-parameter models to tomorrow’s trillion-parameter frontiers while maintaining performance. In this post, we explore how the new Amazon Elastic Compute Cloud (Amazon EC2) P6e-GB200 UltraServers are transforming distributed AI workload through seamless Kubernetes integration. Amazon Web Services (AWS) introduced the EC2 P6e-GB200 UltraServers to meet the growing demand for large-scale AI model training and inference. They represent a significant architectural breakthrough for distributed AI workloads. Furthermore, the EC2 P6e-GB200 UltraServer launch includes support for Amazon Elastic Kubernetes Service (Amazon EKS) , providing a Kubernetes-native environment for deploying and scaling from hundreds-of-billions to trillion-parameter models as the AI landscape continues to evolve. At the heart of EC2 P6e-GB200 UltraServers is the NVIDIA GB200 Grace Blackwell Superchip , which integrates two NVIDIA Blackwell GPUs with a NVIDIA Grace CPU. Furthermore, it provides NVLink-Chip-to-Chip (C2C) connection between these components, delivering 900 GB/s of bidirectional bandwidth, which is substantially faster than traditional PCIe interfaces. When deployed at rack scale, EC2 P6e-GB200 UltraServers participate in NVIDIA’s GB200 NVL72 architecture , creating memory-coherent domains of up to 72 GPUs. Fifth-generation NVLink technology enables GPU-to-GPU communication across discrete servers within the same domain at up to 1.8 TB/s per GPU. Critical to this performance is Elastic Fabric Adapter (EFAv4) networking, which delivers up to 28.8 Tbps of total network bandwidth per UltraServer.</description></item><item><title>Introducing Seekable OCI Parallel Pull mode for Amazon EKS</title><link>https://kubermates.org/docs/2025-08-27-introducing-seekable-oci-parallel-pull-mode-for-amazon-eks/</link><pubDate>Wed, 27 Aug 2025 19:13:44 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-27-introducing-seekable-oci-parallel-pull-mode-for-amazon-eks/</guid><description>Introducing Seekable OCI Parallel Pull mode for Amazon EKS Introducing Parallel Pull mode for the SOCI snapshotter Understanding image pulls SOCI Parallel Pull details Performance consideration SOCI Parallel Pull in action Tuning configuration Benchmark Getting started with SOCI Parallel Pull Mode About the authors Containerization has transformed how customers build and deploy modern cloud native applications, offering unparalleled benefits in portability, scalability, and operational efficiency. Containers provide integrated dependency management and enable a standard distribution and deployment model for any workload. With Amazon Elastic Kubernetes Service (Amazon EKS), Kubernetes has emerged as a go-to solution for customers running large-scale containerized workloads that need to efficiently scale to meet evolving needs. However, one persistent challenge continues to impact specific deployment and scaling aspects of Kubernetes workload operations. Container image pulls, particularly when working with large and complex container images, can directly impact the responsiveness and agility of your systems. With the growth of AI/ML workloads, where we see particularly large images, this directly impacts operations as images may take several minutes to pull and prepare. In our recent Under the Hood post for EKS Ultra Scale Clusters, we briefly touched on our evolving solution for this problem, Seekable OCI (SOCI) Parallel Pull. In this post, we’ll explain how container image pulls work and how they impact deployment and scaling operations, we’ll dive deeper into how SOCI parallel pull works, and finally show how it can help you improve image pull performance with your workloads on Amazon EKS. As average container image sizes have grown in recent years, container startup performance has become a critical element of modern cloud native system performance. Image pull and preparation can account for more than 75% of total startup time for new and scaling workloads. This challenge is particularly acute with the rise of AI/ML workloads on Amazon EKS. These workloads have driven significant growth in container image sizes, where images are commonly tens of gigabytes in size.</description></item><item><title>Refresh cluster insights</title><link>https://kubermates.org/releases/2025-08-27-refresh-cluster-insights/</link><pubDate>Wed, 27 Aug 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-27-refresh-cluster-insights/</guid><description>View cluster insights View configuration insights (Console) View upgrade insights (Console) View cluster insights (AWS CLI) Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Amazon EKS provides two types of insights: Configuration insights and Upgrade insights. Configuration insights identify misconfigurations in your EKS Hybrid Nodes setup that could impair functionality of your cluster or workloads. Upgrade insights identify issues that could impact your ability to upgrade to new versions of Kubernetes. To see the list of insight checks performed and any relevant issues that Amazon EKS has identified, you can call the look in the AWS Management Console, the AWS CLI, AWS SDKs, and Amazon EKS ListInsights API operation. ListInsights Open the Amazon EKS console. Open the Amazon EKS console. From the cluster list, choose the name of the Amazon EKS cluster for which you want to see the insights. Choose Monitor cluster. Choose the Cluster health tab. In the Configuration insights table, you will see the following columns: Name â The check that was performed by Amazon EKS against the cluster. Insight status â An insight with a status of Error means that there is a misconfiguration that is likely impacting cluster functionality.</description></item><item><title>AWS managed policy updates</title><link>https://kubermates.org/releases/2025-08-26-aws-managed-policy-updates/</link><pubDate>Tue, 26 Aug 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-26-aws-managed-policy-updates/</guid><description>AWS managed policies for Amazon Elastic Kubernetes Service AWS managed policy: AmazonEKS_CNI_Policy AWS managed policy: AmazonEKSClusterPolicy AWS managed policy: AmazonEKSDashboardConsoleReadOnly AWS managed policy: AmazonEKSFargatePodExecutionRolePolicy AWS managed policy: AmazonEKSForFargateServiceRolePolicy AWS managed policy: AmazonEKSComputePolicy Permissions details AWS managed policy: AmazonEKSNetworkingPolicy Permissions details AWS managed policy: AmazonEKSBlockStoragePolicy Permissions details AWS managed policy: AmazonEKSLoadBalancingPolicy Permissions details AWS managed policy: AmazonEKSServicePolicy AWS managed policy: AmazonEKSServiceRolePolicy AWS managed policy: AmazonEKSVPCResourceController AWS managed policy: AmazonEKSWorkerNodePolicy AWS managed policy: AmazonEKSWorkerNodeMinimalPolicy AWS managed policy: AWSServiceRoleForAmazonEKSNodegroup AWS managed policy: AmazonEKSDashboardServiceRolePolicy AWS managed policy: AmazonEBSCSIDriverPolicy AWS managed policy: AmazonEFSCSIDriverPolicy AWS managed policy: AmazonEKSLocalOutpostClusterPolicy AWS managed policy: AmazonEKSLocalOutpostServiceRolePolicy Amazon EKS updates to AWS managed policies Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. An AWS managed policy is a standalone policy that is created and administered by AWS. AWS managed policies are designed to provide permissions for many common use cases so that you can start assigning permissions to users, groups, and roles. Keep in mind that AWS managed policies might not grant least-privilege permissions for your specific use cases because theyâre available for all AWS customers to use. We recommend that you reduce permissions further by defining customer managed policies that are specific to your use cases. You cannot change the permissions defined in AWS managed policies. If AWS updates the permissions defined in an AWS managed policy, the update affects all principal identities (users, groups, and roles) that the policy is attached to. AWS is most likely to update an AWS managed policy when a new AWS service is launched or new API operations become available for existing services. For more information, see AWS managed policies in the IAM User Guide. You can attach the AmazonEKS_CNI_Policy to your IAM entities. Before you create an Amazon EC2 node group, this policy must be attached to either the node IAM role , or to an IAM role thatâs used specifically by the Amazon VPC CNI plugin for Kubernetes. This is so that it can perform actions on your behalf.</description></item><item><title>Migrate to Amazon EKS: Data plane cost modeling with Karpenter and KWOK</title><link>https://kubermates.org/docs/2025-08-21-migrate-to-amazon-eks-data-plane-cost-modeling-with-karpenter-and-kwok/</link><pubDate>Thu, 21 Aug 2025 18:18:42 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-21-migrate-to-amazon-eks-data-plane-cost-modeling-with-karpenter-and-kwok/</guid><description>Migrate to Amazon EKS: Data plane cost modeling with Karpenter and KWOK Solution overview Solution walkthrough Prerequisites Step 1: Create the source EKS cluster Step 2: Deploy an example workload Step 3: Extract cluster configuration with Velero Step 4: Create the destination EKS cluster Step 5: Deploy Karpenter with the KWOK provider Step 6: Restore the backup Clean up Conclusion About the authors When migrating Kubernetes clusters to Amazon Elastic Kubernetes Service (Amazon EKS) , organizations typically follow three phases: assessment, mobilize, and migrate and modernize. The assessment phase involves evaluating technical feasibility for Amazon EKS workloads, analyzing current Kubernetes environments, identifying compatibility issues, estimating costs, and determining timelines with business impact considerations. During the mobilize phase, organizations create detailed migration plans, establish EKS environments with proper networking and security, train teams, and develop testing procedures. The final migrate and modernize phase involves transferring applications and data, validating functionality, implementing cloud-centered features, optimizing resources and costs, and enhancing observability to fully use AWS capabilities. One of the most significant challenges organizations face during the process is cost estimation, which happens in the assessment phase. Karpenter is an open source Kubernetes node autoscaler that efficiently provisions just-in-time compute resources to match workload demands. Unlike traditional autoscalers, Karpenter directly integrates with cloud providers to make intelligent, real-time decisions about instance types, availability zones, and capacity options. It evaluates pod requirements and constraints to select optimal instances, considering factors such as CPU, memory, price, and availability. Karpenter can consolidate workloads for cost efficiency and rapidly scale from zero to handle sudden demand spikes. It supports both spot and on-demand instances, and automatically terminates nodes when they’re no longer needed, optimizing cluster resource utilization and reducing cloud costs. Karpenter uses the concept of Providers to interact with different infrastructure platforms for provisioning and managing compute resources. KWOK (Kubernetes WithOut Kubelet) is a toolkit that simulates data plane nodes without allocating actual infrastructure, and can be used as a provider to create lightweight testing environments that enable developers to validate provisioning decisions, try various (virtual) instance types, and debug scaling behaviors.</description></item><item><title>Cross-service confused deputy prevention</title><link>https://kubermates.org/releases/2025-08-19-cross-service-confused-deputy-prevention/</link><pubDate>Tue, 19 Aug 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-19-cross-service-confused-deputy-prevention/</guid><description>Cross-service confused deputy prevention in Amazon EKS Amazon EKS cluster role cross-service confused deputy prevention Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. The confused deputy problem is a security issue where an entity that doesnât have permission to perform an action can coerce a more-privileged entity to perform the action. In AWS, cross-service impersonation can result in the confused deputy problem. Cross-service impersonation can occur when one service (the calling service ) calls another service (the called service ). The calling service can be manipulated to use its permissions to act on another customerâs resources in a way it should not otherwise have permission to access. To prevent this, AWS provides tools that help you protect your data for all services with service principals that have been given access to resources in your account. We recommend using the aws:SourceArn , aws:SourceAccount global condition context keys in resource policies to limit the permissions that Amazon Elastic Kubernetes Service (Amazon EKS) gives another service to the resource. aws:SourceArn aws:SourceAccount aws:SourceArn Use aws:SourceArn to associate only one resource with cross-service access. aws:SourceArn aws:SourceAccount Use aws:SourceAccount to let any resource in that account be associated with the cross-service use. aws:SourceAccount The most effective way to protect against the confused deputy problem is to use the aws:SourceArn global condition context key with the full ARN of the resource. If you donât know the full ARN of the resource or if you are specifying multiple resources, use the aws:SourceArn global context condition key with wildcard characters (&lt;em&gt;) for the unknown portions of the ARN. For example, arn:aws:&lt;servicename&gt;:&lt;/em&gt;:&amp;lt;123456789012&amp;gt;:*.</description></item><item><title>Canary delivery with Argo Rollout and Amazon VPC Lattice for Amazon EKS</title><link>https://kubermates.org/docs/2025-08-12-canary-delivery-with-argo-rollout-and-amazon-vpc-lattice-for-amazon-eks/</link><pubDate>Tue, 12 Aug 2025 23:55:07 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-12-canary-delivery-with-argo-rollout-and-amazon-vpc-lattice-for-amazon-eks/</guid><description>Canary delivery with Argo Rollout and Amazon VPC Lattice for Amazon EKS Solution overview Walkthrough: progressive delivery with VPC Lattice and Argo Rollouts Integration highlights Conclusion About the authors Modern application delivery demands agility and reliability, where updates are rolled out progressively while making sure of the minimal impact on end users. Progressive delivery strategies, such as canary deployments, allow organizations to release new features by shifting traffic incrementally between old and new versions of a service. This allows organizations to first release features to a small subset of users, monitor system behavior and performance in real time, and automatically roll back if anomalies are detected. This is particularly valuable in modern microservices environments running on platforms such as Amazon Elastic Kubernetes Service (Amazon EKS) , where service meshes and traffic routers provide the necessary infrastructure for fine-grained control over traffic routing. This post explores an architectural approach to implementing progressive delivery using Amazon VPC Lattice, Amazon CloudWatch Synthetics , and Argo Rollouts. The solution uses VPC Lattice for enhanced traffic control across microservices, CloudWatch Synthetics for real-time health and validation monitoring, and Argo Rollouts for orchestrating canary updates. The content in this post addresses readers who are already familiar with networking constructs on Amazon Web Services (AWS), such as Amazon Virtual Private Cloud (Amazon VPC) , CloudWatch Synthetics and Amazon EKS. Instead of defining these services, we focus on their capabilities and integration with VPC Lattice. We also build upon your existing understanding of VPC Lattice concepts and Argo Rollouts. For more background on Amazon VPC Lattice, we recommend that you review the post, Build secure multi-account multi-VPC connectivity for your applications with Amazon VPC Lattice , and the collection of resources in the VPC Lattice Getting started guide. The architecture integrates multiple AWS services and Kubernetes-native components, providing a comprehensive solution for progressive delivery: Amazon EKS : A fully managed Kubernetes service to host microservices. VPC Lattice : A service networking layer that enables consistent traffic routing, authentication, and observability across services.</description></item><item><title>Simplify network connectivity using Tailscale with Amazon EKS Hybrid Nodes</title><link>https://kubermates.org/docs/2025-08-06-simplify-network-connectivity-using-tailscale-with-amazon-eks-hybrid-nodes/</link><pubDate>Wed, 06 Aug 2025 22:12:21 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-06-simplify-network-connectivity-using-tailscale-with-amazon-eks-hybrid-nodes/</guid><description>Simplify network connectivity using Tailscale with Amazon EKS Hybrid Nodes Prerequisites Cost and security Architecture overview Step 1: Define a remote pod, and node network Step 2: Install Tailscale on your EKS Hybrid Node Step 3: Add a Tailscale subnet router inside your Amazon VPC Step 4: Update subnet routes in the Amazon VPC Step 5: Verify connectivity between the two Tailscale devices AWS CloudShell connectivity test: Step 6: Instructions for EKS Hybrid Nodes Cleaning up Conclusion Next steps About the authors This post was co-authored with Lee Briggs, Director of Solutions Engineering at Tailscale. In this post, we guide you through integrating Tailscale with your Amazon Elastic Kubernetes Service (EKS) Hybrid Nodes environment. Amazon EKS Hybrid Nodes is a feature of Amazon EKS that enables you to streamline your Kubernetes management by connecting on-premises and edge infrastructure to an EKS cluster running in Amazon Web Services (AWS). This unified approach allows AWS to manage the Kubernetes control plane in the cloud while you maintain your hybrid nodes in on-premises or edge locations. We demonstrate how to configure a remote pod network and node address space. Install Tailscale on your hybrid nodes, set up a subnet router within your Amazon Virtual Private Cloud (Amazon VPC) , and update your AWS routes accordingly. This integration provides direct, encrypted connections that streamline the network architecture needed for EKS Hybrid Nodes. Although EKS Hybrid Nodes streamlines the Kubernetes management challenge, network connectivity between your on-premises infrastructure and AWS remains a critical requirement. Tailscale can help streamline this network connectivity between your EKS Hybrid Nodes data plane and Amazon EKS Kubernetes control plane. Unlike traditional VPNs, which tunnel all network traffic through a central gateway server, Tailscale creates a peer-to-peer mesh network (known as a tailnet ). It enables encrypted point-to-point connections using the open source WireGuard protocol, connecting devices and services across different networks with enhanced security features. However, you can still use Tailscale like a traditional VPN.</description></item><item><title>Amazon EKS platform version update</title><link>https://kubermates.org/releases/2025-07-30-amazon-eks-platform-version-update/</link><pubDate>Wed, 30 Jul 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-30-amazon-eks-platform-version-update/</guid><description>View Amazon EKS platform versions for each Kubernetes version Kubernetes version 1.33 Kubernetes version 1.32 Kubernetes version 1.31 Kubernetes version 1.30 Kubernetes version 1.29 Kubernetes version 1.28 Get current platform version Change platform version Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Amazon EKS platform versions represent the capabilities of the Amazon EKS cluster control plane, such as which Kubernetes API server flags are enabled, as well as the current Kubernetes patch version. Each Kubernetes minor version has one or more associated Amazon EKS platform versions. The platform versions for different Kubernetes minor versions are independent. You can retrieve your clusterâs current platform version using the AWS CLI or AWS Management Console. If you have a local cluster on AWS Outposts, see Learn Kubernetes and Amazon EKS platform versions for AWS Outposts instead of this topic. When a new Kubernetes minor version is available in Amazon EKS, such as 1.33, the initial Amazon EKS platform version for that Kubernetes minor version starts at eks. 1. However, Amazon EKS releases new platform versions periodically to enable new Kubernetes control plane settings and to provide security fixes. eks. 1 When new Amazon EKS platform versions become available for a minor version: The Amazon EKS platform version number is incremented ( eks. &amp;lt;n+1&amp;gt; ).</description></item><item><title>Scaling beyond IPv4: integrating IPv6 Amazon EKS clusters into existing Istio Service Mesh</title><link>https://kubermates.org/docs/2025-07-22-scaling-beyond-ipv4-integrating-ipv6-amazon-eks-clusters-into-existing-istio-ser/</link><pubDate>Tue, 22 Jul 2025 18:54:25 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-22-scaling-beyond-ipv4-integrating-ipv6-amazon-eks-clusters-into-existing-istio-ser/</guid><description>Scaling beyond IPv4: integrating IPv6 Amazon EKS clusters into existing Istio Service Mesh Amazon EKS IPv6 interoperability with IPv4 in Istio Service Mesh Solution overview Istio Multi-Primary Multicluster deployment model on a single network Istio Multi-Primary Multicluster deployment model on multi-network Walkthrough Initial setup Conclusion About the authors Organizations are increasingly adopting IPv6 for their Amazon Elastic Kubernetes Service (Amazon EKS) deployments, driven by three key factors: depletion of private IPv4 addresses, the need to streamline or eliminate overlay networks, and improved network security requirements on Amazon Web Services (AWS). In IPv6-enabled EKS clusters, each pod receives a unique IPv6 address from the Amazon Virtual Private Cloud (Amazon VPC) IPv6 range, with seamless compatibility facilitated by the Amazon EKS VPC Container Network Interface (CNI). This solution effectively addresses two major IPv4 limitations: the scarcity of private addresses and the security vulnerabilities created by overlapping IPv4 spaces that need Network Address Translation (NAT) at the node level. When transitioning to IPv6, you likely need to run both IPv4 and IPv6 EKS clusters simultaneously. This is particularly important for organizations using Istio Service Mesh with Amazon EKS, because IPv6 clusters must integrate with the existing Service Mesh and work smoothly alongside IPv4 clusters. To streamline this transition, you can configure your Istio Service Mesh to support both your current IPv4 EKS clusters and your new IPv6 EKS clusters. If Istio Service Mesh isn’t part of your infrastructure, then we suggest exploring Amazon VPC Lattice as an alternative solution to speed up your IPv6 implementation on AWS. This post provides a step-by-step guide for combining IPv6-enabled EKS clusters with your existing Istio Service Mesh and IPv4 workloads, enabling a graceful transition to IPv6 on AWS. This guide covers detailed instructions for enabling communication between IPv6 and IPv4 EKS clusters, along with recommended practices for implementing IPv6 across both single and multiple VPC configurations. The functionality of Amazon EKS IPv6 builds on the native dual-stack capabilities of VPC. When you enable IPv6 in your VPC, it receives both IPv4 prefixes and a /56 IPv6 prefix. This IPv6 prefix can come from three sources: Amazon’s Global Unicast Address (GUA) space, your own IPv6 range (BYOIPv6), or a Unique Local Address (ULA) space.</description></item><item><title>Deep dive into cluster networking for Amazon EKS Hybrid Nodes</title><link>https://kubermates.org/docs/2025-07-21-deep-dive-into-cluster-networking-for-amazon-eks-hybrid-nodes/</link><pubDate>Mon, 21 Jul 2025 22:22:52 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-21-deep-dive-into-cluster-networking-for-amazon-eks-hybrid-nodes/</guid><description>Deep dive into cluster networking for Amazon EKS Hybrid Nodes Architecture overview CNI considerations Load balancing considerations Prerequisites Walkthrough BGP routing (Cilium example) Static routing (Calico example) On-premises load balancer (MetalLB example) External load balancer (AWS Load Balancer Controller example) Cleaning up Conclusion About the author Amazon Elastic Kubernetes Service ( Amazon EKS ) Hybrid Nodes enables organizations to integrate their existing on-premises and edge computing infrastructure into EKS clusters as remote nodes. EKS Hybrid Nodes provides you with the flexibility to run your containerized applications wherever needed, while maintaining standardized Kubernetes management practices and addressing latency, compliance, and data residency needs. EKS Hybrid Nodes accelerates infrastructure modernization by repurposing existing hardware investments. Organizations can harness the elastic scalability, high availability, and fully managed advantages of Amazon EKS, while making sure of operational consistency through unified workflows and toolsets across hybrid environments. One of the key aspects of the EKS Hybrid Nodes solution is the hybrid network architecture between the cloud-based Amazon EKS control plane and your on-premises nodes. This post dives deep into the cluster networking configurations, guiding you through the process of integrating an EKS cluster with hybrid nodes in your existing infrastructure. In this walkthrough, we set up different Container Network Interface (CNI) options and load balancing solutions on EKS Hybrid Nodes to meet your networking requirements. EKS Hybrid Nodes needs private network connectivity between the cloud-hosted Amazon EKS control plane and the hybrid nodes running in your on-premises environment. This connectivity can be established using either Amazon Web Services (AWS) Direct Connect or AWS Site-to-Site VPN , through an AWS Transit Gateway or the Virtual Private Gateway into your Amazon Virtual Private Cloud (Amazon VPC). For an optimal experience, AWS recommends reliable network connectivity with at least 100 Mbps bandwidth, and a maximum of 200ms round-trip latency, for hybrid nodes connecting to the AWS Region. This is general guidance rather than a strict requirement, and specific bandwidth and latency requirements may differ based on the quantity of hybrid nodes and your application’s unique characteristics. The node and pod Classless Inter-Domain Routing (CIDR) blocks for your hybrid nodes and container workloads must be within the IPv4 RFC-1918 ranges.</description></item><item><title>Under the hood: Amazon EKS ultra scale clusters</title><link>https://kubermates.org/docs/2025-07-16-under-the-hood-amazon-eks-ultra-scale-clusters/</link><pubDate>Wed, 16 Jul 2025 00:14:37 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-16-under-the-hood-amazon-eks-ultra-scale-clusters/</guid><description>This post was co-authored by Shyam Jeedigunta, Principal Engineer, Amazon EKS; Apoorva Kulkarni, Sr. Specialist Solutions Architect, Containers and Raghav Tripathi, Sr. Software Dev Manager, Amazon EKS. Today, Amazon Elastic Kubernetes Service (Amazon EKS) announced support for clusters with up to 100,000 nodes. With Amazon EC2’s new generation accelerated computing instance types, this translates to 1. 6 million AWS Trainium chips or 800,000 NVIDIA GPUs in a single Kubernetes cluster. This unlocks ultra scale artificial intelligence (AI) and machine leaning (ML) workloads such as state-of-the-art model training, fine-tuning and agentic inference. Besides customers directly consuming Amazon EKS today, these improvements also extend to other AI/ML services like Amazon SageMaker HyperPod with EKS that leverage EKS as their compute layer, advancing AWS’s overall ultra scale computing capabilities. Our customers have made it clear that containerization of training jobs and operators such as Kubeflow, the ability to streamline resource provisioning and lifecycle through projects like Karpenter, support for pluggable scheduling strategies, and access to a vast ecosystem of cloud-native tools is critical for their success in the AI/ML domain. Kubernetes has emerged as a key enabler here due to its powerful and extensible API model along with robust container orchestration capabilities, allowing accelerated workloads to scale quickly and run reliably. Through multiple technical innovations, architectural improvements and open-source collaboration, Amazon EKS has built the next generation of its cluster control plane and data plane for ultra scale, with full Kubernetes conformance. At AWS, we recommend customers running general-purpose applications with low coupling and horizontal scalability to follow a cell-based architecture as the strategy to sustain growth.</description></item><item><title>Amazon EKS enables ultra scale AI/ML workloads with support for 100K nodes per cluster</title><link>https://kubermates.org/docs/2025-07-16-amazon-eks-enables-ultra-scale-ai-ml-workloads-with-support-for-100k-nodes-per-c/</link><pubDate>Wed, 16 Jul 2025 00:14:26 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-16-amazon-eks-enables-ultra-scale-ai-ml-workloads-with-support-for-100k-nodes-per-c/</guid><description>&lt;p&gt;Open the original post ↗ &lt;a href="https://aws.amazon.com/blogs/containers/amazon-eks-enables-ultra-scale-ai-ml-workloads-with-support-for-100k-nodes-per-cluster/"&gt;https://aws.amazon.com/blogs/containers/amazon-eks-enables-ultra-scale-ai-ml-workloads-with-support-for-100k-nodes-per-cluster/&lt;/a&gt;&lt;/p&gt;</description></item><item><title>VPC CNI Multi-NIC feature for multi-homed pods</title><link>https://kubermates.org/releases/2025-07-15-vpc-cni-multi-nic-feature-for-multi-homed-pods/</link><pubDate>Tue, 15 Jul 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-15-vpc-cni-multi-nic-feature-for-multi-homed-pods/</guid><description>Attach multiple network interfaces to Pods Background Considerations IPv6 Considerations Usage Frequently Asked Questions 1. What is a network interface card (NIC)? 2. What is a multi-homed pod? 3. Why should I use this feature? 4. How do I use this feature? 5. How do I configure my workloads to use multiple NICs on a supported worker node? 6. What network interface adapters are supported with this feature? 7. Can I see if a node in my cluster has ENA support? 8. Can I see the different IP addresses associated with a pod? 9. Can I control the number of network interfaces for my pods? 10. Can I configure my pods to use a specific NIC? 11. Does this feature work with the other VPC CNI networking features? 12.</description></item><item><title>VPC CNI troubleshooting content update</title><link>https://kubermates.org/releases/2025-06-30-vpc-cni-troubleshooting-content-update/</link><pubDate>Mon, 30 Jun 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-06-30-vpc-cni-troubleshooting-content-update/</guid><description>Troubleshooting Kubernetes network policies For Amazon EKS New policyendpoints CRD and permissions Network policy logs Amazon EKS add-on Self-managed add-on Send network policy logs to Amazon CloudWatch Logs Included eBPF SDK Known issues and solutions Network policy logs generated despite enable-policy-event-logs set to false Network policy map cleanup issues Network policies arenât applied Pods donât return to default deny state after policy deletion in strict mode Security Groups for Pods startup latency FailedScheduling due to insufficient vpc. amazonaws. com/pod-eni IPAM connectivity issues and segmentation faults Failed to find device by name error CVE vulnerabilities in Multus CNI image Flow Info DENY verdicts in logs Pod-to-pod communication issues after migrating from Calico Network policy agent doesnât support standalone pods Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. This is the troubleshooting guide for network policy feature of the Amazon VPC CNI. This guide covers: Install information, CRD and RBAC permissions New policyendpoints CRD and permissions Install information, CRD and RBAC permissions New policyendpoints CRD and permissions Logs to examine when diagnosing network policy problems Network policy logs Logs to examine when diagnosing network policy problems Network policy logs Running the eBPF SDK collection of tools to troubleshoot Running the eBPF SDK collection of tools to troubleshoot Known issues and solutions Known issues and solutions Known issues and solutions Known issues and solutions Note that network policies are only applied to pods that are made by Kubernetes Deployments. For more limitations of the network policies in the VPC CNI, see Considerations. You can troubleshoot and investigate network connections that use network policies by reading the Network policy logs and by running tools from the eBPF SDK. policyendpoints CRD: policyendpoints. networking. k8s. aws CRD: policyendpoints. networking.</description></item><item><title>AWS managed policy updates</title><link>https://kubermates.org/releases/2025-06-26-aws-managed-policy-updates/</link><pubDate>Thu, 26 Jun 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-06-26-aws-managed-policy-updates/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. An AWS managed policy is a standalone policy that is created and administered by AWS. AWS managed policies are designed to provide permissions for many common use cases so that you can start assigning permissions to users, groups, and roles. Keep in mind that AWS managed policies might not grant least-privilege permissions for your specific use cases because theyâre available for all AWS customers to use. We recommend that you reduce permissions further by defining customer managed policies that are specific to your use cases. You cannot change the permissions defined in AWS managed policies. If AWS updates the permissions defined in an AWS managed policy, the update affects all principal identities (users, groups, and roles) that the policy is attached to. AWS is most likely to update an AWS managed policy when a new AWS service is launched or new API operations become available for existing services. For more information, see AWS managed policies in the IAM User Guide. You can attach the AmazonEKS_CNI_Policy to your IAM entities. Before you create an Amazon EC2 node group, this policy must be attached to either the node IAM role , or to an IAM role thatâs used specifically by the Amazon VPC CNI plugin for Kubernetes. This is so that it can perform actions on your behalf.</description></item><item><title>Amazon EKS Auto Mode update to NodeClass</title><link>https://kubermates.org/releases/2025-06-13-amazon-eks-auto-mode-update-to-nodeclass/</link><pubDate>Fri, 13 Jun 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-06-13-amazon-eks-auto-mode-update-to-nodeclass/</guid><description>Create a Node Class for Amazon EKS Create a Node Class Basic Node Class Example Create node class access entry Create access entry with CLI Create access entry with CloudFormation Node Class Specification Considerations Subnet selection for Pods Use cases Example configuration Considerations for subnet selectors for Pods Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Amazon EKS Node Classes are templates that offer granular control over the configuration of your EKS Auto Mode managed nodes. A Node Class defines infrastructure-level settings that apply to groups of nodes in your EKS cluster, including network configuration, storage settings, and resource tagging. This topic explains how to create and configure a Node Class to meet your specific operational requirements. When you need to customize how EKS Auto Mode provisions and configures EC2 instances beyond the default settings, creating a Node Class gives you precise control over critical infrastructure parameters. For example, you can specify private subnet placement for enhanced security, configure instance ephemeral storage for performance-sensitive workloads, or apply custom tagging for cost allocation. To create a NodeClass , follow these steps: NodeClass Create a YAML file (for example, nodeclass. yaml ) with your Node Class configuration Create a YAML file (for example, nodeclass. yaml ) with your Node Class configuration nodeclass. yaml Apply the configuration to your cluster using kubectl Apply the configuration to your cluster using kubectl kubectl Reference the Node Class in your Node Pool configuration. For more information, see Create a Node Pool for EKS Auto Mode. Reference the Node Class in your Node Pool configuration.</description></item><item><title>Target secondary and cross-account roles with EKS Pod Identities</title><link>https://kubermates.org/releases/2025-06-11-target-secondary-and-cross-account-roles-with-eks-pod-identities/</link><pubDate>Wed, 11 Jun 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-06-11-target-secondary-and-cross-account-roles-with-eks-pod-identities/</guid><description>Access AWS Resources using EKS Pod Identity Target IAM Roles Prerequisites How It Works Caching considerations Step 1: Create and associate a Target IAM Role Create the Target IAM Role Update the Target IAM Role trust policy Update the permission policy for EKS Pod Identity role Step 2: Associate the Target IAM Role to a Kubernetes service account Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. When running applications on Amazon Elastic Kubernetes Service (Amazon EKS), you might need to access AWS resources that exist in different AWS accounts. This guide shows you how to set up cross account access using EKS Pod Identity, which enables your Kubernetes pods to access other AWS resources using target roles. Before you begin, ensure you have completed the following steps: Set up the Amazon EKS Pod Identity Agent Set up the Amazon EKS Pod Identity Agent Create an EKS Pod Identity role Create an EKS Pod Identity role Pod Identity enables applications in your EKS cluster to access AWS resources across accounts through a process called role chaining. When creating a Pod Identity association, you can provide two IAM roles: an EKS Pod Identity role in the same account as your EKS cluster and a Target IAM Role from the account containing your AWS resources you wish to access, (like S3 buckets or RDS Databases). The EKS Pod Identity role must be in your EKS clusterâs account due to IAM PassRole requirements, while the Target IAM Role can be in any AWS account. PassRole enables an AWS entity to delegate role assumption to another service. EKS Pod Identity uses PassRole to connect a role to a Kubernetes service account, requiring both the role and the identity passing it to be in the same AWS account as the EKS cluster. When your application pod needs to access AWS resources, it requests credentials from Pod Identity. Pod Identity then automatically performs two role assumptions in sequence: first assuming the EKS Pod Identity role , then using those credentials to assume the Target IAM Role. This process provides your pod with temporary credentials that have the permissions defined in the target role, allowing secure access to resources in other AWS accounts. Due to caching mechanisms, updates to an IAM role in an existing Pod Identity association may not take effect immediately in the pods running on your EKS cluster.</description></item><item><title>Amazon EKS AWS Region expansion</title><link>https://kubermates.org/releases/2025-06-06-amazon-eks-aws-region-expansion/</link><pubDate>Fri, 06 Jun 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-06-06-amazon-eks-aws-region-expansion/</guid><description>Document history Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. The following table describes some of the major updates and new features for the Amazon EKS User Guide. August 27, 2025 AWS managed policy updates Added permission to AmazonEKSServiceRolePolicy. This role can attach new access policy AmazonEKSEventPolicy. Restricted permissions for ec2:DeleteLaunchTemplate and ec2:TerminateInstances. AmazonEKSServiceRolePolicy AmazonEKSEventPolicy ec2:DeleteLaunchTemplate ec2:TerminateInstances August 26, 2025 Cross-service confused deputy prevention Added a topic with an example trust policy that you can apply for Cross-service confused deputy prevention. Amazon EKS accepts the aws:SourceArn and aws:SourceAccount conditions in the trust policy of an EKS cluster role. aws:SourceArn aws:SourceAccount August 19, 2025 Amazon EKS platform version update This is a new platform version with security fixes and enhancements. This includes new patch versions of Kubernetes 1.33.2 , 1.32.6 , 1.31.10 , and 1.30.14. 1.33.2 1.32.6 1.31.10 1.30.14 July 30, 2025 VPC CNI Multi-NIC feature for multi-homed pods Amazon EKS adds multi-homed pods to the VPC CNI. Now you can configure a workload and the VPC CNI assigned IP addresses from every NIC on the EC2 instance to each pod. The application can make concurrent connections to use the bandwidth from each NIC.</description></item><item><title>IPv6 access control for dual-stack public endpoints for new IPv6 clusters</title><link>https://kubermates.org/releases/2025-06-05-ipv6-access-control-for-dual-stack-public-endpoints-for-new-ipv6-clusters/</link><pubDate>Thu, 05 Jun 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-06-05-ipv6-access-control-for-dual-stack-public-endpoints-for-new-ipv6-clusters/</guid><description>Cluster API server endpoint IPv6 cluster endpoint format IPv4 cluster endpoint format Cluster private endpoint Modifying cluster endpoint access Accessing a private only API server Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. This topic helps you to enable private access for your Amazon EKS clusterâs Kubernetes API server endpoint and limit, or completely disable, public access from the internet. When you create a new cluster, Amazon EKS creates an endpoint for the managed Kubernetes API server that you use to communicate with your cluster (using Kubernetes management tools such as kubectl ). By default, this API server endpoint is public to the internet, and access to the API server is secured using a combination of AWS Identity and Access Management (IAM) and native Kubernetes Role Based Access Control (RBAC). This endpoint is known as the cluster public endpoint. Also there is a cluster private endpoint. For more information about the cluster private endpoint, see the following section Cluster private endpoint. kubectl IPv6 EKS creates a unique dual-stack endpoint in the following format for new IPv6 clusters that are made after October 2024. An IPv6 cluster is a cluster that you select IPv6 in the IP family ( ipFamily ) setting of the cluster. IPv6 IPv6 ipFamily EKS cluster public/private endpoint: eks-cluster. region. api.</description></item><item><title>Kubernetes version 1.33</title><link>https://kubermates.org/releases/2025-05-29-kubernetes-version-1-33/</link><pubDate>Thu, 29 May 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-05-29-kubernetes-version-1-33/</guid><description>Review release notes for Kubernetes versions on standard support Kubernetes 1.33 Kubernetes 1.32 Anonymous authentication changes Amazon Linux 2 AMI deprecation Kubernetes 1.31 Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. This topic gives important changes to be aware of for each Kubernetes version in standard support. When upgrading, carefully review the changes that have occurred between the old and new versions for your cluster. Kubernetes 1.33 is now available in Amazon EKS. For more information about Kubernetes 1.33 , see the official release announcement. 1.33 1.33 The Dynamic Resource Allocation beta Kubernetes API is enabled. This beta API improves the experience of scheduling and monitoring workloads that require resources such as GPUs. The beta API is defined by the Kubernetes community, and might change in future versions of Kubernetes. Carefully review Feature stages in the Kubernetes documentation to understand the implications of using beta APIs. The Dynamic Resource Allocation beta Kubernetes API is enabled. This beta API improves the experience of scheduling and monitoring workloads that require resources such as GPUs. The beta API is defined by the Kubernetes community, and might change in future versions of Kubernetes.</description></item><item><title>New cluster insights for EKS Hybrid Nodes</title><link>https://kubermates.org/releases/2025-05-29-new-cluster-insights-for-eks-hybrid-nodes/</link><pubDate>Thu, 29 May 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-05-29-new-cluster-insights-for-eks-hybrid-nodes/</guid><description>Prepare for Kubernetes version upgrades and troubleshoot misconfigurations with cluster insights Cluster insight types Considerations Use cases Upgrade insights Configuration insights Get started Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Amazon EKS cluster insights provide detection of issues and recommendations to resolve them to help you manage your cluster. Every Amazon EKS cluster undergoes automatic, recurring checks against an Amazon EKS curated list of insights. These insight checks are fully managed by Amazon EKS and offer recommendations on how to address any findings. Configuration insights : Identifies misconfigurations in your EKS Hybrid Nodes setup that could impair functionality of your cluster or workloads. Upgrade insights : Identifies issues that could impact your ability to upgrade to new versions of Kubernetes. Frequency : Amazon EKS refreshes cluster insights every 24 hours, or you can manually refresh them to see the latest status. For example, you can manually refresh cluster insights after addressing an issue to see if the issue was resolved. Frequency : Amazon EKS refreshes cluster insights every 24 hours, or you can manually refresh them to see the latest status. For example, you can manually refresh cluster insights after addressing an issue to see if the issue was resolved. Permissions : Amazon EKS automatically creates a cluster access entry for cluster insights in every EKS cluster. This entry gives EKS permission to view information about your cluster.</description></item><item><title>Add-on support for Amazon FSx CSI driver</title><link>https://kubermates.org/releases/2025-05-23-add-on-support-for-amazon-fsx-csi-driver/</link><pubDate>Fri, 23 May 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-05-23-add-on-support-for-amazon-fsx-csi-driver/</guid><description>Use high-performance app storage with Amazon FSx for Lustre Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. The Amazon FSx for Lustre Container Storage Interface (CSI) driver provides a CSI interface that allows Amazon EKS clusters to manage the lifecycle of Amazon FSx for Lustre file systems. For more information, see the Amazon FSx for Lustre User Guide. For details on how to deploy the Amazon FSx for Lustre CSI driver to your Amazon EKS cluster and verify that it works, see Deploy the FSx for Lustre driver. Thanks for letting us know we&amp;rsquo;re doing a good job! If you&amp;rsquo;ve got a moment, please tell us what we did right so we can do more of it. Thanks for letting us know this page needs work. We&amp;rsquo;re sorry we let you down. If you&amp;rsquo;ve got a moment, please tell us how we can make the documentation better.</description></item><item><title>Edit Prometheus scrapers in the console</title><link>https://kubermates.org/releases/2025-05-22-edit-prometheus-scrapers-in-the-console/</link><pubDate>Thu, 22 May 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-05-22-edit-prometheus-scrapers-in-the-console/</guid><description>Monitor your cluster metrics with Prometheus Step 1: Turn on Prometheus metrics Step 2: Use the Prometheus metrics Step 3: Manage Prometheus scrapers Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Prometheus is a monitoring and time series database that scrapes endpoints. It provides the ability to query, aggregate, and store collected data. You can also use it for alerting and alert aggregation. This topic explains how to set up Prometheus as either a managed or open source option. Monitoring Amazon EKS control plane metrics is a common use case. Amazon Managed Service for Prometheus is a Prometheus-compatible monitoring and alerting service that makes it easy to monitor containerized applications and infrastructure at scale. It is a fully-managed service that automatically scales the ingestion, storage, querying, and alerting of your metrics. It also integrates with AWS security services to enable fast and secure access to your data. You can use the open-source PromQL query language to query your metrics and alert on them. Also, you can use alert manager in Amazon Managed Service for Prometheus to set up alerting rules for critical alerts. You can then send these critical alerts as notifications to an Amazon SNS topic.</description></item><item><title>Bottlerocket for hybrid nodes</title><link>https://kubermates.org/releases/2025-04-29-bottlerocket-for-hybrid-nodes/</link><pubDate>Tue, 29 Apr 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-04-29-bottlerocket-for-hybrid-nodes/</guid><description>Connect hybrid nodes with Bottlerocket Prerequisites Step 1: Create the Bottlerocket settings TOML file SSM IAM Roles Anywhere Step 2: Provision the Bottlerocket vSphere VM with user data Creating VM for the first time Updating user data for an existing VM Step 3: Verify the hybrid node connection Step 4: Configure a CNI for hybrid nodes Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. This topic describes how to connect hybrid nodes running Bottlerocket to an Amazon EKS cluster. Bottlerocket is an open source Linux distribution that is sponsored and supported by AWS. Bottlerocket is purpose-built for hosting container workloads. With Bottlerocket, you can improve the availability of containerized deployments and reduce operational costs by automating updates to your container infrastructure. Bottlerocket includes only the essential software to run containers, which improves resource usage, reduces security threats, and lowers management overhead. Only VMware variants of Bottlerocket version v1.37.0 and above are supported with EKS Hybrid Nodes. VMware variants of Bottlerocket are available for Kubernetes versions v1.28 and above. The OS images for these variants include the kubelet, containerd, aws-iam-authenticator and other software prerequisites for EKS Hybrid Nodes. You can configure these components using a Bottlerocket settings file that includes base64 encoded user-data for the Bottlerocket bootstrap and admin containers. Configuring these settings enables Bottlerocket to use your hybrid nodes credentials provider to authenticate hybrid nodes to your cluster. After your hybrid nodes join the cluster, they will appear with status Not Ready in the Amazon EKS console and in Kubernetes-compatible tooling such as kubectl.</description></item><item><title>New concepts pages for hybrid networking</title><link>https://kubermates.org/releases/2025-04-18-new-concepts-pages-for-hybrid-networking/</link><pubDate>Fri, 18 Apr 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-04-18-new-concepts-pages-for-hybrid-networking/</guid><description>Concepts for hybrid nodes Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. With Amazon EKS Hybrid Nodes , you join physical or virtual machines running in on-premises or edge environments to Amazon EKS clusters running in the AWS Cloud. This approach brings many benefits, but also introduces new networking concepts and architectures for those familiar with running Kubernetes clusters in a single network environment. The following sections dive deep into the Kubernetes and networking concepts for EKS Hybrid Nodes and details how traffic flows through the hybrid architecture. These sections require that you are familiar with basic Kubernetes networking knowledge, such as the concepts of pods, nodes, services, Kubernetes control plane, kubelet and kube-proxy. We recommend reading these pages in order, starting with the Networking concepts for hybrid nodes , then the Kubernetes concepts for hybrid nodes , and finally the Network traffic flows for hybrid nodes. Networking concepts for hybrid nodes Networking concepts for hybrid nodes Kubernetes concepts for hybrid nodes Kubernetes concepts for hybrid nodes Network traffic flows for hybrid nodes Network traffic flows for hybrid nodes Thanks for letting us know we&amp;rsquo;re doing a good job! If you&amp;rsquo;ve got a moment, please tell us what we did right so we can do more of it. Thanks for letting us know this page needs work. We&amp;rsquo;re sorry we let you down. If you&amp;rsquo;ve got a moment, please tell us how we can make the documentation better.</description></item><item><title>AWS managed policy updates</title><link>https://kubermates.org/releases/2025-04-14-aws-managed-policy-updates/</link><pubDate>Mon, 14 Apr 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-04-14-aws-managed-policy-updates/</guid><description>AWS managed policies for Amazon Elastic Kubernetes Service AWS managed policy: AmazonEKS_CNI_Policy AWS managed policy: AmazonEKSClusterPolicy AWS managed policy: AmazonEKSDashboardConsoleReadOnly AWS managed policy: AmazonEKSFargatePodExecutionRolePolicy AWS managed policy: AmazonEKSForFargateServiceRolePolicy AWS managed policy: AmazonEKSComputePolicy Permissions details AWS managed policy: AmazonEKSNetworkingPolicy Permissions details AWS managed policy: AmazonEKSBlockStoragePolicy Permissions details AWS managed policy: AmazonEKSLoadBalancingPolicy Permissions details AWS managed policy: AmazonEKSServicePolicy AWS managed policy: AmazonEKSServiceRolePolicy AWS managed policy: AmazonEKSVPCResourceController AWS managed policy: AmazonEKSWorkerNodePolicy AWS managed policy: AmazonEKSWorkerNodeMinimalPolicy AWS managed policy: AWSServiceRoleForAmazonEKSNodegroup AWS managed policy: AmazonEKSDashboardServiceRolePolicy AWS managed policy: AmazonEBSCSIDriverPolicy AWS managed policy: AmazonEFSCSIDriverPolicy AWS managed policy: AmazonEKSLocalOutpostClusterPolicy AWS managed policy: AmazonEKSLocalOutpostServiceRolePolicy Amazon EKS updates to AWS managed policies Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. An AWS managed policy is a standalone policy that is created and administered by AWS. AWS managed policies are designed to provide permissions for many common use cases so that you can start assigning permissions to users, groups, and roles. Keep in mind that AWS managed policies might not grant least-privilege permissions for your specific use cases because theyâre available for all AWS customers to use. We recommend that you reduce permissions further by defining customer managed policies that are specific to your use cases. You cannot change the permissions defined in AWS managed policies. If AWS updates the permissions defined in an AWS managed policy, the update affects all principal identities (users, groups, and roles) that the policy is attached to. AWS is most likely to update an AWS managed policy when a new AWS service is launched or new API operations become available for existing services. For more information, see AWS managed policies in the IAM User Guide. You can attach the AmazonEKS_CNI_Policy to your IAM entities. Before you create an Amazon EC2 node group, this policy must be attached to either the node IAM role , or to an IAM role thatâs used specifically by the Amazon VPC CNI plugin for Kubernetes. This is so that it can perform actions on your behalf.</description></item><item><title>EKS Hybrid Nodes for existing clusters</title><link>https://kubermates.org/releases/2025-03-31-eks-hybrid-nodes-for-existing-clusters/</link><pubDate>Mon, 31 Mar 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-03-31-eks-hybrid-nodes-for-existing-clusters/</guid><description>Enable hybrid nodes on an existing Amazon EKS cluster or modify configuration Prerequisites Considerations Enable hybrid nodes on an existing cluster Enable EKS Hybrid Nodes in an existing cluster - AWS CloudFormation Enable EKS Hybrid Nodes in an existing cluster - AWS CLI Enable EKS Hybrid Nodes in an existing cluster - AWS Management Console Update hybrid nodes configuration in an existing cluster Update hybrid configuration in an existing cluster - AWS CloudFormation Update hybrid configuration in an existing cluster - AWS CLI Update hybrid configuration in an existing cluster - AWS Management Console Disable Hybrid nodes in an existing cluster Disable EKS Hybrid Nodes in an existing cluster - AWS CloudFormation Disable EKS Hybrid Nodes in an existing cluster - AWS CLI Disable EKS Hybrid Nodes in an existing cluster - AWS Management Console Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. This topic provides an overview of the available options and describes what to consider when you add, change, or remove the hybrid nodes configuration for an Amazon EKS cluster. To enable an Amazon EKS cluster to use hybrid nodes, add the IP address CIDR ranges of your on-premises node and optionally pod network in the RemoteNetworkConfig configuration. EKS uses this list of CIDRs to enable connectivity between the cluster and your on-premises networks. For a full list of options when updating your cluster configuration, see the UpdateClusterConfig in the Amazon EKS API Reference. RemoteNetworkConfig You can do any of the following actions to the EKS Hybrid Nodes networking configuration in a cluster: Add remote network configuration to enable EKS Hybrid Nodes in an existing cluster. Add remote network configuration to enable EKS Hybrid Nodes in an existing cluster. Add, change, or remove the remote node networks or the remote pod networks in an existing cluster. Remove all remote node network CIDR ranges to disable EKS Hybrid Nodes in an existing cluster. Before enabling your Amazon EKS cluster for hybrid nodes, ensure your environment meets the requirements outlined at Prerequisite setup for hybrid nodes , and detailed at Prepare networking for hybrid nodes , Prepare operating system for hybrid nodes , and Prepare credentials for hybrid nodes. Your cluster must use IPv4 address family. Your cluster must use either API or API_AND_CONFIG_MAP for the cluster authentication mode.</description></item><item><title>Node health for EKS Hybrid Nodes</title><link>https://kubermates.org/releases/2025-03-31-node-health-for-eks-hybrid-nodes/</link><pubDate>Mon, 31 Mar 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-03-31-node-health-for-eks-hybrid-nodes/</guid><description>Enable node auto repair and investigate node health issues Node monitoring agent Node auto repair Node health issues Kernel node health issues Networking node health issues Neuron node health issues NVIDIA node health issues Runtime node health issues Storage node health issues Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Node health refers to the operational status and capability of a node to effectively run workloads. A healthy node maintains expected connectivity, has sufficient resources, and can successfully run Pods without disruption. For information on getting details about your nodes, see View the health status of your nodes and Retrieve node logs for a managed node using kubectl and S3. To help with maintaining healthy nodes, Amazon EKS offers the node monitoring agent and node auto repair. The node monitoring agent and node auto repair are only available on Linux. These features arenât available on Windows. The node monitoring agent automatically reads node logs to detect certain health issues. It parses through node logs to detect failures and surfaces various status information about worker nodes. A dedicated NodeCondition is applied on the worker nodes for each category of issues detected, such as storage and networking issues. Descriptions of detected health issues are made available in the observability dashboard. For more information, see Node health issues.</description></item><item><title>Rollback: Prevent accidental upgrades with cluster insights</title><link>https://kubermates.org/releases/2025-03-28-rollback-prevent-accidental-upgrades-with-cluster-insights/</link><pubDate>Fri, 28 Mar 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-03-28-rollback-prevent-accidental-upgrades-with-cluster-insights/</guid><description>Update existing cluster to new Kubernetes version Considerations for Amazon EKS Auto Mode Summary Step 1: Prepare for upgrade Step 2: Review upgrade considerations Review upgrade insights Detailed considerations Step 3: Update cluster control plane Update cluster - eksctl Update cluster - AWS console Update cluster - AWS CLI Step 4: Update cluster components Downgrade the Kubernetes version for an Amazon EKS cluster Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. When a new Kubernetes version is available in Amazon EKS, you can update your Amazon EKS cluster to the latest version. Once you upgrade a cluster, you canât downgrade to a previous version. Before you update to a new Kubernetes version, we recommend that you review the information in Understand the Kubernetes version lifecycle on EKS and the update steps in this topic. New Kubernetes versions sometimes introduce significant changes. Therefore, we recommend that you test the behavior of your applications against a new Kubernetes version before you update your production clusters. You can do this by building a continuous integration workflow to test your application behavior before moving to a new Kubernetes version. The update process consists of Amazon EKS launching new API server nodes with the updated Kubernetes version to replace the existing ones. Amazon EKS performs standard infrastructure and readiness health checks for network traffic on these new nodes to verify that theyâre working as expected. However, once youâve started the cluster upgrade, you canât pause or stop it. If any of these checks fail, Amazon EKS reverts the infrastructure deployment, and your cluster remains on the prior Kubernetes version. Running applications arenât affected, and your cluster is never left in a non-deterministic or unrecoverable state.</description></item><item><title>Bottlerocket FIPS AMIs</title><link>https://kubermates.org/releases/2025-03-27-bottlerocket-fips-amis/</link><pubDate>Thu, 27 Mar 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-03-27-bottlerocket-fips-amis/</guid><description>Make your worker nodes FIPS ready with Bottlerocket FIPS AMIs Considerations Create a managed node group with a Bottlerocket FIPS AMI Disable the FIPS endpoint for non-supported AWS Regions Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. The Federal Information Processing Standard (FIPS) Publication 140-3 is a United States and Canadian government standard that specifies the security requirements for cryptographic modules that protect sensitive information. Bottlerocket makes it easier to adhere to FIPS by offering AMIs with a FIPS kernel. These AMIs are preconfigured to use FIPS 140-3 validated cryptographic modules. This includes the Amazon Linux 2023 Kernel Crypto API Cryptographic Module and the AWS-LC Cryptographic Module. Using Bottlerocket FIPS AMIs makes your worker nodes &amp;ldquo;FIPS ready&amp;rdquo; but not automatically &amp;ldquo;FIPS-compliant&amp;rdquo;. For more information, see Federal Information Processing Standard (FIPS) 140-3. If your cluster uses isolated subnets, the Amazon ECR FIPS endpoint may not be accessible. This can cause the node bootstrap to fail. Make sure that your network configuration allows access to the necessary FIPS endpoints. For more information, see Access a resource through a resource VPC endpoint in the AWS PrivateLink Guide. If your cluster uses isolated subnets, the Amazon ECR FIPS endpoint may not be accessible.</description></item><item><title>Update strategies for managed node groups</title><link>https://kubermates.org/releases/2025-01-27-update-strategies-for-managed-node-groups/</link><pubDate>Mon, 27 Jan 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-01-27-update-strategies-for-managed-node-groups/</guid><description>Understand each phase of node updates Setup phase Scale up phase Upgrade phase PodEvictionFailure errors during the upgrade phase Scale down phase Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. The Amazon EKS managed worker node upgrade strategy has four different phases described in the following sections. The setup phase has these steps: It creates a new Amazon EC2 launch template version for the Auto Scaling Group thatâs associated with your node group. The new launch template version uses the target AMI or a custom launch template version for the update. It creates a new Amazon EC2 launch template version for the Auto Scaling Group thatâs associated with your node group. The new launch template version uses the target AMI or a custom launch template version for the update. It updates the Auto Scaling Group to use the latest launch template version. It determines the maximum quantity of nodes to upgrade in parallel using the updateConfig property for the node group. The maximum unavailable has a quota of 100 nodes. The default value is one node. For more information, see the updateConfig property in the Amazon EKS API Reference. It determines the maximum quantity of nodes to upgrade in parallel using the updateConfig property for the node group.</description></item><item><title>Kubernetes version 1.32</title><link>https://kubermates.org/releases/2025-01-23-kubernetes-version-1-32/</link><pubDate>Thu, 23 Jan 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-01-23-kubernetes-version-1-32/</guid><description>Understand the Kubernetes version lifecycle on EKS Available versions on standard support Available versions on extended support Amazon EKS Kubernetes release calendar Get version information with AWS CLI To retrieve information about available Kubernetes versions on EKS using the AWS CLI Amazon EKS version FAQs Amazon EKS extended support FAQs Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Kubernetes rapidly evolves with new features, design updates, and bug fixes. The community releases new Kubernetes minor versions (such as 1.33 ) on average once every four months. Amazon EKS follows the upstream release and deprecation cycle for minor versions. As new Kubernetes versions become available in Amazon EKS, we recommend that you proactively update your clusters to use the latest available version. 1.33 A minor version is under standard support in Amazon EKS for the first 14 months after itâs released. Once a version is past the end of standard support date, it enters extended support for the next 12 months. Extended support allows you to stay at a specific Kubernetes version for longer at an additional cost per cluster hour. If you havenât updated your cluster before the extended support period ends, your cluster is auto-upgraded to the oldest currently supported extended version. Extended support is enabled by default. To disable, see Disable EKS extended support. We recommend that you create your cluster with the latest available Kubernetes version supported by Amazon EKS.</description></item><item><title>Amazon EKS Auto Mode</title><link>https://kubermates.org/releases/2024-12-01-amazon-eks-auto-mode/</link><pubDate>Sun, 01 Dec 2024 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2024-12-01-amazon-eks-auto-mode/</guid><description>Automate cluster infrastructure with EKS Auto Mode Features Automated Components Configuration Shared responsibility model Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. EKS Auto Mode extends AWS management of Kubernetes clusters beyond the cluster itself, to allow AWS to also set up and manage the infrastructure that enables the smooth operation of your workloads. You can delegate key infrastructure decisions and leverage the expertise of AWS for day-to-day operations. Cluster infrastructure managed by AWS includes many Kubernetes capabilities as core components, as opposed to add-ons, such as compute autoscaling, pod and service networking, application load balancing, cluster DNS, block storage, and GPU support. To get started, you can deploy a new EKS Auto Mode cluster or enable EKS Auto Mode on an existing cluster. You can deploy, upgrade, or modify your EKS Auto Mode clusters using eksctl, the AWS CLI, the AWS Management Console, EKS APIs, or your preferred infrastructure-as-code tools. With EKS Auto Mode, you can continue using your preferred Kubernetes-compatible tools. EKS Auto Mode integrates with AWS services like Amazon EC2, Amazon EBS, and ELB, leveraging AWS cloud resources that follow best practices. These resources are automatically scaled, cost-optimized, and regularly updated to help minimize operational costs and overhead. EKS Auto Mode provides the following high-level features: Streamline Kubernetes Cluster Management : EKS Auto Mode streamlines EKS management by providing production-ready clusters with minimal operational overhead. With EKS Auto Mode, you can run demanding, dynamic workloads confidently, without requiring deep EKS expertise. Application Availability : EKS Auto Mode dynamically adds or removes nodes in your EKS cluster based on the demands of your Kubernetes applications.</description></item><item><title>Amazon EKS Hybrid Nodes</title><link>https://kubermates.org/releases/2024-12-01-amazon-eks-hybrid-nodes/</link><pubDate>Sun, 01 Dec 2024 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2024-12-01-amazon-eks-hybrid-nodes/</guid><description>Amazon EKS Hybrid Nodes overview Features Limits Considerations Additional resources Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. With Amazon EKS Hybrid Nodes , you can use your on-premises and edge infrastructure as nodes in Amazon EKS clusters. AWS manages the AWS-hosted Kubernetes control plane of the Amazon EKS cluster, and you manage the hybrid nodes that run in your on-premises or edge environments. This unifies Kubernetes management across your environments and offloads Kubernetes control plane management to AWS for your on-premises and edge applications. Amazon EKS Hybrid Nodes works with any on-premises hardware or virtual machines, bringing the efficiency, scalability, and availability of Amazon EKS to wherever your applications need to run. You can use a wide range of Amazon EKS features with Amazon EKS Hybrid Nodes including Amazon EKS add-ons, Amazon EKS Pod Identity, cluster access entries, cluster insights, and extended Kubernetes version support. Amazon EKS Hybrid Nodes natively integrates with AWS services including AWS Systems Manager, AWS IAM Roles Anywhere, Amazon Managed Service for Prometheus, and Amazon CloudWatch for centralized monitoring, logging, and identity management. With Amazon EKS Hybrid Nodes, there are no upfront commitments or minimum fees, and you are charged per hour for the vCPU resources of your hybrid nodes when they are attached to your Amazon EKS clusters. For more pricing information, see Amazon EKS Pricing. EKS Hybrid Nodes has the following high-level features: Managed Kubernetes control plane : AWS manages the AWS-hosted Kubernetes control plane of the EKS cluster, and you manage the hybrid nodes that run in your on-premises or edge environments. This unifies Kubernetes management across your environments and offloads Kubernetes control plane management to AWS for your on-premises and edge applications. By moving the Kubernetes control plane to AWS, you can conserve on-premises capacity for your applications and trust that the Kubernetes control plane scales with your workloads.</description></item><item><title>Kubernetes version 1.30 is now available for local clusters on AWS Outposts</title><link>https://kubermates.org/releases/2024-11-21-kubernetes-version-1-30-is-now-available-for-local-clusters-on-aws-outposts/</link><pubDate>Thu, 21 Nov 2024 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2024-11-21-kubernetes-version-1-30-is-now-available-for-local-clusters-on-aws-outposts/</guid><description>Learn Kubernetes and Amazon EKS platform versions for AWS Outposts Kubernetes version 1.31 Kubernetes version 1.30 Kubernetes version 1.29 Kubernetes version 1.28 Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Local cluster platform versions represent the capabilities of the Amazon EKS cluster on AWS Outposts. The versions include the components that run on the Kubernetes control plane, which Kubernetes API server flags are enabled. They also include the current Kubernetes patch version. Each Kubernetes minor version has one or more associated platform versions. The platform versions for different Kubernetes minor versions are independent. The platform versions for local clusters and Amazon EKS clusters in the cloud are independent. When a new Kubernetes minor version is available for local clusters, such as 1.31 , the initial platform version for that Kubernetes minor version starts at eks-local-outposts. 1. However, Amazon EKS releases new platform versions periodically to enable new Kubernetes control plane settings and to provide security fixes. 1.31 eks-local-outposts. 1 When new local cluster platform versions become available for a minor version: The platform version number is incremented ( eks-local-outposts.</description></item><item><title>Bottlerocket AMIs that use FIPS 140-3</title><link>https://kubermates.org/releases/2024-11-20-bottlerocket-amis-that-use-fips-140-3/</link><pubDate>Wed, 20 Nov 2024 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2024-11-20-bottlerocket-amis-that-use-fips-140-3/</guid><description>Retrieve recommended Bottlerocket AMI IDs Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. When deploying nodes, you can specify an ID for a pre-built Amazon EKS optimized Amazon Machine Image (AMI). To retrieve an AMI ID that fits your desired configuration, query the AWS Systems Manager Parameter Store API. Using this API eliminates the need to manually look up Amazon EKS optimized AMI IDs. For more information, see GetParameter. The IAM principal that you use must have the ssm:GetParameter IAM permission to retrieve the Amazon EKS optimized AMI metadata. ssm:GetParameter You can retrieve the image ID of the latest recommended Amazon EKS optimized Bottlerocket AMI with the following AWS CLI command, which uses the sub-parameter image_id. Make the following modifications to the command as needed and then run the modified command: image_id Replace kubernetes-version with a supported platform-version. Replace kubernetes-version with a supported platform-version. kubernetes-version Replace -flavor with one of the following options. Remove -flavor for variants without a GPU. Use -nvidia for GPU-enabled variants.</description></item><item><title>Observability dashboard</title><link>https://kubermates.org/releases/2024-11-18-observability-dashboard/</link><pubDate>Mon, 18 Nov 2024 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2024-11-18-observability-dashboard/</guid><description>Monitor your cluster with the observability dashboard Summary Cluster health Control plane monitoring Metrics CloudWatch Log Insights View control plane logs in CloudWatch Cluster insights Node health issues Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. The Amazon EKS console includes an observability dashboard that gives visibility into the performance of your cluster. The information it provides helps you to quickly detect, troubleshoot, and remediate issues. You can open the applicable section of the observability dashboard by choosing an item in the Health and performance summary. This summary is included in several places, including the Observability tab. The observability dashboard is split into several tabs. The Health and performance summary lists the quantity of items in various categories. Each number acts as a hyperlink to a location in the observability dashboard with a list for that category. Cluster health provides important notifications to be aware of, some of which you may need to take action on as soon as possible. With this list, you can see descriptions and the affected resources. Cluster health includes two tables: Health issues and Configuration insights. To refresh the status of Health issues , choose the refresh button ( â» ).</description></item><item><title>New role creation in console for add-ons that support EKS Pod Identities</title><link>https://kubermates.org/releases/2024-11-15-new-role-creation-in-console-for-add-ons-that-support-eks-pod-identities/</link><pubDate>Fri, 15 Nov 2024 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2024-11-15-new-role-creation-in-console-for-add-ons-that-support-eks-pod-identities/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Amazon EKS add-ons are add-on software for Amazon EKS clusters. All Amazon EKS add-ons: Include the latest security patches and bug fixes. Are validated by AWS to work with Amazon EKS. Reduce the amount of work required to manage the add-on software. You can create an Amazon EKS add-on using eksctl , the AWS Management Console, or the AWS CLI. If the add-on requires an IAM role, see the details for the specific add-on in Amazon EKS add-ons for details about creating the role. Complete the following before you create an add-on: The cluster must exist before you create an add-on for it. For more information, see Create an Amazon EKS cluster. Check if your add-on requires an IAM role. For more information, see Verify Amazon EKS add-on version compatibility with a cluster. Verify that the Amazon EKS add-on version is compatabile with your cluster.</description></item></channel></rss>