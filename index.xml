<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubermates</title><link>https://kubermates.org/</link><description>Recent content on Kubermates</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 25 Aug 2025 19:42:02 +0000</lastBuildDate><atom:link href="https://kubermates.org/index.xml" rel="self" type="application/rss+xml"/><item><title>How Should Prometheus Handle OpenTelemetry Resource Attributes? â€“ A UX Research Report</title><link>https://kubermates.org/events/2025-08-25-how-should-prometheus-handle-opentelemetry-resource-attributes-a-ux-research-rep/</link><pubDate>Mon, 25 Aug 2025 19:42:02 +0000</pubDate><guid>https://kubermates.org/events/2025-08-25-how-should-prometheus-handle-opentelemetry-resource-attributes-a-ux-research-rep/</guid><description>&lt;p>On May 29th, 2025, I wrapped up my mentorship with Prometheus through the Linux Foundation Mentorship Program. My project focused on understanding how Prometheus handles OpenTelemetry resource attributes and how that experience could be improved for&amp;hellip;&lt;/p></description></item><item><title>August 25, 2025</title><link>https://kubermates.org/releases/2025-08-25-august-25-2025/</link><pubDate>Mon, 25 Aug 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-08-25-august-25-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#August_25_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#August_25_2025&lt;/a>&lt;/p></description></item><item><title>How I Team Up with GenAI to Craft Conference Talk Proposals</title><link>https://kubermates.org/events/2025-08-22-how-i-team-up-with-genai-to-craft-conference-talk-proposals/</link><pubDate>Fri, 22 Aug 2025 20:09:07 +0000</pubDate><guid>https://kubermates.org/events/2025-08-22-how-i-team-up-with-genai-to-craft-conference-talk-proposals/</guid><description>&lt;p>TL;DR: GenAI can help you write conference abstracts, but if you use it without injecting your own curiosity and humanness, reviewers can tell. This post walks through my personal process for co-writing abstracts with GenAI in&amp;hellip;&lt;/p></description></item><item><title>Migrate to Amazon EKS: Data plane cost modeling with Karpenter and KWOK</title><link>https://kubermates.org/docs/2025-08-21-migrate-to-amazon-eks-data-plane-cost-modeling-with-karpenter-and-kwok/</link><pubDate>Thu, 21 Aug 2025 18:18:42 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-21-migrate-to-amazon-eks-data-plane-cost-modeling-with-karpenter-and-kwok/</guid><description>When migrating Kubernetes clusters to Amazon Elastic Kubernetes Service (Amazon EKS) , organizations typically follow three phases: assessment, mobilize, and migrate and modernize. The assessment phase involves evaluating technical feasibility for Amazon EKS workloads, analyzing current Kubernetes environments, identifying compatibility issues, estimating costs, and determining timelines with business impact considerations. During the mobilize phase, organizations create detailed migration plans, establish EKS environments with proper networking and security, train teams, and develop testing procedures. The final migrate and modernize phase involves transferring applications and data, validating functionality, implementing cloud-centered features, optimizing resources and costs, and enhancing observability to fully use AWS capabilities. One of the most significant challenges organizations face during the process is cost estimation, which happens in the assessment phase. Karpenter is an open source Kubernetes node autoscaler that efficiently provisions just-in-time compute resources to match workload demands. Unlike traditional autoscalers, Karpenter directly integrates with cloud providers to make intelligent, real-time decisions about instance types, availability zones, and capacity options. It evaluates pod requirements and constraints to select optimal instances, considering factors such as CPU, memory, price, and availability. Karpenter can consolidate workloads for cost efficiency and rapidly scale from zero to handle sudden demand spikes. It supports both spot and on-demand instances, and automatically terminates nodes when theyâ€™re no longer needed, optimizing cluster resource utilization and reducing cloud costs. Karpenter uses the concept of Providers to interact with different infrastructure platforms for provisioning and managing compute resources. KWOK (Kubernetes WithOut Kubelet) is a toolkit that simulates data plane nodes without allocating actual infrastructure, and can be used as a provider to create lightweight testing environments that enable developers to validate provisioning decisions, try various (virtual) instance types, and debug scaling behaviors.</description></item><item><title>Celebrating 100 Golden Kubestronauts</title><link>https://kubermates.org/events/2025-08-21-celebrating-100-golden-kubestronauts/</link><pubDate>Thu, 21 Aug 2025 13:59:00 +0000</pubDate><guid>https://kubermates.org/events/2025-08-21-celebrating-100-golden-kubestronauts/</guid><description>&lt;p>The Kubestronauts program has been growing at an incredible pace. We recently celebrated over 2000 kubestronauts across the globe, and today weâ€™re excited to celebrate another milestone: over 100 Golden Kubestronauts have achieved Golden status in&amp;hellip;&lt;/p></description></item><item><title>August 21, 2025</title><link>https://kubermates.org/releases/2025-08-21-august-21-2025/</link><pubDate>Thu, 21 Aug 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-08-21-august-21-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#August_21_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#August_21_2025&lt;/a>&lt;/p></description></item><item><title>Kubernetes v1.34.0-rc.2</title><link>https://kubermates.org/releases/2025-08-20-kubernetes-v1-34-0-rc-2/</link><pubDate>Wed, 20 Aug 2025 21:15:15 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-20-kubernetes-v1-34-0-rc-2/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.34.0-rc.2">https://github.com/kubernetes/kubernetes/releases/tag/v1.34.0-rc.2&lt;/a>&lt;/p></description></item><item><title>Uniting the Cloud Native Community at the Inaugural KCD SF Bay Area</title><link>https://kubermates.org/events/2025-08-20-uniting-the-cloud-native-community-at-the-inaugural-kcd-sf-bay-area/</link><pubDate>Wed, 20 Aug 2025 14:23:00 +0000</pubDate><guid>https://kubermates.org/events/2025-08-20-uniting-the-cloud-native-community-at-the-inaugural-kcd-sf-bay-area/</guid><description>&lt;p>The cloud native landscape is constantly evolving, and staying ahead of the curve means more than just reading documentationâ€”it means connecting with the people who are shaping the future. Thatâ€™s exactly what the inaugural Kubernetes Community&amp;hellip;&lt;/p></description></item><item><title>August 20, 2025</title><link>https://kubermates.org/releases/2025-08-20-august-20-2025/</link><pubDate>Wed, 20 Aug 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-08-20-august-20-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#August_20_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#August_20_2025&lt;/a>&lt;/p></description></item><item><title>Cross-service confused deputy prevention</title><link>https://kubermates.org/releases/2025-08-19-cross-service-confused-deputy-prevention/</link><pubDate>Tue, 19 Aug 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-19-cross-service-confused-deputy-prevention/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/cross-service-confused-deputy-prevention.html">https://docs.aws.amazon.com/eks/latest/userguide/cross-service-confused-deputy-prevention.html&lt;/a>&lt;/p></description></item><item><title>Tuning Linux Swap for Kubernetes: A Deep Dive</title><link>https://kubermates.org/docs/2025-08-19-tuning-linux-swap-for-kubernetes-a-deep-dive/</link><pubDate>Tue, 19 Aug 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-08-19-tuning-linux-swap-for-kubernetes-a-deep-dive/</guid><description>The Kubernetes NodeSwap feature , likely to graduate to stable in the upcoming Kubernetes v1. 34 release, allows swap usage: a significant shift from the conventional practice of disabling swap for performance predictability. This article focuses exclusively on tuning swap on Linux nodes, where this feature is available. By allowing Linux nodes to use secondary storage for additional virtual memory when physical RAM is exhausted, node swap support aims to improve resource utilization and reduce out-of-memory (OOM) kills. However, enabling swap is not a &amp;ldquo;turn-key&amp;rdquo; solution. The performance and stability of your nodes under memory pressure are critically dependent on a set of Linux kernel parameters. Misconfiguration can lead to performance degradation and interfere with Kubelet&amp;rsquo;s eviction logic. In this blogpost, I&amp;rsquo;ll dive into critical Linux kernel parameters that govern swap behavior. I will explore how these parameters influence Kubernetes workload performance, swap utilization, and crucial eviction mechanisms. I will present various test results showcasing the impact of different configurations, and share my findings on achieving optimal settings for stable and high-performing Kubernetes clusters. At a high level, the Linux kernel manages memory through pages, typically 4KiB in size. When physical memory becomes constrained, the kernel&amp;rsquo;s page replacement algorithm decides which pages to move to swap space.</description></item><item><title>Celebrating 100 Days of Kagent</title><link>https://kubermates.org/events/2025-08-19-celebrating-100-days-of-kagent/</link><pubDate>Tue, 19 Aug 2025 13:30:00 +0000</pubDate><guid>https://kubermates.org/events/2025-08-19-celebrating-100-days-of-kagent/</guid><description>&lt;p>When we first introduced kagent on March 17th, 2025, we had a bold vision: to bring agentic AI to cloud nativeâ€”empowering platforms and DevOps engineers to harness AI agents for solving real operational challenges. Fast forward&amp;hellip;&lt;/p></description></item><item><title>launching kubermates</title><link>https://kubermates.org/blog/first-post/</link><pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/blog/first-post/</guid><description>&lt;p>we are live
kubermates will publish practical guides across aks eks gke and onprem&lt;/p></description></item><item><title>Combining GenAI &amp; Agentic AI to build scalable, autonomous systems</title><link>https://kubermates.org/events/2025-08-18-combining-genai-agentic-ai-to-build-scalable-autonomous-systems/</link><pubDate>Mon, 18 Aug 2025 15:00:00 +0000</pubDate><guid>https://kubermates.org/events/2025-08-18-combining-genai-agentic-ai-to-build-scalable-autonomous-systems/</guid><description>&lt;p>A common pattern in todayâ€™s AI adoption is that businesses are investing heavily in GenAI capabilities, yet many are leaving significant value on the table by failing to pair it with Agentic AI.Â  This matters because&amp;hellip;&lt;/p></description></item><item><title>Gear Up for the 5th Annual KCD Washington DC!</title><link>https://kubermates.org/events/2025-08-16-gear-up-for-the-5th-annual-kcd-washington-dc/</link><pubDate>Sat, 16 Aug 2025 13:04:00 +0000</pubDate><guid>https://kubermates.org/events/2025-08-16-gear-up-for-the-5th-annual-kcd-washington-dc/</guid><description>&lt;p>Weâ€™re so thrilled to be organizing another year of cloud native community in the Nationâ€™s Capital.Â  Past years have witnessed amazing presentations ranging from core Kubernetes topics to emerging trends in ML/AI, sustainability, virtual clusters and&amp;hellip;&lt;/p></description></item><item><title>Dragonfly v2.3.0 has been released</title><link>https://kubermates.org/events/2025-08-15-dragonfly-v2-3-0-has-been-released/</link><pubDate>Fri, 15 Aug 2025 18:00:00 +0000</pubDate><guid>https://kubermates.org/events/2025-08-15-dragonfly-v2-3-0-has-been-released/</guid><description>&lt;p>Dragonfly v2.3.0 is released! ðŸŽ‰ðŸŽ‰ðŸŽ‰ Thanks to theÂ contributorsÂ who made this release happen. We welcome you to visitÂ d7y.ioÂ website to learn more. Features Persistent Cache Task It designs to provide persistent caching for tasks. This tool can import&amp;hellip;&lt;/p></description></item><item><title>How Imagine Learning Reduced Operational Overhead by 20% With Linkerd</title><link>https://kubermates.org/events/2025-08-15-how-imagine-learning-reduced-operational-overhead-by-20-with-linkerd/</link><pubDate>Fri, 15 Aug 2025 12:44:00 +0000</pubDate><guid>https://kubermates.org/events/2025-08-15-how-imagine-learning-reduced-operational-overhead-by-20-with-linkerd/</guid><description>&lt;p>At Imagine Learning, we strive to empower educators and inspire breakthrough moments for over 18 million students across the United States. As a digital-first education solutions provider, our mission is to deliver robust, reliable, and secure&amp;hellip;&lt;/p></description></item><item><title>August 15, 2025</title><link>https://kubermates.org/releases/2025-08-15-august-15-2025/</link><pubDate>Fri, 15 Aug 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-08-15-august-15-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#August_15_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#August_15_2025&lt;/a>&lt;/p></description></item><item><title>Cloud Native in the City of Kings</title><link>https://kubermates.org/events/2025-08-14-cloud-native-in-the-city-of-kings/</link><pubDate>Thu, 14 Aug 2025 19:30:22 +0000</pubDate><guid>https://kubermates.org/events/2025-08-14-cloud-native-in-the-city-of-kings/</guid><description>&lt;p>Lima, Peru â€” the â€œCiudad de los Reyesâ€ â€” hosted one of the most energized and well-attended Kubernetes Community Days (KCD) in the region this past weekend. From the start, it was clear that this was&amp;hellip;&lt;/p></description></item><item><title>Beyond Code: Open Source, Mentorship and Microcks</title><link>https://kubermates.org/events/2025-08-14-beyond-code-open-source-mentorship-and-microcks/</link><pubDate>Thu, 14 Aug 2025 15:21:10 +0000</pubDate><guid>https://kubermates.org/events/2025-08-14-beyond-code-open-source-mentorship-and-microcks/</guid><description>&lt;p>Photo by Diva Plavalaguna from Pexels Open source software proves that collective contribution creates collective value. However, as a contributor to Microcks, I have observed a concerning pattern that threatens the sustainability of the project and&amp;hellip;&lt;/p></description></item><item><title>August 14, 2025</title><link>https://kubermates.org/releases/2025-08-14-august-14-2025/</link><pubDate>Thu, 14 Aug 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-08-14-august-14-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#August_14_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#August_14_2025&lt;/a>&lt;/p></description></item><item><title>How OTel Community Day Enriched my Open Source Career: A Tale of Community and Connection</title><link>https://kubermates.org/events/2025-08-14-how-otel-community-day-enriched-my-open-source-career-a-tale-of-community-and-co/</link><pubDate>Thu, 14 Aug 2025 06:28:00 +0000</pubDate><guid>https://kubermates.org/events/2025-08-14-how-otel-community-day-enriched-my-open-source-career-a-tale-of-community-and-co/</guid><description>&lt;p>My name is Diana Todea. Iâ€™m originally from Romania and have been living in Spain for the past nine years. I have worked as a Senior Site Reliability Engineer specializing in Observability for the last three&amp;hellip;&lt;/p></description></item><item><title>Kubernetes v1.31.12</title><link>https://kubermates.org/releases/2025-08-13-kubernetes-v1-31-12/</link><pubDate>Wed, 13 Aug 2025 19:50:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-13-kubernetes-v1-31-12/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.31.12">https://github.com/kubernetes/kubernetes/releases/tag/v1.31.12&lt;/a>&lt;/p></description></item><item><title>Kubernetes v1.33.4</title><link>https://kubermates.org/releases/2025-08-13-kubernetes-v1-33-4/</link><pubDate>Wed, 13 Aug 2025 19:46:26 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-13-kubernetes-v1-33-4/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.33.4">https://github.com/kubernetes/kubernetes/releases/tag/v1.33.4&lt;/a>&lt;/p></description></item><item><title>Kubernetes v1.32.8</title><link>https://kubermates.org/releases/2025-08-13-kubernetes-v1-32-8/</link><pubDate>Wed, 13 Aug 2025 19:45:14 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-13-kubernetes-v1-32-8/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.32.8">https://github.com/kubernetes/kubernetes/releases/tag/v1.32.8&lt;/a>&lt;/p></description></item><item><title>OpenKruise v1.8 Unlocking Infinite Possibilities in Cloud-Native Application Management</title><link>https://kubermates.org/events/2025-08-13-openkruise-v1-8-unlocking-infinite-possibilities-in-cloud-native-application-man/</link><pubDate>Wed, 13 Aug 2025 18:00:00 +0000</pubDate><guid>https://kubermates.org/events/2025-08-13-openkruise-v1-8-unlocking-infinite-possibilities-in-cloud-native-application-man/</guid><description>&lt;p>OpenKruiseÂ is an open-source cloud-native application automation management suite. It is also a current incubating project hosted by the Cloud Native Computing Foundation (CNCF). It is a standard extension component based on Kubernetes that is widely used&amp;hellip;&lt;/p></description></item><item><title>Canary delivery with Argo Rollout and Amazon VPC Lattice for Amazon EKS</title><link>https://kubermates.org/docs/2025-08-12-canary-delivery-with-argo-rollout-and-amazon-vpc-lattice-for-amazon-eks/</link><pubDate>Tue, 12 Aug 2025 23:55:07 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-12-canary-delivery-with-argo-rollout-and-amazon-vpc-lattice-for-amazon-eks/</guid><description>Modern application delivery demands agility and reliability, where updates are rolled out progressively while making sure of the minimal impact on end users. Progressive delivery strategies, such as canary deployments, allow organizations to release new features by shifting traffic incrementally between old and new versions of a service. This allows organizations to first release features to a small subset of users, monitor system behavior and performance in real time, and automatically roll back if anomalies are detected. This is particularly valuable in modern microservices environments running on platforms such as Amazon Elastic Kubernetes Service (Amazon EKS) , where service meshes and traffic routers provide the necessary infrastructure for fine-grained control over traffic routing. This post explores an architectural approach to implementing progressive delivery using Amazon VPC Lattice, Amazon CloudWatch Synthetics , and Argo Rollouts. The solution uses VPC Lattice for enhanced traffic control across microservices, CloudWatch Synthetics for real-time health and validation monitoring, and Argo Rollouts for orchestrating canary updates. The content in this post addresses readers who are already familiar with networking constructs on Amazon Web Services (AWS), such as Amazon Virtual Private Cloud (Amazon VPC) , CloudWatch Synthetics and Amazon EKS. Instead of defining these services, we focus on their capabilities and integration with VPC Lattice. We also build upon your existing understanding of VPC Lattice concepts and Argo Rollouts. For more background on Amazon VPC Lattice, we recommend that you review the post, Build secure multi-account multi-VPC connectivity for your applications with Amazon VPC Lattice , and the collection of resources in the VPC Lattice Getting started guide. The architecture integrates multiple AWS services and Kubernetes-native components, providing a comprehensive solution for progressive delivery: In this section we consider an application running on Amazon EKS, where a new version of a microserviceâ€” prodDetail v2 â€”needs to be rolled out with minimal impact to users relying on the stable version v1. To do this, we implement a canary deployment strategy using VPC Lattice, Argo Rollouts, CloudWatch Synthetics, and AnalysisTemplates.</description></item><item><title>August 12, 2025</title><link>https://kubermates.org/releases/2025-08-12-august-12-2025/</link><pubDate>Tue, 12 Aug 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-08-12-august-12-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#August_12_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#August_12_2025&lt;/a>&lt;/p></description></item><item><title>Kubernetes v1.34.0-rc.1</title><link>https://kubermates.org/releases/2025-08-09-kubernetes-v1-34-0-rc-1/</link><pubDate>Sat, 09 Aug 2025 07:41:31 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-09-kubernetes-v1-34-0-rc-1/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.34.0-rc.1">https://github.com/kubernetes/kubernetes/releases/tag/v1.34.0-rc.1&lt;/a>&lt;/p></description></item><item><title>August 08, 2025</title><link>https://kubermates.org/releases/2025-08-08-august-08-2025/</link><pubDate>Fri, 08 Aug 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-08-08-august-08-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#August_08_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#August_08_2025&lt;/a>&lt;/p></description></item><item><title>Introducing Headlamp AI Assistant</title><link>https://kubermates.org/docs/2025-08-07-introducing-headlamp-ai-assistant/</link><pubDate>Thu, 07 Aug 2025 20:00:00 +0100</pubDate><guid>https://kubermates.org/docs/2025-08-07-introducing-headlamp-ai-assistant/</guid><description>This announcement originally appeared on the Headlamp blog. To simplify Kubernetes management and troubleshooting, we&amp;rsquo;re thrilled to introduce Headlamp AI Assistant : a powerful new plugin for Headlamp that helps you understand and operate your Kubernetes clusters and applications with greater clarity and ease. Whether you&amp;rsquo;re a seasoned engineer or just getting started, the AI Assistant offers: Here is a demo of the AI Assistant in action as it helps troubleshoot an application running with issues in a Kubernetes cluster: Large Language Models (LLMs) have transformed not just how we access data but also how we interact with it. The rise of tools like ChatGPT opened a world of possibilities, inspiring a wave of new applications. Asking questions or giving commands in natural language is intuitive, especially for users who aren&amp;rsquo;t deeply technical. Now everyone can quickly ask how to do X or Y, without feeling awkward or having to traverse pages and pages of documentation like before. Therefore, Headlamp AI Assistant brings a conversational UI to Headlamp , powered by LLMs that Headlamp users can configure with their own API keys. It is available as a Headlamp plugin, making it easy to integrate into your existing setup. Users can enable it by installing the plugin and configuring it with their own LLM API keys, giving them control over which model powers the assistant. Once enabled, the assistant becomes part of the Headlamp UI, ready to respond to contextual queries and perform actions directly from the interface. As expected, the AI Assistant is focused on helping users with Kubernetes concepts. Yet, while there is a lot of value in responding to Kubernetes related questions from Headlamp&amp;rsquo;s UI, we believe that the great benefit of such an integration is when it can use the context of what the user is experiencing in an application.</description></item><item><title>Simplify network connectivity using Tailscale with Amazon EKS Hybrid Nodes</title><link>https://kubermates.org/docs/2025-08-06-simplify-network-connectivity-using-tailscale-with-amazon-eks-hybrid-nodes/</link><pubDate>Wed, 06 Aug 2025 22:12:21 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-06-simplify-network-connectivity-using-tailscale-with-amazon-eks-hybrid-nodes/</guid><description>This post was co-authored with Lee Briggs, Director of Solutions Engineering at Tailscale. In this post, we guide you through integrating Tailscale with your Amazon Elastic Kubernetes Service (EKS) Hybrid Nodes environment. Amazon EKS Hybrid Nodes is a feature of Amazon EKS that enables you to streamline your Kubernetes management by connecting on-premises and edge infrastructure to an EKS cluster running in Amazon Web Services (AWS). This unified approach allows AWS to manage the Kubernetes control plane in the cloud while you maintain your hybrid nodes in on-premises or edge locations. We demonstrate how to configure a remote pod network and node address space. Install Tailscale on your hybrid nodes, set up a subnet router within your Amazon Virtual Private Cloud (Amazon VPC) , and update your AWS routes accordingly. This integration provides direct, encrypted connections that streamline the network architecture needed for EKS Hybrid Nodes. Although EKS Hybrid Nodes streamlines the Kubernetes management challenge, network connectivity between your on-premises infrastructure and AWS remains a critical requirement. Tailscale can help streamline this network connectivity between your EKS Hybrid Nodes data plane and Amazon EKS Kubernetes control plane. Unlike traditional VPNs, which tunnel all network traffic through a central gateway server, Tailscale creates a peer-to-peer mesh network (known as a tailnet ). It enables encrypted point-to-point connections using the open source WireGuard protocol, connecting devices and services across different networks with enhanced security features. However, you can still use Tailscale like a traditional VPN.</description></item><item><title>Kubernetes v1.34.0-rc.0</title><link>https://kubermates.org/releases/2025-08-06-kubernetes-v1-34-0-rc-0/</link><pubDate>Wed, 06 Aug 2025 20:20:08 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-06-kubernetes-v1-34-0-rc-0/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.34.0-rc.0">https://github.com/kubernetes/kubernetes/releases/tag/v1.34.0-rc.0&lt;/a>&lt;/p></description></item><item><title>v1.35.0-alpha.0</title><link>https://kubermates.org/releases/2025-08-06-v1-35-0-alpha-0/</link><pubDate>Wed, 06 Aug 2025 13:26:11 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-06-v1-35-0-alpha-0/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.35.0-alpha.0">https://github.com/kubernetes/kubernetes/releases/tag/v1.35.0-alpha.0&lt;/a>&lt;/p></description></item><item><title>August 06, 2025</title><link>https://kubermates.org/releases/2025-08-06-august-06-2025/</link><pubDate>Wed, 06 Aug 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-08-06-august-06-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#August_06_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#August_06_2025&lt;/a>&lt;/p></description></item><item><title>August 05, 2025</title><link>https://kubermates.org/releases/2025-08-05-august-05-2025/</link><pubDate>Tue, 05 Aug 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-08-05-august-05-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#August_05_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#August_05_2025&lt;/a>&lt;/p></description></item><item><title>August 01, 2025</title><link>https://kubermates.org/releases/2025-08-01-august-01-2025/</link><pubDate>Fri, 01 Aug 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-08-01-august-01-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#August_01_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#August_01_2025&lt;/a>&lt;/p></description></item><item><title>Amazon EKS platform version update</title><link>https://kubermates.org/releases/2025-07-30-amazon-eks-platform-version-update/</link><pubDate>Wed, 30 Jul 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-30-amazon-eks-platform-version-update/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/platform-versions.html">https://docs.aws.amazon.com/eks/latest/userguide/platform-versions.html&lt;/a>&lt;/p></description></item><item><title>July 28, 2025</title><link>https://kubermates.org/releases/2025-07-28-july-28-2025/</link><pubDate>Mon, 28 Jul 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-07-28-july-28-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#July_28_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#July_28_2025&lt;/a>&lt;/p></description></item><item><title>Kubernetes v1.34 Sneak Peek</title><link>https://kubermates.org/docs/2025-07-28-kubernetes-v1-34-sneak-peek/</link><pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-28-kubernetes-v1-34-sneak-peek/</guid><description>Kubernetes v1. 34 is coming at the end of August 2025. This release will not include any removal or deprecation, but it is packed with an impressive number of enhancements. Here are some of the features we are most excited about in this cycle! Please note that this information reflects the current state of v1. 34 development and may change before release. The following list highlights some of the notable enhancements likely to be included in the v1. 34 release, but is not an exhaustive list of all planned changes. This is not a commitment and the release content is subject to change. Dynamic Resource Allocation (DRA) provides a flexible way to categorize, request, and use devices like GPUs or custom hardware in your Kubernetes cluster. Since the v1. 30 release, DRA has been based around claiming devices using structured parameters that are opaque to the core of Kubernetes. The relevant enhancement proposal, KEP-4381 , took inspiration from dynamic provisioning for storage volumes.</description></item><item><title>July 25, 2025</title><link>https://kubermates.org/releases/2025-07-25-july-25-2025/</link><pubDate>Fri, 25 Jul 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-07-25-july-25-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#July_25_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#July_25_2025&lt;/a>&lt;/p></description></item><item><title>Scaling beyond IPv4: integrating IPv6 Amazon EKS clusters into existing Istio Service Mesh</title><link>https://kubermates.org/docs/2025-07-22-scaling-beyond-ipv4-integrating-ipv6-amazon-eks-clusters-into-existing-istio-ser/</link><pubDate>Tue, 22 Jul 2025 18:54:25 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-22-scaling-beyond-ipv4-integrating-ipv6-amazon-eks-clusters-into-existing-istio-ser/</guid><description>Organizations are increasingly adopting IPv6 for their Amazon Elastic Kubernetes Service (Amazon EKS) deployments, driven by three key factors: depletion of private IPv4 addresses, the need to streamline or eliminate overlay networks, and improved network security requirements on Amazon Web Services (AWS). In IPv6-enabled EKS clusters, each pod receives a unique IPv6 address from the Amazon Virtual Private Cloud (Amazon VPC) IPv6 range, with seamless compatibility facilitated by the Amazon EKS VPC Container Network Interface (CNI). This solution effectively addresses two major IPv4 limitations: the scarcity of private addresses and the security vulnerabilities created by overlapping IPv4 spaces that need Network Address Translation (NAT) at the node level. When transitioning to IPv6, you likely need to run both IPv4 and IPv6 EKS clusters simultaneously. This is particularly important for organizations using Istio Service Mesh with Amazon EKS, because IPv6 clusters must integrate with the existing Service Mesh and work smoothly alongside IPv4 clusters. To streamline this transition, you can configure your Istio Service Mesh to support both your current IPv4 EKS clusters and your new IPv6 EKS clusters. If Istio Service Mesh isnâ€™t part of your infrastructure, then we suggest exploring Amazon VPC Lattice as an alternative solution to speed up your IPv6 implementation on AWS. This post provides a step-by-step guide for combining IPv6-enabled EKS clusters with your existing Istio Service Mesh and IPv4 workloads, enabling a graceful transition to IPv6 on AWS. This guide covers detailed instructions for enabling communication between IPv6 and IPv4 EKS clusters, along with recommended practices for implementing IPv6 across both single and multiple VPC configurations. The functionality of Amazon EKS IPv6 builds on the native dual-stack capabilities of VPC. When you enable IPv6 in your VPC, it receives both IPv4 prefixes and a /56 IPv6 prefix. This IPv6 prefix can come from three sources: Amazonâ€™s Global Unicast Address (GUA) space, your own IPv6 range (BYOIPv6), or a Unique Local Address (ULA) space.</description></item><item><title>Deep dive into cluster networking for Amazon EKS Hybrid Nodes</title><link>https://kubermates.org/docs/2025-07-21-deep-dive-into-cluster-networking-for-amazon-eks-hybrid-nodes/</link><pubDate>Mon, 21 Jul 2025 22:22:52 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-21-deep-dive-into-cluster-networking-for-amazon-eks-hybrid-nodes/</guid><description>Amazon Elastic Kubernetes Service ( Amazon EKS ) Hybrid Nodes enables organizations to integrate their existing on-premises and edge computing infrastructure into EKS clusters as remote nodes. EKS Hybrid Nodes provides you with the flexibility to run your containerized applications wherever needed, while maintaining standardized Kubernetes management practices and addressing latency, compliance, and data residency needs. EKS Hybrid Nodes accelerates infrastructure modernization by repurposing existing hardware investments. Organizations can harness the elastic scalability, high availability, and fully managed advantages of Amazon EKS, while making sure of operational consistency through unified workflows and toolsets across hybrid environments. One of the key aspects of the EKS Hybrid Nodes solution is the hybrid network architecture between the cloud-based Amazon EKS control plane and your on-premises nodes. This post dives deep into the cluster networking configurations, guiding you through the process of integrating an EKS cluster with hybrid nodes in your existing infrastructure. In this walkthrough, we set up different Container Network Interface (CNI) options and load balancing solutions on EKS Hybrid Nodes to meet your networking requirements. EKS Hybrid Nodes needs private network connectivity between the cloud-hosted Amazon EKS control plane and the hybrid nodes running in your on-premises environment. This connectivity can be established using either Amazon Web Services (AWS) Direct Connect or AWS Site-to-Site VPN , through an AWS Transit Gateway or the Virtual Private Gateway into your Amazon Virtual Private Cloud (Amazon VPC). For an optimal experience, AWS recommends reliable network connectivity with at least 100 Mbps bandwidth, and a maximum of 200ms round-trip latency, for hybrid nodes connecting to the AWS Region. This is general guidance rather than a strict requirement, and specific bandwidth and latency requirements may differ based on the quantity of hybrid nodes and your applicationâ€™s unique characteristics. The node and pod Classless Inter-Domain Routing (CIDR) blocks for your hybrid nodes and container workloads must be within the IPv4 RFC-1918 ranges.</description></item><item><title>July 21, 2025</title><link>https://kubermates.org/releases/2025-07-21-july-21-2025/</link><pubDate>Mon, 21 Jul 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-07-21-july-21-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#July_21_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#July_21_2025&lt;/a>&lt;/p></description></item><item><title>ðŸ§© GitHub Actions Composite vs Reusable Workflows</title><link>https://kubermates.org/blog/github-actions-composite-vs-reusable-workflows-4bih/</link><pubDate>Fri, 18 Jul 2025 08:41:53 +0000</pubDate><guid>https://kubermates.org/blog/github-actions-composite-vs-reusable-workflows-4bih/</guid><description>&lt;h2 id="how-to-standardize-and-supercharge-your-cicd-pipelines-across-projects">How to standardize and supercharge your CI/CD pipelines across projects&lt;/h2>
&lt;p>When your teams manage multiple projects with similar deployment patterns, repeating the same GitHub Actions steps over and over can become tedious, error-prone, and hard to maintain&lt;/p>
&lt;p>Thankfully, GitHub Actions offers two powerful solutions to help &lt;strong>standardize, reuse, and scale your CI/CD pipelines&lt;/strong>: &lt;strong>Composite Actions&lt;/strong> and &lt;strong>Reusable Workflows&lt;/strong>. When used together, they form a clean, modular, and DRY (donâ€™t repeat yourself) CI/CD strategy&lt;/p></description></item><item><title>Post-Quantum Cryptography in Kubernetes</title><link>https://kubermates.org/docs/2025-07-18-post-quantum-cryptography-in-kubernetes/</link><pubDate>Fri, 18 Jul 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-18-post-quantum-cryptography-in-kubernetes/</guid><description>The world of cryptography is on the cusp of a major shift with the advent of quantum computing. While powerful quantum computers are still largely theoretical for many applications, their potential to break current cryptographic standards is a serious concern, especially for long-lived systems. This is where Post-Quantum Cryptography (PQC) comes in. In this article, I&amp;rsquo;ll dive into what PQC means for TLS and, more specifically, for the Kubernetes ecosystem. I&amp;rsquo;ll explain what the (suprising) state of PQC in Kubernetes is and what the implications are for current and future clusters. Post-Quantum Cryptography refers to cryptographic algorithms that are thought to be secure against attacks by both classical and quantum computers. The primary concern is that quantum computers, using algorithms like Shor&amp;rsquo;s Algorithm , could efficiently break widely used public-key cryptosystems such as RSA and Elliptic Curve Cryptography (ECC), which underpin much of today&amp;rsquo;s secure communication, including TLS. The industry is actively working on standardizing and adopting PQC algorithms. One of the first to be standardized by NIST is the Module-Lattice Key Encapsulation Mechanism ( ML-KEM ), formerly known as Kyber, and now standardized as FIPS-203 (PDF download). It is difficult to predict when quantum computers will be able to break classical algorithms. However, it is clear that we need to start migrating to PQC algorithms now, as the next section shows. To get a feeling for the predicted timeline we can look at a NIST report covering the transition to post-quantum cryptography standards.</description></item><item><title>Kubernetes v1.34.0-beta.0</title><link>https://kubermates.org/releases/2025-07-16-kubernetes-v1-34-0-beta-0/</link><pubDate>Wed, 16 Jul 2025 14:40:38 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-16-kubernetes-v1-34-0-beta-0/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.34.0-beta.0">https://github.com/kubernetes/kubernetes/releases/tag/v1.34.0-beta.0&lt;/a>&lt;/p></description></item><item><title>July 16, 2025</title><link>https://kubermates.org/releases/2025-07-16-july-16-2025/</link><pubDate>Wed, 16 Jul 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-07-16-july-16-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#July_16_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#July_16_2025&lt;/a>&lt;/p></description></item><item><title>Under the hood: Amazon EKS ultra scale clusters</title><link>https://kubermates.org/docs/2025-07-16-under-the-hood-amazon-eks-ultra-scale-clusters/</link><pubDate>Wed, 16 Jul 2025 00:14:37 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-16-under-the-hood-amazon-eks-ultra-scale-clusters/</guid><description>This post was co-authored by Shyam Jeedigunta, Principal Engineer, Amazon EKS; Apoorva Kulkarni, Sr. Specialist Solutions Architect, Containers and Raghav Tripathi, Sr. Software Dev Manager, Amazon EKS. Today, Amazon Elastic Kubernetes Service (Amazon EKS) announced support for clusters with up to 100,000 nodes. With Amazon EC2â€™s new generation accelerated computing instance types, this translates to 1. 6 million AWS Trainium chips or 800,000 NVIDIA GPUs in a single Kubernetes cluster. This unlocks ultra scale artificial intelligence (AI) and machine leaning (ML) workloads such as state-of-the-art model training, fine-tuning and agentic inference. Besides customers directly consuming Amazon EKS today, these improvements also extend to other AI/ML services like Amazon SageMaker HyperPod with EKS that leverage EKS as their compute layer, advancing AWSâ€™s overall ultra scale computing capabilities. Our customers have made it clear that containerization of training jobs and operators such as Kubeflow, the ability to streamline resource provisioning and lifecycle through projects like Karpenter, support for pluggable scheduling strategies, and access to a vast ecosystem of cloud-native tools is critical for their success in the AI/ML domain. Kubernetes has emerged as a key enabler here due to its powerful and extensible API model along with robust container orchestration capabilities, allowing accelerated workloads to scale quickly and run reliably. Through multiple technical innovations, architectural improvements and open-source collaboration, Amazon EKS has built the next generation of its cluster control plane and data plane for ultra scale, with full Kubernetes conformance. At AWS, we recommend customers running general-purpose applications with low coupling and horizontal scalability to follow a cell-based architecture as the strategy to sustain growth.</description></item><item><title>Amazon EKS enables ultra scale AI/ML workloads with support for 100K nodes per cluster</title><link>https://kubermates.org/docs/2025-07-16-amazon-eks-enables-ultra-scale-ai-ml-workloads-with-support-for-100k-nodes-per-c/</link><pubDate>Wed, 16 Jul 2025 00:14:26 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-16-amazon-eks-enables-ultra-scale-ai-ml-workloads-with-support-for-100k-nodes-per-c/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://aws.amazon.com/blogs/containers/amazon-eks-enables-ultra-scale-ai-ml-workloads-with-support-for-100k-nodes-per-cluster/">https://aws.amazon.com/blogs/containers/amazon-eks-enables-ultra-scale-ai-ml-workloads-with-support-for-100k-nodes-per-cluster/&lt;/a>&lt;/p></description></item><item><title>Kubernetes v1.33.3</title><link>https://kubermates.org/releases/2025-07-15-kubernetes-v1-33-3/</link><pubDate>Tue, 15 Jul 2025 22:19:31 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-15-kubernetes-v1-33-3/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.33.3">https://github.com/kubernetes/kubernetes/releases/tag/v1.33.3&lt;/a>&lt;/p></description></item><item><title>Kubernetes v1.32.7</title><link>https://kubermates.org/releases/2025-07-15-kubernetes-v1-32-7/</link><pubDate>Tue, 15 Jul 2025 22:18:27 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-15-kubernetes-v1-32-7/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.32.7">https://github.com/kubernetes/kubernetes/releases/tag/v1.32.7&lt;/a>&lt;/p></description></item><item><title>VPC CNI Multi-NIC feature for multi-homed pods</title><link>https://kubermates.org/releases/2025-07-15-vpc-cni-multi-nic-feature-for-multi-homed-pods/</link><pubDate>Tue, 15 Jul 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-15-vpc-cni-multi-nic-feature-for-multi-homed-pods/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/pod-multiple-network-interfaces.html">https://docs.aws.amazon.com/eks/latest/userguide/pod-multiple-network-interfaces.html&lt;/a>&lt;/p></description></item><item><title>July 14, 2025</title><link>https://kubermates.org/releases/2025-07-14-july-14-2025/</link><pubDate>Mon, 14 Jul 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-07-14-july-14-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#July_14_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#July_14_2025&lt;/a>&lt;/p></description></item><item><title>July 11, 2025</title><link>https://kubermates.org/releases/2025-07-11-july-11-2025/</link><pubDate>Fri, 11 Jul 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-07-11-july-11-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#July_11_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#July_11_2025&lt;/a>&lt;/p></description></item><item><title>Navigating Failures in Pods With Devices</title><link>https://kubermates.org/docs/2025-07-03-navigating-failures-in-pods-with-devices/</link><pubDate>Thu, 03 Jul 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-03-navigating-failures-in-pods-with-devices/</guid><description>Kubernetes is the de facto standard for container orchestration, but when it comes to handling specialized hardware like GPUs and other accelerators, things get a bit complicated. This blog post dives into the challenges of managing failure modes when operating pods with devices in Kubernetes, based on insights from Sergey Kanzhelev and Mrunal Patel&amp;rsquo;s talk at KubeCon NA 2024. You can follow the links to slides and recording. The rise of AI/ML workloads has brought new challenges to Kubernetes. These workloads often rely heavily on specialized hardware, and any device failure can significantly impact performance and lead to frustrating interruptions. As highlighted in the 2024 Llama paper , hardware issues, particularly GPU failures, are a major cause of disruption in AI/ML training. You can also learn how much effort NVIDIA spends on handling devices failures and maintenance in the KubeCon talk by Ryan Hallisey and Piotr Prokop All-Your-GPUs-Are-Belong-to-Us: An Inside Look at NVIDIA&amp;rsquo;s Self-Healing GeForce NOW Infrastructure ( recording ) as they see 19 remediation requests per 1000 nodes a day! We also see data centers offering spot consumption models and overcommit on power, making device failures commonplace and a part of the business model. However, Kubernetesâ€™s view on resources is still very static. The resource is either there or not. And if it is there, the assumption is that it will stay there fully functional - Kubernetes lacks good support for handling full or partial hardware failures. These long-existing assumptions combined with the overall complexity of a setup lead to a variety of failure modes, which we discuss here. Generally, all AI/ML workloads require specialized hardware, have challenging scheduling requirements, and are expensive when idle.</description></item><item><title>July 02, 2025</title><link>https://kubermates.org/releases/2025-07-02-july-02-2025/</link><pubDate>Wed, 02 Jul 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-07-02-july-02-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#July_02_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#July_02_2025&lt;/a>&lt;/p></description></item><item><title>VPC CNI troubleshooting content update</title><link>https://kubermates.org/releases/2025-06-30-vpc-cni-troubleshooting-content-update/</link><pubDate>Mon, 30 Jun 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-06-30-vpc-cni-troubleshooting-content-update/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/network-policies-troubleshooting.html">https://docs.aws.amazon.com/eks/latest/userguide/network-policies-troubleshooting.html&lt;/a>&lt;/p></description></item><item><title>June 27, 2025</title><link>https://kubermates.org/releases/2025-06-27-june-27-2025/</link><pubDate>Fri, 27 Jun 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-06-27-june-27-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#June_27_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#June_27_2025&lt;/a>&lt;/p></description></item><item><title>AWS managed policy updates</title><link>https://kubermates.org/releases/2025-06-26-aws-managed-policy-updates/</link><pubDate>Thu, 26 Jun 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-06-26-aws-managed-policy-updates/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/security-iam-awsmanpol.html">https://docs.aws.amazon.com/eks/latest/userguide/security-iam-awsmanpol.html&lt;/a>&lt;/p></description></item><item><title>June 25, 2025</title><link>https://kubermates.org/releases/2025-06-25-june-25-2025/</link><pubDate>Wed, 25 Jun 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-06-25-june-25-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#June_25_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#June_25_2025&lt;/a>&lt;/p></description></item><item><title>Image Compatibility In Cloud Native Environments</title><link>https://kubermates.org/docs/2025-06-25-image-compatibility-in-cloud-native-environments/</link><pubDate>Wed, 25 Jun 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-25-image-compatibility-in-cloud-native-environments/</guid><description>In industries where systems must run very reliably and meet strict performance criteria such as telecommunication, high-performance or AI computing, containerized applications often need specific operating system configuration or hardware presence. It is common practice to require the use of specific versions of the kernel, its configuration, device drivers, or system components. Despite the existence of the Open Container Initiative (OCI) , a governing community to define standards and specifications for container images, there has been a gap in expression of such compatibility requirements. The need to address this issue has led to different proposals and, ultimately, an implementation in Kubernetes&amp;rsquo; Node Feature Discovery (NFD). NFD is an open source Kubernetes project that automatically detects and reports hardware and system features of cluster nodes. This information helps users to schedule workloads on nodes that meet specific system requirements, which is especially useful for applications with strict hardware or operating system dependencies. A container image is built on a base image, which provides a minimal runtime environment, often a stripped-down Linux userland, completely empty or distroless. When an application requires certain features from the host OS, compatibility issues arise. These dependencies can manifest in several ways: While containers in Kubernetes are the most likely unit of abstraction for these needs, the definition of compatibility can extend further to include other container technologies such as Singularity and other OCI artifacts such as binaries from a spack binary cache. Containerized applications are deployed across various Kubernetes distributions and cloud providers, where different host operating systems introduce compatibility challenges. Often those have to be pre-configured before workload deployment or are immutable. For instance, different cloud providers will include different operating systems like: Each OS comes with unique kernel versions, configurations, and drivers, making compatibility a non-trivial issue for applications requiring specific features.</description></item><item><title>June 24, 2025</title><link>https://kubermates.org/releases/2025-06-24-june-24-2025/</link><pubDate>Tue, 24 Jun 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-06-24-june-24-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#June_24_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#June_24_2025&lt;/a>&lt;/p></description></item><item><title>June 18, 2025</title><link>https://kubermates.org/releases/2025-06-18-june-18-2025/</link><pubDate>Wed, 18 Jun 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-06-18-june-18-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#June_18_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#June_18_2025&lt;/a>&lt;/p></description></item><item><title>June 16, 2025</title><link>https://kubermates.org/releases/2025-06-16-june-16-2025/</link><pubDate>Mon, 16 Jun 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-06-16-june-16-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#June_16_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#June_16_2025&lt;/a>&lt;/p></description></item><item><title>Changes to Kubernetes Slack</title><link>https://kubermates.org/docs/2025-06-16-changes-to-kubernetes-slack/</link><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-16-changes-to-kubernetes-slack/</guid><description>UPDATE : Weâ€™ve received notice from Salesforce that our Slack workspace WILL NOT BE DOWNGRADED on June 20th. Stand by for more details, but for now, there is no urgency to back up private channels or direct messages. Kubernetes Slack will lose its special status and will be changing into a standard free Slack on June 20, 2025. Sometime later this year, our community may move to a new platform. If you are responsible for a channel or private channel, or a member of a User Group, you will need to take some actions as soon as you can. For the last decade, Slack has supported our project with a free customized enterprise account. They have let us know that they can no longer do so, particularly since our Slack is one of the largest and more active ones on the platform. As such, they will be downgrading it to a standard free Slack while we decide on, and implement, other options. On Friday, June 20, we will be subject to the feature limitations of free Slack. The primary ones which will affect us will be only retaining 90 days of history, and having to disable several apps and workflows which we are currently using. The Slack Admin team will do their best to manage these limitations. Responsible channel owners, members of private channels, and members of User Groups should take some actions to prepare for the upgrade and preserve information as soon as possible.</description></item><item><title>Amazon EKS Auto Mode update to NodeClass</title><link>https://kubermates.org/releases/2025-06-13-amazon-eks-auto-mode-update-to-nodeclass/</link><pubDate>Fri, 13 Jun 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-06-13-amazon-eks-auto-mode-update-to-nodeclass/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/create-node-class.html#auto-node-class-spec">https://docs.aws.amazon.com/eks/latest/userguide/create-node-class.html#auto-node-class-spec&lt;/a>&lt;/p></description></item><item><title>June 12, 2025</title><link>https://kubermates.org/releases/2025-06-12-june-12-2025/</link><pubDate>Thu, 12 Jun 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-06-12-june-12-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#June_12_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#June_12_2025&lt;/a>&lt;/p></description></item><item><title>Target secondary and cross-account roles with EKS Pod Identities</title><link>https://kubermates.org/releases/2025-06-11-target-secondary-and-cross-account-roles-with-eks-pod-identities/</link><pubDate>Wed, 11 Jun 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-06-11-target-secondary-and-cross-account-roles-with-eks-pod-identities/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/pod-id-assign-target-role.html">https://docs.aws.amazon.com/eks/latest/userguide/pod-id-assign-target-role.html&lt;/a>&lt;/p></description></item><item><title>June 10, 2025</title><link>https://kubermates.org/releases/2025-06-10-june-10-2025/</link><pubDate>Tue, 10 Jun 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-06-10-june-10-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#June_10_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#June_10_2025&lt;/a>&lt;/p></description></item><item><title>Enhancing Kubernetes Event Management with Custom Aggregation</title><link>https://kubermates.org/docs/2025-06-10-enhancing-kubernetes-event-management-with-custom-aggregation/</link><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-10-enhancing-kubernetes-event-management-with-custom-aggregation/</guid><description>Kubernetes Events provide crucial insights into cluster operations, but as clusters grow, managing and analyzing these events becomes increasingly challenging. This blog post explores how to build custom event aggregation systems that help engineering teams better understand cluster behavior and troubleshoot issues more effectively. In a Kubernetes cluster, events are generated for various operations - from pod scheduling and container starts to volume mounts and network configurations. While these events are invaluable for debugging and monitoring, several challenges emerge in production environments: To learn more about Events in Kubernetes, read the Event API reference. Consider a production environment with tens of microservices where the users report intermittent transaction failures: Traditional event aggregation process: Engineers are wasting hours sifting through thousands of standalone events spread across namespaces. By the time they look into it, the older events have long since purged, and correlating pod restarts to node-level issues is practically impossible. With its event aggregation in its custom events: The system groups events across resources, instantly surfacing correlation patterns such as volume mount timeouts before pod restarts. History indicates it occurred during past record traffic spikes, highlighting a storage scalability issue in minutes rather than hours. The beneï¬t of this approach is that organizations that implement it commonly cut down their troubleshooting time significantly along with increasing the reliability of systems by detecting patterns early. This post explores how to build a custom event aggregation system that addresses these challenges, aligned to Kubernetes best practices. I&amp;rsquo;ve picked the Go programming language for my example. This event aggregation system consists of three main components: Here&amp;rsquo;s a sketch for how to implement the event watcher: The event processor enriches events with additional context and classification: One of the key features you could implement is a way of correlating related Events.</description></item><item><title>Amazon EKS AWS Region expansion</title><link>https://kubermates.org/releases/2025-06-06-amazon-eks-aws-region-expansion/</link><pubDate>Fri, 06 Jun 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-06-06-amazon-eks-aws-region-expansion/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/doc-history.html">https://docs.aws.amazon.com/eks/latest/userguide/doc-history.html&lt;/a>&lt;/p></description></item><item><title>IPv6 access control for dual-stack public endpoints for new IPv6 clusters</title><link>https://kubermates.org/releases/2025-06-05-ipv6-access-control-for-dual-stack-public-endpoints-for-new-ipv6-clusters/</link><pubDate>Thu, 05 Jun 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-06-05-ipv6-access-control-for-dual-stack-public-endpoints-for-new-ipv6-clusters/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/#IPv6_access_control_for_dual-stack_public_endpoints_for_new_IPv6_clusters_2025-06-05">https://docs.aws.amazon.com/eks/latest/userguide/#IPv6_access_control_for_dual-stack_public_endpoints_for_new_IPv6_clusters_2025-06-05&lt;/a>&lt;/p></description></item><item><title>June 05, 2025</title><link>https://kubermates.org/releases/2025-06-05-june-05-2025/</link><pubDate>Thu, 05 Jun 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-06-05-june-05-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#June_05_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#June_05_2025&lt;/a>&lt;/p></description></item><item><title>Introducing Gateway API Inference Extension</title><link>https://kubermates.org/docs/2025-06-05-introducing-gateway-api-inference-extension/</link><pubDate>Thu, 05 Jun 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-05-introducing-gateway-api-inference-extension/</guid><description>Modern generative AI and large language model (LLM) services create unique traffic-routing challenges on Kubernetes. Unlike typical short-lived, stateless web requests, LLM inference sessions are often long-running, resource-intensive, and partially stateful. For example, a single GPU-backed model server may keep multiple inference sessions active and maintain in-memory token caches. Traditional load balancers focused on HTTP path or round-robin lack the specialized capabilities needed for these workloads. They also donâ€™t account for model identity or request criticality (e. g. , interactive chat vs. batch jobs). Organizations often patch together ad-hoc solutions, but a standardized approach is missing. Gateway API Inference Extension was created to address this gap by building on the existing Gateway API , adding inference-specific routing capabilities while retaining the familiar model of Gateways and HTTPRoutes. By adding an inference extension to your existing gateway, you effectively transform it into an Inference Gateway , enabling you to self-host GenAI/LLMs with a â€œmodel-as-a-serviceâ€ mindset. The projectâ€™s goal is to improve and standardize routing to inference workloads across the ecosystem.</description></item><item><title>Start Sidecar First: How To Avoid Snags</title><link>https://kubermates.org/docs/2025-06-03-start-sidecar-first-how-to-avoid-snags/</link><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-03-start-sidecar-first-how-to-avoid-snags/</guid><description>From the Kubernetes Multicontainer Pods: An Overview blog post you know what their job is, what are the main architectural patterns, and how they are implemented in Kubernetes. The main thing Iâ€™ll cover in this article is how to ensure that your sidecar containers start before the main app. Itâ€™s more complicated than you might think! I&amp;rsquo;d just like to remind readers that the v1. 29. 0 release of Kubernetes added native support for sidecar containers , which can now be defined within the. spec. initContainers field, but with restartPolicy: Always. You can see that illustrated in the following example Pod manifest snippet: What are the specifics of defining sidecars with a. spec. initContainers block, rather than as a legacy multi-container pod with multiple. spec. containers ? Well, all.</description></item><item><title>Gateway API v1.3.0: Advancements in Request Mirroring, CORS, Gateway Merging, and Retry Budgets</title><link>https://kubermates.org/docs/2025-06-02-gateway-api-v1-3-0-advancements-in-request-mirroring-cors-gateway-merging-and-re/</link><pubDate>Mon, 02 Jun 2025 09:00:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-06-02-gateway-api-v1-3-0-advancements-in-request-mirroring-cors-gateway-merging-and-re/</guid><description>Join us in the Kubernetes SIG Network community in celebrating the general availability of Gateway API v1. 3. 0! We are also pleased to announce that there are already a number of conformant implementations to try, made possible by postponing this blog announcement. Version 1. 3. 0 of the API was released about a month ago on April 24, 2025. Gateway API v1. 3. 0 brings a new feature to the Standard channel (Gateway API&amp;rsquo;s GA release channel): percentage-based request mirroring , and introduces three new experimental features: cross-origin resource sharing (CORS) filters, a standardized mechanism for listener and gateway merging, and retry budgets. Also see the full release notes and applaud the v1. 3. 0 release team next time you see them.</description></item><item><title>May 30, 2025</title><link>https://kubermates.org/releases/2025-05-30-may-30-2025/</link><pubDate>Fri, 30 May 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-05-30-may-30-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#May_30_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#May_30_2025&lt;/a>&lt;/p></description></item><item><title>Kubernetes version 1.33</title><link>https://kubermates.org/releases/2025-05-29-kubernetes-version-1-33/</link><pubDate>Thu, 29 May 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-05-29-kubernetes-version-1-33/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions-standard.html#kubernetes-1-33">https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions-standard.html#kubernetes-1-33&lt;/a>&lt;/p></description></item><item><title>New cluster insights for EKS Hybrid Nodes</title><link>https://kubermates.org/releases/2025-05-29-new-cluster-insights-for-eks-hybrid-nodes/</link><pubDate>Thu, 29 May 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-05-29-new-cluster-insights-for-eks-hybrid-nodes/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/cluster-insights.html">https://docs.aws.amazon.com/eks/latest/userguide/cluster-insights.html&lt;/a>&lt;/p></description></item><item><title>May 27, 2025</title><link>https://kubermates.org/releases/2025-05-27-may-27-2025/</link><pubDate>Tue, 27 May 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-05-27-may-27-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#May_27_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#May_27_2025&lt;/a>&lt;/p></description></item><item><title>Add-on support for Amazon FSx CSI driver</title><link>https://kubermates.org/releases/2025-05-23-add-on-support-for-amazon-fsx-csi-driver/</link><pubDate>Fri, 23 May 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-05-23-add-on-support-for-amazon-fsx-csi-driver/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/fsx-csi.html">https://docs.aws.amazon.com/eks/latest/userguide/fsx-csi.html&lt;/a>&lt;/p></description></item><item><title>May 23, 2025</title><link>https://kubermates.org/releases/2025-05-23-may-23-2025/</link><pubDate>Fri, 23 May 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-05-23-may-23-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#May_23_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#May_23_2025&lt;/a>&lt;/p></description></item><item><title>Edit Prometheus scrapers in the console</title><link>https://kubermates.org/releases/2025-05-22-edit-prometheus-scrapers-in-the-console/</link><pubDate>Thu, 22 May 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-05-22-edit-prometheus-scrapers-in-the-console/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/prometheus.html#viewing-prometheus-scraper-details">https://docs.aws.amazon.com/eks/latest/userguide/prometheus.html#viewing-prometheus-scraper-details&lt;/a>&lt;/p></description></item><item><title>May 22, 2025</title><link>https://kubermates.org/releases/2025-05-22-may-22-2025/</link><pubDate>Thu, 22 May 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-05-22-may-22-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#May_22_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#May_22_2025&lt;/a>&lt;/p></description></item><item><title>May 20, 2025</title><link>https://kubermates.org/releases/2025-05-20-may-20-2025/</link><pubDate>Tue, 20 May 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-05-20-may-20-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#May_20_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#May_20_2025&lt;/a>&lt;/p></description></item><item><title>Kubernetes v1.33: In-Place Pod Resize Graduated to Beta</title><link>https://kubermates.org/docs/2025-05-16-kubernetes-v1-33-in-place-pod-resize-graduated-to-beta/</link><pubDate>Fri, 16 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-16-kubernetes-v1-33-in-place-pod-resize-graduated-to-beta/</guid><description>On behalf of the Kubernetes project, I am excited to announce that the in-place Pod resize feature (also known as In-Place Pod Vertical Scaling), first introduced as alpha in Kubernetes v1. 27, has graduated to Beta and will be enabled by default in the Kubernetes v1. 33 release! This marks a significant milestone in making resource management for Kubernetes workloads more flexible and less disruptive. Traditionally, changing the CPU or memory resources allocated to a container required restarting the Pod. While acceptable for many stateless applications, this could be disruptive for stateful services, batch jobs, or any workloads sensitive to restarts. In-place Pod resizing allows you to change the CPU and memory requests and limits assigned to containers within a running Pod, often without requiring a container restart. Here&amp;rsquo;s the core idea: You can try it out on a v1. 33 Kubernetes cluster by using kubectl to edit a Pod (requires kubectl v1. 32+): For detailed usage instructions and examples, please refer to the official Kubernetes documentation: Resize CPU and Memory Resources assigned to Containers. Kubernetes still excels at scaling workloads horizontally (adding or removing replicas), but in-place Pod resizing unlocks several key benefits for vertical scaling: Since the alpha release in v1. 27, significant work has gone into maturing the feature, improving its stability, and refining the user experience based on feedback and further development. Here are the key changes: Graduating to Beta means the feature is ready for broader adoption, but development doesn&amp;rsquo;t stop here! Here&amp;rsquo;s what the community is focusing on next: With the InPlacePodVerticalScaling feature gate enabled by default in v1.</description></item><item><title>May 16, 2025</title><link>https://kubermates.org/releases/2025-05-16-may-16-2025/</link><pubDate>Fri, 16 May 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-05-16-may-16-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#May_16_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#May_16_2025&lt;/a>&lt;/p></description></item><item><title>Announcing etcd v3.6.0</title><link>https://kubermates.org/docs/2025-05-15-announcing-etcd-v3-6-0/</link><pubDate>Thu, 15 May 2025 16:00:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-15-announcing-etcd-v3-6-0/</guid><description>This announcement originally appeared on the etcd blog. Today, we are releasing etcd v3. 6. 0 , the first minor release since etcd v3. 5. 0 on June 15, 2021. This release introduces several new features, makes significant progress on long-standing efforts like downgrade support and migration to v3store, and addresses numerous critical &amp;amp; major issues. It also includes major optimizations in memory usage, improving efficiency and performance. In addition to the features of v3. 6. 0, etcd has joined Kubernetes as a SIG (sig-etcd), enabling us to improve project sustainability. We&amp;rsquo;ve introduced systematic robustness testing to ensure correctness and reliability.</description></item><item><title>Kubernetes 1.33: Job's SuccessPolicy Goes GA</title><link>https://kubermates.org/docs/2025-05-15-kubernetes-1-33-job-s-successpolicy-goes-ga/</link><pubDate>Thu, 15 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-15-kubernetes-1-33-job-s-successpolicy-goes-ga/</guid><description>On behalf of the Kubernetes project, I&amp;rsquo;m pleased to announce that Job success policy has graduated to General Availability (GA) as part of the v1. 33 release. In batch workloads, you might want to use leader-follower patterns like MPI , in which the leader controls the execution, including the followers&amp;rsquo; lifecycle. In this case, you might want to mark it as succeeded even if some of the indexes failed. Unfortunately, a leader-follower Kubernetes Job that didn&amp;rsquo;t use a success policy, in most cases, would have to require all Pods to finish successfully for that Job to reach an overall succeeded state. For Kubernetes Jobs, the API allows you to specify the early exit criteria using the. spec. successPolicy field (you can only use the. spec. successPolicy field for an indexed Job ). Which describes a set of rules either using a list of succeeded indexes for a job, or defining a minimal required size of succeeded indexes. This newly stable field is especially valuable for scientific simulation, AI/ML and High-Performance Computing (HPC) batch workloads.</description></item><item><title>Kubernetes v1.33: Updates to Container Lifecycle</title><link>https://kubermates.org/docs/2025-05-14-kubernetes-v1-33-updates-to-container-lifecycle/</link><pubDate>Wed, 14 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-14-kubernetes-v1-33-updates-to-container-lifecycle/</guid><description>Kubernetes v1. 33 introduces a few updates to the lifecycle of containers. The Sleep action for container lifecycle hooks now supports a zero sleep duration (feature enabled by default). There is also alpha support for customizing the stop signal sent to containers when they are being terminated. This blog post goes into the details of these new aspects of the container lifecycle, and how you can use them. Kubernetes v1. 29 introduced the Sleep action for container PreStop and PostStart Lifecycle hooks. The Sleep action lets your containers pause for a specified duration after the container is started or before it is terminated. This was needed to provide a straightforward way to manage graceful shutdowns. Before the Sleep action, folks used to run the sleep command using the exec action in their container lifecycle hooks. If you wanted to do this you&amp;rsquo;d need to have the binary for the sleep command in your container image. This is difficult if you&amp;rsquo;re using third party images.</description></item><item><title>Kubernetes v1.33: Job's Backoff Limit Per Index Goes GA</title><link>https://kubermates.org/docs/2025-05-13-kubernetes-v1-33-job-s-backoff-limit-per-index-goes-ga/</link><pubDate>Tue, 13 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-13-kubernetes-v1-33-job-s-backoff-limit-per-index-goes-ga/</guid><description>In Kubernetes v1. 33, the Backoff Limit Per Index feature reaches general availability (GA). This blog describes the Backoff Limit Per Index feature and its benefits. When you run workloads on Kubernetes, you must consider scenarios where Pod failures can affect the completion of your workloads. Ideally, your workload should tolerate transient failures and continue running. To achieve failure tolerance in a Kubernetes Job, you can set the spec. backoffLimit field. This field specifies the total number of tolerated failures. However, for workloads where every index is considered independent, like embarassingly parallel workloads - the spec. backoffLimit field is often not flexible enough. For example, you may choose to run multiple suites of integration tests by representing each suite as an index within an Indexed Job. In that setup, a fast-failing index (test suite) is likely to consume your entire budget for tolerating Pod failures, and you might not be able to run the other indexes.</description></item><item><title>May 13, 2025</title><link>https://kubermates.org/releases/2025-05-13-may-13-2025/</link><pubDate>Tue, 13 May 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-05-13-may-13-2025/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#May_13_2025">https://cloud.google.com/kubernetes-engine/docs/release-notes#May_13_2025&lt;/a>&lt;/p></description></item><item><title>Kubernetes v1.33: Image Pull Policy the way you always thought it worked!</title><link>https://kubermates.org/docs/2025-05-12-kubernetes-v1-33-image-pull-policy-the-way-you-always-thought-it-worked/</link><pubDate>Mon, 12 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-12-kubernetes-v1-33-image-pull-policy-the-way-you-always-thought-it-worked/</guid><description>Some things in Kubernetes are surprising, and the way imagePullPolicy behaves might be one of them. Given Kubernetes is all about running pods, it may be peculiar to learn that there has been a caveat to restricting pod access to authenticated images for over 10 years in the form of issue 18787 ! It is an exciting release when you can resolve a ten-year-old issue. The gist of the problem is that the imagePullPolicy: IfNotPresent strategy has done precisely what it says, and nothing more. Let&amp;rsquo;s set up a scenario. To begin, Pod A in Namespace X is scheduled to Node 1 and requires image Foo from a private repository. For it&amp;rsquo;s image pull authentication material, the pod references Secret 1 in its imagePullSecrets. Secret 1 contains the necessary credentials to pull from the private repository. The Kubelet will utilize the credentials from Secret 1 as supplied by Pod A and it will pull container image Foo from the registry. This is the intended (and secure) behavior. But now things get curious. If Pod B in Namespace Y happens to also be scheduled to Node 1 , unexpected (and potentially insecure) things happen. Pod B may reference the same private image, specifying the IfNotPresent image pull policy.</description></item><item><title>Kubernetes v1.33: Streaming List responses</title><link>https://kubermates.org/docs/2025-05-09-kubernetes-v1-33-streaming-list-responses/</link><pubDate>Fri, 09 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-09-kubernetes-v1-33-streaming-list-responses/</guid><description>Managing Kubernetes cluster stability becomes increasingly critical as your infrastructure grows. One of the most challenging aspects of operating large-scale clusters has been handling List requests that fetch substantial datasets - a common operation that could unexpectedly impact your cluster&amp;rsquo;s stability. Today, the Kubernetes community is excited to announce a significant architectural improvement: streaming encoding for List responses. Current API response encoders just serialize an entire response into a single contiguous memory and perform one ResponseWriter. Write call to transmit data to the client. Despite HTTP/2&amp;rsquo;s capability to split responses into smaller frames for transmission, the underlying HTTP server continues to hold the complete response data as a single buffer. Even as individual frames are transmitted to the client, the memory associated with these frames cannot be freed incrementally. When cluster size grows, the single response body can be substantial - like hundreds of megabytes in size. At large scale, the current approach becomes particularly inefficient, as it prevents incremental memory release during transmission. Imagining that when network congestion occurs, that large response bodyâ€™s memory block stays active for tens of seconds or even minutes. This limitation leads to unnecessarily high and prolonged memory consumption in the kube-apiserver process. If multiple large List requests occur simultaneously, the cumulative memory consumption can escalate rapidly, potentially leading to an Out-of-Memory (OOM) situation that compromises cluster stability.</description></item><item><title>Kubernetes 1.33: Volume Populators Graduate to GA</title><link>https://kubermates.org/docs/2025-05-08-kubernetes-1-33-volume-populators-graduate-to-ga/</link><pubDate>Thu, 08 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-08-kubernetes-1-33-volume-populators-graduate-to-ga/</guid><description>Kubernetes volume populators are now generally available (GA)! The AnyVolumeDataSource feature gate is treated as always enabled for Kubernetes v1. 33, which means that users can specify any appropriate custom resource as the data source of a PersistentVolumeClaim (PVC). An example of how to use dataSourceRef in PVC: There are four major enhancements from beta. During the beta phase, contributors to Kubernetes identified potential resource leaks with PersistentVolumeClaim (PVC) deletion while volume population was in progress; these leaks happened due to limitations in finalizer handling. Ahead of the graduation to general availability, the Kubernetes project added support to delete temporary resources (PVC prime, etc. ) if the original PVC is deleted. To accommodate this, we&amp;rsquo;ve introduced three new plugin-based functions: A provider example is added in lib-volume-populator/example. For GA, the CSI volume populator controller code gained a MutatorConfig , allowing the specification of mutator functions to modify Kubernetes resources. For example, if the PVC prime is not an exact copy of the PVC and you need provider-specific information for the driver, you can include this information in the optional MutatorConfig. This allows you to customize the Kubernetes objects in the volume populator. Our beta phase highlighted a new requirement: the need to aggregate metrics not just from lib-volume-populator, but also from other components within the provider&amp;rsquo;s codebase. To address this, SIG Storage introduced a provider metric manager.</description></item><item><title>Kubernetes v1.33: From Secrets to Service Accounts: Kubernetes Image Pulls Evolved</title><link>https://kubermates.org/docs/2025-05-07-kubernetes-v1-33-from-secrets-to-service-accounts-kubernetes-image-pulls-evolved/</link><pubDate>Wed, 07 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-07-kubernetes-v1-33-from-secrets-to-service-accounts-kubernetes-image-pulls-evolved/</guid><description>Kubernetes has steadily evolved to reduce reliance on long-lived credentials stored in the API. A prime example of this shift is the transition of Kubernetes Service Account (KSA) tokens from long-lived, static tokens to ephemeral, automatically rotated tokens with OpenID Connect (OIDC)-compliant semantics. This advancement enables workloads to securely authenticate with external services without needing persistent secrets. However, one major gap remains: image pull authentication. Today, Kubernetes clusters rely on image pull secrets stored in the API, which are long-lived and difficult to rotate, or on node-level kubelet credential providers, which allow any pod running on a node to access the same credentials. This presents security and operational challenges. To address this, Kubernetes is introducing Service Account Token Integration for Kubelet Credential Providers , now available in alpha. This enhancement allows credential providers to use pod-specific service account tokens to obtain registry credentials, which kubelet can then use for image pulls â€” eliminating the need for long-lived image pull secrets. Currently, Kubernetes administrators have two primary options for handling private container image pulls: Image pull secrets stored in the Kubernetes API Kubelet credential providers Neither approach aligns with the principles of least privilege or ephemeral authentication , leaving Kubernetes with a security gap. This new enhancement enables kubelet credential providers to use workload identity when fetching image registry credentials. Instead of relying on long-lived secrets, credential providers can use service account tokens to request short-lived credentials tied to a specific podâ€™s identity. This approach provides: Kubelet generates short-lived, automatically rotated tokens for service accounts if the credential provider it communicates with has opted into receiving a service account token for image pulls.</description></item><item><title>Kubernetes v1.33: Fine-grained SupplementalGroups Control Graduates to Beta</title><link>https://kubermates.org/docs/2025-05-06-kubernetes-v1-33-fine-grained-supplementalgroups-control-graduates-to-beta/</link><pubDate>Tue, 06 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-06-kubernetes-v1-33-fine-grained-supplementalgroups-control-graduates-to-beta/</guid><description>The new field, supplementalGroupsPolicy , was introduced as an opt-in alpha feature for Kubernetes v1. 31 and has graduated to beta in v1. 33; the corresponding feature gate ( SupplementalGroupsPolicy ) is now enabled by default. This feature enables to implement more precise control over supplemental groups in containers that can strengthen the security posture, particularly in accessing volumes. Moreover, it also enhances the transparency of UID/GID details in containers, offering improved security oversight. Please be aware that this beta release contains some behavioral breaking change. See The Behavioral Changes Introduced In Beta and Upgrade Considerations sections for details. Although the majority of Kubernetes cluster admins/users may not be aware, kubernetes, by default, merges group information from the Pod with information defined in /etc/group in the container image. Let&amp;rsquo;s see an example, below Pod manifest specifies runAsUser=1000 , runAsGroup=3000 and supplementalGroups=4000 in the Pod&amp;rsquo;s security context. What is the result of id command in the ctr container? The output should be similar to this: Where does group ID 50000 in supplementary groups ( groups field) come from, even though 50000 is not defined in the Pod&amp;rsquo;s manifest at all? The answer is /etc/group file in the container image. Checking the contents of /etc/group in the container image should show below: This shows that the container&amp;rsquo;s primary user 1000 belongs to the group 50000 in the last entry. Thus, the group membership defined in /etc/group in the container image for the container&amp;rsquo;s primary user is implicitly merged to the information from the Pod.</description></item><item><title>Kubernetes v1.33: Prevent PersistentVolume Leaks When Deleting out of Order graduates to GA</title><link>https://kubermates.org/docs/2025-05-05-kubernetes-v1-33-prevent-persistentvolume-leaks-when-deleting-out-of-order-gradu/</link><pubDate>Mon, 05 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-05-kubernetes-v1-33-prevent-persistentvolume-leaks-when-deleting-out-of-order-gradu/</guid><description>I am thrilled to announce that the feature to prevent PersistentVolume (or PVs for short) leaks when deleting out of order has graduated to General Availability (GA) in Kubernetes v1. 33! This improvement, initially introduced as a beta feature in Kubernetes v1. 31, ensures that your storage resources are properly reclaimed, preventing unwanted leaks. PersistentVolumeClaim (or PVC for short) is a user&amp;rsquo;s request for storage. A PV and PVC are considered Bound if a newly created PV or a matching PV is found. The PVs themselves are backed by volumes allocated by the storage backend. Normally, if the volume is to be deleted, then the expectation is to delete the PVC for a bound PV-PVC pair. However, there are no restrictions on deleting a PV before deleting a PVC. For a Bound PV-PVC pair, the ordering of PV-PVC deletion determines whether the PV reclaim policy is honored. The reclaim policy is honored if the PVC is deleted first; however, if the PV is deleted prior to deleting the PVC, then the reclaim policy is not exercised. As a result of this behavior, the associated storage asset in the external infrastructure is not removed. With the graduation to GA in Kubernetes v1.</description></item><item><title>Kubernetes v1.33: Mutable CSI Node Allocatable Count</title><link>https://kubermates.org/docs/2025-05-02-kubernetes-v1-33-mutable-csi-node-allocatable-count/</link><pubDate>Fri, 02 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-02-kubernetes-v1-33-mutable-csi-node-allocatable-count/</guid><description>Scheduling stateful applications reliably depends heavily on accurate information about resource availability on nodes. Kubernetes v1. 33 introduces an alpha feature called mutable CSI node allocatable count , allowing Container Storage Interface (CSI) drivers to dynamically update the reported maximum number of volumes that a node can handle. This capability significantly enhances the accuracy of pod scheduling decisions and reduces scheduling failures caused by outdated volume capacity information. Traditionally, Kubernetes CSI drivers report a static maximum volume attachment limit when initializing. However, actual attachment capacities can change during a node&amp;rsquo;s lifecycle for various&amp;hellip;</description></item><item><title>Kubernetes v1.33: New features in DRA</title><link>https://kubermates.org/docs/2025-05-01-kubernetes-v1-33-new-features-in-dra/</link><pubDate>Thu, 01 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-01-kubernetes-v1-33-new-features-in-dra/</guid><description>Kubernetes Dynamic Resource Allocation (DRA) was originally introduced as an alpha feature in the v1. 26 release, and then went through a significant redesign for Kubernetes v1. 31. The main DRA feature went to beta in v1. 32, and the project hopes it will be generally available in Kubernetes v1. 34.</description></item><item><title>Kubernetes v1.33: Storage Capacity Scoring of Nodes for Dynamic Provisioning (alpha)</title><link>https://kubermates.org/docs/2025-04-30-kubernetes-v1-33-storage-capacity-scoring-of-nodes-for-dynamic-provisioning-alph/</link><pubDate>Wed, 30 Apr 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-04-30-kubernetes-v1-33-storage-capacity-scoring-of-nodes-for-dynamic-provisioning-alph/</guid><description>Kubernetes v1. 33 introduces a new alpha feature called StorageCapacityScoring. This feature adds a scoring method for pod scheduling with the topology-aware volume provisioning. This feature eases to schedule pods on nodes with either the most or least available storage capacity. This feature extends the kube-scheduler&amp;rsquo;s VolumeBinding plugin to perform scoring using node storage capacity information obtained from Storage Capacity. Currently, you can only filter out nodes with insufficient storage capacity.</description></item><item><title>Bottlerocket for hybrid nodes</title><link>https://kubermates.org/releases/2025-04-29-bottlerocket-for-hybrid-nodes/</link><pubDate>Tue, 29 Apr 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-04-29-bottlerocket-for-hybrid-nodes/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/hybrid-nodes-bottlerocket.html">https://docs.aws.amazon.com/eks/latest/userguide/hybrid-nodes-bottlerocket.html&lt;/a>&lt;/p></description></item><item><title>Kubernetes v1.33: Image Volumes graduate to beta!</title><link>https://kubermates.org/docs/2025-04-29-kubernetes-v1-33-image-volumes-graduate-to-beta/</link><pubDate>Tue, 29 Apr 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-04-29-kubernetes-v1-33-image-volumes-graduate-to-beta/</guid><description>Image Volumes were introduced as an Alpha feature with the Kubernetes v1. 31 release as part of KEP-4639. In Kubernetes v1. 33, this feature graduates to beta. Please note that the feature is still disabled by default, because not all container runtimes have full support for it. CRI-O supports the initial feature since version v1.</description></item><item><title>Kubernetes v1.33: HorizontalPodAutoscaler Configurable Tolerance</title><link>https://kubermates.org/docs/2025-04-28-kubernetes-v1-33-horizontalpodautoscaler-configurable-tolerance/</link><pubDate>Mon, 28 Apr 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-04-28-kubernetes-v1-33-horizontalpodautoscaler-configurable-tolerance/</guid><description>This post describes configurable tolerance for horizontal Pod autoscaling , a new alpha feature first available in Kubernetes 1. 33. Horizontal Pod Autoscaling is a well-known Kubernetes feature that allows your workload to automatically resize by adding or removing replicas based on resource utilization. Let&amp;rsquo;s say you have a web application running in a Kubernetes cluster with 50 replicas. You configure the HorizontalPodAutoscaler (HPA) to scale based on CPU utilization, with a target of 75% utilization. Now, imagine that the current CPU utilization across all replicas is 90%, which is higher than the desired 75%.</description></item><item><title>Kubernetes v1.33: User Namespaces enabled by default!</title><link>https://kubermates.org/docs/2025-04-25-kubernetes-v1-33-user-namespaces-enabled-by-default/</link><pubDate>Fri, 25 Apr 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-04-25-kubernetes-v1-33-user-namespaces-enabled-by-default/</guid><description>In Kubernetes v1. 33 support for user namespaces is enabled by default. This means that, when the stack requirements are met, pods can opt-in to use user namespaces. To use the feature there is no need to enable any Kubernetes feature flag anymore! In this blog post we answer some common questions about user namespaces. But, before we dive into that, let&amp;rsquo;s recap what user namespaces are and why they are important. Note: Linux user namespaces are a different concept from Kubernetes namespaces.</description></item><item><title>Kubernetes v1.33: Continuing the transition from Endpoints to EndpointSlices</title><link>https://kubermates.org/docs/2025-04-24-kubernetes-v1-33-continuing-the-transition-from-endpoints-to-endpointslices/</link><pubDate>Thu, 24 Apr 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-04-24-kubernetes-v1-33-continuing-the-transition-from-endpoints-to-endpointslices/</guid><description>Since the addition of EndpointSlices ( KEP-752 ) as alpha in v1. 15 and later GA in v1. 21, the Endpoints API in Kubernetes has been gathering dust. New Service features like dual-stack networking and traffic distribution are only supported via the EndpointSlice API, so all service proxies, Gateway API implementations, and similar controllers have had to be ported from using Endpoints to using EndpointSlices. At this point, the Endpoints API is really only there to avoid breaking end user workloads and scripts that still make use of it. As of Kubernetes 1.</description></item><item><title>Kubernetes v1.33: Octarine</title><link>https://kubermates.org/docs/2025-04-23-kubernetes-v1-33-octarine/</link><pubDate>Wed, 23 Apr 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-04-23-kubernetes-v1-33-octarine/</guid><description>Editors: Agustina Barbetta, Aakanksha Bhende, Udi Hofesh, Ryota Sawada, Sneha Yadav Similar to previous releases, the release of Kubernetes v1. 33 introduces new stable, beta, and alpha features. The consistent delivery of high-quality releases underscores the strength of our development cycle and the vibrant support from our community. This release consists of 64 enhancements. Of those enhancements, 18 have graduated to Stable, 20 are entering Beta, 24 have entered Alpha, and 2 are deprecated or withdrawn. There are also several notable deprecations and removals in this release; make sure to read about those if you already run an older version of Kubernetes.</description></item><item><title>Kubernetes Multicontainer Pods: An Overview</title><link>https://kubermates.org/docs/2025-04-22-kubernetes-multicontainer-pods-an-overview/</link><pubDate>Tue, 22 Apr 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-22-kubernetes-multicontainer-pods-an-overview/</guid><description>As cloud-native architectures continue to evolve, Kubernetes has become the go-to platform for deploying complex, distributed systems. One of the most powerful yet nuanced design patterns in this ecosystem is the sidecar patternâ€”a technique that allows developers to extend application functionality without diving deep into source code. Think of a sidecar like a trusty companion motorcycle attachment. Historically, IT infrastructures have always used auxiliary services to handle critical tasks. Before containers, we relied on background processes and helper daemons to manage logging, monitoring, and networking. The microservices revolution transformed this approach, making sidecars a struc&amp;hellip;</description></item><item><title>New concepts pages for hybrid networking</title><link>https://kubermates.org/releases/2025-04-18-new-concepts-pages-for-hybrid-networking/</link><pubDate>Fri, 18 Apr 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-04-18-new-concepts-pages-for-hybrid-networking/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/hybrid-nodes-concepts.html">https://docs.aws.amazon.com/eks/latest/userguide/hybrid-nodes-concepts.html&lt;/a>&lt;/p></description></item><item><title>ðŸš€ðŸŒ Elevating Infrastructure: From Terraform/Terragrunt Foundations to Platform Engineering ðŸ˜Š</title><link>https://kubermates.org/blog/elevating-infrastructure-from-terraformterragrunt-foundations-to-platform-engineering-41fc/</link><pubDate>Fri, 18 Apr 2025 10:24:24 +0000</pubDate><guid>https://kubermates.org/blog/elevating-infrastructure-from-terraformterragrunt-foundations-to-platform-engineering-41fc/</guid><description>&lt;p>Hey there, cloud adventurers! ðŸš€ Letâ€™s chat about why keeping &lt;strong>Terraform&lt;/strong> (or &lt;strong>OpenTofu&lt;/strong>) and &lt;strong>Terragrunt&lt;/strong> in their own lanes is absolutely essentialâ€”and how using &lt;strong>Terraform JSON tfvars&lt;/strong> makes life easier when youâ€™re building nifty tools on top. Ready? Letâ€™s dive in! ðŸ˜„&lt;/p>
&lt;h3 id="why-separation-is-a-must-not-an-option-">Why Separation Is a Must, Not an Option ðŸ™…â€â™‚ï¸ðŸ™…â€â™€ï¸&lt;/h3>
&lt;p>It might be tempting to mix Terraform and Terragrunt into one big fileâ€”after all, they work together, right? But trust me, keeping them decoupled is a gameâ€‘changer:&lt;/p></description></item><item><title>AWS managed policy updates</title><link>https://kubermates.org/releases/2025-04-14-aws-managed-policy-updates/</link><pubDate>Mon, 14 Apr 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-04-14-aws-managed-policy-updates/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/security-iam-awsmanpol.html#security-iam-awsmanpol-updates">https://docs.aws.amazon.com/eks/latest/userguide/security-iam-awsmanpol.html#security-iam-awsmanpol-updates&lt;/a>&lt;/p></description></item><item><title>Introducing kube-scheduler-simulator</title><link>https://kubermates.org/docs/2025-04-07-introducing-kube-scheduler-simulator/</link><pubDate>Mon, 07 Apr 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-07-introducing-kube-scheduler-simulator/</guid><description>The Kubernetes Scheduler is a crucial control plane component that determines which node a Pod will run on. Thus, anyone utilizing Kubernetes relies on a scheduler. kube-scheduler-simulator is a simulator for the Kubernetes scheduler, that started as a Google Summer of Code 2021 project developed by me (Kensei Nakada) and later received a lot of contributions. This tool allows users to closely examine the schedulerâ€™s behavior and decisions. It is useful for casual users who employ scheduling constraints (for example, inter-Pod affinity ) and experts who extend the scheduler with custom plugins. The scheduler often appears as a black box, composed of many plugins that each contribute to th&amp;hellip;</description></item><item><title>EKS Hybrid Nodes for existing clusters</title><link>https://kubermates.org/releases/2025-03-31-eks-hybrid-nodes-for-existing-clusters/</link><pubDate>Mon, 31 Mar 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-03-31-eks-hybrid-nodes-for-existing-clusters/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/hybrid-nodes-cluster-update.html">https://docs.aws.amazon.com/eks/latest/userguide/hybrid-nodes-cluster-update.html&lt;/a>&lt;/p></description></item><item><title>Node health for EKS Hybrid Nodes</title><link>https://kubermates.org/releases/2025-03-31-node-health-for-eks-hybrid-nodes/</link><pubDate>Mon, 31 Mar 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-03-31-node-health-for-eks-hybrid-nodes/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/node-health.html">https://docs.aws.amazon.com/eks/latest/userguide/node-health.html&lt;/a>&lt;/p></description></item><item><title>Rollback: Prevent accidental upgrades with cluster insights</title><link>https://kubermates.org/releases/2025-03-28-rollback-prevent-accidental-upgrades-with-cluster-insights/</link><pubDate>Fri, 28 Mar 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-03-28-rollback-prevent-accidental-upgrades-with-cluster-insights/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html#update-cluster-control-plane">https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html#update-cluster-control-plane&lt;/a>&lt;/p></description></item><item><title>Bottlerocket FIPS AMIs</title><link>https://kubermates.org/releases/2025-03-27-bottlerocket-fips-amis/</link><pubDate>Thu, 27 Mar 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-03-27-bottlerocket-fips-amis/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/bottlerocket-fips-amis.html">https://docs.aws.amazon.com/eks/latest/userguide/bottlerocket-fips-amis.html&lt;/a>&lt;/p></description></item><item><title>Kubernetes v1.33 sneak peek</title><link>https://kubermates.org/docs/2025-03-26-kubernetes-v1-33-sneak-peek/</link><pubDate>Wed, 26 Mar 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-03-26-kubernetes-v1-33-sneak-peek/</guid><description>As the release of Kubernetes v1. 33 approaches, the Kubernetes project continues to evolve. Features may be deprecated, removed, or replaced to improve the overall health of the project. This blog post outlines some planned changes for the v1. 33 release, which the release team believes you should be aware of to ensure the continued smooth operation of your Kubernetes environment and to keep you up-to-date with the latest developments. The information below is based on the current status of the v1.</description></item><item><title>Fresh Swap Features for Linux Users in Kubernetes 1.32</title><link>https://kubermates.org/docs/2025-03-25-fresh-swap-features-for-linux-users-in-kubernetes-1-32/</link><pubDate>Tue, 25 Mar 2025 10:00:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-03-25-fresh-swap-features-for-linux-users-in-kubernetes-1-32/</guid><description>Swap is a fundamental and an invaluable Linux feature. It offers numerous benefits, such as effectively increasing a nodeâ€™s memory by swapping out unused data, shielding nodes from system-level memory spikes, preventing Pods from crashing when they hit their memory limits, and much more. As a result, the node special interest group within the Kubernetes project has invested significant effort into supporting swap on Linux nodes. The 1. 22 release introduced Alpha support for configuring swap memory usage for Kubernetes workloads running on Linux on a per-node basis. Later, in release 1.</description></item><item><title>Ingress-nginx CVE-2025-1974: What You Need to Know</title><link>https://kubermates.org/docs/2025-03-24-ingress-nginx-cve-2025-1974-what-you-need-to-know/</link><pubDate>Mon, 24 Mar 2025 12:00:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-03-24-ingress-nginx-cve-2025-1974-what-you-need-to-know/</guid><description>Today, the ingress-nginx maintainers have released patches for a batch of critical vulnerabilities that could make it easy for attackers to take over your Kubernetes cluster: ingress-nginx v1. 12. 1 and ingress-nginx v1. 11. 5. If you are among the over 40% of Kubernetes administrators using ingress-nginx , you should take action immediately to protect your users and data.</description></item><item><title>Introducing JobSet</title><link>https://kubermates.org/docs/2025-03-23-introducing-jobset/</link><pubDate>Sun, 23 Mar 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-23-introducing-jobset/</guid><description>Authors : Daniel Vega-Myhre (Google), Abdullah Gharaibeh (Google), Kevin Hannon (Red Hat) In this article, we introduce JobSet , an open source API for representing distributed jobs. The goal of JobSet is to provide a unified API for distributed ML training and HPC workloads on Kubernetes. The Kubernetes communityâ€™s recent enhancements to the batch ecosystem on Kubernetes has attracted ML engineers who have found it to be a natural fit for the requirements of running distributed training workloads. Large ML models (particularly LLMs) which cannot fit into the memory of the GPU or TPU chips on a single host are often distributed across tens of thousands of accelerator chips, which in turn&amp;hellip;</description></item><item><title>Spotlight on SIG Apps</title><link>https://kubermates.org/docs/2025-03-12-spotlight-on-sig-apps/</link><pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-12-spotlight-on-sig-apps/</guid><description>In our ongoing SIG Spotlight series, we dive into the heart of the Kubernetes project by talking to the leaders of its various Special Interest Groups (SIGs). This time, we focus on SIG Apps , the group responsible for everything related to developing, deploying, and operating applications on Kubernetes. Sandipan Panda ( DevZero ) had the opportunity to interview Maciej Szulik ( Defense Unicorns ) and Janet Kuo ( Google ), the chairs and tech leads of SIG Apps. They shared their experiences, challenges, and visions for the future of application management within the Kubernetes ecosystem. Sandipan: Hello, could you start by telling us a bit about yourself, your role, and your journey withi&amp;hellip;</description></item><item><title>Spotlight on SIG etcd</title><link>https://kubermates.org/docs/2025-03-04-spotlight-on-sig-etcd/</link><pubDate>Tue, 04 Mar 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-04-spotlight-on-sig-etcd/</guid><description>In this SIG etcd spotlight we talked with James Blair , Marek Siarkowicz , Wenjia Zhang , and Benjamin Wang to learn a bit more about this Kubernetes Special Interest Group. Frederico: Hello, thank you for the time! Letâ€™s start with some introductions, could you tell us a bit about yourself, your role and how you got involved in Kubernetes. Benjamin: Hello, I am Benjamin. I am a SIG etcd Tech Lead and one of the etcd maintainers. I work for VMware, which is part of the Broadcom group. I got involved in Kubernetes &amp;amp; etcd &amp;amp; CSI ( Container Storage Interface ) because of work and also a big passion for open source.</description></item><item><title>NFTables mode for kube-proxy</title><link>https://kubermates.org/docs/2025-02-28-nftables-mode-for-kube-proxy/</link><pubDate>Fri, 28 Feb 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-02-28-nftables-mode-for-kube-proxy/</guid><description>A new nftables mode for kube-proxy was introduced as an alpha feature in Kubernetes 1. 29. Currently in beta, it is expected to be GA as of 1. 33. The new mode fixes long-standing performance problems with the iptables mode and all users running on systems with reasonably-recent kernels are encouraged to try it out. (For compatibility reasons, even once nftables becomes GA, iptables will still be the default.</description></item><item><title>ðŸ” Secure Secret Management with SOPS in Helm ðŸš€</title><link>https://kubermates.org/blog/secure-secret-management-with-sops-in-helm-1940/</link><pubDate>Thu, 27 Feb 2025 08:15:10 +0000</pubDate><guid>https://kubermates.org/blog/secure-secret-management-with-sops-in-helm-1940/</guid><description>&lt;p>When managing applications deployed on Kubernetes, keeping secrets safe while still making them accessible to Helm charts is a challenge. Storing secrets in plaintext is a &lt;strong>security risk&lt;/strong> ðŸš¨ â€” and thatâ€™s where &lt;strong>SOPS&lt;/strong> (Secrets OPerationS) and the &lt;strong>Helm Secrets plugin&lt;/strong> come in!&lt;/p>
&lt;p>In this guide, weâ€™ll cover:&lt;/p>
&lt;ul>
&lt;li>âœ… How to use &lt;strong>SOPS&lt;/strong> with &lt;strong>age&lt;/strong> and &lt;strong>GPG&lt;/strong>&lt;/li>
&lt;li>âœ… How to configure &lt;strong>SOPS with &lt;code>sops.yaml&lt;/code>&lt;/strong> for better management&lt;/li>
&lt;li>âœ… How to use &lt;strong>Helm Secrets Plugin&lt;/strong> to manage encrypted secrets directly in your Helm charts&lt;/li>
&lt;li>âœ… A &lt;strong>GitHub Actions workflow&lt;/strong> to securely deploy Helm charts using encrypted secrets&lt;/li>
&lt;/ul>
&lt;h2 id="-why-use-sops-with-helm">ðŸ“Œ Why Use SOPS with Helm?&lt;/h2>
&lt;p>SOPS is an open-source tool from Mozilla that lets you &lt;strong>encrypt and decrypt&lt;/strong> secrets with ease. When combined with the Helm Secrets plugin, you can safely store your sensitive data in Git repositories and automatically decrypt them during Helm deployments. Hereâ€™s why itâ€™s awesome:&lt;/p></description></item><item><title>ðŸ” Secure Secret Management with SOPS in Terraform &amp; Terragrunt</title><link>https://kubermates.org/blog/secure-secret-management-with-sops-in-terraform-terragrunt-231a/</link><pubDate>Wed, 26 Feb 2025 14:44:59 +0000</pubDate><guid>https://kubermates.org/blog/secure-secret-management-with-sops-in-terraform-terragrunt-231a/</guid><description>&lt;p>When managing infrastructure as code (IaC), keeping secrets &lt;strong>safe&lt;/strong> while still making them accessible to Terraform/Terragrunt is a challenge. Storing secrets in plaintext is a &lt;strong>security risk&lt;/strong> ðŸš¨â€”and thatâ€™s where &lt;strong>SOPS&lt;/strong> (Secrets OPerationS) comes in!&lt;/p>
&lt;p>In this guide, weâ€™ll cover:&lt;/p>
&lt;ul>
&lt;li>âœ… How to use &lt;strong>SOPS&lt;/strong> with &lt;strong>age&lt;/strong> and &lt;strong>GPG&lt;/strong>&lt;/li>
&lt;li>âœ… How to configure &lt;strong>SOPS with &lt;code>sops.yaml&lt;/code>&lt;/strong> for better management&lt;/li>
&lt;li>âœ… How to use &lt;strong>Terragruntâ€™s built-in SOPS decryption&lt;/strong> (without &lt;code>run_cmd&lt;/code>)&lt;/li>
&lt;li>âœ… A &lt;strong>GitHub Actions workflow&lt;/strong> to securely use secrets in CI/CD&lt;/li>
&lt;/ul>
&lt;h2 id="-why-use-sops">ðŸ“Œ Why Use SOPS?&lt;/h2>
&lt;p>SOPS is an open-source tool from Mozilla that lets you &lt;strong>encrypt and decrypt&lt;/strong> secrets easily. It supports multiple encryption methods, including &lt;strong>GPG&lt;/strong>, &lt;strong>AWS KMS&lt;/strong>, &lt;strong>Azure Key Vault&lt;/strong>, &lt;strong>Google Cloud KMS&lt;/strong>, and &lt;strong>age&lt;/strong>.&lt;/p></description></item><item><title>The Cloud Controller Manager Chicken and Egg Problem</title><link>https://kubermates.org/docs/2025-02-14-the-cloud-controller-manager-chicken-and-egg-problem/</link><pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-02-14-the-cloud-controller-manager-chicken-and-egg-problem/</guid><description>Kubernetes 1. 31 completed the largest migration in Kubernetes history , removing the in-tree cloud provider. While the component migration is now done, this leaves some additional complexity for users and installer projects (for example, kOps or Cluster API). We will go over those additional steps and failure points and make recommendations for cluster owners. This migration was complex and some logic had to be extracted from the core components, building four new subsystems. The cloud controller manager is part of the control plane.</description></item><item><title>Update strategies for managed node groups</title><link>https://kubermates.org/releases/2025-01-27-update-strategies-for-managed-node-groups/</link><pubDate>Mon, 27 Jan 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-01-27-update-strategies-for-managed-node-groups/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/managed-node-update-behavior.html#managed-node-update-upgrade">https://docs.aws.amazon.com/eks/latest/userguide/managed-node-update-behavior.html#managed-node-update-upgrade&lt;/a>&lt;/p></description></item><item><title>Kubernetes version 1.32</title><link>https://kubermates.org/releases/2025-01-23-kubernetes-version-1-32/</link><pubDate>Thu, 23 Jan 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-01-23-kubernetes-version-1-32/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html#kubernetes-1-32">https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html#kubernetes-1-32&lt;/a>&lt;/p></description></item><item><title>Helm Chart Essentials &amp; Writing Effective Charts ðŸš€</title><link>https://kubermates.org/blog/helm-chart-essentials-writing-effective-charts-11ca/</link><pubDate>Thu, 23 Jan 2025 10:50:53 +0000</pubDate><guid>https://kubermates.org/blog/helm-chart-essentials-writing-effective-charts-11ca/</guid><description>&lt;p>Helm charts are a powerful way to define, install, and upgrade Kubernetes applications. By packaging all the Kubernetes manifests and parameters in a neat, reproducible format, Helm simplifies the deployment process for engineers and DevOps teams. In this article, weâ€™ll explore some best practices for writing effective Helm charts, introduce the &lt;strong>Helm Schema plugin&lt;/strong> for validation, show how to include tests to ensure reliability, discuss &lt;strong>helm-docs&lt;/strong> for automated documentation generation, and share an additional resource for testing and linting. Letâ€™s get started! ðŸŽ‰&lt;/p></description></item><item><title>Spotlight on SIG Architecture: Enhancements</title><link>https://kubermates.org/docs/2025-01-21-spotlight-on-sig-architecture-enhancements/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-01-21-spotlight-on-sig-architecture-enhancements/</guid><description>This is the fourth interview of a SIG Architecture Spotlight series that will cover the different subprojects, and we will be covering SIG Architecture: Enhancements. In this SIG Architecture spotlight we talked with Kirsten Garrison , lead of the Enhancements subproject. Frederico (FSM): Hi Kirsten, very happy to have the opportunity to talk about the Enhancements subproject. Let&amp;rsquo;s start with some quick information about yourself and your role. Kirsten Garrison (KG) : Iâ€™m a lead of the Enhancements subproject of SIG-Architecture and currently work at Google. I first got involved by contributing to the service-catalog project with the help of Carolyn Van Slyck.</description></item><item><title>Unlocking Secrets with External Secrets Operator ðŸ”âœ¨</title><link>https://kubermates.org/blog/unlocking-secrets-with-external-secrets-operator-2f89/</link><pubDate>Fri, 03 Jan 2025 20:00:01 +0000</pubDate><guid>https://kubermates.org/blog/unlocking-secrets-with-external-secrets-operator-2f89/</guid><description>&lt;p>In modern cloud-native applications, securely managing sensitive data like API keys, database credentials, and certificates is a top priority. Two powerful tools stand out when integrating secrets into Kubernetes: &lt;strong>External Secrets Operator&lt;/strong> and &lt;strong>SecretStoreProviders plugin&lt;/strong> (for Azure and AWS). Letâ€™s dive into how to use them, their differences, and when to pick one over the other. ðŸš€&lt;/p>
&lt;h2 id="what-is-external-secrets-operator-">&lt;strong>What Is External Secrets Operator?&lt;/strong> ðŸ¤”&lt;/h2>
&lt;p>The &lt;strong>External Secrets Operator&lt;/strong> (ESO) simplifies secret management in Kubernetes by integrating external secret stores directly into your cluster. Instead of manually creating Kubernetes Secrets, ESO syncs secrets from providers like AWS Secrets Manager, Azure Key Vault, or HashiCorp Vault.&lt;/p></description></item><item><title>Automating DNS in Azure Private DNS with External-DNS â˜ï¸ðŸ”</title><link>https://kubermates.org/blog/automating-dns-in-azure-private-dns-with-external-dns-3knk/</link><pubDate>Sun, 29 Dec 2024 17:07:48 +0000</pubDate><guid>https://kubermates.org/blog/automating-dns-in-azure-private-dns-with-external-dns-3knk/</guid><description>&lt;p>&lt;em>When running Kubernetes in Azure, one of the biggest time-savers you can implement is automatic DNS record managementâ€”especially for internal (private) services. By integrating &lt;a href="https://github.com/kubernetes-sigs/external-dns">External-DNS&lt;/a> with Azure Private DNS, you can say goodbye to manual record updates. Better yet, you can skip traditional service principals and use Azure Workload Identity to make your cluster more secure and secrets-free!&lt;/em> ðŸš€&lt;/p>
&lt;p>In this article, weâ€™ll show you how to configure External-DNS for Azure Private DNS using &lt;strong>Azure Workload Identity&lt;/strong>, leveraging a snippet of a &lt;code>values.yaml&lt;/code> that highlights the relevant settings.&lt;/p></description></item><item><title>Using Nginx Ingress Controller and Cert-Manager for HTTPS with Letâ€™s Encrypt âš¡</title><link>https://kubermates.org/blog/using-nginx-ingress-controller-and-cert-manager-for-https-with-lets-encrypt-2flh/</link><pubDate>Thu, 26 Dec 2024 09:39:11 +0000</pubDate><guid>https://kubermates.org/blog/using-nginx-ingress-controller-and-cert-manager-for-https-with-lets-encrypt-2flh/</guid><description>&lt;p>Hey there! In todayâ€™s world, serving your web apps over HTTPS is a must. Luckily, combining the power of &lt;strong>Nginx Ingress Controller&lt;/strong> with &lt;strong>Cert-Manager&lt;/strong> helps you easily request, issue, and renew TLS certificates from &lt;strong>Letâ€™s Encrypt&lt;/strong>. In this friendly guide, weâ€™ll walk you through:&lt;/p>
&lt;ol>
&lt;li>Installing the &lt;strong>Nginx Ingress Controller&lt;/strong>&lt;/li>
&lt;li>Installing &lt;strong>Cert-Manager&lt;/strong>&lt;/li>
&lt;li>Creating a &lt;strong>ClusterIssuer&lt;/strong> to fetch certificates from Letâ€™s Encrypt&lt;/li>
&lt;li>Configuring an example &lt;strong>Ingress&lt;/strong> to serve traffic via HTTPS&lt;/li>
&lt;/ol>
&lt;p>Letâ€™s get started! ðŸš€&lt;/p></description></item><item><title>Kubernetes 1.32: Moving Volume Group Snapshots to Beta</title><link>https://kubermates.org/docs/2024-12-18-kubernetes-1-32-moving-volume-group-snapshots-to-beta/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-12-18-kubernetes-1-32-moving-volume-group-snapshots-to-beta/</guid><description>Volume group snapshots were introduced as an Alpha feature with the Kubernetes 1. 27 release. The recent release of Kubernetes v1. 32 moved that support to beta. The support for volume group snapshots relies on a set of extension APIs for group snapshots. These APIs allow users to take crash consistent snapshots for a set of volumes.</description></item><item><title>Enhancing Kubernetes API Server Efficiency with API Streaming</title><link>https://kubermates.org/docs/2024-12-17-enhancing-kubernetes-api-server-efficiency-with-api-streaming/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-12-17-enhancing-kubernetes-api-server-efficiency-with-api-streaming/</guid><description>Managing Kubernetes clusters efficiently is critical, especially as their size is growing. A significant challenge with large clusters is the memory overhead caused by list requests. In the existing implementation, the kube-apiserver processes list requests by assembling the entire response in-memory before transmitting any data to the client. But what if the response body is substantial, say hundreds of megabytes? Additionally, imagine a scenario where multiple list requests flood in simultaneously, perhaps after a brief network outage. While API Priority and Fairness has proven to reasonably protect kube-apiserver from CPU overload, its impact is visibly smaller for memory protection. T&amp;hellip;</description></item><item><title>Kubernetes v1.32 Adds A New CPU Manager Static Policy Option For Strict CPU Reservation</title><link>https://kubermates.org/docs/2024-12-16-kubernetes-v1-32-adds-a-new-cpu-manager-static-policy-option-for-strict-cpu-rese/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-12-16-kubernetes-v1-32-adds-a-new-cpu-manager-static-policy-option-for-strict-cpu-rese/</guid><description>In Kubernetes v1. 32, after years of community discussion, we are excited to introduce a strict-cpu-reservation option for the CPU Manager static policy. This feature is currently in alpha, with the associated policy hidden by default. You can only use the policy if you explicitly enable the alpha behavior in your cluster. The CPU Manager static policy is used to reduce latency or improve performance. The reservedSystemCPUs defines an explicit CPU set for OS system daemons and kubernetes system daemons.</description></item><item><title>Kubernetes v1.32: Memory Manager Goes GA</title><link>https://kubermates.org/docs/2024-12-13-kubernetes-v1-32-memory-manager-goes-ga/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-12-13-kubernetes-v1-32-memory-manager-goes-ga/</guid><description>With Kubernetes 1. 32, the memory manager has officially graduated to General Availability (GA), marking a significant milestone in the journey toward efficient and predictable memory allocation for containerized applications. Since Kubernetes v1. 22, where it graduated to beta, the memory manager has proved itself reliable, stable and a good complementary feature for the CPU Manager. As part of kubelet&amp;rsquo;s workload admission process, the memory manager provides topology hints to optimize memory allocation and alignment. This enables users to allocate exclusive memory for Pods in the Guaranteed QoS class.</description></item><item><title>Kubernetes v1.32: QueueingHint Brings a New Possibility to Optimize Pod Scheduling</title><link>https://kubermates.org/docs/2024-12-12-kubernetes-v1-32-queueinghint-brings-a-new-possibility-to-optimize-pod-schedulin/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-12-12-kubernetes-v1-32-queueinghint-brings-a-new-possibility-to-optimize-pod-schedulin/</guid><description>The Kubernetes scheduler is the core component that selects the nodes on which new Pods run. The scheduler processes these new Pods one by one. Therefore, the larger your clusters, the more important the throughput of the scheduler becomes. Over the years, Kubernetes SIG Scheduling has improved the throughput of the scheduler in multiple enhancements. This blog post describes a major improvement to the scheduler in Kubernetes v1. 32: a scheduling context element named QueueingHint.</description></item><item><title>Kubernetes v1.32: Penelope</title><link>https://kubermates.org/docs/2024-12-11-kubernetes-v1-32-penelope/</link><pubDate>Wed, 11 Dec 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-12-11-kubernetes-v1-32-penelope/</guid><description>Editors: Matteo Bianchi, Edith Puclla, William Rizzo, Ryota Sawada, Rashan Smith Announcing the release of Kubernetes v1. 32: Penelope! In line with previous releases, the release of Kubernetes v1. 32 introduces new stable, beta, and alpha features. The consistent delivery of high-quality releases underscores the strength of our development cycle and the vibrant support from our community. This release consists of 44 enhancements in total. Of those enhancements, 13 have graduated to Stable, 12 are entering Beta, and 19 have entered in Alpha.</description></item><item><title>Amazon EKS Auto Mode</title><link>https://kubermates.org/releases/2024-12-01-amazon-eks-auto-mode/</link><pubDate>Sun, 01 Dec 2024 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2024-12-01-amazon-eks-auto-mode/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/automode.html">https://docs.aws.amazon.com/eks/latest/userguide/automode.html&lt;/a>&lt;/p></description></item><item><title>Amazon EKS Hybrid Nodes</title><link>https://kubermates.org/releases/2024-12-01-amazon-eks-hybrid-nodes/</link><pubDate>Sun, 01 Dec 2024 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2024-12-01-amazon-eks-hybrid-nodes/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/hybrid-nodes-overview.html">https://docs.aws.amazon.com/eks/latest/userguide/hybrid-nodes-overview.html&lt;/a>&lt;/p></description></item><item><title>Kubernetes version 1.30 is now available for local clusters on AWS Outposts</title><link>https://kubermates.org/releases/2024-11-21-kubernetes-version-1-30-is-now-available-for-local-clusters-on-aws-outposts/</link><pubDate>Thu, 21 Nov 2024 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2024-11-21-kubernetes-version-1-30-is-now-available-for-local-clusters-on-aws-outposts/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-outposts-platform-versions.html">https://docs.aws.amazon.com/eks/latest/userguide/eks-outposts-platform-versions.html&lt;/a>&lt;/p></description></item><item><title>Gateway API v1.2: WebSockets, Timeouts, Retries, and More</title><link>https://kubermates.org/docs/2024-11-21-gateway-api-v1-2-websockets-timeouts-retries-and-more/</link><pubDate>Thu, 21 Nov 2024 09:00:00 -0800</pubDate><guid>https://kubermates.org/docs/2024-11-21-gateway-api-v1-2-websockets-timeouts-retries-and-more/</guid><description>Kubernetes SIG Network is delighted to announce the general availability of Gateway API v1. 2! This version of the API was released on October 3, and we&amp;rsquo;re delighted to report that we now have a number of conformant implementations of it for you to try out. Gateway API v1. 2 brings a number of new features to the Standard channel (Gateway API&amp;rsquo;s GA release channel), introduces some new experimental features, and inaugurates our new release process â€” but it also brings two breaking changes that you&amp;rsquo;ll want to be careful of. Now that the v1 versions of GRPCRoute and ReferenceGrant have graduated to Standard, the old v1alpha2 versions have been removed from both the Standard and Experimental &amp;hellip;</description></item><item><title>How we built a dynamic Kubernetes API Server for the API Aggregation Layer in Cozystack</title><link>https://kubermates.org/docs/2024-11-21-how-we-built-a-dynamic-kubernetes-api-server-for-the-api-aggregation-layer-in-co/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-11-21-how-we-built-a-dynamic-kubernetes-api-server-for-the-api-aggregation-layer-in-co/</guid><description>Hi there! I&amp;rsquo;m Andrei Kvapil, but you might know me as @kvaps in communities dedicated to Kubernetes and cloud-native tools. In this article, I want to share how we implemented our own extension api-server in the open-source PaaS platform, Cozystack. Kubernetes truly amazes me with its powerful extensibility features. You&amp;rsquo;re probably already familiar with the controller concept and frameworks like kubebuilder and operator-sdk that help you implement it. In a nutshell, they allow you to extend your Kubernetes cluster by defining custom resources (CRDs) and writing additional controllers that handle your business logic for reconciling and managing these kinds of resources. This approach is w&amp;hellip;</description></item><item><title>Bottlerocket AMIs that use FIPS 140-3</title><link>https://kubermates.org/releases/2024-11-20-bottlerocket-amis-that-use-fips-140-3/</link><pubDate>Wed, 20 Nov 2024 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2024-11-20-bottlerocket-amis-that-use-fips-140-3/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/retrieve-ami-id-bottlerocket.html">https://docs.aws.amazon.com/eks/latest/userguide/retrieve-ami-id-bottlerocket.html&lt;/a>&lt;/p></description></item><item><title>Observability dashboard</title><link>https://kubermates.org/releases/2024-11-18-observability-dashboard/</link><pubDate>Mon, 18 Nov 2024 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2024-11-18-observability-dashboard/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/observability-dashboard.html">https://docs.aws.amazon.com/eks/latest/userguide/observability-dashboard.html&lt;/a>&lt;/p></description></item><item><title>New role creation in console for add-ons that support EKS Pod Identities</title><link>https://kubermates.org/releases/2024-11-15-new-role-creation-in-console-for-add-ons-that-support-eks-pod-identities/</link><pubDate>Fri, 15 Nov 2024 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2024-11-15-new-role-creation-in-console-for-add-ons-that-support-eks-pod-identities/</guid><description>&lt;p>Open the original post â†— &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/creating-an-add-on.html#_create_add_on_console">https://docs.aws.amazon.com/eks/latest/userguide/creating-an-add-on.html#_create_add_on_console&lt;/a>&lt;/p></description></item><item><title>Kubernetes v1.32 sneak peek</title><link>https://kubermates.org/docs/2024-11-08-kubernetes-v1-32-sneak-peek/</link><pubDate>Fri, 08 Nov 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-11-08-kubernetes-v1-32-sneak-peek/</guid><description>As we get closer to the release date for Kubernetes v1. 32, the project develops and matures. Features may be deprecated, removed, or replaced with better ones for the project&amp;rsquo;s overall health. This blog outlines some of the planned changes for the Kubernetes v1. 32 release, that the release team feels you should be aware of, for the continued maintenance of your Kubernetes environment and keeping up to date with the latest changes. Information listed below is based on the current status of the v1.</description></item><item><title>Ensuring Effective Helm Charts with Linting, Testing, and Diff Checks ðŸš€</title><link>https://kubermates.org/blog/ensuring-effective-helm-charts-with-linting-testing-and-diff-checks-ni0/</link><pubDate>Tue, 01 Oct 2024 16:25:51 +0000</pubDate><guid>https://kubermates.org/blog/ensuring-effective-helm-charts-with-linting-testing-and-diff-checks-ni0/</guid><description>&lt;p>When deploying applications to Kubernetes, using Helm charts is a great way to simplify the process. But how do you make sure your Helm charts are high-quality and wonâ€™t cause issues down the line? Donâ€™t worry! In this guide, weâ€™ll show you how to:&lt;/p>
&lt;ul>
&lt;li>Use &lt;strong>Helm Chart-Testing&lt;/strong> for linting and validation ðŸ•µï¸â€â™€ï¸&lt;/li>
&lt;li>Perform &lt;strong>Unit Testing&lt;/strong> with the Helm Unit Test plugin ðŸ”§&lt;/li>
&lt;li>Use &lt;strong>Helm Diff&lt;/strong> to check changes before installing or upgrading ðŸš¦&lt;/li>
&lt;/ul>
&lt;p>By following these steps, youâ€™ll catch potential issues early and ensure smooth deployments. Weâ€™ll also build a fully tested &lt;strong>NGINX Helm chart&lt;/strong> at the end!&lt;/p></description></item><item><title>Capsule: How to Use It and First Steps to Set It Up on an AKS Cluster ðŸš€</title><link>https://kubermates.org/blog/capsule-how-to-use-it-and-first-steps-to-set-it-up-on-an-aks-cluster-5hl5/</link><pubDate>Tue, 24 Sep 2024 19:24:03 +0000</pubDate><guid>https://kubermates.org/blog/capsule-how-to-use-it-and-first-steps-to-set-it-up-on-an-aks-cluster-5hl5/</guid><description>&lt;p>Capsule is an awesome, open-source solution that helps you manage multiple tenants in Kubernetes clusters, making it super easy to handle multi-tenancy. Whether youâ€™re running Kubernetes for a big company or providing services for others, Capsule ensures your clusters stay organized and secure!&lt;/p>
&lt;p>In this guide, weâ€™ll dive into what Capsule is, why itâ€™s a great choice, and show you how to set it up on an Azure Kubernetes Service (AKS) cluster. Ready? Letâ€™s go! ðŸŽ‰&lt;/p></description></item><item><title>Understanding AKS NAP: Azure Kubernetes Service Node Auto-Provisioning (Powered by Karpenter) ðŸš€</title><link>https://kubermates.org/blog/understanding-aks-nap-azure-kubernetes-service-node-auto-provisioning-powered-by-karpenter-3djh/</link><pubDate>Fri, 20 Sep 2024 06:50:50 +0000</pubDate><guid>https://kubermates.org/blog/understanding-aks-nap-azure-kubernetes-service-node-auto-provisioning-powered-by-karpenter-3djh/</guid><description>&lt;p>As more organizations embrace cloud-native technologies like &lt;strong>Kubernetes&lt;/strong>, keeping your infrastructure lean and scalable is key to success. Thankfully, Azure Kubernetes Service (AKS) offers a powerful featureâ€”&lt;strong>Node Auto-Provisioning (NAP)&lt;/strong>. NAP is powered by &lt;strong>Karpenter&lt;/strong>, a Kubernetes-native tool, and allows automatic node provisioning based on demand, ensuring that your clusterâ€™s resources are always just right. In this article, weâ€™ll explore how AKS NAP works, how it&amp;rsquo;s powered by &lt;strong>Karpenter&lt;/strong>, and why it can make your life a lot easier! ðŸŒŸ&lt;/p></description></item><item><title>Admission Controllers in Kubernetes: OPA GateKeeper, Kyverno, and Azure Policy Add-on for AKSâ€”Which One Wins? ðŸ†</title><link>https://kubermates.org/blog/admission-controllers-in-kubernetes-opa-gatekeeper-kyverno-and-azure-policy-add-on-for-aks-which-one-wins-237d/</link><pubDate>Tue, 17 Sep 2024 07:25:52 +0000</pubDate><guid>https://kubermates.org/blog/admission-controllers-in-kubernetes-opa-gatekeeper-kyverno-and-azure-policy-add-on-for-aks-which-one-wins-237d/</guid><description>&lt;p>When managing a Kubernetes cluster, controlling what gets deployed and ensuring resources comply with security, governance, and operational policies is essential. Admission controllers act as &amp;ldquo;gatekeepers&amp;rdquo; for your cluster, ensuring only compliant resources get through. ðŸ›¡ï¸&lt;/p>
&lt;p>Three popular options for extending Kubernetes&amp;rsquo; admission control functionality are &lt;strong>OPA GateKeeper&lt;/strong>, &lt;strong>Kyverno&lt;/strong>, and the &lt;strong>Azure Policy Add-on for AKS&lt;/strong> (which incorporates OPA GateKeeper&amp;rsquo;s engine). In this article, weâ€™ll compare these solutions and show why &lt;strong>Kyverno&lt;/strong> is still the most user-friendly and versatile option for Kubernetes users. ðŸŒŠ&lt;/p></description></item><item><title>Upgrading AKS: In-Place, Blue-Green, and Canary Upgrades Explained ðŸš€</title><link>https://kubermates.org/blog/upgrading-aks-in-place-blue-green-and-canary-upgrades-explained-3aap/</link><pubDate>Sun, 15 Sep 2024 10:09:30 +0000</pubDate><guid>https://kubermates.org/blog/upgrading-aks-in-place-blue-green-and-canary-upgrades-explained-3aap/</guid><description>&lt;p>Keeping your Azure Kubernetes Service (AKS) cluster up to date is crucial for security, performance, and accessing new features. AKS offers different strategies for upgrading, and in this guide, weâ€™ll walk through the main methods: &lt;strong>In-place upgrades&lt;/strong>, &lt;strong>Blue-Green deployments&lt;/strong>, and &lt;strong>Canary upgrades&lt;/strong>, complete with real-world examples to help you understand the process!&lt;/p>
&lt;h2 id="types-of-aks-upgrades-">Types of AKS Upgrades ðŸš§&lt;/h2>
&lt;h3 id="1-in-place-upgrades-">1. &lt;strong>In-Place Upgrades&lt;/strong> ðŸ”„&lt;/h3>
&lt;p>An &lt;strong>In-place upgrade&lt;/strong> is the most straightforward method, where the upgrade happens directly on your current AKS cluster. It updates the control plane and worker nodes without creating new resources.&lt;/p></description></item><item><title>Monitor and Optimize Multi-Cluster AKS Costs ðŸ’°</title><link>https://kubermates.org/blog/monitor-and-optimize-multi-cluster-aks-costs-4627/</link><pubDate>Thu, 12 Sep 2024 07:42:06 +0000</pubDate><guid>https://kubermates.org/blog/monitor-and-optimize-multi-cluster-aks-costs-4627/</guid><description>&lt;p>As businesses scale their Kubernetes workloads across multiple Azure Kubernetes Service (AKS) clusters, managing and optimizing cloud costs becomes critical. Deploying and managing observability tools such as KubeCost and OpenTelemetry (OTel) across multiple clusters can be simplified using &lt;a href="https://github.com/Azure/fleet/blob/main/docs/concepts/README.md">AKS Fleet Manager&lt;/a>, Microsoft Managed Prometheus, and Grafana.&lt;/p>
&lt;p>This guide will explain how to:&lt;/p>
&lt;ol>
&lt;li>Deploy &lt;strong>KubeCost&lt;/strong> and &lt;strong>OpenTelemetry&lt;/strong> across multiple AKS clusters using &lt;strong>AKS Fleet Manager&lt;/strong>.&lt;/li>
&lt;li>Expose metrics through OpenTelemetry.&lt;/li>
&lt;li>Centralize monitoring via &lt;strong>Managed Prometheus&lt;/strong> and &lt;strong>Grafana&lt;/strong>.&lt;/li>
&lt;/ol>
&lt;p>Youâ€™ll gain a single-pane-of-glass view into your multi-cluster environment, enabling more efficient resource utilization and cost management.&lt;/p></description></item><item><title>ðŸš€ Enhancing Container Security: The Complete Guide to Secure and Clean Kubernetes Clusters ðŸ›¡ï¸ðŸ§¼</title><link>https://kubermates.org/blog/enhancing-container-security-the-complete-guide-to-secure-and-clean-kubernetes-clusters-1ida/</link><pubDate>Wed, 11 Sep 2024 07:19:50 +0000</pubDate><guid>https://kubermates.org/blog/enhancing-container-security-the-complete-guide-to-secure-and-clean-kubernetes-clusters-1ida/</guid><description>&lt;p>As Kubernetes continues to grow in popularity, ensuring the security and cleanliness of your container images is crucial. In this guide, weâ€™ll cover two key strategies: &lt;strong>image signing using Notary&lt;/strong> ðŸ–‹ï¸ and the &lt;strong>AKS Image Cleaner (Eraser)&lt;/strong> add-on ðŸ§¼. Together, they form a robust, secure, and efficient container management workflow.&lt;/p>
&lt;p>By the end, you&amp;rsquo;ll know how to ensure that your AKS cluster pulls only verified, trusted images and stays free of unused images that could pose a risk to your environment.&lt;/p></description></item><item><title>Karmada: Deep Dive into Managing Multiple AKS Clusters ðŸš€</title><link>https://kubermates.org/blog/karmada-deep-dive-into-managing-multiple-aks-clusters-1j08/</link><pubDate>Mon, 09 Sep 2024 07:00:23 +0000</pubDate><guid>https://kubermates.org/blog/karmada-deep-dive-into-managing-multiple-aks-clusters-1j08/</guid><description>&lt;p>In todayâ€™s cloud-driven world, Kubernetes has become the go-to platform for running containerized applications. If you&amp;rsquo;re using Microsoft Azure Kubernetes Service (AKS), you know how powerful it can be. But what if youâ€™re managing not just one, but multiple AKS clusters across different environments? Sounds a bit overwhelming, right? ðŸ˜…&lt;/p>
&lt;p>Thatâ€™s where &lt;strong>Karmada&lt;/strong> (Kubernetes Armada) comes to the rescue! Karmada is like your multi-cluster superhero, helping you deploy and manage applications across multiple AKS clusters as if they were one big happy family. This deep dive will take you through Karmadaâ€™s architecture, installation process, advanced deployment scenarios, best strategies, and how to integrate Karmada into your CI/CD pipelines with practical examples. ðŸŒŸ&lt;/p></description></item><item><title>Understanding eBPF and Its Application in Modern Cloud Environments ðŸš€</title><link>https://kubermates.org/blog/understanding-ebpf-and-its-application-in-modern-cloud-environments-3f99/</link><pubDate>Sun, 08 Sep 2024 06:53:32 +0000</pubDate><guid>https://kubermates.org/blog/understanding-ebpf-and-its-application-in-modern-cloud-environments-3f99/</guid><description>&lt;h2 id="what-is-ebpf-">What is eBPF? ðŸ¤”&lt;/h2>
&lt;p>&lt;strong>eBPF&lt;/strong> (Extended Berkeley Packet Filter) is like magic for the Linux kernel! ðŸª„ It lets developers run custom code directly within the kernel, safely and efficiently, without needing to modify the kernel&amp;rsquo;s source code or load new modules. Originally, eBPF was created to help with network packet filtering, but it has evolved into a Swiss Army knife ðŸ› ï¸ for all sorts of tasks, from observability to security and system performance monitoring.&lt;/p></description></item><item><title>ðŸŒ Securing Kubernetes Secrets in AKS: Using Azure Key Vault with Managed and User Assigned Identities ðŸš€</title><link>https://kubermates.org/blog/securing-kubernetes-secrets-in-aks-using-azure-key-vault-with-managed-and-user-assigned-identities-569k/</link><pubDate>Wed, 04 Sep 2024 11:35:22 +0000</pubDate><guid>https://kubermates.org/blog/securing-kubernetes-secrets-in-aks-using-azure-key-vault-with-managed-and-user-assigned-identities-569k/</guid><description>&lt;p>Hello Kubernetes enthusiast! ðŸ‘‹ Letâ€™s dive into a critical aspect of securing your applications running in Azure Kubernetes Service (AKS): managing secrets. While Kubernetes Secrets provide a way to manage sensitive information like passwords and API keys, they arenâ€™t encrypted by default and can be vulnerable if not handled correctly. In this guide, we&amp;rsquo;ll explore how to securely manage secrets by integrating Azure Key Vault with AKS using both &lt;strong>VM Managed Identities&lt;/strong> and &lt;strong>User Assigned Identities&lt;/strong>. Plus, we&amp;rsquo;ll show you how to enable the Secret Store CSI Driver directly in AKS.&lt;/p></description></item><item><title>ðŸš€ Automating Image Vulnerability Patching in Kubernetes with Trivy Operator, Copacetic, and GitHub Actions</title><link>https://kubermates.org/blog/automating-image-vulnerability-patching-in-kubernetes-with-trivy-operator-copacetic-and-github-actions-13l/</link><pubDate>Tue, 03 Sep 2024 14:52:24 +0000</pubDate><guid>https://kubermates.org/blog/automating-image-vulnerability-patching-in-kubernetes-with-trivy-operator-copacetic-and-github-actions-13l/</guid><description>&lt;h1 id="-automating-image-vulnerability-patching-in-kubernetes-with-trivy-operator-copacetic-and-github-actions">ðŸš€ Automating Image Vulnerability Patching in Kubernetes with Trivy Operator, Copacetic, and GitHub Actions&lt;/h1>
&lt;h2 id="1-installing-and-using-copacetic-copa-cli">1. Installing and Using Copacetic (Copa) CLI&lt;/h2>
&lt;p>Copacetic is a CLI tool (&lt;code>copa&lt;/code>) designed to help automate the patching of vulnerabilities in your container images. Hereâ€™s how to install it:&lt;/p>
&lt;h3 id="11-clone-the-copacetic-repository">1.1. Clone the Copacetic Repository&lt;/h3>
&lt;p>Start by cloning the Copacetic repository to your local machine:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">git clone https://github.com/project-copacetic/copacetic
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">cd&lt;/span> copacetic
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="12-build-copacetic">1.2. Build Copacetic&lt;/h3>
&lt;p>Inside the cloned directory, build the Copacetic CLI:&lt;/p></description></item><item><title>Autoscaling in Kubernetes: KEDA, Karpenter, and Native Autoscalers</title><link>https://kubermates.org/blog/autoscaling-in-kubernetes-keda-karpenter-and-native-autoscalers-1gpo/</link><pubDate>Mon, 02 Sep 2024 11:41:41 +0000</pubDate><guid>https://kubermates.org/blog/autoscaling-in-kubernetes-keda-karpenter-and-native-autoscalers-1gpo/</guid><description>&lt;p>Autoscaling is a critical component of any robust Kubernetes environment, ensuring your applications and infrastructure can dynamically adjust to meet demand. In this guide, we&amp;rsquo;ll explore three powerful autoscaling tools: &lt;strong>KEDA&lt;/strong> for event-driven pod autoscaling, &lt;strong>Karpenter&lt;/strong> for dynamic node scaling, and Kubernetes&amp;rsquo; native autoscalers (HPA and VPA). We&amp;rsquo;ll dive into how to use them effectively, with plenty of examples to get you started. ðŸš€&lt;/p>
&lt;h2 id="introduction-to-keda-">Introduction to KEDA ðŸš€&lt;/h2>
&lt;p>KEDA (Kubernetes-based Event Driven Autoscaling) allows you to scale applications based on custom event metrics, not just CPU or memory usage. Itâ€™s ideal for scenarios where workloads are triggered by external events, such as message queues, databases, or HTTP requests. Whether you&amp;rsquo;re processing incoming orders, reacting to sensor data, or scaling based on custom Prometheus metrics, KEDA has you covered! ðŸ’¥&lt;/p></description></item><item><title>Insights on Securing Your Kubernetes Cluster with Falco ðŸš€ðŸ”’</title><link>https://kubermates.org/blog/insights-on-securing-your-kubernetes-cluster-with-falco-4b9g/</link><pubDate>Sun, 01 Sep 2024 12:11:29 +0000</pubDate><guid>https://kubermates.org/blog/insights-on-securing-your-kubernetes-cluster-with-falco-4b9g/</guid><description>&lt;p>Falco is a powerful open-source security tool designed to monitor your Kubernetes cluster in real-time, detecting suspicious activities based on customizable rules. Implementing Falco effectively can significantly enhance your clusterâ€™s security. In this comprehensive guide, weâ€™ll cover everything from installing Falco to best practices for implementing rules, and how to defend against potential bypasses.&lt;/p>
&lt;hr>
&lt;h2 id="-1-installing-falco-in-kubernetes">ðŸŒŸ 1. Installing Falco in Kubernetes&lt;/h2>
&lt;p>Getting started with Falco is straightforward. Hereâ€™s how you can install it using Helm, a popular package manager for Kubernetes.&lt;/p></description></item><item><title>ðŸš€ Building a Kubernetes Operator with an NGINX CRD</title><link>https://kubermates.org/blog/building-a-kubernetes-operator-with-an-nginx-crd-3lil/</link><pubDate>Thu, 29 Aug 2024 08:25:36 +0000</pubDate><guid>https://kubermates.org/blog/building-a-kubernetes-operator-with-an-nginx-crd-3lil/</guid><description>&lt;p>Kubernetes is a powerful platform that automates the deployment, scaling, and management of containerized applications. One of the coolest features of Kubernetes is its ability to be extended with &lt;strong>Custom Resource Definitions (CRDs)&lt;/strong> and &lt;strong>Operators&lt;/strong>. In this guide, we&amp;rsquo;ll build a simple Kubernetes operator using an NGINX CRD to manage NGINX instances in your cluster.&lt;/p>
&lt;h3 id="-understanding-kubernetes-controllers-operators-and-crds">ðŸ¤– Understanding Kubernetes Controllers, Operators, and CRDs&lt;/h3>
&lt;h4 id="what-is-a-kubernetes-controller">What is a Kubernetes Controller?&lt;/h4>
&lt;p>A Kubernetes controller is like a robot ðŸ¤– that continuously monitors your cluster. It checks whether the actual state of the resources matches the desired state (what you want) and makes adjustments to align them.&lt;/p></description></item><item><title>ðŸŽ¨ Hacking the Helm Operator with Flux: Creating Self-Installable Services for Easier App Deployment</title><link>https://kubermates.org/blog/hacking-the-helm-operator-with-flux-creating-self-installable-services-for-easier-app-deployment-5a8l/</link><pubDate>Wed, 28 Aug 2024 08:01:26 +0000</pubDate><guid>https://kubermates.org/blog/hacking-the-helm-operator-with-flux-creating-self-installable-services-for-easier-app-deployment-5a8l/</guid><description>&lt;p>Managing applications in Kubernetes can be tricky, but with tools like Helm, operators, and Flux, you can make the process smoother and even fun! In this guide, we&amp;rsquo;ll walk you through how to hack the Helm Operator using the Operator SDK and Flux to create powerful, self-installable services that make deploying apps like NGINX, Apache Tomcat, and even Redis a breeze. ðŸŒ¬ï¸&lt;/p>
&lt;p>By the end, you&amp;rsquo;ll have your very own GitOps-powered system, making deployments as simple as pushing to a Git repository. Let&amp;rsquo;s dive in!&lt;/p></description></item><item><title>Chaos Engineering Let's Break Everything! ðŸ˜ˆ</title><link>https://kubermates.org/blog/chaos-engineering-lets-break-everything-io0/</link><pubDate>Tue, 27 Aug 2024 10:51:45 +0000</pubDate><guid>https://kubermates.org/blog/chaos-engineering-lets-break-everything-io0/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Hey there! ðŸ‘‹ If you&amp;rsquo;re running your applications on Kubernetes, you might already know that things can go wrong in unexpected ways. That&amp;rsquo;s where &lt;strong>chaos engineering&lt;/strong> comes in! Chaos engineering is all about intentionally injecting failures into your system to see how it behaves under stress. The idea is to discover weaknesses and fix them before they can cause real problems.&lt;/p>
&lt;p>Today, we&amp;rsquo;re diving into &lt;strong>Chaos Mesh&lt;/strong>, an awesome tool that makes chaos engineering in Kubernetes super easy and fun (well, as fun as breaking things can be!). We&amp;rsquo;ll go step-by-step through setting up Chaos Mesh and show you how to run some cool chaos experiments to test your app&amp;rsquo;s resilience.&lt;/p></description></item><item><title>A Guide to Modern Kubernetes Service Networking ðŸš€</title><link>https://kubermates.org/blog/a-guide-to-modern-kubernetes-service-networking-4017/</link><pubDate>Mon, 26 Aug 2024 19:32:17 +0000</pubDate><guid>https://kubermates.org/blog/a-guide-to-modern-kubernetes-service-networking-4017/</guid><description>&lt;p>As Kubernetes becomes the go-to platform for managing cloud-native applications, how we handle network traffic has evolved. If youâ€™re already using Kubernetes, you might have heard about service meshes like Istio and Linkerd, which have been popular choices for managing service-to-service communication. But now, thereâ€™s a new player in townâ€”the Kubernetes Gateway API! Letâ€™s dive into what the Gateway API is, how it compares to Istio and Linkerd, and when you might want to use each. ðŸŒŸ&lt;/p></description></item><item><title>ðŸ›¡ï¸ Effective Vulnerability Monitoring in Kubernetes</title><link>https://kubermates.org/blog/effective-vulnerability-monitoring-in-kubernetes-1mge/</link><pubDate>Mon, 26 Aug 2024 10:32:34 +0000</pubDate><guid>https://kubermates.org/blog/effective-vulnerability-monitoring-in-kubernetes-1mge/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Hey there, Kubernetes explorer! ðŸŒŸ As your Kubernetes environment grows, keeping it secure becomes more challenging, especially when dealing with multiple clusters. Imagine managing several clusters (spokes) and needing a single source of truth for all your security metricsâ€”sounds like a big task, right? ðŸ¤” That&amp;rsquo;s where &lt;strong>Trivy&lt;/strong>, &lt;strong>Trivy Operator&lt;/strong>, &lt;strong>OpenTelemetry&lt;/strong>, &lt;strong>Prometheus&lt;/strong>, and &lt;strong>Grafana&lt;/strong> come to the rescue.&lt;/p>
&lt;p>In this guide, Iâ€™ll show you how to set up Trivy and Trivy Operator in a federated Kubernetes environment, collect vulnerability data using OpenTelemetry, and centralize it using either an in-cluster Prometheus setup or managed services like &lt;strong>Azure Monitor (for Prometheus)&lt;/strong> and &lt;strong>Grafana Cloud&lt;/strong> or &lt;strong>Azure Managed Grafana&lt;/strong>. By the end of this, youâ€™ll have a system that monitors vulnerabilities across all your clusters from one place. Letâ€™s dive in! ðŸŠâ€â™‚ï¸&lt;/p></description></item><item><title>Kubernetes Multi-Cluster Management ðŸ“¦</title><link>https://kubermates.org/blog/kubernetes-multi-cluster-management-1nek/</link><pubDate>Fri, 23 Aug 2024 15:10:22 +0000</pubDate><guid>https://kubermates.org/blog/kubernetes-multi-cluster-management-1nek/</guid><description>&lt;p>Managing Kubernetes deployments across multiple clusters is a complex yet crucial task for scaling modern applications. Whether ensuring consistency across environments or automating deployments for high availability, choosing the right tools and approach is essential. In this guide, weâ€™ll explore five powerful toolsâ€”&lt;strong>Helmfile&lt;/strong>, &lt;strong>FluxCD&lt;/strong>, &lt;strong>ArgoCD&lt;/strong>, &lt;strong>ClusterAPI&lt;/strong>, and &lt;strong>Karmada&lt;/strong>â€”and how they can help you efficiently manage multi-cluster Kubernetes environments. Letâ€™s dive into the details and discover which strategy suits your needs best! ðŸŒ&lt;/p></description></item><item><title>How to Make Your Docker Images Go on a Diet ðŸŠâ€â™‚ï¸</title><link>https://kubermates.org/blog/how-to-make-your-docker-images-go-on-a-diet-88l/</link><pubDate>Wed, 21 Aug 2024 07:50:27 +0000</pubDate><guid>https://kubermates.org/blog/how-to-make-your-docker-images-go-on-a-diet-88l/</guid><description>&lt;p>Hey there, Docker enthusiast! ðŸŒŸ Are your Docker images feeling a little bloated lately? Don&amp;rsquo;t worry, you&amp;rsquo;re not alone. As our applications grow, so do our Docker images, but with a few nifty tricks, you can slim them down and keep everything running smooth as butter. ðŸ§ˆ In this guide, we&amp;rsquo;ll explore how to optimize your Docker images using Alpine, Distroless, Scratch images, and multi-stage builds. Let&amp;rsquo;s dive in! ðŸŠâ€â™‚ï¸&lt;/p></description></item><item><title>A quick navigation through Service Mesh in Kubernetes ðŸ‘€</title><link>https://kubermates.org/blog/a-quick-navigation-through-service-mesh-in-kubernetes-5dea/</link><pubDate>Tue, 20 Aug 2024 10:52:45 +0000</pubDate><guid>https://kubermates.org/blog/a-quick-navigation-through-service-mesh-in-kubernetes-5dea/</guid><description>&lt;p>If you&amp;rsquo;re working with Kubernetes, you know that managing communication between microservices can get complicated as your application grows. Enter &lt;strong>Linkerd&lt;/strong>, a powerful yet user-friendly service mesh that simplifies this process by handling traffic management, security, and observability for your microservices. In this article, we&amp;rsquo;ll walk you through what Linkerd is, how to set it up, and how to use it to manage your services, including examples of Blue-Green and Canary deployments. Weâ€™ll also compare Linkerd with Istio, another popular service mesh, and provide references to official documentation to help you along the way.&lt;/p></description></item><item><title>ðŸš€ Kubernetes RBAC and Role Aggregation Made Easy</title><link>https://kubermates.org/blog/kubernetes-rbac-and-role-aggregation-made-easy-3j4o/</link><pubDate>Mon, 19 Aug 2024 21:03:46 +0000</pubDate><guid>https://kubermates.org/blog/kubernetes-rbac-and-role-aggregation-made-easy-3j4o/</guid><description>&lt;h2 id="what-is-kubernetes-rbac-">What is Kubernetes RBAC? ðŸ¤”&lt;/h2>
&lt;p>Kubernetes, the platform that helps you automate, scale, and manage your containerized applications, comes with a cool feature called &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">Role-Based Access Control (RBAC)&lt;/a>. Think of RBAC as a gatekeeper that controls who can do what within your Kubernetes cluster. Itâ€™s super important because it ensures that everyone and everything (like users, applications, and services) only have the permissions they needâ€”nothing more, nothing less.&lt;/p>
&lt;h2 id="the-four-pillars-of-kubernetes-rbac-">The Four Pillars of Kubernetes RBAC ðŸ›ï¸&lt;/h2>
&lt;p>Kubernetes RBAC revolves around four main building blocks:&lt;/p></description></item><item><title>Essential Tips for Setting Resource Limits in Kubernetes ðŸ“ˆ</title><link>https://kubermates.org/blog/essential-tips-for-setting-resource-limits-in-kubernetes-3b54/</link><pubDate>Mon, 19 Aug 2024 07:31:56 +0000</pubDate><guid>https://kubermates.org/blog/essential-tips-for-setting-resource-limits-in-kubernetes-3b54/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Kubernetes is like the ultimate conductor ðŸŽ¶ in an orchestra, ensuring that all your applications (the musicians) play in harmony without hogging too many resources (the instruments). But if you don&amp;rsquo;t set the right limits, things can get out of tune quickly! ðŸŽ» Setting the right resource limitations in Kubernetes helps keep everything running smoothly, preventing any one application from using too much CPU or memory and leaving the rest high and dry.&lt;/p></description></item><item><title>ðŸŒ Kubernetes DNS: How to Use In-Cluster and Azure Private DNS Together</title><link>https://kubermates.org/blog/kubernetes-dns-how-to-use-in-cluster-and-azure-private-dns-together-48h3/</link><pubDate>Sat, 17 Aug 2024 17:51:44 +0000</pubDate><guid>https://kubermates.org/blog/kubernetes-dns-how-to-use-in-cluster-and-azure-private-dns-together-48h3/</guid><description>&lt;p>Kubernetes is a powerful platform for managing containerized applications, providing robust tools for networking, service discovery, and DNS resolution. This guide will explore how Kubernetes handles DNS resolution within the cluster and how you can integrate it with Azure Private DNS to securely resolve external, private resources.&lt;/p>
&lt;h2 id="-understanding-kubernetes-services-and-dns-resolution">ðŸ” Understanding Kubernetes Services and DNS Resolution&lt;/h2>
&lt;p>Kubernetes Services provide a stable network identity for a set of Pods, allowing other components to communicate with them reliably. This is crucial since Pods in Kubernetes are ephemeral and their IP addresses can change over time.&lt;/p></description></item><item><title>How to Manage Kubernetes App Storage Like a Pro ðŸ“</title><link>https://kubermates.org/blog/how-to-manage-kubernetes-app-storage-like-a-pro-o33/</link><pubDate>Fri, 16 Aug 2024 20:25:33 +0000</pubDate><guid>https://kubermates.org/blog/how-to-manage-kubernetes-app-storage-like-a-pro-o33/</guid><description>&lt;p>Managing storage in Kubernetes might seem a bit tricky at first, but donâ€™t worryâ€”we&amp;rsquo;re here to help! This guide will walk you through everything you need to know about Kubernetes volumes, how they work, and how to use them effectively, especially if you&amp;rsquo;re using Azure.&lt;/p>
&lt;h2 id="-what-are-kubernetes-volumes">ðŸ“‚ What Are Kubernetes Volumes?&lt;/h2>
&lt;p>Think of &lt;strong>volumes&lt;/strong> as a way to store data in your Kubernetes pods that doesnâ€™t disappear when the pod shuts down. This is super important for things like saving files, databases, or anything else that needs to stick around.&lt;/p></description></item><item><title>Understanding Pod Topology Spread Constraints and Node Affinity in Kubernetes</title><link>https://kubermates.org/blog/understanding-pod-topology-spread-constraints-and-node-affinity-in-kubernetes-49a2/</link><pubDate>Thu, 15 Aug 2024 14:26:38 +0000</pubDate><guid>https://kubermates.org/blog/understanding-pod-topology-spread-constraints-and-node-affinity-in-kubernetes-49a2/</guid><description>&lt;p>When you&amp;rsquo;re running applications in Kubernetes, it&amp;rsquo;s important to think about where your Pods (the units that make up your application) are placed in your cluster. Getting this right helps keep your application available, resilient, and running smoothly. Two tools that can help you do this are &lt;strong>Pod Topology Spread Constraints&lt;/strong> and &lt;strong>Node Affinity&lt;/strong>. Letâ€™s break these down with some easy-to-understand examples.&lt;/p>
&lt;h2 id="1-pod-topology-spread-constraints">1. Pod Topology Spread Constraints&lt;/h2>
&lt;p>Think of &lt;strong>Pod Topology Spread Constraints&lt;/strong> as a way to tell Kubernetes, &amp;ldquo;Hey, I want my Pods spread out evenly across different parts of my cluster.&amp;rdquo; This helps prevent all your Pods from ending up in the same spot, which could be a problem if that spot has an issue.&lt;/p></description></item><item><title>How to Test the Latest Kubernetes Changes in Version 1.31 "Elli"</title><link>https://kubermates.org/blog/how-to-test-the-latest-kubernetes-changes-in-version-131-elli-39ec/</link><pubDate>Wed, 14 Aug 2024 21:38:00 +0000</pubDate><guid>https://kubermates.org/blog/how-to-test-the-latest-kubernetes-changes-in-version-131-elli-39ec/</guid><description>&lt;p>Testing Kubernetes 1.31 &amp;ldquo;Elli&amp;rdquo; involves setting up a dedicated environment, verifying new features, validating API changes, running automated tests, and closely monitoring your cluster. Hereâ€™s a detailed guide with examples for each step.&lt;/p>
&lt;h2 id="1-set-up-a-testing-environment">1. Set Up a Testing Environment&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Create a Kubernetes Cluster&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Example&lt;/strong>: Use Minikube to create a local cluster. Run:
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">minikube start --kubernetes-version&lt;span class="o">=&lt;/span>v1.31.0
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>This command sets up a Kubernetes 1.31 cluster locally, allowing you to test the new features and changes in a controlled environment.&lt;/li>
&lt;li>&lt;strong>Cloud-Based Testing&lt;/strong>: For cloud environments, use a tool like &lt;code>eksctl&lt;/code> for Amazon EKS:
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">eksctl create cluster --version 1.31 --name test-cluster
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>This command creates an Amazon EKS cluster with Kubernetes 1.31, suitable for more extensive testing scenarios.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Isolate the Environment&lt;/strong>:&lt;/p></description></item><item><title>KubeCon + CloudNativeCon Europe 2026</title><link>https://kubermates.org/events/kubecon-eu-2026/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubermates.org/events/kubecon-eu-2026/</guid><description/></item><item><title>KubeCon + CloudNativeCon North America 2026</title><link>https://kubermates.org/events/kubecon-na-2026/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubermates.org/events/kubecon-na-2026/</guid><description/></item><item><title>PlatformCon 2026</title><link>https://kubermates.org/events/platformcon-2026/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubermates.org/events/platformcon-2026/</guid><description/></item></channel></rss>