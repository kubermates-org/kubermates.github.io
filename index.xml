<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubermates</title><link>https://kubermates.org/</link><description>Recent content on Kubermates</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 04 Sep 2025 15:30:00 +0000</lastBuildDate><atom:link href="https://kubermates.org/index.xml" rel="self" type="application/rss+xml"/><item><title>Cloud Native Talks: Tales of SREs and Kubernetes Security</title><link>https://kubermates.org/events/2025-09-04-cloud-native-talks-tales-of-sres-and-kubernetes-security/</link><pubDate>Thu, 04 Sep 2025 15:30:00 +0000</pubDate><guid>https://kubermates.org/events/2025-09-04-cloud-native-talks-tales-of-sres-and-kubernetes-security/</guid><description>&lt;p&gt;Rafael Ordo√±ez, Ionik Tech (M√°laga):¬†Walking Through Our SRE Journey ‚Äì A Storytelling Approach (English)Saed Farah,¬†Goog&amp;hellip;&lt;/p&gt;</description></item><item><title>Cloud Native Timisoara - Autumn Edition 2025</title><link>https://kubermates.org/events/2025-09-04-cloud-native-timisoara-autumn-edition-2025/</link><pubDate>Thu, 04 Sep 2025 15:00:00 +0000</pubDate><guid>https://kubermates.org/events/2025-09-04-cloud-native-timisoara-autumn-edition-2025/</guid><description>&lt;p&gt;Don&amp;rsquo;t miss this opportunity to expand your knowledge and network with like-minded professionals.&lt;/p&gt;</description></item><item><title>#cTENcf Birthday Bash Taipei</title><link>https://kubermates.org/events/2025-09-04-ctencf-birthday-bash-taipei/</link><pubDate>Thu, 04 Sep 2025 11:30:00 +0000</pubDate><guid>https://kubermates.org/events/2025-09-04-ctencf-birthday-bash-taipei/</guid><description>&lt;p&gt;ÔºàÊú¨Ê¥ªÂãïÊñº 2025/08/21 12:00 PM (UTC+8) ÈñãÊîæ RSVPÔºåÈúÄÊî∂ 50 ÂÖÉÂ†¥Âú∞Ë≤ªÔºåÊ≠°ËøéÂ§ßÂÆ∂‰æÜ‰∏ÄÂêåÂèÉËàáÔºÅÔºâ&lt;/p&gt;
&lt;p&gt;CNCF 10 ÈÄ±Âπ¥Á¥ÄÂøµÊ¥ªÂãïÔºÅ&lt;/p&gt;</description></item><item><title>CNCF On-Demand: One API to Rule Them All - Building a Unified Platform with Kubernetes Aggregation</title><link>https://kubermates.org/events/2025-09-04-cncf-on-demand-one-api-to-rule-them-all-building-a-unified-platform-with-kuberne/</link><pubDate>Thu, 04 Sep 2025 07:00:00 +0000</pubDate><guid>https://kubermates.org/events/2025-09-04-cncf-on-demand-one-api-to-rule-them-all-building-a-unified-platform-with-kuberne/</guid><description>&lt;p&gt;How do you build a unified product from open-source tools? A Cozystack maintainer shares the journey of integrating Helm, Operators, and Kubernetes‚Äô Aggregation Layer to create a general-purpose API‚Äîwithout etcd. Learn how they built a dynamic API server, universal GUI for operators, and real-time b&lt;/p&gt;</description></item><item><title>#cTENcf Birthday Bash Barcelona</title><link>https://kubermates.org/events/2025-09-04-ctencf-birthday-bash-barcelona/</link><pubDate>Thu, 04 Sep 2025 04:30:00 +0000</pubDate><guid>https://kubermates.org/events/2025-09-04-ctencf-birthday-bash-barcelona/</guid><description>&lt;p&gt;#cTENcf #celebrateCNCF, which will celebrate 10 years of the CNCF&lt;/p&gt;</description></item><item><title>Paving a Golden Path: Cloud-Native Lessons on Developer Experience with Thomas Schuetz</title><link>https://kubermates.org/events/2025-09-03-paving-a-golden-path-cloud-native-lessons-on-developer-experience-with-thomas-sc/</link><pubDate>Wed, 03 Sep 2025 17:00:00 +0000</pubDate><guid>https://kubermates.org/events/2025-09-03-paving-a-golden-path-cloud-native-lessons-on-developer-experience-with-thomas-sc/</guid><description>&lt;p&gt;Join us for an enlightening session titled &amp;ldquo;Paving a Golden Path: What Going Cloud-Native Taught us About Developer Expe&amp;hellip;&lt;/p&gt;</description></item><item><title>Fall Kickoff at UW-Madison</title><link>https://kubermates.org/events/2025-09-02-fall-kickoff-at-uw-madison/</link><pubDate>Tue, 02 Sep 2025 22:00:00 +0000</pubDate><guid>https://kubermates.org/events/2025-09-02-fall-kickoff-at-uw-madison/</guid><description>&lt;p&gt;Big congrats to UW‚ÄìMadison for being named to LinkedIn‚Äôs inaugural Top Colleges list for long-term career success‚Äîperfec&amp;hellip;&lt;/p&gt;</description></item><item><title>Cartografos Community Meeting</title><link>https://kubermates.org/events/2025-09-02-cartografos-community-meeting/</link><pubDate>Tue, 02 Sep 2025 17:00:00 +0000</pubDate><guid>https://kubermates.org/events/2025-09-02-cartografos-community-meeting/</guid><description>&lt;p&gt;Bi-weekly community meeting for the Cartografos Working Group.
The Cartografos working group aims to provide tools to help adopters and end-users to navigate the CNCF landscape and the wider cloud native ecosystem.
Please come along - we invite all attendees.&lt;/p&gt;</description></item><item><title>Merge Forward Monthly Meeting</title><link>https://kubermates.org/events/2025-09-02-merge-forward-monthly-meeting/</link><pubDate>Tue, 02 Sep 2025 17:00:00 +0000</pubDate><guid>https://kubermates.org/events/2025-09-02-merge-forward-monthly-meeting/</guid><description>&lt;p&gt;This is the monthly Merge Forward meeting, where representatives from all Merge Forward diversity groups get together. E&amp;hellip;&lt;/p&gt;</description></item><item><title>Chaos Mesh Development Meeting</title><link>https://kubermates.org/events/2025-09-02-chaos-mesh-development-meeting/</link><pubDate>Tue, 02 Sep 2025 01:00:00 +0000</pubDate><guid>https://kubermates.org/events/2025-09-02-chaos-mesh-development-meeting/</guid><description>&lt;p&gt;This is a biweekly meeting to discuss anything related to the project development.&lt;/p&gt;</description></item><item><title>Unlocking Platform Engineering for Organizational Success and The Journey to the Beginning of the Edge with Talos Kubernetes</title><link>https://kubermates.org/events/2025-09-01-unlocking-platform-engineering-for-organizational-success-and-the-journey-to-the/</link><pubDate>Mon, 01 Sep 2025 14:30:00 +0000</pubDate><guid>https://kubermates.org/events/2025-09-01-unlocking-platform-engineering-for-organizational-success-and-the-journey-to-the/</guid><description>&lt;p&gt;Join us for an exciting evening where we dive into two transformative topics:First, discover the world of Team Topologie&amp;hellip;&lt;/p&gt;</description></item><item><title>#cTENcf party - USAC - 10 a√±os de CNCF</title><link>https://kubermates.org/events/2025-08-30-ctencf-party-usac-10-a-os-de-cncf/</link><pubDate>Sat, 30 Aug 2025 17:00:00 +0000</pubDate><guid>https://kubermates.org/events/2025-08-30-ctencf-party-usac-10-a-os-de-cncf/</guid><description>&lt;p&gt;Celebrate 10 years of CNCF!, Nuestra comunidad esta celebrando los 10 a√±os de CNCF, 10 a√±os, 200 proyectos y m√°s de 275000 contribuidores, √∫nete a nosotros en la USAC para celebrar juntos.&lt;/p&gt;</description></item><item><title>CKA Bootcamp by CNCG Gandhinagar - Week 1</title><link>https://kubermates.org/events/2025-08-30-cka-bootcamp-by-cncg-gandhinagar-week-1/</link><pubDate>Sat, 30 Aug 2025 13:30:00 +0000</pubDate><guid>https://kubermates.org/events/2025-08-30-cka-bootcamp-by-cncg-gandhinagar-week-1/</guid><description>&lt;p&gt;üöÄ CKA Bootcamp ‚Äì Master Kubernetes with Hands-On LearningGet ready to take your Kubernetes skills to the next level!Foll&amp;hellip;&lt;/p&gt;</description></item><item><title>#cTENcf Birthday Bash S√£o Paulo</title><link>https://kubermates.org/events/2025-08-30-ctencf-birthday-bash-s-o-paulo/</link><pubDate>Sat, 30 Aug 2025 12:30:00 +0000</pubDate><guid>https://kubermates.org/events/2025-08-30-ctencf-birthday-bash-s-o-paulo/</guid><description>&lt;p&gt;Bem-vindos ao chapter Cloud Native S√£o Paulo! üáßüá∑
Somos um grupo oficial da CNCF (Cloud Native Computing Foundation) que tem como prop√≥sito contribuir para a comunidade Cloud Native e Open Source.
Estimulamos a conex√£o entre entusiastas da tecnologia atrav√©s de encontros virtuais e presenciais, com w&lt;/p&gt;</description></item><item><title>#cTENcf Birthday Bash Dhaka</title><link>https://kubermates.org/events/2025-08-30-ctencf-birthday-bash-dhaka/</link><pubDate>Sat, 30 Aug 2025 08:30:00 +0000</pubDate><guid>https://kubermates.org/events/2025-08-30-ctencf-birthday-bash-dhaka/</guid><description>&lt;p&gt;Celebrating the 10th year of CNCF Communities and the CNCF LandscapeRegister using the form: &lt;a href="https://forms.gle/UjHUpuPvF"&gt;https://forms.gle/UjHUpuPvF&lt;/a&gt;&amp;hellip;&lt;/p&gt;</description></item><item><title>Cloud Native Mauritius August Meetup</title><link>https://kubermates.org/events/2025-08-30-cloud-native-mauritius-august-meetup/</link><pubDate>Sat, 30 Aug 2025 06:00:41 +0000</pubDate><guid>https://kubermates.org/events/2025-08-30-cloud-native-mauritius-august-meetup/</guid><description>&lt;p&gt;Following DevCon, Cloud Native meetups are back. Join us on the 30th of August to learn about Cloud Native technologies &amp;hellip;&lt;/p&gt;</description></item><item><title>#cTENcf Birthday Bash Gurugram</title><link>https://kubermates.org/events/2025-08-30-ctencf-birthday-bash-gurugram/</link><pubDate>Sat, 30 Aug 2025 04:00:00 +0000</pubDate><guid>https://kubermates.org/events/2025-08-30-ctencf-birthday-bash-gurugram/</guid><description>&lt;p&gt;A #cTENcf regional event Exploring the Future of Cloud This edition is extra special ‚Äî we‚Äôre celebrating 10 Years of CN&amp;hellip;&lt;/p&gt;</description></item><item><title>Deep Dive into Kubernetes Networking</title><link>https://kubermates.org/events/2025-08-29-deep-dive-into-kubernetes-networking/</link><pubDate>Fri, 29 Aug 2025 18:00:00 +0000</pubDate><guid>https://kubermates.org/events/2025-08-29-deep-dive-into-kubernetes-networking/</guid><description>&lt;p&gt;Kubernetes networking is a core component of every cluster, yet it remains one of the most misunderstood areas. In this &amp;hellip;&lt;/p&gt;</description></item><item><title>Istio- it Is! &amp; Optimizing DNS Queries</title><link>https://kubermates.org/events/2025-08-28-istio-it-is-optimizing-dns-queries/</link><pubDate>Thu, 28 Aug 2025 23:00:00 +0000</pubDate><guid>https://kubermates.org/events/2025-08-28-istio-it-is-optimizing-dns-queries/</guid><description>&lt;p&gt;Everyone is invited!&lt;/p&gt;
&lt;p&gt;We are in for a treat with two amazing talks!&lt;/p&gt;
&lt;p&gt;Vishwa Gandhi from Oracle on &amp;ldquo;Istio- it Is!&amp;rdquo;&lt;/p&gt;
&lt;p&gt;We had a last-minute change, and the second talk of the evening will now be our friend Goutam Tadi, Astronomer.io. Infra Engineering, Staff Software Engineer on &amp;ldquo;Optimizing DNS queries&amp;rdquo;&lt;/p&gt;</description></item><item><title>#cTENcf Birthday Bash Birmingham</title><link>https://kubermates.org/events/2025-08-28-ctencf-birthday-bash-birmingham/</link><pubDate>Thu, 28 Aug 2025 17:00:00 +0000</pubDate><guid>https://kubermates.org/events/2025-08-28-ctencf-birthday-bash-birmingham/</guid><description>&lt;p&gt;Join us for an exciting meetup to celebrate 10 years of the Cloud Native Computing Foundation (CNCF) and its impact on t&amp;hellip;&lt;/p&gt;</description></item><item><title>#cTENcf Birthday Bash Dublin</title><link>https://kubermates.org/events/2025-08-28-ctencf-birthday-bash-dublin/</link><pubDate>Thu, 28 Aug 2025 17:00:00 +0000</pubDate><guid>https://kubermates.org/events/2025-08-28-ctencf-birthday-bash-dublin/</guid><description>&lt;p&gt;Date: Aug 28, 6:00‚Äâ‚Äì‚Äâ9:30‚ÄØPM @ Microsoft DublinJoin us for an exciting Meetup to celebrate 10 years of the Cloud Native &amp;hellip;&lt;/p&gt;</description></item><item><title>Argo CD Up and Running: Cluster Management</title><link>https://kubermates.org/events/2025-08-28-argo-cd-up-and-running-cluster-management/</link><pubDate>Thu, 28 Aug 2025 16:00:00 +0000</pubDate><guid>https://kubermates.org/events/2025-08-28-argo-cd-up-and-running-cluster-management/</guid><description>&lt;p&gt;Join us for an engaging session as we delve into &amp;lsquo;Argo CD: Up and Running&amp;rsquo; by Christian Hernandez and Andrew Block. Get &amp;hellip;&lt;/p&gt;</description></item><item><title>Odesa Cloud Native #43: Performance Testing with GenAI</title><link>https://kubermates.org/events/2025-08-28-odesa-cloud-native-43-performance-testing-with-genai/</link><pubDate>Thu, 28 Aug 2025 16:00:00 +0000</pubDate><guid>https://kubermates.org/events/2025-08-28-odesa-cloud-native-43-performance-testing-with-genai/</guid><description>&lt;p&gt;Speaker: Oleg Chornyi, &lt;a href="mailto:SRE@Airalo.Agenda"&gt;SRE@Airalo.Agenda&lt;/a&gt;:Srelock Holmes and Dr. Claude Watson: better togetherThe Elephant In The Room: &amp;hellip;&lt;/p&gt;</description></item><item><title>KCD Colombia 2025</title><link>https://kubermates.org/events/2025-08-28-kcd-colombia-2025/</link><pubDate>Thu, 28 Aug 2025 13:00:00 +0000</pubDate><guid>https://kubermates.org/events/2025-08-28-kcd-colombia-2025/</guid><description>&lt;p&gt;Kubernetes Community Days (KCD) Colombia 2025, hosted in Bogot√° on August 29, 2025, is the premier cloud-native and open&amp;hellip;&lt;/p&gt;</description></item><item><title>v2.10.9</title><link>https://kubermates.org/releases/2025-08-27-v2-10-9/</link><pubDate>Wed, 27 Aug 2025 23:00:47 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-27-v2-10-9/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>v2.9.11</title><link>https://kubermates.org/releases/2025-08-27-v2-9-11/</link><pubDate>Wed, 27 Aug 2025 21:37:32 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-27-v2-9-11/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Extreme Performance Series 2025: VMware Cloud Foundation 9.0 in the Hands-On Labs</title><link>https://kubermates.org/docs/2025-08-27-extreme-performance-series-2025-vmware-cloud-foundation-9-0-in-the-hands-on-labs/</link><pubDate>Wed, 27 Aug 2025 21:29:32 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-27-extreme-performance-series-2025-vmware-cloud-foundation-9-0-in-the-hands-on-labs/</guid><description>The Extreme Performance Series is back for 2025! This video blog series highlights recent performance work on VMware technology. In this episode, Todd Muirhead talks with Josh Schnee about the inclusion of VMware Cloud Foundation 9. 0 (VCF 9. 0) in the Hands-on Labs at VMware Explore 2025. Links to additional resources:.</description></item><item><title>Pre-release v2.11.5-rc1</title><link>https://kubermates.org/releases/2025-08-27-pre-release-v2-11-5-rc1/</link><pubDate>Wed, 27 Aug 2025 20:04:54 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-27-pre-release-v2-11-5-rc1/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Introducing Seekable OCI Parallel¬†Pull mode for Amazon EKS</title><link>https://kubermates.org/docs/2025-08-27-introducing-seekable-oci-parallel-pull-mode-for-amazon-eks/</link><pubDate>Wed, 27 Aug 2025 19:13:44 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-27-introducing-seekable-oci-parallel-pull-mode-for-amazon-eks/</guid><description>Containerization has transformed how customers build and deploy modern cloud native applications, offering unparalleled benefits in portability, scalability, and operational efficiency. Containers provide integrated dependency management and enable a standard distribution and deployment model for any workload. With Amazon Elastic Kubernetes Service (Amazon EKS), Kubernetes has emerged as a go-to solution for customers running large-scale containerized workloads that need to efficiently scale to meet evolving needs. However, one persistent challenge continues to impact specific deployment and scaling aspects of Kubernetes workload operations. Container image pulls, particularly when working with large and complex container images, can directly impact the responsiveness and agility of your systems. With the growth of AI/ML workloads, where we see particularly large images, this directly impacts operations as images may take several minutes to pull and prepare. In our recent Under the Hood post for EKS Ultra Scale Clusters, we briefly touched on our evolving solution for this problem, Seekable OCI (SOCI) Parallel Pull. In this post, we‚Äôll explain how container image pulls work and how they impact deployment and scaling operations, we‚Äôll dive deeper into how SOCI parallel pull works, and finally show how it can help you improve image pull performance with your workloads on Amazon EKS. As average container image sizes have grown in recent years, container startup performance has become a critical element of modern cloud native system performance. Image pull and preparation can account for more than 75% of total startup time for new and scaling workloads. This challenge is particularly acute with the rise of AI/ML workloads on Amazon EKS. These workloads have driven significant growth in container image sizes, where images are commonly tens of gigabytes in size.</description></item><item><title>Metal3.io becomes a CNCF incubating project</title><link>https://kubermates.org/docs/2025-08-27-metal3-io-becomes-a-cncf-incubating-project/</link><pubDate>Wed, 27 Aug 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-27-metal3-io-becomes-a-cncf-incubating-project/</guid><description>Posted on August 27, 2025 by Metal3. io Project Maintainers CNCF projects highlighted in this post The CNCF Technical Oversight Committee (TOC) has voted to accept Metal3. io as a CNCF incubating project. Metal3. io joins a growing ecosystem of technologies tackling real-world challenges at the edge of cloud native infrastructure. The Metal3. io project (pronounced: ‚ÄúMetal Kubed‚Äù) provides components for bare metal host management with Kubernetes. You can enroll your bare metal machines, provision operating system images, and then, if you like, deploy Kubernetes clusters to them. From there, operating and upgrading your Kubernetes clusters can be handled by Metal3. io. Moreover, Metal3. io is itself a Kubernetes application, so it runs on Kubernetes and uses Kubernetes resources and APIs as its interface.</description></item><item><title>Refresh cluster insights</title><link>https://kubermates.org/releases/2025-08-27-refresh-cluster-insights/</link><pubDate>Wed, 27 Aug 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-27-refresh-cluster-insights/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Amazon EKS provides two types of insights: Configuration insights and Upgrade insights. Configuration insights identify misconfigurations in your EKS Hybrid Nodes setup that could impair functionality of your cluster or workloads. Upgrade insights identify issues that could impact your ability to upgrade to new versions of Kubernetes. To see the list of insight checks performed and any relevant issues that Amazon EKS has identified, you can call the look in the AWS Management Console, the AWS CLI, AWS SDKs, and Amazon EKS ListInsights API operation. Open the Amazon EKS console. From the cluster list, choose the name of the Amazon EKS cluster for which you want to see the insights. Choose Monitor cluster. Choose the Cluster health tab. In the Configuration insights table, you will see the following columns: Name √¢¬Ä¬ì The check that was performed by Amazon EKS against the cluster. Insight status √¢¬Ä¬ì An insight with a status of Error means that there is a misconfiguration that is likely impacting cluster functionality. An insight with a status of Warning means that the configuration doesn√¢¬Ä¬ôt match the documented approach, but that cluster functionality might work if you configured it intentionally.</description></item><item><title>Kubernetes v1.34: Of Wind &amp; Will (O' WaW)</title><link>https://kubermates.org/docs/2025-08-27-kubernetes-v1-34-of-wind-will-o-waw/</link><pubDate>Wed, 27 Aug 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-08-27-kubernetes-v1-34-of-wind-will-o-waw/</guid><description>Editors: Agustina Barbetta, Alejandro Josue Leon Bellido, Graziano Casto, Melony Qin, Dipesh Rawat Similar to previous releases, the release of Kubernetes v1. 34 introduces new stable, beta, and alpha features. The consistent delivery of high-quality releases underscores the strength of our development cycle and the vibrant support from our community. This release consists of 58 enhancements. Of those enhancements, 23 have graduated to Stable, 22 have entered Beta, and 13 have entered Alpha. There are also some deprecations and removals in this release; make sure to read about those. A release powered by the wind around us ‚Äî and the will within us. Every release cycle, we inherit winds that we don&amp;rsquo;t really control ‚Äî the state of our tooling, documentation, and the historical quirks of our project. Sometimes these winds fill our sails, sometimes they push us sideways or die down. What keeps Kubernetes moving isn&amp;rsquo;t the perfect winds, but the will of our sailors who adjust the sails, man the helm, chart the courses and keep the ship steady. The release happens not because conditions are always ideal, but because of the people who build it, the people who release it, and the bears ^ , cats, dogs, wizards, and curious minds who keep Kubernetes sailing strong ‚Äî no matter which way the wind blows. This release, Of Wind &amp;amp; Will (O&amp;rsquo; WaW) , honors the winds that have shaped us, and the will that propels us forward.</description></item><item><title>Pre-release v2.9.11-rc1</title><link>https://kubermates.org/releases/2025-08-27-pre-release-v2-9-11-rc1/</link><pubDate>Wed, 27 Aug 2025 17:32:18 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-27-pre-release-v2-9-11-rc1/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Kubernetes v1.34.0</title><link>https://kubermates.org/releases/2025-08-27-kubernetes-v1-34-0/</link><pubDate>Wed, 27 Aug 2025 17:19:17 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-27-kubernetes-v1-34-0/</guid><description>See kubernetes-announce@. Additional binary downloads are linked in the CHANGELOG. See the CHANGELOG for more details. There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Pre-release v2.10.9-rc1</title><link>https://kubermates.org/releases/2025-08-27-pre-release-v2-10-9-rc1/</link><pubDate>Wed, 27 Aug 2025 17:13:31 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-27-pre-release-v2-10-9-rc1/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>v1.33.4+k3s1</title><link>https://kubermates.org/releases/2025-08-27-v1-33-4-k3s1/</link><pubDate>Wed, 27 Aug 2025 16:40:07 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-27-v1-33-4-k3s1/</guid><description>This release updates Kubernetes to v1. 33. 4, and fixes a number of issues. For more details on what&amp;rsquo;s new, see the Kubernetes release notes. As always, we welcome and appreciate feedback from our community of users. Please feel free to: There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>v1.32.8+k3s1</title><link>https://kubermates.org/releases/2025-08-27-v1-32-8-k3s1/</link><pubDate>Wed, 27 Aug 2025 16:40:02 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-27-v1-32-8-k3s1/</guid><description>This release updates Kubernetes to v1. 32. 8, and fixes a number of issues. For more details on what&amp;rsquo;s new, see the Kubernetes release notes. As always, we welcome and appreciate feedback from our community of users. Please feel free to: There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>v1.31.12+k3s1</title><link>https://kubermates.org/releases/2025-08-27-v1-31-12-k3s1/</link><pubDate>Wed, 27 Aug 2025 16:39:58 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-27-v1-31-12-k3s1/</guid><description>This release updates Kubernetes to v1. 31. 12, and fixes a number of issues. For more details on what&amp;rsquo;s new, see the Kubernetes release notes. As always, we welcome and appreciate feedback from our community of users. Please feel free to: There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>How OCI Artifacts Will Drive Future AI Use Cases</title><link>https://kubermates.org/docs/2025-08-27-how-oci-artifacts-will-drive-future-ai-use-cases/</link><pubDate>Wed, 27 Aug 2025 14:24:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-27-how-oci-artifacts-will-drive-future-ai-use-cases/</guid><description>Posted on August 27, 2025 by Sascha Grunert, CNCF Member Project Maintainer (Graduate Project) CNCF | Special Interest Group (SIG) | CNCF Ambassador In recent years, the software industry has seen a strong shift toward enabling and supporting Artificial Intelligence (AI) workloads. While a variety of high level tools like Large Language Models (LLMs) already exist to support generic use cases, many domain specific solutions are either not yet available or come with significant development costs and risks, particularly when targeting more niche problems. This raises an important challenge: how can we avoid building a fragmented AI tooling landscape with limited real world applicability? To truly support the next wave of AI innovation, especially in cloud native environments, we need to rethink and reinforce our software component foundation. This includes standardizing formats, improving cross platform interoperability, and evolving Kubernetes with AI native features in mind. One of the most promising developments in this space revolves around Open Container Initiative (OCI) artifacts. OCI artifacts enable users to store and distribute arbitrary files and metadata using OCI compliant container registries. While originally used for generic purposes (like ORAS supports them), they‚Äôre now finding critical roles in AI/ML workflows, especially with the rise of specifications like the CNCF ModelPack. The CNCF ModelPack Specification builds on top of OCI artifacts and aims to standardize the packaging, distribution, and execution of AI models in cloud native environments. By moving away from proprietary formats, ModelPack facilitates reproducibility, portability, and vendor neutrality in machine learning workflows. This opens the door to several important use cases: Having standards around OCI artifacts are essential for building robust AI features into Kubernetes. For example, the Kubernetes Image Volume feature allows pods to mount container image layers as read-only volumes. Image volumes could also support OCI artifacts as a special use case.</description></item><item><title>Simplify Linux management across your systems‚Äô lifecycles with Red Hat Insights</title><link>https://kubermates.org/docs/2025-08-27-simplify-linux-management-across-your-systems-lifecycles-with-red-hat-insights/</link><pubDate>Wed, 27 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-27-simplify-linux-management-across-your-systems-lifecycles-with-red-hat-insights/</guid><description>Share As a system administrator, keeping a Linux fleet running efficiently and securely can feel like you‚Äôre constantly putting out fires. Just when you address one issue, another one pops up, often because of a lack of visibility across your environment and too many manual, repetitive tasks. But what if you could spend less time reacting to problems and more time on the work that drives your business forward? That&amp;rsquo;s why we&amp;rsquo;re committed to building and improving new capabilities in Red Hat Insights for Red Hat Enterprise Linux (RHEL) to help you stay ahead of the curve. Insights for RHEL helps you proactively manage your environment across the entire system lifecycle. With exciting new features ‚Äì some you may have already heard about at Red Hat Summit this past May - you can make better decisions, access targeted information when you need it, and automate repetitive tasks to increase efficiency. Planning for new systems can be difficult when you don&amp;rsquo;t have a clear, centralized view of the future. The new Insights planning for RHEL capability provides a centralized view of future roadmap details, package lifecycle information, and deprecations. This helps you proactively plan system builds, anticipate potential impacts of updates, and align your infrastructure with your long-term strategy. You can build new images with confidence, avoiding last-minute scrambling and unexpected issues. We&amp;rsquo;re also making it easier to create more complete and functional images with package recommendations in image builder. This new feature provides proactive package recommendations based on user inputs and real-world usage patterns, helping you discover valuable components you might otherwise overlook. This is like having a second set of eyes on your work, helping to minimize post-deployment headaches.</description></item><item><title>v1.33.4+rke2r1</title><link>https://kubermates.org/releases/2025-08-26-v1-33-4-rke2r1/</link><pubDate>Tue, 26 Aug 2025 23:04:44 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-26-v1-33-4-rke2r1/</guid><description>This release updates Kubernetes to v1. 33. 4. Important Note If your server (control-plane) nodes were not started with the &amp;ndash;token CLI flag or config file key, a randomized token was generated during initial cluster startup. This key is used both for joining new nodes to the cluster, and for encrypting cluster bootstrap data within the datastore. Ensure that you retain a copy of this token, as is required when restoring from backup. You may retrieve the token value from any server already joined to the cluster: As always, we welcome and appreciate feedback from our community of users. Please feel free to: There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>v1.32.8+rke2r1</title><link>https://kubermates.org/releases/2025-08-26-v1-32-8-rke2r1/</link><pubDate>Tue, 26 Aug 2025 23:04:37 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-26-v1-32-8-rke2r1/</guid><description>This release updates Kubernetes to v1. 32. 8. Important Note If your server (control-plane) nodes were not started with the &amp;ndash;token CLI flag or config file key, a randomized token was generated during initial cluster startup. This key is used both for joining new nodes to the cluster, and for encrypting cluster bootstrap data within the datastore. Ensure that you retain a copy of this token, as is required when restoring from backup. You may retrieve the token value from any server already joined to the cluster: As always, we welcome and appreciate feedback from our community of users. Please feel free to: There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>v1.31.12+rke2r1</title><link>https://kubermates.org/releases/2025-08-26-v1-31-12-rke2r1/</link><pubDate>Tue, 26 Aug 2025 23:04:30 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-26-v1-31-12-rke2r1/</guid><description>This release updates Kubernetes to v1. 31. 12. Important Note If your server (control-plane) nodes were not started with the &amp;ndash;token CLI flag or config file key, a randomized token was generated during initial cluster startup. This key is used both for joining new nodes to the cluster, and for encrypting cluster bootstrap data within the datastore. Ensure that you retain a copy of this token, as is required when restoring from backup. You may retrieve the token value from any server already joined to the cluster: As always, we welcome and appreciate feedback from our community of users. Please feel free to: There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>AWS managed policy updates</title><link>https://kubermates.org/releases/2025-08-26-aws-managed-policy-updates/</link><pubDate>Tue, 26 Aug 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-26-aws-managed-policy-updates/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. An AWS managed policy is a standalone policy that is created and administered by AWS. AWS managed policies are designed to provide permissions for many common use cases so that you can start assigning permissions to users, groups, and roles. Keep in mind that AWS managed policies might not grant least-privilege permissions for your specific use cases because they√¢¬Ä¬ôre available for all AWS customers to use. We recommend that you reduce permissions further by defining customer managed policies that are specific to your use cases. You cannot change the permissions defined in AWS managed policies. If AWS updates the permissions defined in an AWS managed policy, the update affects all principal identities (users, groups, and roles) that the policy is attached to. AWS is most likely to update an AWS managed policy when a new AWS service is launched or new API operations become available for existing services. For more information, see AWS managed policies in the IAM User Guide. You can attach the AmazonEKS_CNI_Policy to your IAM entities. Before you create an Amazon EC2 node group, this policy must be attached to either the node IAM role , or to an IAM role that√¢¬Ä¬ôs used specifically by the Amazon VPC CNI plugin for Kubernetes. This is so that it can perform actions on your behalf.</description></item><item><title>v1.31.12+k3s1</title><link>https://kubermates.org/releases/2025-08-26-v1-31-12-k3s1/</link><pubDate>Tue, 26 Aug 2025 18:58:55 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-26-v1-31-12-k3s1/</guid><description>This release updates Kubernetes to v1. 31. 12, and fixes a number of issues. For more details on what&amp;rsquo;s new, see the Kubernetes release notes. As always, we welcome and appreciate feedback from our community of users. Please feel free to: There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>v1.32.8+k3s1</title><link>https://kubermates.org/releases/2025-08-26-v1-32-8-k3s1/</link><pubDate>Tue, 26 Aug 2025 18:58:21 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-26-v1-32-8-k3s1/</guid><description>This release updates Kubernetes to v1. 32. 8, and fixes a number of issues. For more details on what&amp;rsquo;s new, see the Kubernetes release notes. As always, we welcome and appreciate feedback from our community of users. Please feel free to: There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>v1.33.4+k3s1</title><link>https://kubermates.org/releases/2025-08-26-v1-33-4-k3s1/</link><pubDate>Tue, 26 Aug 2025 18:58:05 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-26-v1-33-4-k3s1/</guid><description>This release updates Kubernetes to v1. 33. 4, and fixes a number of issues. For more details on what&amp;rsquo;s new, see the Kubernetes release notes. As always, we welcome and appreciate feedback from our community of users. Please feel free to: There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Use Envoy Gateway as the Unified Ingress Gateway and Waypoint Proxy for Ambient Mesh</title><link>https://kubermates.org/docs/2025-08-26-use-envoy-gateway-as-the-unified-ingress-gateway-and-waypoint-proxy-for-ambient-/</link><pubDate>Tue, 26 Aug 2025 13:09:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-26-use-envoy-gateway-as-the-unified-ingress-gateway-and-waypoint-proxy-for-ambient-/</guid><description>Posted on August 26, 2025 by Huabing (Robin) Zhao, Software Engineer &amp;amp; Ric Hincapi√©, DevOps and Support Engineer at Tetrate In this article, we‚Äôll look at how you can use Envoy Gateway , an Envoy project open source solution, together with Istio when running in Ambient mode. This allows you to easily leverage the power of Envoy‚Äôs L7 capabilities for Ingress and east-west traffic in your mesh with easy-to-use CRDs. To understand how this integration works, let‚Äôs first take a quick look at Ambient Mesh itself. Also known as Istio Ambient mode , it‚Äôs a sidecar-less service mesh architecture that aims to simplify deployments and can boost efficiency for specific use cases. Unlike sidecar-based meshes, Ambient splits the data plane into two key components: the ztunnel , which secures service-to-service communication, and the Waypoint Proxy , which handles Layer 7 traffic routing and policy enforcement. On the other side, Envoy Gateway is a Kubernetes-native API gateway built on top of Envoy Proxy. It‚Äôs designed to work seamlessly with the Kubernetes Gateway API and takes a batteries-included approach‚Äîoffering built-in support for authentication, authorization, rate limiting, CORS handling, header manipulation, and more. These capabilities are exposed through familiar Kubernetes-style APIs, letting you fully tap into Envoy‚Äôs power without needing complex configurations. Because both Ambient Mesh and Envoy Gateway are built on top of Envoy, they share a common foundation. This makes integration straightforward and allows Envoy Gateway to act as both the Ingress Gateway and Waypoint Proxy ‚Äîgiving you a consistent and powerful way to manage traffic and apply Layer 7 policies across your mesh. While Ambient Mesh simplifies service mesh operations by removing sidecars, its feature set doesn‚Äôt yet match the maturity of the sidecar-based model. Some advanced Layer 7 capabilities are either missing, considered experimental, or require extra complexity to configure in native Ambient mode.</description></item><item><title>Broadcom and Canonical Partner to Fast-track and Secure Containerized Workload Deployments on VMware Cloud Foundation</title><link>https://kubermates.org/docs/2025-08-26-broadcom-and-canonical-partner-to-fast-track-and-secure-containerized-workload-d/</link><pubDate>Tue, 26 Aug 2025 12:35:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-26-broadcom-and-canonical-partner-to-fast-track-and-secure-containerized-workload-d/</guid><description>A survey on the State of Developers found that most developers spend roughly one day each week tackling development and IT inefficiencies 1. This is way too much overhead for an organization‚Äôs most important source of innovation. That‚Äôs why we are very excited about the newly expanded partnership between Broadcom and Canonical announced today at Explore 2025 in Las Vegas that will power high velocity, no friction containerized workload deployments on VMware Cloud Foundation (VCF). It brings together the #1 Private Cloud platform with the #1 Cloud OS to help customers who are building Kubernetes-based modern applications streamline support, improve developer efficiency, manage security risks, and simplify AI workload deployment. VCF 9. 0 delivers a unified, AI-ready private cloud platform to manage traditional as well as modern containerized applications. Customers get a unified cloud experience reducing friction and significantly increasing developer productivity and experience. This partnership will further streamline the deployment of container-based and AI applications. Fueling Private Cloud Momentum VCF with vSphere Kubernetes Service (VKS) is the leading platform of choice for modern private clouds. VKS is Broadcom‚Äôs enterprise-grade upstream conformant CNCF certified Kubernetes distribution, offered as part of VCF. It is easy to install, upgrade, scale, and manage as a multi-cluster deployment delivering a seamless self-service experience. Centralized policy management for access control, networking, security, image registries, and runtime configurations across diverse Kubernetes environments simplifies compliance and governance at scale.</description></item><item><title>Advancing AI for enterprises: Announcing Expanded Collaboration between Broadcom and AMD on AI</title><link>https://kubermates.org/docs/2025-08-26-advancing-ai-for-enterprises-announcing-expanded-collaboration-between-broadcom-/</link><pubDate>Tue, 26 Aug 2025 12:30:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-26-advancing-ai-for-enterprises-announcing-expanded-collaboration-between-broadcom-/</guid><description>Artificial Intelligence (AI) is rapidly transforming industries, and Generative AI (Gen AI) is pushing the boundaries of what‚Äôs possible, creating new content and redefining value creation. However, enterprises face significant challenges in AI adoption, especially concerning privacy, data security, and the need for adaptable infrastructure. This is why Broadcom announced VMware Private AI. Today Broadcom and AMD are announcing the expansion of our collaboration to Advance AI for enterprises with a private, secure, and high-performance AI infrastructure. We will release in the future a joint platform bringing VMware Cloud Foundation (VCF) together with AMD Enterprise AI software and AMD Instinct‚Ñ¢ GPUs. Together this will enable privacy and security of infrastructure, simplify infrastructure management and streamline AI model deployment. We will also enable Broadcom‚Äôs Enhanced DirectPath (I/O) driver models with AMD GPUs to take advantage of VCF‚Äôs unique virtualization capabilities for delivering a scalable and efficient platform for AI workloads. Our expanded efforts directly tackle the core issues enterprises encounter with AI deployments: The core of this expanded collaboration is a robust architecture we will release in the future for enterprise AI. Let‚Äôs review the components of this: Note: Supermicro Computer Inc. server AS-8125GS-TNMR2 with MI300 GPUs are now supported in DirectPath I/O mode with VCF. Let‚Äôs get into some of the capabilities that this expanded collaboration will enable for customers in future. This collaboration will enable privacy and control of corporate data, integrated security and management and help enterprises build and deploy private and secure AI models.</description></item><item><title>Building your GenAI Agents on VCF with Private AI Services</title><link>https://kubermates.org/docs/2025-08-26-building-your-genai-agents-on-vcf-with-private-ai-services/</link><pubDate>Tue, 26 Aug 2025 12:30:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-26-building-your-genai-agents-on-vcf-with-private-ai-services/</guid><description>Today at VMware Explore‚Äôs general session you saw Chris Wolf demonstrate Intelligent Assist for VMware Cloud Foundation, providing AI-powered assistance for our users. In this blog, we‚Äôll take a step behind the curtain to see how these capabilities are running in VCF, using AI features that our customers can also use to build their own AI experiences with their own private data. VMware Private AI services enable administrators to safely and securely import and share approved AI models (Model Gallery and Model Governance); scale and run Models as a Service for their organization (Model Runtime and ML API gateway); create Knowledge bases and regularly refresh data in a fully supported vector database for creating RAG applications (Data Indexing and Retrieval Service in partnership with Data Services Manager); and provide developers a UI where they can compose models, knowledge bases, and tools together to create Agents (Agent Builder). The Intelligent Assist service is using these capabilities to run the Intelligent Assist agent, and VCF engineering teams are using these services as a common AI platform to deliver joint services and AI workflows. Customers can also use these same capabilities for their own teams. These features give private cloud administrators what they need to safely download, validate, and share models with teams across their cloud. Learn about how to safely onboard popular models from upstream and ensure the model‚Äôs behavior meets your enterprises‚Äô expectations and requirements ‚Äì and behavior doesn‚Äôt drift over time in this blog post. Now that you have models securely imported and shared with the right folks in your organization, you will want to run them in an efficient and scalable way. Gone are the days of every division running their own separate copies of the same popular models ‚Äì instead your team can provide Models as a Service using the Model Runtime. Deploy models on a fully maintained runtime stack from directly within VCF, and then horizontally scale them as they come under load with no end user impact, as users broker their requests via the ML API gateway. This also gives you flexibility to do rolling upgrades of models with zero end user impact. This method of deploying models allows separate lines of business or tenants within a Cloud Service Provider to keep their data separate from each other while ensuring high GPU utilization.</description></item><item><title>Engineering the Next Generation of VMware Cloud Foundation</title><link>https://kubermates.org/docs/2025-08-26-engineering-the-next-generation-of-vmware-cloud-foundation/</link><pubDate>Tue, 26 Aug 2025 12:30:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-26-engineering-the-next-generation-of-vmware-cloud-foundation/</guid><description>The private cloud is entering a new era. The workloads our infrastructure must serve have shifted from traditional applications only to also include distributed systems, data-intensive analytics, and AI-driven models that demand unprecedented agility and scale. VMware Cloud Foundation (VCF) has always been about giving customers a single, integrated platform to build and operate their private cloud with consistency. But the requirements are evolving‚Äîand so must the platform. And VMware Cloud Foundation isn‚Äôt just adapting to enterprise transformation ‚Äì it‚Äôs leading it. VCFis the engine driving how modern businesses reimagine their infrastructure, applications, and data strategies. Each new innovation isn‚Äôt a reaction to where the industry is going‚Äîit sets the pace for what‚Äôs possible in the private cloud ‚Äì whether it‚Äôs in a central datacenter, edge or in the hyperscaler environment. VCF 9. 0 set the benchmark for the modern private cloud: Missed it? What‚Äôs New In VMware Cloud Foundation 9. 0 VCF 9. 0 was just the beginning of that journey. We‚Äôre here to solve the next set of challenges that the world of IT needs to overcome.</description></item><item><title>Strengthened Cyber-Risk Management and Compliance for Large-Scale VMware Cloud Foundation Environments</title><link>https://kubermates.org/docs/2025-08-26-strengthened-cyber-risk-management-and-compliance-for-large-scale-vmware-cloud-f/</link><pubDate>Tue, 26 Aug 2025 12:30:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-26-strengthened-cyber-risk-management-and-compliance-for-large-scale-vmware-cloud-f/</guid><description>Is data an organization‚Äôs most valuable asset, or biggest liability? The ever-evolving risks presented by today‚Äôs modern threat landscape pose unprecedented challenges that most today are ill-equipped to address. With ransomware attacks becoming increasingly sophisticated and regulatory guidelines more stringent, cyber-risk management and compliance lie at the core of every strategic IT decision made by C-Suite executives and board members. 51% of leaders reported security, data protection and privacy as key priorities to enable compliant operations 1. However, the path to achieve successful outcomes is beset with obstacles. Why Organizations Struggle to Manage Cyber-Risk and Maintain Compliance Reactive vs. Proactive Response: data is scattered across different applications in multiple geographic locations, each with its own rules and regulations. 63% of organizations report that complexity from the disaggregated nature of data as the main challenge to maintain compliance 2. Policy monitoring for virtual machines (VMs), containers, databases, infrastructure and technology stack components is siloed, which hinders early detection of drifts and leads to a reactive versus proactive response. Manual Remediation and Recovery: policy management and recovery lack automation, which directly impacts an organization‚Äôs ability to scale operations and preserve compliance as applications are changed, moved or deleted. Intense manual intervention inevitably delays time to audit-readiness and leaves unaddressed gaps that could expose critical workloads to increased damage. Stitching of Multiple Tools: consolidation of cyber-risk management, resilience and recovery is easier said than done. Most organizations rely on a piecemeal approach, plagued with operational complexity and inefficiencies that increase chances of human error.</description></item><item><title>Unleashing the Power of Private AI: New Innovations from Broadcom with NVIDIA</title><link>https://kubermates.org/docs/2025-08-26-unleashing-the-power-of-private-ai-new-innovations-from-broadcom-with-nvidia/</link><pubDate>Tue, 26 Aug 2025 12:30:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-26-unleashing-the-power-of-private-ai-new-innovations-from-broadcom-with-nvidia/</guid><description>Enterprises can get tremendous productivity and business transformation from AI. With VMware Private AI Foundation with NVIDIA , Broadcom and NVIDIA aim to unlock AI and unleash productivity with lower TCO. Recently with VCF 9. 0 , Broadcom and NVIDIA released several features in VMware Private AI Foundation with NVIDIA to further our mission of providing private and secure AI models for enterprises. Today we are happy to announce additional capabilities to help enterprises in this mission. Previously sold separately, VCF will now include VCF Private AI services as part of the platform. Private AI services enable privacy and security, simplify infrastructure management and streamline model deployment. They include capabilities such as GPU Monitoring, Model Store, Model Runtime, Agent Builder, Vector Database and Data Indexing and Retrieval. By embedding all the benefits of Private AI into VMware Cloud Foundation, enterprises will get a unified platform for their AI and non-AI workloads without an additional purchase. We are releasing an exciting new capability in the platform. NVIDIA Blackwell GPUs unlock the potential of generative, agentic and physical AI by delivering exceptional performance, efficiency and scale for enterprises. VCF will now support the NVIDIA Blackwell architecture, enabling enterprises to get the industry-leading AI training and inference capabilities at unprecedented scale.</description></item><item><title>DigitalOcean MCP Server is now available</title><link>https://kubermates.org/docs/2025-08-26-digitalocean-mcp-server-is-now-available/</link><pubDate>Tue, 26 Aug 2025 02:32:43 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-26-digitalocean-mcp-server-is-now-available/</guid><description>By Mavis Franco , Nicole Ghalwash , Amit Jotwani , and Bikram Gupta The new DigitalOcean MCP (Model Context Protocol) Server enables you to manage your cloud resources with simple, natural language commands through AI-powered tools like Cursor, Claude, or your own custom Large Language Models (LLM). Running locally, it connects seamlessly to 9 services √¢¬Ä¬ì making cloud operations faster, easier, and more intuitive for developers. √¢¬Ü¬í Try the new DigitalOcean MCP Server today by following the specific configuration guidelines for the MCP client from the DigitalOcean MCP GitHub repo √¢¬Ü¬í Watch our most recent MCP Server video walkthrough: √¢¬Ü¬í Explore the Model Context Protocol (MCP) Overview Model Context Protocol (MCP) is an open-source standard that streamlines how AI systems like large language models (LLMs) connect with external tools, systems, and data sources. It defines a standard and consistent way to manage and share context across machine learning components, replacing fragmented integrations. An MCP Server acts as a bridge between an AI application and these external resources. Learn more about the DigitalOcean MCP Server Now supports 9 services (and growing): Accounts , App Platform , Databases , DOKS , Droplets , Insights , Marketplace , Networking , and Spaces Storage. Instead of juggling multiple dashboards or tools, you can manage common cloud operations right inside your favorite MCP-compatible tools. Turn plain English into real API actions. No more digging through docs or writing scripts √¢¬Ä¬ì just ask your AI assistant. Examples: Deploy and Manage Apps: Run commands like deploy a Ruby on Rails app from my GitHub repo or check which apps are on my account. Create and Manage Databases: Easily provision a new PostgreSQL database or create a new database. For example, you could use the command create a new PostgreSQL database named &amp;ldquo;my-project-db&amp;rdquo; with PostgreSQL version 14 and it would do so in no time.</description></item><item><title>Accelerating 5G standalone rollout: continuous testing to enhance robustness, interoperability and efficiency</title><link>https://kubermates.org/docs/2025-08-26-accelerating-5g-standalone-rollout-continuous-testing-to-enhance-robustness-inte/</link><pubDate>Tue, 26 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-26-accelerating-5g-standalone-rollout-continuous-testing-to-enhance-robustness-inte/</guid><description>Share In this post: Deploying a 5G core network involves several key components, including the Access and Mobility Management Function (AMF) for connection and mobility management, the Session Management Function (SMF) for managing user data sessions, the User Plane Function (UPF) for service delivery, the Network Repository Function (NRF) for service discovery, and the Policy Control Function (PCF) for policy enforcement among others. A significant challenge for telecommunication (telco) service providers is the management of different software versions of these network functions, especially in a multivendor environment. A particular version may include new features, bug fixes, or require specific configurations. Consequently, testing of the network functions is complex, going beyond the verification of each component&amp;rsquo;s functionality to ensure that various versions of these network functions can interoperate. Dealing with this complexity necessitates extensive testing automation to guarantee streamlined operations across the service provider‚Äôs network. Adopting a cloud-native design for the 5G core gives service providers a competitive edge in building networks that easily scale and adapt. A distributed and disaggregated 5G core network introduces additional challenges, particularly around ensuring that all the different components can work together. As 5G core cloud-native network functions (CNFs) are made up of microservices that are constantly being updated and deployed in a dynamic environment, keeping tabs on security and adherence to standards becomes an overwhelming task. To ensure that all independently developed components of the 5G core operate effectively, the various network functions within the 5G core must interoperate efficiently, adhering to 3GPP standards. Still, as each 5G core vendor implements their microservices slightly differently, rigorous testing of these interactions is crucial. To ensure robustness and performance, the 5G core needs to be tested at different levels: Red Hat and Rebaca have collaborated to produce an autonomous continuous testing framework to address the challenges outlined with 5G core lifecycle management. The framework is engineered to facilitate the consistent delivery of compliant and interoperable 5G core CNFs.</description></item><item><title>What‚Äôs New in Calico ‚Äì Summer 2025</title><link>https://kubermates.org/docs/2025-08-25-what-s-new-in-calico-summer-2025/</link><pubDate>Mon, 25 Aug 2025 22:17:49 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-25-what-s-new-in-calico-summer-2025/</guid><description>As Kubernetes adoption scales across enterprise architectures, platform architects face mounting pressure to implement consistent security guardrails across distributed, multi-cluster environments while maintaining operational velocity. Modern infrastructure demands a security architecture that can adapt without introducing complexity or performance penalties. Traditional approaches force architects to cobble together separate solutions for ingress protection, network policies, and application-layer security, creating operational friction and increasing attack surface. Today, we‚Äôre announcing significant enhancements to Calico that eliminate this architectural complexity. This release introduces native Web Application Firewall (WAF) capabilities integrated directly into Calico‚Äôs Ingress Gateway, enabling platform architects to deploy a single technology stack for both ingress management and HTTP-layer threat protection. Combined with enhanced Role-Based Access Controls (RBAC) controls, and centralized observability across heterogeneous workloads, platform architects can now design and implement comprehensive security all within a unified platform. The new features in this release can be grouped under two main categories: Ingress traffic into a Kubernetes cluster is a common entry point for attacks, so it‚Äôs critical to inspect and proactively secure it. Since clusters often receive traffic directly from the public internet, analyzing application-layer protocols like HTTP and gRPC for threats is a fundamental security requirement. While there are options to deploy a standalone Web Application Firewall (WAF) with your ingress controller, using an integrated WAF simplifies operations and can reduce both complexity and cost. Calico Ingress Gateway , our implementation of the Kubernetes Gateway API, now includes a built-in WAF that allows you to inspect, authorize, and secure ingress traffic at runtime. The WAF that is integrated with Ingress Gateway is the same as the one used in Calico‚Äôs workload-based WAF , giving you consistent threat detection across both ingress points and internal services. With this built-in WAF, you can define and enforce security rules directly within the Ingress Gateway, enabling deep inspection of HTTP and gRPC traffic and blocking known threats before they reach your workloads.</description></item><item><title>Pre-release v2.11.5-alpha3</title><link>https://kubermates.org/releases/2025-08-25-pre-release-v2-11-5-alpha3/</link><pubDate>Mon, 25 Aug 2025 22:02:52 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-25-pre-release-v2-11-5-alpha3/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Extreme Performance Series 2025: vSAN ESA vs Traditional Storage Array</title><link>https://kubermates.org/docs/2025-08-25-extreme-performance-series-2025-vsan-esa-vs-traditional-storage-array/</link><pubDate>Mon, 25 Aug 2025 20:34:53 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-25-extreme-performance-series-2025-vsan-esa-vs-traditional-storage-array/</guid><description>The Extreme Performance Series is back for 2025! This video blog series highlights recent performance work on VMware technology. In this video, Todd Muirhead talks with Pete Koehler about the performance advantage of vSAN ESA over a traditional storage array that he discussed in a recent blog. Links to additional resources:.</description></item><item><title>Extreme Performance Video Blog Series 2025</title><link>https://kubermates.org/docs/2025-08-25-extreme-performance-video-blog-series-2025/</link><pubDate>Mon, 25 Aug 2025 20:34:24 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-25-extreme-performance-video-blog-series-2025/</guid><description>Back for a fifth year, the Extreme Performance Series will continue the tradition of exploring some of the exciting work being done at Broadcom. The following series of blog posts is all new for 2025 and will be released starting during the week of VMware Explore. Looking forward to a great year of extreme performance! Previous years:.</description></item><item><title>Pre-release v2.12.1-alpha4</title><link>https://kubermates.org/releases/2025-08-25-pre-release-v2-12-1-alpha4/</link><pubDate>Mon, 25 Aug 2025 19:53:29 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-25-pre-release-v2-12-1-alpha4/</guid><description>rancher/rancher-agent:v2. 12. 1-alpha4 rancher/rancher-webhook:v0. 8. 1-rc. 3 rancher/rancher:v2. 12. 1-alpha4 rancher/rke2-cloud-provider:v1. 30. 13-rc1. 0. 20250516172343-e77f78ee9466-build20250613 rancher/rke2-cloud-provider:v1.</description></item><item><title>How Should Prometheus Handle OpenTelemetry Resource Attributes? ‚Äì A UX Research Report</title><link>https://kubermates.org/docs/2025-08-25-how-should-prometheus-handle-opentelemetry-resource-attributes-a-ux-research-rep/</link><pubDate>Mon, 25 Aug 2025 19:42:02 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-25-how-should-prometheus-handle-opentelemetry-resource-attributes-a-ux-research-rep/</guid><description>Posted on August 25, 2025 by Victoria Nduka, User Experience Designer CNCF projects highlighted in this post On May 29th, 2025, I wrapped up my mentorship with Prometheus through the Linux Foundation Mentorship Program. My project focused on understanding how Prometheus handles OpenTelemetry resource attributes and how that experience could be improved for users. My job was to conduct user research to get the user perspective on this challenge. In three months, I conducted user and stakeholder interviews, ran a survey, and analyzed the findings. In this article, I‚Äôll share how I conducted the research, what I uncovered and where the communities involved could go from here. OpenTelemetry (OTel) has something called a resource attribute, which is extra information about the source of a metric, like the service, host, or environment that generated it. Prometheus, a time-series database, uses labels to identify and query metrics. If resource attributes are converted to labels, they can cause what‚Äôs known as ‚Äúa cardinality explosion‚Äù, essentially creating too many unique combinations that overwhelm the system. This usually happens if the attributes change often or include a lot of unique values, like user IDs or pod names. Currently, there are three main approaches to handling this challenge: These aren‚Äôt bad solutions technically, but they don‚Äôt provide the best user experience. So, I conducted this research to understand what the Prometheus maintainers might be missing about the user experience. The research objectives were: My research approach was a mix of qualitative and quantitative research.</description></item><item><title>4.20.0-okd-scos.ec.13</title><link>https://kubermates.org/releases/2025-08-25-4-20-0-okd-scos-ec-13/</link><pubDate>Mon, 25 Aug 2025 07:54:47 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-25-4-20-0-okd-scos-ec-13/</guid><description>These archives contain the client tooling for OKD on CentOS Stream CoreOS. To verify the contents of this directory, use the &amp;lsquo;gpg&amp;rsquo; and &amp;lsquo;shasum&amp;rsquo; tools to ensure the archives you have downloaded match those published from this location. The openshift-install binary has been preconfigured to install the following release: There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>August 25, 2025</title><link>https://kubermates.org/releases/2025-08-25-august-25-2025/</link><pubDate>Mon, 25 Aug 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-08-25-august-25-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>VCF Breakroom Chats Episode 56: FinOps in VMware Cloud Foundation</title><link>https://kubermates.org/docs/2025-08-25-vcf-breakroom-chats-episode-56-finops-in-vmware-cloud-foundation/</link><pubDate>Mon, 25 Aug 2025 04:32:39 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-25-vcf-breakroom-chats-episode-56-finops-in-vmware-cloud-foundation/</guid><description>Welcome to the next episode of the VCF Breakroom Chats. Today, we are happy to present this vLog with Soumya Kapoor, Product Manager, VCF Division, Broadcom. In this episode, Soumya Kapoor and Sachin Alex discuss all things FinOps and show you how to implement cost controls in your VMware Cloud Foundation environment. Want to learn more about VCF Operations? About the VCF Breakroom Chat Series This webinar series focuses on VMware Cloud Foundation. In this series, we share conversations with industry-recognized experts from Broadcom and with solution partners and customers. You‚Äôll gain relevant insights whether you‚Äôre an IT practitioner, IT admin, cloud or platform architect, developer, DevOps, senior IT manager, IT executive, or AI/ML professional. If you are new to the series, please check out our previous episodes.</description></item><item><title>Accelerate issue resolution with a Dedicated Operations Technical Account Manager</title><link>https://kubermates.org/docs/2025-08-25-accelerate-issue-resolution-with-a-dedicated-operations-technical-account-manage/</link><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-25-accelerate-issue-resolution-with-a-dedicated-operations-technical-account-manage/</guid><description>Share Building and managing applications and IT infrastructure securely is a complex task for even the most skilled individual or team. While Red Hat makes it easier to work across platforms and environments with strengthened open source solutions, organizations may lack the time and expertise required to take full advantage of their current Red Hat investments. With a Dedicated Operations Technical Account Manager (TAM), organizations have an embedded resource that serves as an extension of their team within Red Hat. Focused on Day 2 diagnostics, remediation, and risk mitigation, a Dedicated Operations TAM is the ideal resource for complex, high-uptime, or regulated environments. Unlike Red Hat‚Äôs standard Technical Account Management subscriptions, the Dedicated Operations TAM offers hands-on issue resolution, optional on-site presence, and security-cleared personnel, making it a value add for OpenShift, Ansible, and compliance-driven customers. TAMs help ensure that you have the tools and knowledge needed to continually innovate while maintaining regulatory compliance and mitigating risk. Quickly reacting to production downtime and minimizing disruptions while building a reliable IT infrastructure and deploying applications is no easy task. Today‚Äôs IT teams are under pressure to deliver more with fewer resources. Bringing in a dedicated expert can provide much-needed support, thereby increasing flexibility, improving scalability, and streamlining the management of complex systems. Your Dedicated Operations TAM allows you to address your specific needs more effectively. Additionally, they provide: A Dedicated Operations TAM is a vital asset for busy teams with critical availability requirements, serving as a single point of contact for technical guidance, issue resolution, and strategic planning. With deep knowledge of your organization‚Äôs systems and goals, a TAM helps prioritize tasks, streamlines support, and proactively identifies potential challenges before they escalate.</description></item><item><title>From core to tactical edge: A unified platform for defense innovation</title><link>https://kubermates.org/docs/2025-08-25-from-core-to-tactical-edge-a-unified-platform-for-defense-innovation/</link><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-25-from-core-to-tactical-edge-a-unified-platform-for-defense-innovation/</guid><description>Share Driven by the need for agility, security, and sovereignty, the defence sector is undergoing a rapid digital transformation. Military organizations are increasingly operating across a hybrid infrastructure, spanning the strategic core, deployed edge, and tactical edge, while maintaining absolute control over their systems. However, this evolution presents significant challenges, from siloed technologies to cybersecurity threats. Defence organizations must navigate these obstacles by adopting a unified platform approach, leveraging an open framework based on open standards to strengthen autonomy, security, and seamless operations across all environments. Traditionally, defence operations began in the strategic core, which were large, on-premise data centers disconnected from the internet. These handled everything from mission planning to logistics. The first wave of transformation introduced private cloud solutions, allowing military organizations to scale infrastructure dynamically while maintaining digital sovereignty. Many defence entities remain cautious about using the public cloud due to security concerns, opting instead for air-gapped private clouds‚Äîphysically isolated environments within their facilities‚Äîso only screened personnel and verified secure networks have access. Today, the focus has shifted to edge computing, highlighting the need for smarter, real-time, and autonomous decision-making in the field. Defence organizations now operate across two key edge environments, the deployed edge and the tactical edge. The deployed edge consists of mini data centres, capable of running mission-critical applications, processing intelligence and supporting AI workloads. Meanwhile, the tactical edge encompasses lightweight, ruggedized devices such as drones, autonomous vehicles, and soldier-worn systems that can collect, analyse, and act on data in real time, often completely disconnected from central networks.</description></item><item><title>How I Team Up with GenAI to Craft Conference Talk Proposals</title><link>https://kubermates.org/docs/2025-08-22-how-i-team-up-with-genai-to-craft-conference-talk-proposals/</link><pubDate>Fri, 22 Aug 2025 20:09:07 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-22-how-i-team-up-with-genai-to-craft-conference-talk-proposals/</guid><description>Posted on August 22, 2025 by Whitney Lee, CNCF Ambassador and Senior Technical Advocate at Datadog TL;DR: GenAI can help you write conference abstracts, but if you use it without injecting your own curiosity and humanness, reviewers can tell. This post walks through my personal process for co-writing abstracts with GenAI in a way that preserves my voice and experience. I developed this process for technical talks, but it adapts well to other formats too. I‚Äôve spoken at a lot of conferences, big and small. I‚Äôve given keynotes. Breakouts. Workshops. I‚Äôve been around the block and written many conference talk proposals. I‚Äôve also been on program committees and reviewed others‚Äô proposals. There‚Äôs always room for improvement, but I like to think I know what good is. In my view, here are some elements of a good talk proposal: ‚Äì A clear, engaging idea (or a run-of-the-mill idea with a compelling angle) ‚Äì Some clear details about the talk, like concrete examples or stories, or talk format and tools covered ‚Äì Clear audience ‚Äì Clear takeaways As you can imagine, the widespread use of GenAI to generate talk proposals has changed the game for both submitters and reviewers. As a submitter, you can type just one line into ChatGPT, for example: Write me a KubeCon + CloudNativeCon talk abstract on Validating Admission Policy And it produces something that, on the surface, looks pretty good!* Title: Enforcing What Matters: Building Safer Kubernetes with Validating Admission Policies Abstract: As Kubernetes adoption continues to grow across organizations, ensuring workloads meet security, compliance, and operational standards before hitting the cluster is more critical than ever.</description></item><item><title>Pre-release v2.9.11-alpha3</title><link>https://kubermates.org/releases/2025-08-22-pre-release-v2-9-11-alpha3/</link><pubDate>Fri, 22 Aug 2025 19:25:24 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-22-pre-release-v2-9-11-alpha3/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Pre-release v2.11.5-alpha2</title><link>https://kubermates.org/releases/2025-08-22-pre-release-v2-11-5-alpha2/</link><pubDate>Fri, 22 Aug 2025 18:57:29 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-22-pre-release-v2-11-5-alpha2/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Pre-release v2.12.1-alpha3</title><link>https://kubermates.org/releases/2025-08-22-pre-release-v2-12-1-alpha3/</link><pubDate>Fri, 22 Aug 2025 15:26:41 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-22-pre-release-v2-12-1-alpha3/</guid><description>rancher/fleet-agent:v0. 13. 1-rc. 6 rancher/fleet:v0. 13. 1-rc. 6 rancher/prometheus-federator:v4. 1. 0-rc. 3 rancher/rancher-agent:v2. 12. 1-alpha3 rancher/rancher-webhook:v0.</description></item><item><title>Pre-release v2.10.9-alpha2</title><link>https://kubermates.org/releases/2025-08-22-pre-release-v2-10-9-alpha2/</link><pubDate>Fri, 22 Aug 2025 14:39:38 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-22-pre-release-v2-10-9-alpha2/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Virtually Speaking Podcast: Explore Las Vegas 2025 Preview</title><link>https://kubermates.org/docs/2025-08-22-virtually-speaking-podcast-explore-las-vegas-2025-preview/</link><pubDate>Fri, 22 Aug 2025 14:20:04 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-22-virtually-speaking-podcast-explore-las-vegas-2025-preview/</guid><description>Get ready for VMware Explore 2025 in Las Vegas! In this episode of Virtually Speaking, hosts Pete Flecha and John Nicholson are joined by Brad Tompkins, Executive Director of VMUG, to preview what attendees can expect at this year‚Äôs event. They cover: Whether you‚Äôre coming for the deep technical content, the networking, or the community, VMware Explore 2025 has something for everyone. We hope to see you there! The Virtually Speaking Podcast is a technical podcast dedicated to discussing VMware topics related to private and hybrid cloud. Each week Pete Flecha and John Nicholson bring in various subject matter experts from within the industry to discuss their respective areas of expertise. If you‚Äôre new to the Virtually Speaking Podcast check out all episodes on vspeakingpodcast. com and follow on Twitter\X @VirtSpeaking.</description></item><item><title>DenizBank drives AI innovation with Red Hat OpenShift AI</title><link>https://kubermates.org/docs/2025-08-22-denizbank-drives-ai-innovation-with-red-hat-openshift-ai/</link><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-22-denizbank-drives-ai-innovation-with-red-hat-openshift-ai/</guid><description>Share In the competitive world of modern finance, staying ahead means embracing innovation. For DenizBank, one of T√ºrkiye&amp;rsquo;s leading private banks, this means a strong commitment to integrating artificial intelligence (AI) and machine learning (ML) into the core of its operations. Driving this technological evolution within DenizBank is Intertech, DenizBank&amp;rsquo;s IT subsidiary, which has played a key role in redefining what&amp;rsquo;s possible in banking. Before this modernization, DenizBank&amp;rsquo;s data science teams faced significant hurdles that hindered their ability to innovate quickly. Their workflows were characterized by: Intertech recognized that a piecemeal approach wouldn&amp;rsquo;t work. They needed a comprehensive and standardized solution. Their goals were clear: improve time-to-market, reduce AI costs, automate the entire data science pipeline, and empower their teams with self-service capabilities. The foundation was already in place. With over three years of experience running an enterprise application platform environment‚ÄîRed Hat OpenShift‚ÄîIntertech had a deep well of expertise with robust, container-based architectures. The natural next step was to adopt Red Hat OpenShift AI. It was chosen for its powerful ability to leverage Kubernetes for scalability and its GitOps-friendly, code-based approach to infrastructure. Because OpenShift AI is a fully integrated product with OpenShift, the MLOps team adopted GitOps practices to manage the entire AI lifecycle, from experiments to production models, with full trackability.</description></item><item><title>Friday Five ‚Äî August 22, 2025</title><link>https://kubermates.org/docs/2025-08-22-friday-five-august-22-2025/</link><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-22-friday-five-august-22-2025/</guid><description>Share Red Hat is formally endorsing the United Nations (UN) Open Source Principles, which align closely with their own long-standing practices of open collaboration and community building. This endorsement aims to further drive open source adoption and innovation globally, emphasizing transparency, sustainability, and accountability. Learn more Gen AI has the potential to significantly improve customer engagement, reduce costs, and boost productivity. However, many enterprises struggle to move beyond initial experiments to widespread and scalable implementations. This article examines 6 common obstacles to enterprise AI adoption and how Red Hat can help you overcome them. Learn more This is the first post in a new series that looks at the people and planning that went into building and releasing Red Hat Enterprise Linux 10. From the earliest conceptual stages to the launch at Red Hat Summit 2025, we‚Äôll hear firsthand accounts of how RHEL 10 came into being. Learn more The infrastructure decisions you make today shape your organization&amp;rsquo;s flexibility tomorrow. But for many IT leaders, the challenge isn&amp;rsquo;t choosing the next technology, it&amp;rsquo;s figuring out how to move forward without breaking what already works. Running your Citrix Virtual Apps and Desktops on a unified platform with OpenShift Virtualization respects your existing investment while setting you up for what&amp;rsquo;s next. Learn more The line between agentic AI and gen AI can feel blurry because they both begin with a prompt from a user and typically exist in a chatbot-like format. This article looks at the the differences between the 2 technologies.</description></item><item><title>Optimize your virtualization platform: IBM Turbonomic now manages VMs on Red Hat OpenShift</title><link>https://kubermates.org/docs/2025-08-22-optimize-your-virtualization-platform-ibm-turbonomic-now-manages-vms-on-red-hat-/</link><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-22-optimize-your-virtualization-platform-ibm-turbonomic-now-manages-vms-on-red-hat-/</guid><description>Share With many businesses rethinking their reliance on their traditional virtualization platform, IT teams are looking for ways to manage costs without compromising performance or control. For organizations migrating VM workloads or running them side by side with containers, IBM Turbonomic and Red Hat OpenShift Virtualization now offer a unified, intelligent platform to optimize and enable innovation. The powerful new integration between IBM Turbonomic, a leader in application resource management (ARM) and Red Hat OpenShift Virtualization extends OpenShift by adding support for virtual machines (VMs) alongside containers. Powered by KubeVirt and the KVM hypervisor, OpenShift Virtualization lets you run, manage, and migrate Linux and Windows VMs. For organizations focused solely on virtualization, Red Hat also offers OpenShift Virtualization Engine , a dedicated edition of OpenShift that provides the same proven virtualization capabilities without the container platform and application development and delivery features found in the broader OpenShift platform. IBM Turbonomic already helps OpenShift users automatically manage and optimize container workloads. With this new integration, Turbonomic extends those same capabilities to VMs running on OpenShift Virtualization, providing teams with unified control of resource optimization across both VMs and containers in hybrid environments. IBM Turbonomic is a platform designed to manage and optimize application resources automatically. It continuously checks resource demands and provides clear recommendations and automated adjustments to keep applications running smoothly and efficiently. At its core, Turbonomic takes a unique application-centric approach to resource management. It automatically discovers your entire environment, from applications to infrastructure, and maps the dependencies between them. Using real-time analytics and AI-driven decision making, it determines the exact resources each application needs and takes action to ensure those needs are met.</description></item><item><title>Release v1.8.6</title><link>https://kubermates.org/releases/2025-08-21-release-v1-8-6/</link><pubDate>Thu, 21 Aug 2025 21:56:35 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-21-release-v1-8-6/</guid><description>Rancher Kubernetes Engine (RKE) is reaching its end of life. Version 1. 8 is the final release in the RKE 1. x series. We strongly recommend migrating to Rancher&amp;rsquo;s newer Kubernetes distribution, RKE2, to stay supported, secure, and take advantage of the latest features and updates. For more details, please refer to the official SUSE EOL article. NOTE: v1. 32. 7-rancher1-1 and v1. 31. 11-rancher1-1 are not available without an active RKE Extended Life subscription. For more details, please refer to the SUSE EOL article.</description></item><item><title>Pre-release v2.12.1-alpha2</title><link>https://kubermates.org/releases/2025-08-21-pre-release-v2-12-1-alpha2/</link><pubDate>Thu, 21 Aug 2025 21:03:27 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-21-pre-release-v2-12-1-alpha2/</guid><description>rancher/fleet-agent:v0. 13. 1-rc. 6 rancher/fleet:v0. 13. 1-rc. 6 rancher/prometheus-federator:v4. 1. 0-rc. 3 rancher/rancher-agent:v2. 12. 1-alpha2 rancher/rancher-webhook:v0.</description></item><item><title>Migrate to Amazon EKS: Data plane cost modeling with Karpenter and KWOK</title><link>https://kubermates.org/docs/2025-08-21-migrate-to-amazon-eks-data-plane-cost-modeling-with-karpenter-and-kwok/</link><pubDate>Thu, 21 Aug 2025 18:18:42 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-21-migrate-to-amazon-eks-data-plane-cost-modeling-with-karpenter-and-kwok/</guid><description>When migrating Kubernetes clusters to Amazon Elastic Kubernetes Service (Amazon EKS) , organizations typically follow three phases: assessment, mobilize, and migrate and modernize. The assessment phase involves evaluating technical feasibility for Amazon EKS workloads, analyzing current Kubernetes environments, identifying compatibility issues, estimating costs, and determining timelines with business impact considerations. During the mobilize phase, organizations create detailed migration plans, establish EKS environments with proper networking and security, train teams, and develop testing procedures. The final migrate and modernize phase involves transferring applications and data, validating functionality, implementing cloud-centered features, optimizing resources and costs, and enhancing observability to fully use AWS capabilities. One of the most significant challenges organizations face during the process is cost estimation, which happens in the assessment phase. Karpenter is an open source Kubernetes node autoscaler that efficiently provisions just-in-time compute resources to match workload demands. Unlike traditional autoscalers, Karpenter directly integrates with cloud providers to make intelligent, real-time decisions about instance types, availability zones, and capacity options. It evaluates pod requirements and constraints to select optimal instances, considering factors such as CPU, memory, price, and availability. Karpenter can consolidate workloads for cost efficiency and rapidly scale from zero to handle sudden demand spikes. It supports both spot and on-demand instances, and automatically terminates nodes when they‚Äôre no longer needed, optimizing cluster resource utilization and reducing cloud costs. Karpenter uses the concept of Providers to interact with different infrastructure platforms for provisioning and managing compute resources. KWOK (Kubernetes WithOut Kubelet) is a toolkit that simulates data plane nodes without allocating actual infrastructure, and can be used as a provider to create lightweight testing environments that enable developers to validate provisioning decisions, try various (virtual) instance types, and debug scaling behaviors.</description></item><item><title>v1.31.12-rc5+rke2r1</title><link>https://kubermates.org/releases/2025-08-21-v1-31-12-rc5-rke2r1/</link><pubDate>Thu, 21 Aug 2025 16:20:55 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-21-v1-31-12-rc5-rke2r1/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>v1.33.4-rc5+rke2r1</title><link>https://kubermates.org/releases/2025-08-21-v1-33-4-rc5-rke2r1/</link><pubDate>Thu, 21 Aug 2025 16:16:16 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-21-v1-33-4-rc5-rke2r1/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>v1.32.8-rc5+rke2r1</title><link>https://kubermates.org/releases/2025-08-21-v1-32-8-rc5-rke2r1/</link><pubDate>Thu, 21 Aug 2025 16:11:16 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-21-v1-32-8-rc5-rke2r1/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Stop Building SaaS from Scratch: Meet the SeaNotes Starter Kit</title><link>https://kubermates.org/docs/2025-08-21-stop-building-saas-from-scratch-meet-the-seanotes-starter-kit/</link><pubDate>Thu, 21 Aug 2025 14:20:21 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-21-stop-building-saas-from-scratch-meet-the-seanotes-starter-kit/</guid><description>By Amit Jotwani and Haimantika Mitra There√¢¬Ä¬ôs a moment in every SaaS project where you realize√¢¬Ä¬¶ You√¢¬Ä¬ôre not building your product yet. You√¢¬Ä¬ôre setting up auth. You√¢¬Ä¬ôre wiring up Stripe. You√¢¬Ä¬ôre figuring out how to send emails, where to store files, how to deploy it √¢¬Ä¬î and now, how to sprinkle in just enough AI to make it feel modern. Even in 2025, LLMs still struggle with this part. They√¢¬Ä¬ôre great at scaffolding UI and generating business logic. But they don√¢¬Ä¬ôt know how to spin up a database, integrate Stripe, or deploy an actual app. That√¢¬Ä¬ôs exactly what SeaStack solves. SeaStack is a new series of open-source starter kits and reference apps from DigitalOcean √¢¬Ä¬î built to help developers ship real apps, faster. It√¢¬Ä¬ôs our way of saying: √¢¬Ä¬ú Here√¢¬Ä¬ôs how you can build real things with DigitalOcean √¢¬Ä¬î and here√¢¬Ä¬ôs the source code to get started. √¢¬Ä¬ù And SeaNotes is the first one. SeaNotes SaaS Starter Kit is an open source GitHub repo that gives developers a simple, production-ready foundation to build real SaaS apps √¢¬Ä¬î fast.</description></item><item><title>Celebrating 100 Golden Kubestronauts</title><link>https://kubermates.org/docs/2025-08-21-celebrating-100-golden-kubestronauts/</link><pubDate>Thu, 21 Aug 2025 13:59:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-21-celebrating-100-golden-kubestronauts/</guid><description>Posted on August 21, 2025 by Christophe Sauthier, Cloud Native Training and Certification Lead The Kubestronauts program has been growing at an incredible pace. We recently celebrated over 2000 kubestronauts across the globe, and today we‚Äôre excited to celebrate another milestone: over 100 Golden Kubestronauts have achieved Golden status in less than five months, thanks to their dedication and hard work. One of the most inspiring aspects of the Golden Kubestronauts community is its global reach. With more than 100 Golden badges earned so far, we see representation from every corner of the world‚Äîspanning North and South America, Europe, Asia, and beyond. From Austria, Brazil, and Canada to India, Japan, and Singapore , and from Germany, the Netherlands, France and the United States to places like Bangladesh, Mongolia, and Vietnam , Golden Kubestronauts truly embody the global spirit of CNCF. This diversity highlights not only the worldwide adoption of cloud native technologies, but also the strength of a community that learns, grows, and achieves together‚Äîregardless of geography. Becoming a Golden Kubestronaut is the highest level of recognition in the program. It requires completing all CNCF certifications as well as the Linux Foundation Certified Sysadmin (LFCS). Starting October 15th, the Certified Cloud Native Platform Engineering Associate (CNPA) will also be added as part of this journey‚Äîjust as all future CNCF certifications will be included as the program continues to evolve. Golden Kubestronauts are proof of what‚Äôs possible when passion, knowledge, persistence, and a love of cloud native come together. They embody the breadth of skills across the ecosystem, from Kubernetes administration to observability, security, platform engineering, and more. It‚Äôs important to highlight that once you‚Äôve earned Golden Kubestronaut status, it‚Äôs yours for life.</description></item><item><title>August 21, 2025</title><link>https://kubermates.org/releases/2025-08-21-august-21-2025/</link><pubDate>Thu, 21 Aug 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-08-21-august-21-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>An open world: Why Red Hat supports the United Nations Open Source Principles</title><link>https://kubermates.org/docs/2025-08-21-an-open-world-why-red-hat-supports-the-united-nations-open-source-principles/</link><pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-21-an-open-world-why-red-hat-supports-the-united-nations-open-source-principles/</guid><description>Share At Red Hat, we have always believed that the best technology comes from open collaboration. It‚Äôs no secret that the components of Red Hat‚Äôs products, from the world‚Äôs leading enterprise Linux platform to the latest AI innovations of Red Hat AI Inference Server , are created in the upstream, in the open, with communities and projects that make them possible. For three decades, open source has been a core tenet for Red Hat. While the broader technology world was slower to embrace it, more and more organizations are turning to open source-first approaches each year. This is why Red Hat, as a champion of open source technologies and practices, is pleased to formally endorse the United Nations (UN) Open Source Principles. These principles echo how we have always worked by building in the open, contributing back, and nurturing communities that thrive because of inclusion and collaboration. These guidelines aim to drive collaboration and open source adoption within the UN and globally, and were produced by the Open Source United Community as part of the UN Chief Executive Board‚Äôs Digital Technology Network. The principles encompass: Open by default: Making open source the standard approach for projects. Contribute back: Encouraging active participation in the open source ecosystem. Secure by design: Making security a priority in all software projects. Foster inclusive participation and community building: Enabling and facilitating diverse and inclusive contributions. Design for reusability: Designing projects to be interoperable across various platforms and ecosystems.</description></item><item><title>Production AI success: From gen AI promise to business impact</title><link>https://kubermates.org/docs/2025-08-21-production-ai-success-from-gen-ai-promise-to-business-impact/</link><pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-21-production-ai-success-from-gen-ai-promise-to-business-impact/</guid><description>Share Generative AI (gen AI) has the potential to significantly improve customer engagement, reduce costs, and boost productivity. However, many enterprises struggle to move beyond initial experiments to widespread and scalable implementations. This article examines 6 common obstacles to enterprise AI adoption ‚Äîas highlighted in this Harvard Business Review report ‚Äîand how Red Hat can help you overcome them. Many organizations initiate gen AI projects without a defined strategy or roadmap. This can lead to fragmented efforts that make it difficult to demonstrate a return on investment (ROI). A well-articulated strategy is crucial for successful adoption. You may also find your organization struggling to align AI solutions with specific, impactful business use cases. Demonstrating clear business value can often be a challenge. How Red Hat helps: A global shortage of skilled AI talent is a growing problem. This isn&amp;rsquo;t just a bottleneck‚Äîit can prevent your organization from fully harnessing AI&amp;rsquo;s transformative potential. Addressing this gap requires strategies for developing both future and current workforces. How Red Hat helps: Many existing IT and data infrastructures are unprepared for gen AI integration.</description></item><item><title>Red Hat: a leader in driving sustainability efforts within the IT industry</title><link>https://kubermates.org/docs/2025-08-21-red-hat-a-leader-in-driving-sustainability-efforts-within-the-it-industry/</link><pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-21-red-hat-a-leader-in-driving-sustainability-efforts-within-the-it-industry/</guid><description>Share Red Hat makes it a strategic priority to offer energy-efficient products, addressing the growing demand of our customers for sustainable IT solutions. Our efforts are driven by minimizing the increase in energy consumption of AI, cloud computing, and IT infrastructure. Red Hat focuses on open source technologies designed to improve energy efficiency across IT environments. This includes: InstructLab is an open source project that enhances large language models (LLMs) used in generative AI (gen AI), upgrading an LLM with less human input and fewer resources than retraining. One of the key aspects of InstructLab is its focus on smaller, fine-tuned language models. This approach directly translates to improved energy efficiency compared to larger, more resource-intensive LLMs deployed by competitors. Smaller language models require significantly less computational power for training and inference, leading to reduced energy consumption. InstructLab optimizes performance by tailoring SLMs to specific tasks or domains, enhancing efficiency, minimizing wasteful energy, and enabling models to run on lower-power CPUs as opposed to power-hungry GPUs. This lower energy consumption results in reduced operational costs, providing both financial benefits and support for sustainability goals. Red Hat&amp;rsquo;s energy-efficient solutions offer significant benefits for enterprises, from reducing operational costs to helping achieve ambitious sustainability targets. By optimizing resource utilization and enhancing performance, these solutions also provide a crucial competitive advantage in today&amp;rsquo;s market, which include: Red Hat and SoftBank have collaborate d on Artificial Intelligence Radio Access Network (AI-RAN) to optimize power consumption in datacenters powering AI applications and vRAN. This project integrates Kepler, an open source project founded at Red Hat, into SoftBank&amp;rsquo;s AI and Telecom Radio Access System (AITRAS) platform.</description></item><item><title>Honoring Styra‚Äôs Contributions To OPA</title><link>https://kubermates.org/docs/2025-08-20-honoring-styra-s-contributions-to-opa/</link><pubDate>Wed, 20 Aug 2025 23:03:50 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-20-honoring-styra-s-contributions-to-opa/</guid><description>As the creator of Open Policy Agent (OPA), Styra has played an extraordinary role in shaping the Policy as Code (PaC) movement and advancing cloud-native security and governance. This week, the founders and core team behind Styra announced that they are joining Apple. I first met the Styra team at KubeCon 2017 in Austin, when Brian Grant saw an early demo of what we were building, and suggested that I talk to them. Although we have significantly different approaches to PaC, one of my favorite moments at each KubeCon since has been catching up with folks from Styra and exchanging notes on the space. We here at Nirmata, want to take a moment to recognize and thank the entire Styra team ‚Äì their founders Teemu, Tim, Torin, along with Anders, Charlie and many others ‚Äì for their pioneering work. Without their vision and drive, there would be no PaC ecosystem as we know it today. At Nirmata, we know that Kyverno‚Äôs journey, and our own, would not have been possible without the foundation laid by the OPA community. The early adoption of OPA proved the need for declarative policies and automated governance. It set the standard for how organizations think about declarative policies across cloud and infrastructure environments and it will continue to play an important role in the CNCF landscape. We are also encouraged that the Styra team has found a great home at Apple. With such a strong technology leader supporting them, the OPA community can look forward to ongoing innovation, stability, and new opportunities for growth. We are excited to see what comes next in the OPA roadmap and how the project continues to evolve as a critical part of the cloud native landscape.</description></item><item><title>Kubernetes v1.34.0-rc.2</title><link>https://kubermates.org/releases/2025-08-20-kubernetes-v1-34-0-rc-2/</link><pubDate>Wed, 20 Aug 2025 21:15:15 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-20-kubernetes-v1-34-0-rc-2/</guid><description>See kubernetes-announce@. Additional binary downloads are linked in the CHANGELOG. See the CHANGELOG for more details. There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Announcing OpenAI gpt-oss Models on the DigitalOcean Gradient‚Ñ¢ AI Platform</title><link>https://kubermates.org/docs/2025-08-20-announcing-openai-gpt-oss-models-on-the-digitalocean-gradient-ai-platform/</link><pubDate>Wed, 20 Aug 2025 16:21:33 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-20-announcing-openai-gpt-oss-models-on-the-digitalocean-gradient-ai-platform/</guid><description>By Grace Morgan OpenAI√¢¬Ä¬ôs first open-source GPT models (20b and 120b) are now available on the Gradient AI Platform. This launch brings even more flexibility and choice to developers building AI-powered applications, whether you√¢¬Ä¬ôre starting with a quick prototype or scaling a production agent. With code: Call the models directly through our Serverless Inference API. In the UI: Head to Agent Creation and select gpt-oss 20b or 120b from the model dropdown. This launch marks another step toward making Gradient AI Platform the simplest, most flexible way to build real, production-ready AI applications. For code-first developers: √∞¬ü¬ë¬â Deploy gpt-oss via API: Spin up the 20b or 120b models directly through the Gradient Serverless Inference API and start building in just a few lines of code. For UI users: √∞¬ü¬ë¬â Build with gpt-oss in console: Create an agent in the Gradient AI Platform, select gpt-oss from the model dropdown, and deploy instantly√¢¬Ä¬îno code required. Share Read more Read more Read more.</description></item><item><title>Uniting the Cloud Native Community at the Inaugural KCD SF Bay Area</title><link>https://kubermates.org/docs/2025-08-20-uniting-the-cloud-native-community-at-the-inaugural-kcd-sf-bay-area/</link><pubDate>Wed, 20 Aug 2025 14:23:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-20-uniting-the-cloud-native-community-at-the-inaugural-kcd-sf-bay-area/</guid><description>Posted on August 20, 2025 by Lisa-Marie Namphy and Matthew Cascio, CNCF Ambassadors The cloud native landscape is constantly evolving, and staying ahead of the curve means more than just reading documentation‚Äîit means connecting with the people who are shaping the future. That‚Äôs exactly what the inaugural Kubernetes Community Day (KCD) SF Bay Area event is all about. This full-day gathering, proudly sponsored by the Cloud Native Computing Foundation (CNCF), is the ultimate opportunity for enthusiasts, developers, and industry leaders to come together for a day of insightful talks and unparalleled networking. On September 9th , the historic Computer History Museum in Mountain View will open its doors to the Bay Area‚Äôs thriving cloud native community. This isn‚Äôt just a conference; it‚Äôs a celebration of collaboration and knowledge sharing. As an added bonus, your event registration grants you access to the museum‚Äôs fascinating exhibits, allowing you to explore the history of technology during your breaks. The KCD SF Bay Area event boasts an incredible lineup of speakers and visionaries. You‚Äôll have the chance to hear directly from some of the most influential minds in the cloud native space. Confirmed speakers include: These are just a few of the thought leaders you can expect to learn from. The full schedule, which you can view at https://schedule. kcdsfbayarea. com , is packed with sessions designed to expand your knowledge and inspire new ideas.</description></item><item><title>August 20, 2025</title><link>https://kubermates.org/releases/2025-08-20-august-20-2025/</link><pubDate>Wed, 20 Aug 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-08-20-august-20-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>Disaster Recovery: Achieving Instantaneous Hot-Hot with OpenShift</title><link>https://kubermates.org/docs/2025-08-20-disaster-recovery-achieving-instantaneous-hot-hot-with-openshift/</link><pubDate>Wed, 20 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-20-disaster-recovery-achieving-instantaneous-hot-hot-with-openshift/</guid><description>Share The biggest challenge for disaster recovery in traditional environments is that every environment looks and feels different. If you&amp;rsquo;re moving from a colo that someone manages to VMware, the cloud, or a different VMware data center, they all look and feel different. Even if you&amp;rsquo;re using the same virtualization provider, storage is probably handled differently. There has to be a lot of planning and strategy around how to copy data from one area to another since that‚Äôs not something natively built into your virtualization provider. You also have to figure out networking, DNS, routing, etc. With Red Hat OpenShift , all of these components are software-defined. You get software-defined DNS, networking, and storage layers all because it‚Äôs based on Kubernetes. Out-of-the-box OpenShift won‚Äôt ‚Äúautomagically‚Äù failover without additional levels of configuration, much like VMware or other traditional infrastructure environments. But the tools required are mostly there for you, either pre-packaged and open-source or robustly tested and a standard in the space. You can set up OpenShift in any environment because the least common denominator for everything, no matter where you are (bare metal or virtual), is the operating system This is where OpenShift lives. That‚Äôs why it is the perfect tool for building a truly agile infrastructure that can move freely between environments, providing instantaneous disaster recovery for businesses. There are different benefits of infrastructure agility.</description></item><item><title>Cross-service confused deputy prevention</title><link>https://kubermates.org/releases/2025-08-19-cross-service-confused-deputy-prevention/</link><pubDate>Tue, 19 Aug 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-19-cross-service-confused-deputy-prevention/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. The confused deputy problem is a security issue where an entity that doesn√¢¬Ä¬ôt have permission to perform an action can coerce a more-privileged entity to perform the action. In AWS, cross-service impersonation can result in the confused deputy problem. Cross-service impersonation can occur when one service (the calling service ) calls another service (the called service ). The calling service can be manipulated to use its permissions to act on another customer√¢¬Ä¬ôs resources in a way it should not otherwise have permission to access. To prevent this, AWS provides tools that help you protect your data for all services with service principals that have been given access to resources in your account. We recommend using the aws:SourceArn , aws:SourceAccount global condition context keys in resource policies to limit the permissions that Amazon Elastic Kubernetes Service (Amazon EKS) gives another service to the resource. Use aws:SourceArn to associate only one resource with cross-service access. Use aws:SourceAccount to let any resource in that account be associated with the cross-service use. The most effective way to protect against the confused deputy problem is to use the aws:SourceArn global condition context key with the full ARN of the resource. If you don√¢¬Ä¬ôt know the full ARN of the resource or if you are specifying multiple resources, use the aws:SourceArn global context condition key with wildcard characters (&lt;em&gt;) for the unknown portions of the ARN. For example, arn:aws:&lt;servicename&gt;:&lt;/em&gt;:&amp;lt;123456789012&amp;gt;:*.</description></item><item><title>Tuning Linux Swap for Kubernetes: A Deep Dive</title><link>https://kubermates.org/docs/2025-08-19-tuning-linux-swap-for-kubernetes-a-deep-dive/</link><pubDate>Tue, 19 Aug 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-08-19-tuning-linux-swap-for-kubernetes-a-deep-dive/</guid><description>The Kubernetes NodeSwap feature , likely to graduate to stable in the upcoming Kubernetes v1. 34 release, allows swap usage: a significant shift from the conventional practice of disabling swap for performance predictability. This article focuses exclusively on tuning swap on Linux nodes, where this feature is available. By allowing Linux nodes to use secondary storage for additional virtual memory when physical RAM is exhausted, node swap support aims to improve resource utilization and reduce out-of-memory (OOM) kills. However, enabling swap is not a &amp;ldquo;turn-key&amp;rdquo; solution. The performance and stability of your nodes under memory pressure are critically dependent on a set of Linux kernel parameters. Misconfiguration can lead to performance degradation and interfere with Kubelet&amp;rsquo;s eviction logic. In this blogpost, I&amp;rsquo;ll dive into critical Linux kernel parameters that govern swap behavior. I will explore how these parameters influence Kubernetes workload performance, swap utilization, and crucial eviction mechanisms. I will present various test results showcasing the impact of different configurations, and share my findings on achieving optimal settings for stable and high-performing Kubernetes clusters. At a high level, the Linux kernel manages memory through pages, typically 4KiB in size. When physical memory becomes constrained, the kernel&amp;rsquo;s page replacement algorithm decides which pages to move to swap space.</description></item><item><title>Introducing langchain-gradient: Seamless LangChain Integration with DigitalOcean Gradient‚Ñ¢ AI Platform</title><link>https://kubermates.org/docs/2025-08-19-introducing-langchain-gradient-seamless-langchain-integration-with-digitalocean-/</link><pubDate>Tue, 19 Aug 2025 17:37:45 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-19-introducing-langchain-gradient-seamless-langchain-integration-with-digitalocean-/</guid><description>By Narasimha Badrinath We√¢¬Ä¬ôre excited to introduce langchain-gradient , a new open-source integration that brings the power of LangChain to DigitalOcean Gradient AI Platform. LangChain is a popular framework for building applications powered by large language models (LLMs), with tools for chaining prompts, managing context, and connecting to external data sources. With this package, you can now seamlessly connect LangChain√¢¬Ä¬ôs flexible orchestration framework to DigitalOcean√¢¬Ä¬ôs scalable, developer-friendly AI infrastructure√¢¬Ä¬îmaking it easier than ever to build, and scale AI-powered applications. As AI adoption accelerates, developers are looking for ways to streamline the process of building and deploying intelligent applications. LangChain has been a go-to framework for chaining together LLMs, tools, and data sources. DigitalOcean Gradient AI, meanwhile, offers a simple, cost-effective platform for running AI workloads at scale. With langchain-gradient , you can now: With LangChain and Gradient AI Platform working together, you can: Build retrieval-augmented chatbots that pull knowledge from your own files, databases, or cloud storage. Automate customer support workflows by chaining together intent detection, document search, and AI-powered responses. Create AI copilots for developers or teams that summarize logs, generate code snippets, or draft documentation. Experiment faster with different LLMs for tasks like summarization, classification, or content generation√¢¬Ä¬îthen scale instantly with serverless inference. You can install the package directly from PyPI. For step-by-step installation instructions and setup guidance, visit the PyPI page or DigitalOcean Gradient page in LangChain.</description></item><item><title>Celebrating 100 Days of Kagent</title><link>https://kubermates.org/docs/2025-08-19-celebrating-100-days-of-kagent/</link><pubDate>Tue, 19 Aug 2025 13:30:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-19-celebrating-100-days-of-kagent/</guid><description>Posted on August 19, 2025 by Lin Sun, VP of Open Source at Solo. io CNCF projects highlighted in this post When we first introduced kagent on March 17th, 2025, we had a bold vision: to bring agentic AI to cloud native‚Äîempowering platforms and DevOps engineers to harness AI agents for solving real operational challenges. Fast forward 100 days, and today we‚Äôre celebrating a major milestone: 100 days of the kagent project ! üéâThank you everyone for being part of this journey! üöÄ What began as a tool to address our own customer challenges has grown into a thriving open-source project. Kagent is now a CNCF Sandbox project , and it‚Äôs evolving into a powerful declarative agentic AI framework ‚Äîenabling like-minded engineers to run AI agents in Kubernetes, automating complex operations and streamlining troubleshooting workflows. Source: LinkedIn post In just 100 days, kagent has hit two incredible milestones: üöÄ 100 contributors , with over 85% from outside Solo. io ‚≠ê 1000+ GitHub stars from our amazing community! We‚Äôve been actively connecting with contributors through GitHub, Discord, weekly community calls, livestreams, and Contributor Spotlights. We‚Äôre incredibly grateful for everyone who‚Äôs explored, supported, and contributed to kagent. A huge shoutout to our top 20 contributors , and a special thanks to our top 3 : @eitanya, @peterj, and @sbx03 ‚Äî we deeply appreciate your dedication and impact!! üôå Source: https://kagent. devstats. cncf. io/d/66/developer-activity-counts-by-companies?orgId=1 Over the past 100 days, we‚Äôve been blown away by how our early adopters are using kagent in creative and powerful ways‚Äîincluding in production environments ! Across social media, community meetings , and beyond, users are not just experimenting‚Äîthey‚Äôre innovating. üí° Here are just a few examples of how the community is putting kagent to work: Source: Linkedin post Source: Linkedin post Source: LinkedIn post Source: LinkedIn post Source: LinkedIn post Source: LinkedIn post In just 100 days, kagent has made its mark on the global stage.</description></item><item><title>launching kubermates</title><link>https://kubermates.org/blog/first-post/</link><pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/blog/first-post/</guid><description>&lt;p&gt;we are live
kubermates will publish practical guides across aks eks gke and onprem&lt;/p&gt;</description></item><item><title>v1.31.12-rc4+rke2r1</title><link>https://kubermates.org/releases/2025-08-18-v1-31-12-rc4-rke2r1/</link><pubDate>Mon, 18 Aug 2025 19:00:33 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-18-v1-31-12-rc4-rke2r1/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>v1.33.4-rc4+rke2r1</title><link>https://kubermates.org/releases/2025-08-18-v1-33-4-rc4-rke2r1/</link><pubDate>Mon, 18 Aug 2025 18:55:22 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-18-v1-33-4-rc4-rke2r1/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>v1.32.8-rc4+rke2r1</title><link>https://kubermates.org/releases/2025-08-18-v1-32-8-rc4-rke2r1/</link><pubDate>Mon, 18 Aug 2025 18:50:30 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-18-v1-32-8-rc4-rke2r1/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>v1.30.14-rc6+rke2r3</title><link>https://kubermates.org/releases/2025-08-18-v1-30-14-rc6-rke2r3/</link><pubDate>Mon, 18 Aug 2025 17:30:40 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-18-v1-30-14-rc6-rke2r3/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Combining GenAI &amp; Agentic AI to build scalable, autonomous systems</title><link>https://kubermates.org/docs/2025-08-18-combining-genai-agentic-ai-to-build-scalable-autonomous-systems/</link><pubDate>Mon, 18 Aug 2025 15:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-18-combining-genai-agentic-ai-to-build-scalable-autonomous-systems/</guid><description>Posted on August 18, 2025 by Marta Fernandes, Engineering Operations Manager, YLD A common pattern in today‚Äôs AI adoption is that businesses are investing heavily in GenAI capabilities, yet many are leaving significant value on the table by failing to pair it with Agentic AI. This matters because while GenAI can generate content, ideas, or responses, it can‚Äôt act on them. However, when combined with Agentic AI, your systems shift from being reactive and prompt-driven to autonomous and outcome-oriented. This article aims to help senior technology leaders better understand the value Agentic AI can add to existing AI infrastructure, while outlining key considerations to determine whether its implementation aligns with your business needs. Image caption: Core components of an Agentic AI Architecture from Markovate. Agentic AI systems are designed to operate autonomously, perceiving their environment, making decisions, and executing actions without continuous human oversight. This autonomy enables businesses to streamline operations, reduce costs, and enhance scalability. The image above illustrates how the following systems function: Ultimately, Agentic AI systems are equipped with learning mechanisms that improve over time. Through techniques like reinforcement learning, where the system learns from the outcomes of its actions, and supervised or unsupervised learning, where it identifies patterns in data, the AI adapts to new situations and refines its decision-making processes. The most disruptive AI solutions over the next few years will combine GenAI with Agentic AI, and the key to achieving better business outcomes is knowing when to use one, the other, or both. Not every use case requires a full agentic system, but many businesses will be surprised at how many should. GenAI alone: GenAI excels at generating new content from existing data patterns, significantly boosting creativity and productivity across various domains.</description></item><item><title>Gear Up for the 5th Annual KCD Washington DC!</title><link>https://kubermates.org/docs/2025-08-16-gear-up-for-the-5th-annual-kcd-washington-dc/</link><pubDate>Sat, 16 Aug 2025 13:04:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-16-gear-up-for-the-5th-annual-kcd-washington-dc/</guid><description>Posted on August 16, 2025 by Matthew Cascio, CNCF Ambassador and KCD Washington DC Organizer We‚Äôre so thrilled to be organizing another year of cloud native community in the Nation‚Äôs Capital. Past years have witnessed amazing presentations ranging from core Kubernetes topics to emerging trends in ML/AI, sustainability, virtual clusters and more. This year promises to be even more amazing. Our program is set, and includes two new innovations for our KCD: Special Government &amp;amp; Cybersecurity Focus ‚Äì and free admission for our government employee community members! You won‚Äôt want to miss Ashley Jones‚Äô keynote if you‚Äôre a cybersecurity professional or involved in government technology in any way. A limited number of free tickets will be available soon for our community members working for federal, state or local government agencies. You can request a free government ticket here: https://forms. gle/aLNfRwpvFn3vQ5Vo8 Plus our regular program is amazing! All of that rests on top of our regular program that includes presentations like: ‚Ä¶. and a whole lot more, which you can see in the Full Schedule [https://kcd-washington-dc-2025. sessionize. com/] And there‚Äôs plenty of food, drinks and networking Once again, we plan on hosting an evening Social Hour free for all attendees where you can meet fellow attendees, speakers, and sponsors, in a relaxed, community environment while enjoying drinks and appetizers. And, of course, you can enjoy all-day snacks, beverages (tea, coffee, soft drinks) and a nutritious lunch, all included with your one-day ticket. That‚Äôs a pretty good value for $75, and to make it even more inviting, we‚Äôre offering 25% for all CNCF readers using code FRIENDS25 when you Get Tickets [https://bit.</description></item><item><title>Calico at KubeCon + CloudNativeCon North America 2025!</title><link>https://kubermates.org/docs/2025-08-15-calico-at-kubecon-cloudnativecon-north-america-2025/</link><pubDate>Fri, 15 Aug 2025 13:34:02 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-15-calico-at-kubecon-cloudnativecon-north-america-2025/</guid><description>Get ready, North America! The Calico team is thrilled to announce our participation in KubeCon + CloudNativeCon North America 2025, where we‚Äôll be showcasing the latest advancements in Kubernetes networking, security, and observability. We‚Äôre excited to connect with the vibrant cloud-native community, share insights, and demonstrate how Calico Open Source continues to empower organizations worldwide. We have a packed agenda designed to offer you multiple ways to engage with our team and learn more about Calico. Mark your calendars for these exciting opportunities! Join us at CalicoCon North America 2025 , your go-to event for the latest in Kubernetes networking, security, and observability. Hosted by the Calico team, this hybrid event is your chance to hear directly from Calico engineers and leadership, get hands-on with new features, and take an in-depth look at the state of Project Calico. We‚Äôll dive into Calico 3. 30, Calico eBPF, and Calico Whisker: open source observability for Kubernetes. Add CalicoCon to your existing KubeCon + CloudNativeCon registration ‚Äåto secure your spot. If you are not attending KubeCon + CloudNativeCon North America but would still like to attend CalicoCon, please reach out to us ‚Äåon the Calico User Slack. Event Details Date : November 10, 2025 Time : 1:00pm to 5:00pm EST Location : Virtual | The Westin Peachtree Plaza Atlanta Register Now After a day of deep dives and technical discussions, unwind and network with other Calico users and the team at our exclusive Happy Hour with Calico ! This is a fantastic opportunity to relax, mingle with fellow Kubernetes enthusiasts, and connect with Calico engineers in a casual setting. Enjoy good food, drinks, and great company as we celebrate the cloud-native community. Event Details Date : November 10, 2025 Time : 5:00pm to 7:00pm EST Location : The Sun Dial Restaurant, Atlanta, GA Register Now Be sure to stop by booth #521 to discuss Calico‚Äôs latest Kubernetes network security and observability advancements.</description></item><item><title>Why KubeCon India 2025 Meant More to KodeKloud</title><link>https://kubermates.org/docs/2025-08-15-why-kubecon-india-2025-meant-more-to-kodekloud/</link><pubDate>Fri, 15 Aug 2025 07:29:57 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-15-why-kubecon-india-2025-meant-more-to-kodekloud/</guid><description>That big queue at your booth? Brilliant. It‚Äôs exactly what KodeKloud is about. When attendees, speakers, and industry peers tell you that at KubeCon India, it‚Äôs a clear sign that what KodeKloud is doing is making a real impact. For KodeKloud, KubeCon India 2025 was more than a tech conference. It was a celebration of community, connection, and the power of practical learning in the Cloud Native space. From conversations with global leaders to meeting hundreds of learners face-to-face, this year meant more than ever before. One of the proudest moments for us at KubeCon India 2025 was standing alongside the Cloud Native Computing Foundation (CNCF) and the Linux Foundation Education team to announce a new initiative tailored specifically for learners in India. This partnership is designed to make Cloud Native certifications more accessible, affordable, and achievable in the region through: KodeKloud ‚Äì Linux Foundation Partnership for Localized Certification Payment Click to learn more and explore our partnership ‚Üí For us at KodeKloud, this is about much more than payments - it‚Äôs about removing barriers that keep talented engineers from reaching their potential. India is home to some of the most passionate DevOps and Cloud Native professionals in the world, and now, more of them will have a clear, affordable pathway to earning globally recognized certifications. The announcement was made live at KubeCon, with CNCF and Linux Foundation leaders sharing the stage - a powerful reminder that when communities and education providers work together, opportunities multiply. One of the standout moments was when Mumshad Mannambeth , KodeKloud‚Äôs founder, met Chris Aniszczyk , the CTO of the Cloud Native Computing Foundation (CNCF). Their conversation touched on: It was a meeting of minds that reflected exactly what KubeCon stands for - collaboration, knowledge-sharing, and driving the industry forward together.</description></item><item><title>August 15, 2025</title><link>https://kubermates.org/releases/2025-08-15-august-15-2025/</link><pubDate>Fri, 15 Aug 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-08-15-august-15-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>v1.33.4-rc1+k3s1</title><link>https://kubermates.org/releases/2025-08-14-v1-33-4-rc1-k3s1/</link><pubDate>Thu, 14 Aug 2025 22:12:05 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-14-v1-33-4-rc1-k3s1/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>v1.32.8-rc1+k3s1</title><link>https://kubermates.org/releases/2025-08-14-v1-32-8-rc1-k3s1/</link><pubDate>Thu, 14 Aug 2025 22:11:23 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-14-v1-32-8-rc1-k3s1/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>v1.31.12-rc1+k3s1</title><link>https://kubermates.org/releases/2025-08-14-v1-31-12-rc1-k3s1/</link><pubDate>Thu, 14 Aug 2025 22:11:10 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-14-v1-31-12-rc1-k3s1/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Pre-release v2.12.1-alpha1</title><link>https://kubermates.org/releases/2025-08-14-pre-release-v2-12-1-alpha1/</link><pubDate>Thu, 14 Aug 2025 22:03:02 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-14-pre-release-v2-12-1-alpha1/</guid><description>rancher/aks-operator:v1. 12. 1-rc. 1 rancher/eks-operator:v1. 12. 1-rc. 1 rancher/fleet-agent:v0. 13. 1-rc. 4 rancher/fleet:v0. 13. 1-rc.</description></item><item><title>Pre-release v2.11.5-alpha1</title><link>https://kubermates.org/releases/2025-08-14-pre-release-v2-11-5-alpha1/</link><pubDate>Thu, 14 Aug 2025 21:23:44 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-14-pre-release-v2-11-5-alpha1/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>v2.11.4-hotfix-aa32.1: Fix CAValidator (#51375) (#51399)</title><link>https://kubermates.org/releases/2025-08-14-v2-11-4-hotfix-aa32-1-fix-cavalidator-51375-51399/</link><pubDate>Thu, 14 Aug 2025 15:04:12 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-14-v2-11-4-hotfix-aa32-1-fix-cavalidator-51375-51399/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Build smarter AI agents: new tools now available for the DigitalOcean Gradient‚Ñ¢ AI Platform</title><link>https://kubermates.org/docs/2025-08-14-build-smarter-ai-agents-new-tools-now-available-for-the-digitalocean-gradient-ai/</link><pubDate>Thu, 14 Aug 2025 14:37:02 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-14-build-smarter-ai-agents-new-tools-now-available-for-the-digitalocean-gradient-ai/</guid><description>By Grace Morgan Building with AI should be fast, flexible, and frustration-free. That√¢¬Ä¬ôs why we√¢¬Ä¬ôve leveled up the Gradient AI Platform with new tools that let you ship smarter agents, debug in real time, and tap into the knowledge sources you already use. Now you can: Streamline AI app development with the unified Gradient AI Python SDK -&amp;gt; Optimize agent performance via log stream insights -&amp;gt; Augment your agents with Dropbox knowledge bases -&amp;gt; Whether you√¢¬Ä¬ôre launching your first AI MVP or scaling production workloads, these new tools help you build real, production-ready AI apps. Access all major Gradient AI services, like Agents, Inference APIs, Knowledge Bases, and even GPU Droplets , through one modern Python SDK, now available on PyPI. No more jumping between tools or stitching together API calls. The Gradient AI SDK gives you a unified way to access end-to-end AI building blocks, from bare metal GPU infrastructure to a complete agent builder, perfect for quick prototyping, scalable backends, or custom internal tools. Explore the SDK -&amp;gt; To install the SDK: When your agent isn√¢¬Ä¬ôt behaving how it should, combing through logs can slow you down. Log Stream Insights helps you skip the guesswork by analyzing your agent trace data in real time√¢¬Ä¬îand surfacing optimization recommendations automatically. If you√¢¬Ä¬ôre already storing traces, you can turn on Insights with a single toggle√¢¬Ä¬îand start seeing recommendations instantly. Add Log Stream Insights √¢¬Ü¬í Your team already stores knowledge in Dropbox, why not use it to power your agents? With the new Dropbox Data Connector, you can ingest content directly from Dropbox into your Gradient knowledge bases. Use it to build onboarding assistants, search agents, or custom copilots grounded in your docs. Connect Dropbox data √¢¬Ü¬í We built these tools to meet you where you are√¢¬Ä¬îand help you level up.</description></item><item><title>Introducing GPU Droplets accelerated by NVIDIA HGX H200</title><link>https://kubermates.org/docs/2025-08-14-introducing-gpu-droplets-accelerated-by-nvidia-hgx-h200/</link><pubDate>Thu, 14 Aug 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-14-introducing-gpu-droplets-accelerated-by-nvidia-hgx-h200/</guid><description>By Waverly Swinton Key Takeaways NVIDIA HGX H200 is now available as a DigitalOcean GPU Droplet (virtual, on-demand machines). NVIDIA HGX H200 GPU Droplets offer significant performance improvements - up to 2x faster inference speeds and double the memory capacity - compared to the H100. NVIDIA H200 GPU Droplets are designed for simplicity, scalability, and cost-effectiveness, with on-demand pricing at just $3. 44/GPU/hr. We√¢¬Ä¬ôre expanding our AI/ML offerings with the introduction of new DigitalOcean Gradient√¢¬Ñ¬¢ AI GPU Droplets , virtual machines accelerated by NVIDIA HGX H200. This is a significant step forward in our mission to provide the simplest, most scalable cloud for builders everywhere. For developers working on the cutting edge of AI, the right hardware can make all the difference. NVIDIA HGX H200 is built to handle the most demanding tasks in generative AI and high-performance computing (HPC). We√¢¬Ä¬ôre making it easier and more affordable for you to tap into this power without the complexity and high costs often seen with other providers. NVIDIA H200 is a state-of-the-art GPU that brings some serious advantages to the table: Iterate and deploy faster: H200 GPU offers up to 2x faster inference speeds than the NVIDIA H100 on large language models like Llama 2 70B. Access larger memory capacity: First GPU to feature HBM3e memory, delivering a substantial increase in memory bandwidth and capacity for better inference performance. The H200 GPU features Its 141 GB of memory which is 76% higher than H100 and its memory bandwidth is 43% higher than the H100.</description></item><item><title>August 14, 2025</title><link>https://kubermates.org/releases/2025-08-14-august-14-2025/</link><pubDate>Thu, 14 Aug 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-08-14-august-14-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>Pre-release v1.8.6-rc.1</title><link>https://kubermates.org/releases/2025-08-14-pre-release-v1-8-6-rc-1/</link><pubDate>Thu, 14 Aug 2025 05:36:53 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-14-pre-release-v1-8-6-rc-1/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Sharks of DigitalOcean: Darian Wilkin, Senior Manager, Solutions Engineering</title><link>https://kubermates.org/docs/2025-08-14-sharks-of-digitalocean-darian-wilkin-senior-manager-solutions-engineering/</link><pubDate>Thu, 14 Aug 2025 04:29:13 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-14-sharks-of-digitalocean-darian-wilkin-senior-manager-solutions-engineering/</guid><description>By Sujatha R Technical Writer Darian Wilkin has been part of DigitalOcean√¢¬Ä¬ôs story for over eight years, witnessing firsthand the company√¢¬Ä¬ôs transformation from scaling startup to public company. As a Senior Manager of Solutions Architecture, he helps customers build smarter, more scalable cloud solutions. Darian has seen how DigitalOcean has grown to serve more complex workloads for digital native enterprises, while staying true to its roots as a developer-friendly cloud. For Darian, the answer is simple: the people and the culture that bring them together. √¢¬Ä¬úI think you√¢¬Ä¬ôll hear this from just about anyone at DigitalOcean, the people are what make this place special. Not only do I get to work with extremely talented folks, but everyone genuinely enjoys being here. That creates an environment and culture that√¢¬Ä¬ôs really pleasant to be a part of. √¢¬Ä¬ù It√¢¬Ä¬ôs this mix of collaboration, shared passion, and a love for the work that keeps Darian energized after nearly a decade at DO. √∞¬ü¬é¬• Have a look at Darian Wilkin√¢¬Ä¬ôs full conversation √¢¬¨¬á√Ø¬∏¬è √¢¬Ä¬úI like to think of our team as the lubricant in the gears that keep things moving when businesses are adopting cloud technologies. We meet with customers one-on-one to help them design robust, fault-tolerant, and performant architectures, while keeping costs in check. If our customers can√¢¬Ä¬ôt expand their cloud presence in a reliable and cost-effective way, their business is going to struggle. So they rely on us for best practices and forward-looking advice on how to get better in the cloud.</description></item><item><title>Kubernetes v1.31.12</title><link>https://kubermates.org/releases/2025-08-13-kubernetes-v1-31-12/</link><pubDate>Wed, 13 Aug 2025 19:50:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-13-kubernetes-v1-31-12/</guid><description>See kubernetes-announce@. Additional binary downloads are linked in the CHANGELOG. See the CHANGELOG for more details. There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Kubernetes v1.33.4</title><link>https://kubermates.org/releases/2025-08-13-kubernetes-v1-33-4/</link><pubDate>Wed, 13 Aug 2025 19:46:26 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-13-kubernetes-v1-33-4/</guid><description>See kubernetes-announce@. Additional binary downloads are linked in the CHANGELOG. See the CHANGELOG for more details. There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Kubernetes v1.32.8</title><link>https://kubermates.org/releases/2025-08-13-kubernetes-v1-32-8/</link><pubDate>Wed, 13 Aug 2025 19:45:14 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-13-kubernetes-v1-32-8/</guid><description>See kubernetes-announce@. Additional binary downloads are linked in the CHANGELOG. See the CHANGELOG for more details. There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Canary delivery with Argo Rollout and Amazon VPC Lattice for Amazon EKS</title><link>https://kubermates.org/docs/2025-08-12-canary-delivery-with-argo-rollout-and-amazon-vpc-lattice-for-amazon-eks/</link><pubDate>Tue, 12 Aug 2025 23:55:07 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-12-canary-delivery-with-argo-rollout-and-amazon-vpc-lattice-for-amazon-eks/</guid><description>Modern application delivery demands agility and reliability, where updates are rolled out progressively while making sure of the minimal impact on end users. Progressive delivery strategies, such as canary deployments, allow organizations to release new features by shifting traffic incrementally between old and new versions of a service. This allows organizations to first release features to a small subset of users, monitor system behavior and performance in real time, and automatically roll back if anomalies are detected. This is particularly valuable in modern microservices environments running on platforms such as Amazon Elastic Kubernetes Service (Amazon EKS) , where service meshes and traffic routers provide the necessary infrastructure for fine-grained control over traffic routing. This post explores an architectural approach to implementing progressive delivery using Amazon VPC Lattice, Amazon CloudWatch Synthetics , and Argo Rollouts. The solution uses VPC Lattice for enhanced traffic control across microservices, CloudWatch Synthetics for real-time health and validation monitoring, and Argo Rollouts for orchestrating canary updates. The content in this post addresses readers who are already familiar with networking constructs on Amazon Web Services (AWS), such as Amazon Virtual Private Cloud (Amazon VPC) , CloudWatch Synthetics and Amazon EKS. Instead of defining these services, we focus on their capabilities and integration with VPC Lattice. We also build upon your existing understanding of VPC Lattice concepts and Argo Rollouts. For more background on Amazon VPC Lattice, we recommend that you review the post, Build secure multi-account multi-VPC connectivity for your applications with Amazon VPC Lattice , and the collection of resources in the VPC Lattice Getting started guide. The architecture integrates multiple AWS services and Kubernetes-native components, providing a comprehensive solution for progressive delivery: In this section we consider an application running on Amazon EKS, where a new version of a microservice‚Äî prodDetail v2 ‚Äîneeds to be rolled out with minimal impact to users relying on the stable version v1. To do this, we implement a canary deployment strategy using VPC Lattice, Argo Rollouts, CloudWatch Synthetics, and AnalysisTemplates.</description></item><item><title>August 12, 2025</title><link>https://kubermates.org/releases/2025-08-12-august-12-2025/</link><pubDate>Tue, 12 Aug 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-08-12-august-12-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>4.20.0-okd-scos.ec.12</title><link>https://kubermates.org/releases/2025-08-11-4-20-0-okd-scos-ec-12/</link><pubDate>Mon, 11 Aug 2025 02:51:53 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-11-4-20-0-okd-scos-ec-12/</guid><description>These archives contain the client tooling for OKD on CentOS Stream CoreOS. To verify the contents of this directory, use the &amp;lsquo;gpg&amp;rsquo; and &amp;lsquo;shasum&amp;rsquo; tools to ensure the archives you have downloaded match those published from this location. The openshift-install binary has been preconfigured to install the following release: There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Red Hat Named a Leader in 2025 Gartner¬Æ Magic Quadrant‚Ñ¢ for Container Management for the Third Consecutive Year</title><link>https://kubermates.org/docs/2025-08-11-red-hat-named-a-leader-in-2025-gartner-magic-quadrant-for-container-management-f/</link><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-11-red-hat-named-a-leader-in-2025-gartner-magic-quadrant-for-container-management-f/</guid><description>Share This week we announced that for the third consecutive year, Red Hat has been named a Leader in the Gartner¬Æ Magic Quadrant‚Ñ¢ for Container Management. We‚Äôre thrilled by this recognition and believe it represents continued validation of Red Hat OpenShift‚Äôs strong execution and strategy for delivering an industry leading application platform across hybrid environments from the data center, to the cloud, and to the edge. Red Hat OpenShift is recognized for its ability to execute and completeness of vision. Red Hat‚Äôs investments in OpenShift as a platform for AI workloads, and in building AI capabilities directly into the product to make users‚Äô jobs easier. We believe it is also reflected in Red Hat‚Äôs position in the Gartner Magic Quadrant. OpenShift provides a comprehensive application platform with container management capabilities, including support for virtual machine workloads and AI-powered services. Magic Quadrant reports are a culmination of rigorous, fact-based research in specific markets, providing a wide-angle view of the relative positions of the providers in markets where growth is high and provider differentiation is distinct. Providers are positioned into four quadrants: Leaders, Challengers, Visionaries and Niche Players. Leaders execute well against their current vision and are well positioned for tomorrow. The research enables you to get the most from market analysis in alignment with your unique business and technology needs. Gartner defines container management as offerings that support the deployment and operation of containerized workloads and associated resources. It uses a combination of technologies (many open source) that enable agile application deployments and infrastructure modernization.</description></item><item><title>4.20.0-okd-scos.ec.11</title><link>https://kubermates.org/releases/2025-08-10-4-20-0-okd-scos-ec-11/</link><pubDate>Sun, 10 Aug 2025 03:42:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-10-4-20-0-okd-scos-ec-11/</guid><description>These archives contain the client tooling for OKD on CentOS Stream CoreOS. To verify the contents of this directory, use the &amp;lsquo;gpg&amp;rsquo; and &amp;lsquo;shasum&amp;rsquo; tools to ensure the archives you have downloaded match those published from this location. The openshift-install binary has been preconfigured to install the following release: There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Kubernetes v1.34.0-rc.1</title><link>https://kubermates.org/releases/2025-08-09-kubernetes-v1-34-0-rc-1/</link><pubDate>Sat, 09 Aug 2025 07:41:31 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-09-kubernetes-v1-34-0-rc-1/</guid><description>See kubernetes-announce@. Additional binary downloads are linked in the CHANGELOG. See the CHANGELOG for more details. There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>August 08, 2025</title><link>https://kubermates.org/releases/2025-08-08-august-08-2025/</link><pubDate>Fri, 08 Aug 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-08-08-august-08-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>Now Live: GPT-5 on the DigitalOcean Gradient‚Ñ¢ AI Platform</title><link>https://kubermates.org/docs/2025-08-07-now-live-gpt-5-on-the-digitalocean-gradient-ai-platform/</link><pubDate>Thu, 07 Aug 2025 23:00:48 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-07-now-live-gpt-5-on-the-digitalocean-gradient-ai-platform/</guid><description>By Grace Morgan , Yogesh Sharma , and Amit Jotwani We√¢¬Ä¬ôre excited to announce that GPT-5 is now available on the DigitalOcean Gradient√¢¬Ñ¬¢ AI Platform. With this update, developers can start using GPT-5 immediately via serverless inference APIs or the Gradient AI Platform SDK. Alternatively, you can bring your own OpenAI API key to integrate the new model into your Gradient AI Platform agent workflow. Curl command Gradient AI Platform SDK The latest model from OpenAI, GPT-5 brings major improvements in reasoning, specialization, and overall performance. It excels at complex tasks like financial planning, medical document analysis, code generation, and creative content creation. Whether you√¢¬Ä¬ôre building advanced copilots, research tools, or generative experiences, GPT-5 makes it easier to deliver high-quality, domain-aware results, faster and with greater accuracy. This release is ideal for developers who need production-ready access to cutting-edge models without GPU setup or model management. You can start using GPT-5 on the Gradient AI Platform now: -&amp;gt; New to DigitalOcean? Create an account and then generate your model access key. -&amp;gt; Existing customers can start using GPT-5 immediately via serverless inference APIs , the Gradient AI Platform SDK , or by bringing your own OpenAI Key for your Gradient AI Platform agent workflow. Share Read more Read more Read more.</description></item><item><title>Introducing Headlamp AI Assistant</title><link>https://kubermates.org/docs/2025-08-07-introducing-headlamp-ai-assistant/</link><pubDate>Thu, 07 Aug 2025 20:00:00 +0100</pubDate><guid>https://kubermates.org/docs/2025-08-07-introducing-headlamp-ai-assistant/</guid><description>This announcement originally appeared on the Headlamp blog. To simplify Kubernetes management and troubleshooting, we&amp;rsquo;re thrilled to introduce Headlamp AI Assistant : a powerful new plugin for Headlamp that helps you understand and operate your Kubernetes clusters and applications with greater clarity and ease. Whether you&amp;rsquo;re a seasoned engineer or just getting started, the AI Assistant offers: Here is a demo of the AI Assistant in action as it helps troubleshoot an application running with issues in a Kubernetes cluster: Large Language Models (LLMs) have transformed not just how we access data but also how we interact with it. The rise of tools like ChatGPT opened a world of possibilities, inspiring a wave of new applications. Asking questions or giving commands in natural language is intuitive, especially for users who aren&amp;rsquo;t deeply technical. Now everyone can quickly ask how to do X or Y, without feeling awkward or having to traverse pages and pages of documentation like before. Therefore, Headlamp AI Assistant brings a conversational UI to Headlamp , powered by LLMs that Headlamp users can configure with their own API keys. It is available as a Headlamp plugin, making it easy to integrate into your existing setup. Users can enable it by installing the plugin and configuring it with their own LLM API keys, giving them control over which model powers the assistant. Once enabled, the assistant becomes part of the Headlamp UI, ready to respond to contextual queries and perform actions directly from the interface. As expected, the AI Assistant is focused on helping users with Kubernetes concepts. Yet, while there is a lot of value in responding to Kubernetes related questions from Headlamp&amp;rsquo;s UI, we believe that the great benefit of such an integration is when it can use the context of what the user is experiencing in an application.</description></item><item><title>Red Hat Named a Leader in 2025 Gartner¬Æ Magic Quadrant‚Ñ¢ for Cloud-Native Application Platforms for the Second Consecutive Year</title><link>https://kubermates.org/docs/2025-08-07-red-hat-named-a-leader-in-2025-gartner-magic-quadrant-for-cloud-native-applicati/</link><pubDate>Thu, 07 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-07-red-hat-named-a-leader-in-2025-gartner-magic-quadrant-for-cloud-native-applicati/</guid><description>Share This week we announced that Red Hat has been positioned as a Leader for the second year in a row in the 2025 Gartner¬Æ Magic Quadrant‚Ñ¢ for Cloud-Native Application Platforms. We are proud of this recognition, as we believe this is a strong validation of Red Hat OpenShift cloud services as a flexible, complete, and secure cloud-native application platform for AI-enabled applications and virtualized and containerized workloads across the hybrid cloud. The Red Hat OpenShift cloud services portfolio consists of jointly engineered solutions with hyperscalers, including Red Hat OpenShift Service on AWS , Azure Red Hat OpenShift , Red Hat OpenShift on IBM Cloud and Red Hat OpenShift Dedicated on Google Cloud. Source: Gartner, ‚ÄúMagic Quadrant for Cloud-Native Application Platforms,‚Äù August 2025 To help further explain the evaluation of the Gartner Magic Quadrant from our point of view, we‚Äôre answering some of the most frequently asked questions. ‚ÄúMagic Quadrant reports are a culmination of rigorous, fact-based research in specific markets, providing a wide-angle view of the relative positions of the providers in markets where growth is high and provider differentiation is distinct. Providers are positioned into four quadrants: Leaders, Challengers, Visionaries and Niche Players. The research enables you to get the most from market analysis in alignment with your unique business and technology needs. ‚Äù ‚ÄúGartner defines cloud-native application platforms as those that provide managed application runtime environments for applications and integrated capabilities to manage the life cycle of an application or application component. They typically enable distributed application deployments and support cloud style operations ‚Äî such as elasticity, multitenancy and self-service ‚Äî without requiring the development team to provision infrastructure or manage containers. Cloud-native application platforms are designed to facilitate the deployment, runtime execution, and management of modern cloud-native or cloud-optimized applications without the need to manage any underlying infrastructure. Also, they are designed to enhance developer productivity, accelerate development and deployment cycles, and increase operational effectiveness by making it easier to scale on demand. Cloud-native application platforms offer a structured execution environment for applications, effectively hiding the complexities of the underlying infrastructure and computing resources.</description></item><item><title>Top 10 articles Red Hat customers are reading right now</title><link>https://kubermates.org/docs/2025-08-07-top-10-articles-red-hat-customers-are-reading-right-now/</link><pubDate>Thu, 07 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-07-top-10-articles-red-hat-customers-are-reading-right-now/</guid><description>Share Staying on top of the latest advancements in open source technology is more critical than ever. At Red Hat, we&amp;rsquo;re at the forefront of this innovation, continuously delivering solutions that empower enterprises to navigate complex challenges, encourage efficiency, and build a more secure future. In this roundup, our customers and communities have been particularly drawn to topics spanning the future of AI in IT, the ongoing evolution of our core platforms, and the strategic shifts in critical industries like telecommunications. From groundbreaking product releases that enhance virtualization and accelerate AI workloads to essential insights on security, developer productivity, and sovereign cloud, these articles offer a pulse on what&amp;rsquo;s driving the open hybrid cloud. Explore the top stories that are shaping the future of enterprise IT and empowering you to innovate with confidence. Red Hat OpenShift 4. 19, now generally available, is designed to propel your enterprise forward in the AI era while offering enhanced flexibility for virtualization and security. This release, built on Kubernetes 1. 32 and CRI-O 1. 32, brings powerful capabilities to the trusted OpenShift application platform. From dynamic GPU slicing to accelerated expertise with Red Hat OpenShift Lightspeed, OpenShift 4. 19 provides the comprehensive and consistent foundation needed to innovate faster across your hybrid cloud environment without compromising on security.</description></item><item><title>Simplify network connectivity using Tailscale with Amazon EKS Hybrid Nodes</title><link>https://kubermates.org/docs/2025-08-06-simplify-network-connectivity-using-tailscale-with-amazon-eks-hybrid-nodes/</link><pubDate>Wed, 06 Aug 2025 22:12:21 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-06-simplify-network-connectivity-using-tailscale-with-amazon-eks-hybrid-nodes/</guid><description>This post was co-authored with Lee Briggs, Director of Solutions Engineering at Tailscale. In this post, we guide you through integrating Tailscale with your Amazon Elastic Kubernetes Service (EKS) Hybrid Nodes environment. Amazon EKS Hybrid Nodes is a feature of Amazon EKS that enables you to streamline your Kubernetes management by connecting on-premises and edge infrastructure to an EKS cluster running in Amazon Web Services (AWS). This unified approach allows AWS to manage the Kubernetes control plane in the cloud while you maintain your hybrid nodes in on-premises or edge locations. We demonstrate how to configure a remote pod network and node address space. Install Tailscale on your hybrid nodes, set up a subnet router within your Amazon Virtual Private Cloud (Amazon VPC) , and update your AWS routes accordingly. This integration provides direct, encrypted connections that streamline the network architecture needed for EKS Hybrid Nodes. Although EKS Hybrid Nodes streamlines the Kubernetes management challenge, network connectivity between your on-premises infrastructure and AWS remains a critical requirement. Tailscale can help streamline this network connectivity between your EKS Hybrid Nodes data plane and Amazon EKS Kubernetes control plane. Unlike traditional VPNs, which tunnel all network traffic through a central gateway server, Tailscale creates a peer-to-peer mesh network (known as a tailnet ). It enables encrypted point-to-point connections using the open source WireGuard protocol, connecting devices and services across different networks with enhanced security features. However, you can still use Tailscale like a traditional VPN.</description></item><item><title>Kubernetes v1.34.0-rc.0</title><link>https://kubermates.org/releases/2025-08-06-kubernetes-v1-34-0-rc-0/</link><pubDate>Wed, 06 Aug 2025 20:20:08 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-06-kubernetes-v1-34-0-rc-0/</guid><description>See kubernetes-announce@. Additional binary downloads are linked in the CHANGELOG. See the CHANGELOG for more details. There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>v1.35.0-alpha.0</title><link>https://kubermates.org/releases/2025-08-06-v1-35-0-alpha-0/</link><pubDate>Wed, 06 Aug 2025 13:26:11 +0000</pubDate><guid>https://kubermates.org/releases/2025-08-06-v1-35-0-alpha-0/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>August 06, 2025</title><link>https://kubermates.org/releases/2025-08-06-august-06-2025/</link><pubDate>Wed, 06 Aug 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-08-06-august-06-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>How 1&amp;1 Mail &amp; Media Scaled Kubernetes Networking with eBPF and Calico</title><link>https://kubermates.org/docs/2025-08-05-how-1-1-mail-media-scaled-kubernetes-networking-with-ebpf-and-calico/</link><pubDate>Tue, 05 Aug 2025 16:12:16 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-05-how-1-1-mail-media-scaled-kubernetes-networking-with-ebpf-and-calico/</guid><description>‚ÄúWe started in 2017 with Calico and never regretted it!‚Äù ‚ÄîStephan Fudeus, Product Owner/Lead Architect, 1&amp;amp;1 Mail &amp;amp; Media 1&amp;amp;1 Mail &amp;amp; Media, part of the United Internet, powers popular European internet brands including GMX and Web. de, serving more than 50% of Germany‚Äôs population with critical identity and email infrastructure. With roughly 45 to 50 million users, network reliability is non-negotiable. Any downtime could affect millions. By 2022, the company had containerized 80% of its workloads on Kubernetes across three self-managed data centers. While the platform, backed by bare metal nodes and custom network layers, was highly scalable, network throughput bottlenecks began to emerge. Pods were limited to 2. 5 Gbps of bandwidth due to IP encapsulation overhead, despite 10 Gbps network interfaces. The team needed a solution that: 1&amp;amp;1 Mail &amp;amp; Media had adopted Calico back in 2017, largely for its unique Kubernetes NetworkPolicy standard support. As their Kubernetes platform evolved, with clusters scaling to 300 bare metal nodes, 16,000 pods, and over 4 million conntrack entries, the team turned to Calico‚Äôs eBPF data plane to unlock performance gains. Following successful initial trials of eBPF in development and integration environments, the team moved forward with production migrations in 2023. While early versions of Calico on older Linux kernel versions presented some limitations, these challenges were quickly addressed with proactive collaboration between the Calico maintainers and the team.</description></item><item><title>August 05, 2025</title><link>https://kubermates.org/releases/2025-08-05-august-05-2025/</link><pubDate>Tue, 05 Aug 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-08-05-august-05-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>Introducing OpenShift Service Mesh 3.1</title><link>https://kubermates.org/docs/2025-08-05-introducing-openshift-service-mesh-3-1/</link><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-05-introducing-openshift-service-mesh-3-1/</guid><description>Share Red Hat OpenShift Service Mesh 3. 1 has been released and is included with the Red Hat OpenShift Container Platform and Red Hat OpenShift Platform Plus. Based on the Istio, Envoy, and Kiali projects, this release updates the version of Istio to 1. 26 and Kiali to 2. 11 , and is supported on OpenShift Container Platform 4. 16 and above. This is the first minor release following Red Hat OpenShift Service Mesh 3. 0, a major update to converge OpenShift Service Mesh with the community Istio project, with installation and management using the Sail operator. This change helps ensure that OpenShift Service Mesh can offer the latest stable Istio features with Red Hat support. If you are running OpenShift Service Mesh 2. 6 or earlier releases, you must upgrade to OpenShift Service Mesh 3. 0 before upgrading to 3.</description></item><item><title>August 01, 2025</title><link>https://kubermates.org/releases/2025-08-01-august-01-2025/</link><pubDate>Fri, 01 Aug 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-08-01-august-01-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>Amazon EKS platform version update</title><link>https://kubermates.org/releases/2025-07-30-amazon-eks-platform-version-update/</link><pubDate>Wed, 30 Jul 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-30-amazon-eks-platform-version-update/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Amazon EKS platform versions represent the capabilities of the Amazon EKS cluster control plane, such as which Kubernetes API server flags are enabled, as well as the current Kubernetes patch version. Each Kubernetes minor version has one or more associated Amazon EKS platform versions. The platform versions for different Kubernetes minor versions are independent. You can retrieve your cluster√¢¬Ä¬ôs current platform version using the AWS CLI or AWS Management Console. If you have a local cluster on AWS Outposts, see Learn Kubernetes and Amazon EKS platform versions for AWS Outposts instead of this topic. When a new Kubernetes minor version is available in Amazon EKS, such as 1. 33, the initial Amazon EKS platform version for that Kubernetes minor version starts at eks. 1. However, Amazon EKS releases new platform versions periodically to enable new Kubernetes control plane settings and to provide security fixes. When new Amazon EKS platform versions become available for a minor version: The Amazon EKS platform version number is incremented ( eks. &amp;lt;n+1&amp;gt; ).</description></item><item><title>4.20.0-okd-scos.ec.10</title><link>https://kubermates.org/releases/2025-07-29-4-20-0-okd-scos-ec-10/</link><pubDate>Tue, 29 Jul 2025 21:00:36 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-29-4-20-0-okd-scos-ec-10/</guid><description>These archives contain the client tooling for OKD on CentOS Stream CoreOS. To verify the contents of this directory, use the &amp;lsquo;gpg&amp;rsquo; and &amp;lsquo;shasum&amp;rsquo; tools to ensure the archives you have downloaded match those published from this location. The openshift-install binary has been preconfigured to install the following release: There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Top 5 Kubernetes Network Issues You Can Catch Early with Calico Whisker</title><link>https://kubermates.org/docs/2025-07-29-top-5-kubernetes-network-issues-you-can-catch-early-with-calico-whisker/</link><pubDate>Tue, 29 Jul 2025 18:54:43 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-29-top-5-kubernetes-network-issues-you-can-catch-early-with-calico-whisker/</guid><description>Kubernetes networking is deceptively simple on the surface, until it breaks, silently leaks data, or opens the door to a full-cluster compromise. As modern workloads become more distributed and ephemeral, traditional logging and metrics just can‚Äôt keep up with the complexity of cloud-native traffic flows. That‚Äôs where Calico Whisker comes in. Whisker is a lightweight Kubernetes-native observability tool created by Tigera. It offers deep insights into real-time traffic flow patterns, without requiring you to deploy heavyweight service meshes or packet sniffer. And here‚Äôs something you won‚Äôt get anywhere else: Whisker is data plane-agnostic. Whether you run Calico eBPF data plane, nftables, or iptables, you‚Äôll get the same high-fidelity flow logs with consistent fields, format, and visibility. You don‚Äôt have to change your data plane, Whisker fits right in and shows you the truth, everywhere. Let‚Äôs walk through 5 network issues Whisker helps you catch early, before they turn into outages or security incidents. Traditional observability tools often show whether a packet was forwarded, accepted or dropped, but not why. They lack visibility into which Kubernetes network policy was responsible or if one was even applied. With Whisker, each network flow is paired with: This lets you immediately spot: This makes it easy to answer questions like: You get proactive visibility into gaps in enforcement long before someone accidentally exposes an internal app to the public internet.</description></item><item><title>v1.33.3+k3s1</title><link>https://kubermates.org/releases/2025-07-29-v1-33-3-k3s1/</link><pubDate>Tue, 29 Jul 2025 17:43:05 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-29-v1-33-3-k3s1/</guid><description>This release updates Kubernetes to v1. 33. 3, and fixes a number of issues. For more details on what&amp;rsquo;s new, see the Kubernetes release notes. As always, we welcome and appreciate feedback from our community of users. Please feel free to: There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>v1.30.14+k3s2</title><link>https://kubermates.org/releases/2025-07-29-v1-30-14-k3s2/</link><pubDate>Tue, 29 Jul 2025 17:41:51 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-29-v1-30-14-k3s2/</guid><description>This release updates Kubernetes to v1. 30. 14, and fixes a number of issues. For more details on what&amp;rsquo;s new, see the Kubernetes release notes. As always, we welcome and appreciate feedback from our community of users. Please feel free to: There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Release v1.6.12</title><link>https://kubermates.org/releases/2025-07-29-release-v1-6-12/</link><pubDate>Tue, 29 Jul 2025 16:51:42 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-29-release-v1-6-12/</guid><description>Rancher Kubernetes Engine (RKE) is reaching its end of life. Version 1. 8 is the final release in the RKE 1. x series. We strongly recommend migrating to Rancher&amp;rsquo;s newer Kubernetes distribution, RKE2, to stay supported, secure, and take advantage of the latest features and updates. For more details, please refer to the official SUSE EOL article. Full Changelog : v1. 6. 10. v1. 6. 11 There was an error while loading.</description></item><item><title>Release v1.7.9</title><link>https://kubermates.org/releases/2025-07-29-release-v1-7-9/</link><pubDate>Tue, 29 Jul 2025 16:51:20 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-29-release-v1-7-9/</guid><description>Rancher Kubernetes Engine (RKE) is reaching its end of life. Version 1. 8 is the final release in the RKE 1. x series. We strongly recommend migrating to Rancher&amp;rsquo;s newer Kubernetes distribution, RKE2, to stay supported, secure, and take advantage of the latest features and updates. For more details, please refer to the official SUSE EOL article. Full Changelog : v1. 7. 7. v1. 7. 8 There was an error while loading.</description></item><item><title>Release v1.8.5</title><link>https://kubermates.org/releases/2025-07-29-release-v1-8-5/</link><pubDate>Tue, 29 Jul 2025 16:50:50 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-29-release-v1-8-5/</guid><description>Rancher Kubernetes Engine (RKE) is reaching its end of life. Version 1. 8 is the final release in the RKE 1. x series. We strongly recommend migrating to Rancher&amp;rsquo;s newer Kubernetes distribution, RKE2, to stay supported, secure, and take advantage of the latest features and updates. For more details, please refer to the official SUSE EOL article. Full Changelog : v1. 8. 4. v1. 8. 5 Full Changelog : v1.</description></item><item><title>Innovating DigitalOcean Managed Databases: Our H1 Progress and Improvements</title><link>https://kubermates.org/docs/2025-07-29-innovating-digitalocean-managed-databases-our-h1-progress-and-improvements/</link><pubDate>Tue, 29 Jul 2025 13:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-29-innovating-digitalocean-managed-databases-our-h1-progress-and-improvements/</guid><description>By Nicole Ghalwash Databases are the cornerstone of modern applications, and at DigitalOcean, we√¢¬Ä¬ôre committed to providing you with powerful, scalable, and easy-to-use managed database solutions. The first half of 2025 has been a busy and exciting time for our Managed Databases team, as we√¢¬Ä¬ôve rolled out a series of significant new features and engines designed to enhance your development experience. From expanding support for popular database versions to introducing advanced observability tools, we√¢¬Ä¬ôve been focused on delivering the innovations you need to build and scale with confidence. In chronological order, let√¢¬Ä¬ôs take a look at some of the key launches and improvements we√¢¬Ä¬ôve made within the first half of the 2025 fiscal year [H1]. Launched February 11, support for PostgreSQL 17 offers improved performance, expanded developer tools, enhanced high availability and replication, and advanced security and monitoring capabilities. To read more about all of the new features, such as indexing improvements and expanding monitoring and analysis tools, check out our blog post from this past February. Continuing the theme of database upgrades, as of March 13, DigitalOcean Managed MongoDB supports MongoDB v8 (8. 0. 12). This latest release brings significant improvements in performance, scalability, and security, including faster replication through improved concurrent writes, optimized time-series data handling, and enhanced client-side encryption with support for range queries. Developers also gain greater control over performance with features like persistent query settings and default maximum execution times. To learn more about this release and our Managed MongoDB service, check out our blog or visit the MongoDB homepage.</description></item><item><title>July 28, 2025</title><link>https://kubermates.org/releases/2025-07-28-july-28-2025/</link><pubDate>Mon, 28 Jul 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-07-28-july-28-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>Kubernetes v1.34 Sneak Peek</title><link>https://kubermates.org/docs/2025-07-28-kubernetes-v1-34-sneak-peek/</link><pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-28-kubernetes-v1-34-sneak-peek/</guid><description>Kubernetes v1. 34 is coming at the end of August 2025. This release will not include any removal or deprecation, but it is packed with an impressive number of enhancements. Here are some of the features we are most excited about in this cycle! Please note that this information reflects the current state of v1. 34 development and may change before release. The following list highlights some of the notable enhancements likely to be included in the v1. 34 release, but is not an exhaustive list of all planned changes. This is not a commitment and the release content is subject to change. Dynamic Resource Allocation (DRA) provides a flexible way to categorize, request, and use devices like GPUs or custom hardware in your Kubernetes cluster. Since the v1. 30 release, DRA has been based around claiming devices using structured parameters that are opaque to the core of Kubernetes. The relevant enhancement proposal, KEP-4381 , took inspiration from dynamic provisioning for storage volumes.</description></item><item><title>July 25, 2025</title><link>https://kubermates.org/releases/2025-07-25-july-25-2025/</link><pubDate>Fri, 25 Jul 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-07-25-july-25-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>Kubernetes Is Powerful, But Not Secure (at least not by default)</title><link>https://kubermates.org/docs/2025-07-24-kubernetes-is-powerful-but-not-secure-at-least-not-by-default/</link><pubDate>Thu, 24 Jul 2025 19:38:47 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-24-kubernetes-is-powerful-but-not-secure-at-least-not-by-default/</guid><description>Kubernetes has transformed how we deploy and manage applications. It gives us the ability to spin up a virtual data center in minutes, scaling infrastructure with ease. But with great power comes great complexities, and in the case of Kubernetes, that complexity is security. By default, Kubernetes permits all traffic between workloads in a cluster. This ‚Äúallow by default‚Äù stance is convenient during development, and testing but it‚Äôs dangerous in production. It‚Äôs up to DevOps, DevSecOps, and cloud platform teams to lock things down. To improve the security posture of a Kubernetes cluster, we can use microsegmentation , a practice that limits each workload‚Äôs network reach so it can only talk to the specific resources it needs. This is an essential security method in today‚Äôs cloud-native environments. We all understand that network policies can achieve microsegmentation; or in other words, it can divide our Kubernetes network model into isolated pieces. This is important since Kubernetes is usually used to provide multiple teams with their infrastructural needs or host multiple workloads for different tenants. With that, you would think network policies are first citizens of clusters. However, when we dig into implementing them, three operational challenges make most practitioners reluctant about implementing policies.</description></item><item><title>Pre-release v1.8.5-rc.3</title><link>https://kubermates.org/releases/2025-07-23-pre-release-v1-8-5-rc-3/</link><pubDate>Wed, 23 Jul 2025 06:31:09 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-23-pre-release-v1-8-5-rc-3/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Pre-release v1.7.9-rc.3</title><link>https://kubermates.org/releases/2025-07-23-pre-release-v1-7-9-rc-3/</link><pubDate>Wed, 23 Jul 2025 06:29:10 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-23-pre-release-v1-7-9-rc-3/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>4.20.0-okd-scos.ec.9</title><link>https://kubermates.org/releases/2025-07-22-4-20-0-okd-scos-ec-9/</link><pubDate>Tue, 22 Jul 2025 21:00:04 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-22-4-20-0-okd-scos-ec-9/</guid><description>These archives contain the client tooling for OKD on CentOS Stream CoreOS. To verify the contents of this directory, use the &amp;lsquo;gpg&amp;rsquo; and &amp;lsquo;shasum&amp;rsquo; tools to ensure the archives you have downloaded match those published from this location. The openshift-install binary has been preconfigured to install the following release: There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Scaling beyond IPv4: integrating IPv6 Amazon EKS clusters into existing Istio Service Mesh</title><link>https://kubermates.org/docs/2025-07-22-scaling-beyond-ipv4-integrating-ipv6-amazon-eks-clusters-into-existing-istio-ser/</link><pubDate>Tue, 22 Jul 2025 18:54:25 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-22-scaling-beyond-ipv4-integrating-ipv6-amazon-eks-clusters-into-existing-istio-ser/</guid><description>Organizations are increasingly adopting IPv6 for their Amazon Elastic Kubernetes Service (Amazon EKS) deployments, driven by three key factors: depletion of private IPv4 addresses, the need to streamline or eliminate overlay networks, and improved network security requirements on Amazon Web Services (AWS). In IPv6-enabled EKS clusters, each pod receives a unique IPv6 address from the Amazon Virtual Private Cloud (Amazon VPC) IPv6 range, with seamless compatibility facilitated by the Amazon EKS VPC Container Network Interface (CNI). This solution effectively addresses two major IPv4 limitations: the scarcity of private addresses and the security vulnerabilities created by overlapping IPv4 spaces that need Network Address Translation (NAT) at the node level. When transitioning to IPv6, you likely need to run both IPv4 and IPv6 EKS clusters simultaneously. This is particularly important for organizations using Istio Service Mesh with Amazon EKS, because IPv6 clusters must integrate with the existing Service Mesh and work smoothly alongside IPv4 clusters. To streamline this transition, you can configure your Istio Service Mesh to support both your current IPv4 EKS clusters and your new IPv6 EKS clusters. If Istio Service Mesh isn‚Äôt part of your infrastructure, then we suggest exploring Amazon VPC Lattice as an alternative solution to speed up your IPv6 implementation on AWS. This post provides a step-by-step guide for combining IPv6-enabled EKS clusters with your existing Istio Service Mesh and IPv4 workloads, enabling a graceful transition to IPv6 on AWS. This guide covers detailed instructions for enabling communication between IPv6 and IPv4 EKS clusters, along with recommended practices for implementing IPv6 across both single and multiple VPC configurations. The functionality of Amazon EKS IPv6 builds on the native dual-stack capabilities of VPC. When you enable IPv6 in your VPC, it receives both IPv4 prefixes and a /56 IPv6 prefix. This IPv6 prefix can come from three sources: Amazon‚Äôs Global Unicast Address (GUA) space, your own IPv6 range (BYOIPv6), or a Unique Local Address (ULA) space.</description></item><item><title>Four Powerful, New Features to Help You Build and Deploy More Efficient Apps On DigitalOcean Kubernetes</title><link>https://kubermates.org/docs/2025-07-22-four-powerful-new-features-to-help-you-build-and-deploy-more-efficient-apps-on-d/</link><pubDate>Tue, 22 Jul 2025 16:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-22-four-powerful-new-features-to-help-you-build-and-deploy-more-efficient-apps-on-d/</guid><description>By Nicole Ghalwash We√¢¬Ä¬ôre adding to March√¢¬Ä¬ôs updates with even more Managed Kubernetes features that will help you get even more utility out of the product√¢¬Ä¬ìincluding newly supported Droplet types, the ability to automatically scale nodes to zero when you√¢¬Ä¬ôre not using them, and more. Let√¢¬Ä¬ôs walk through these new features and how they can benefit both your Kubernetes environment and your business. TL;DR: We built some new features for Kubernetes platform. Get started with some quick-links below: √¢¬Ä¬ì&amp;gt;Not a customer yet? Spin up a Kubernetes cluster in minutes. √¢¬Ä¬ì&amp;gt;Already a customer? Explore the new features by logging into your DigitalOcean account. You can now deploy GPU-accelerated workloads on DigitalOcean Managed Kubernetes using our latest GPU Droplet types (both NVIDIA and AMD ). These new instance types are ideal for AI/ML training and inference, image and video processing, and other compute-intensive workloads. With native support for GPU nodes in your Kubernetes clusters, you get the flexibility of containers with the raw power of high-performance GPUs-fully integrated into the DOKS experience. Here are the new GPU Droplet types: This feature allows a node pool to automatically scale down to zero nodes when there are no active workloads that require those nodes. You can now enable the node pools within your Kubernetes environment to automatically scale down to zero when idle, stopping compute charges during those periods of inactivity. This feature is optimal for development or testing environments, applications with usage patterns tied to business hours that naturally yield idle periods, or workloads that use specialized node pools like GPU or CPU-Optimized for intermittent jobs. The main components of this feature include: 1.</description></item><item><title>Introducing ERNIE 4.5-21B-A3B-Base</title><link>https://kubermates.org/docs/2025-07-22-introducing-ernie-4-5-21b-a3b-base/</link><pubDate>Tue, 22 Jul 2025 14:51:42 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-22-introducing-ernie-4-5-21b-a3b-base/</guid><description>By Waverly Swinton and Quinn Eckart TL;DR: Baidu recently released ERNIE 4. 5-21B-A3B-Base, a powerful open-source LLM You can launch ERNIE 4. 5-21B directly on DigitalOcean GPU Droplets with only 1-click - deploy in the cloud console We put ERNIE 4. 5-21B to the test for translation and it outperformed models like Qwen3 - check out our demo ERNIE (Enhanced Representation through kNowledge IntEgration) 4. 5-21B-A3B-Base represents a significant leap in large language model development. Originating from Baidu√¢¬Ä¬ôs extensive research, this model is celebrated for its unique knowledge-enhanced architecture, which allows for robust performance across complex natural language processing tasks, including sophisticated text generation, nuanced conversational AI, and comprehensive summarization. Its ability to integrate real-world knowledge into its understanding sets it apart, making it a compelling choice for intricate AI applications. Deploying ERNIE 4. 5-21B on DigitalOcean empowers you to rapidly prototype and scale applications powered by a model that goes beyond surface-level text processing, without the overhead of intricate infrastructure configuration. ERNIE 4. 5-21B is praised for its advanced multimodal capabilities, handling text, images, audio, and video, as well as its remarkable computational efficiency, allowing you to save significantly on AI infrastructure deployment costs while achieving high performance. We specifically put it to the test for translation, using an ERNIE 4.</description></item><item><title>Pre-release v1.6.12-rc.1</title><link>https://kubermates.org/releases/2025-07-22-pre-release-v1-6-12-rc-1/</link><pubDate>Tue, 22 Jul 2025 10:34:13 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-22-pre-release-v1-6-12-rc-1/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>From CFP to Stage: Win Your Tech Talk Slot</title><link>https://kubermates.org/docs/2025-07-22-from-cfp-to-stage-win-your-tech-talk-slot/</link><pubDate>Tue, 22 Jul 2025 06:19:59 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-22-from-cfp-to-stage-win-your-tech-talk-slot/</guid><description>You‚Äôve devoted hours into a project, discovered something incredible, and decided you want to share it with the world. You find the perfect conference, open the Call for Proposals (CFP) form, and pour your heart into it. You hit &amp;ldquo;submit. &amp;quot; And then you wait‚Ä¶ That rejection email always stings. We‚Äôve all been there. But what if you could change the odds? What if you could move from hoping your talk gets chosen to expecting it? After speaking at numerous global conferences and analyzing hundreds of accepted and rejected talks, I‚Äôve found that a winning proposal isn‚Äôt about luck. It‚Äôs about strategy. This guide will break down that strategy, making it applicable for any tech event, from a local Google Community Day to the global stages of KubeCon or a Linux Foundation Summit. Before you write a single word of your abstract, you need to pick the right topic. The best topics sit at the intersection of three things: Your title is the first - and sometimes only - thing a reviewer reads. It needs to be a powerful hook. This is where you sell the story you hinted at in your title.</description></item><item><title>Deep dive into cluster networking for Amazon EKS Hybrid Nodes</title><link>https://kubermates.org/docs/2025-07-21-deep-dive-into-cluster-networking-for-amazon-eks-hybrid-nodes/</link><pubDate>Mon, 21 Jul 2025 22:22:52 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-21-deep-dive-into-cluster-networking-for-amazon-eks-hybrid-nodes/</guid><description>Amazon Elastic Kubernetes Service ( Amazon EKS ) Hybrid Nodes enables organizations to integrate their existing on-premises and edge computing infrastructure into EKS clusters as remote nodes. EKS Hybrid Nodes provides you with the flexibility to run your containerized applications wherever needed, while maintaining standardized Kubernetes management practices and addressing latency, compliance, and data residency needs. EKS Hybrid Nodes accelerates infrastructure modernization by repurposing existing hardware investments. Organizations can harness the elastic scalability, high availability, and fully managed advantages of Amazon EKS, while making sure of operational consistency through unified workflows and toolsets across hybrid environments. One of the key aspects of the EKS Hybrid Nodes solution is the hybrid network architecture between the cloud-based Amazon EKS control plane and your on-premises nodes. This post dives deep into the cluster networking configurations, guiding you through the process of integrating an EKS cluster with hybrid nodes in your existing infrastructure. In this walkthrough, we set up different Container Network Interface (CNI) options and load balancing solutions on EKS Hybrid Nodes to meet your networking requirements. EKS Hybrid Nodes needs private network connectivity between the cloud-hosted Amazon EKS control plane and the hybrid nodes running in your on-premises environment. This connectivity can be established using either Amazon Web Services (AWS) Direct Connect or AWS Site-to-Site VPN , through an AWS Transit Gateway or the Virtual Private Gateway into your Amazon Virtual Private Cloud (Amazon VPC). For an optimal experience, AWS recommends reliable network connectivity with at least 100 Mbps bandwidth, and a maximum of 200ms round-trip latency, for hybrid nodes connecting to the AWS Region. This is general guidance rather than a strict requirement, and specific bandwidth and latency requirements may differ based on the quantity of hybrid nodes and your application‚Äôs unique characteristics. The node and pod Classless Inter-Domain Routing (CIDR) blocks for your hybrid nodes and container workloads must be within the IPv4 RFC-1918 ranges.</description></item><item><title>v1.30.14-rc1+k3s2</title><link>https://kubermates.org/releases/2025-07-21-v1-30-14-rc1-k3s2/</link><pubDate>Mon, 21 Jul 2025 18:52:11 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-21-v1-30-14-rc1-k3s2/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>July 21, 2025</title><link>https://kubermates.org/releases/2025-07-21-july-21-2025/</link><pubDate>Mon, 21 Jul 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-07-21-july-21-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>Democratizing AI Model Training on Kubernetes: Introducing Kubeflow Trainer V2</title><link>https://kubermates.org/docs/2025-07-21-democratizing-ai-model-training-on-kubernetes-introducing-kubeflow-trainer-v2/</link><pubDate>Mon, 21 Jul 2025 00:00:00 -0500</pubDate><guid>https://kubermates.org/docs/2025-07-21-democratizing-ai-model-training-on-kubernetes-introducing-kubeflow-trainer-v2/</guid><description>Jul 21, 2025 ‚Ä¢ Kubeflow Trainer Team ‚Ä¢ 11 min read trainer Running machine learning workloads on Kubernetes can be challenging. Distributed training and LLMs fine-tuning, in particular, involves managing multiple nodes, GPUs, large datasets, and fault tolerance, which often requires deep Kubernetes knowledge. The Kubeflow Trainer v2 (KF Trainer) was created to hide this complexity, by abstracting Kubernetes from AI Practitioners and providing the easiest, most scalable way to run distributed PyTorch jobs. The main goals of Kubeflow Trainer v2 include: We‚Äôre deeply grateful to all contributors and community members who made the Trainer v2 possible with their hard work and valuable feedback. We‚Äôd like to give special recognition to andreyvelich , tenzen-y , electronic-waste , astefanutti , ironicbo , mahdikhashan , kramaranya , harshal292004 , akshaychitneni , chenyi015 and the rest of the contributors. We would also like to highlight ahg-g , kannon92 , and vsoch whose feedback was essential while we designed the Kubeflow Trainer architecture together with the Batch WG. See the full contributor list for everyone who helped make this release possible. Kubeflow Trainer v2 represents the next evolution of the Kubeflow Training Operator , building on over seven years of experience running ML workloads on Kubernetes. The journey began in 2017 when the Kubeflow project introduced TFJob to orchestrate TensorFlow training on Kubernetes. At that time, Kubernetes lacked many of the advanced batch processing features needed for distributed ML training, so the community had to implement these capabilities from scratch. Over the years, the project expanded to support multiple ML frameworks including PyTorch , MXNet , MPI , and XGBoost through various specialized operators. In 2021, these were consolidated into the unified Training Operator v1.</description></item><item><title>üß© GitHub Actions Composite vs Reusable Workflows</title><link>https://kubermates.org/blog/github-actions-composite-vs-reusable-workflows-4bih/</link><pubDate>Fri, 18 Jul 2025 08:41:53 +0000</pubDate><guid>https://kubermates.org/blog/github-actions-composite-vs-reusable-workflows-4bih/</guid><description>&lt;h2 id="how-to-standardize-and-supercharge-your-cicd-pipelines-across-projects"&gt;How to standardize and supercharge your CI/CD pipelines across projects&lt;/h2&gt;
&lt;p&gt;When your teams manage multiple projects with similar deployment patterns, repeating the same GitHub Actions steps over and over can become tedious, error-prone, and hard to maintain&lt;/p&gt;
&lt;p&gt;Thankfully, GitHub Actions offers two powerful solutions to help &lt;strong&gt;standardize, reuse, and scale your CI/CD pipelines&lt;/strong&gt;: &lt;strong&gt;Composite Actions&lt;/strong&gt; and &lt;strong&gt;Reusable Workflows&lt;/strong&gt;. When used together, they form a clean, modular, and DRY (don‚Äôt repeat yourself) CI/CD strategy&lt;/p&gt;</description></item><item><title>Blog: Post-Quantum Cryptography in Kubernetes</title><link>https://kubermates.org/docs/2025-07-18-blog-post-quantum-cryptography-in-kubernetes/</link><pubDate>Fri, 18 Jul 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-18-blog-post-quantum-cryptography-in-kubernetes/</guid><description>The world of cryptography is on the cusp of a major shift with the advent of quantum computing. While powerful quantum computers are still largely theoretical for many applications, their potential to break current cryptographic standards is a serious concern, especially for long-lived systems. This is where Post-Quantum Cryptography (PQC) comes in. In this article, I&amp;rsquo;ll dive into what PQC means for TLS and, more specifically, for the Kubernetes ecosystem. I‚Äôll explain what the (suprising) state of PQC in Kubernetes is and what the implications are for current and future clusters. Post-Quantum Cryptography refers to cryptographic algorithms that are thought to be secure against attacks by both classical and quantum computers. The primary concern is that quantum computers, using algorithms like Shor&amp;rsquo;s Algorithm , could efficiently break widely used public-key cryptosystems such as RSA and Elliptic Curve Cryptography (ECC), which underpin much of today&amp;rsquo;s secure communication, including TLS. The industry is actively working on standardizing and adopting PQC algorithms. One of the first to be standardized by NIST is the Module-Lattice Key Encapsulation Mechanism ( ML-KEM ), formerly known as Kyber, and now standardized as FIPS-203 (PDF download). It is difficult to predict when quantum computers will be able to break classical algorithms. However, it is clear that we need to start migrating to PQC algorithms now, as the next section shows. To get a feeling for the predicted timeline we can look at a NIST report covering the transition to post-quantum cryptography standards.</description></item><item><title>Post-Quantum Cryptography in Kubernetes</title><link>https://kubermates.org/docs/2025-07-18-post-quantum-cryptography-in-kubernetes/</link><pubDate>Fri, 18 Jul 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-18-post-quantum-cryptography-in-kubernetes/</guid><description>The world of cryptography is on the cusp of a major shift with the advent of quantum computing. While powerful quantum computers are still largely theoretical for many applications, their potential to break current cryptographic standards is a serious concern, especially for long-lived systems. This is where Post-Quantum Cryptography (PQC) comes in. In this article, I&amp;rsquo;ll dive into what PQC means for TLS and, more specifically, for the Kubernetes ecosystem. I&amp;rsquo;ll explain what the (suprising) state of PQC in Kubernetes is and what the implications are for current and future clusters. Post-Quantum Cryptography refers to cryptographic algorithms that are thought to be secure against attacks by both classical and quantum computers. The primary concern is that quantum computers, using algorithms like Shor&amp;rsquo;s Algorithm , could efficiently break widely used public-key cryptosystems such as RSA and Elliptic Curve Cryptography (ECC), which underpin much of today&amp;rsquo;s secure communication, including TLS. The industry is actively working on standardizing and adopting PQC algorithms. One of the first to be standardized by NIST is the Module-Lattice Key Encapsulation Mechanism ( ML-KEM ), formerly known as Kyber, and now standardized as FIPS-203 (PDF download). It is difficult to predict when quantum computers will be able to break classical algorithms. However, it is clear that we need to start migrating to PQC algorithms now, as the next section shows. To get a feeling for the predicted timeline we can look at a NIST report covering the transition to post-quantum cryptography standards.</description></item><item><title>Elevate Your AI Workloads: AMD Instinct‚Ñ¢ MI325X GPU Droplets are Now Available on DigitalOcean</title><link>https://kubermates.org/docs/2025-07-17-elevate-your-ai-workloads-amd-instinct-mi325x-gpu-droplets-are-now-available-on-/</link><pubDate>Thu, 17 Jul 2025 15:07:55 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-17-elevate-your-ai-workloads-amd-instinct-mi325x-gpu-droplets-are-now-available-on-/</guid><description>By Waverly Swinton At DigitalOcean, we√¢¬Ä¬ôre constantly striving to equip developers and digital native enterprises with the most powerful and accessible tools to fuel AI innovation. Following the introduction of our AMD Instinct√¢¬Ñ¬¢ MI300X GPU Droplets last month, we√¢¬Ä¬ôre thrilled to announce the availability of the next generation of AI accelerators - AMD Instinct√¢¬Ñ¬¢ MI325X. Built on the advanced AMD CDNA√¢¬Ñ¬¢ 3 architecture, the AMD Instinct√¢¬Ñ¬¢ MI325X accelerators are engineered to deliver exceptional performance for AI workloads, including large model training, fine-tuning, inference, and high-performance computing (HPC). These GPUs are available as GradientAI GPU Droplets , powerful virtual machines with a simplified setup. Key benefits of the AMD Instinct√¢¬Ñ¬¢ MI325X that enable you to accelerate complex computations, ensure your inference tasks run faster, and provide more flexibility and control: Memory capacity and bandwidth : With 256GB of HBM3E memory and 6. 0TB/s of bandwidth, the MI325X can hold massive models entirely in memory, significantly reducing the need for model splitting across GPUs and accelerating complex computations. This represents a substantial leap in memory capabilities, offering 1. 8x more capacity and 1. 3x more bandwidth compared to previous generations. Accelerated AI inference performance : The MI325X boasts 1. 3x greater peak theoretical FP16 and FP8 compute performance than competing solutions, ensuring your AI models run faster and more efficiently, especially for inference tasks. ROCm√¢¬Ñ¬¢ Software Platform : The MI325X fully leverages the AMD ROCm√¢¬Ñ¬¢ open software platform, enabling developers to build and deploy powerful HPC and AI systems.</description></item><item><title>Pre-release v1.7.9-rc.2</title><link>https://kubermates.org/releases/2025-07-17-pre-release-v1-7-9-rc-2/</link><pubDate>Thu, 17 Jul 2025 04:24:24 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-17-pre-release-v1-7-9-rc-2/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Pre-release v1.8.5-rc.2</title><link>https://kubermates.org/releases/2025-07-17-pre-release-v1-8-5-rc-2/</link><pubDate>Thu, 17 Jul 2025 00:49:16 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-17-pre-release-v1-8-5-rc-2/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>v1.33.3-rc1+k3s1</title><link>https://kubermates.org/releases/2025-07-16-v1-33-3-rc1-k3s1/</link><pubDate>Wed, 16 Jul 2025 22:15:37 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-16-v1-33-3-rc1-k3s1/</guid><description>There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Kubernetes v1.34.0-beta.0</title><link>https://kubermates.org/releases/2025-07-16-kubernetes-v1-34-0-beta-0/</link><pubDate>Wed, 16 Jul 2025 14:40:38 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-16-kubernetes-v1-34-0-beta-0/</guid><description>See kubernetes-announce@. Additional binary downloads are linked in the CHANGELOG. See the CHANGELOG for more details. There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>July 16, 2025</title><link>https://kubermates.org/releases/2025-07-16-july-16-2025/</link><pubDate>Wed, 16 Jul 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-07-16-july-16-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>Under the hood: Amazon EKS ultra scale clusters</title><link>https://kubermates.org/docs/2025-07-16-under-the-hood-amazon-eks-ultra-scale-clusters/</link><pubDate>Wed, 16 Jul 2025 00:14:37 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-16-under-the-hood-amazon-eks-ultra-scale-clusters/</guid><description>This post was co-authored by Shyam Jeedigunta, Principal Engineer, Amazon EKS; Apoorva Kulkarni, Sr. Specialist Solutions Architect, Containers and Raghav Tripathi, Sr. Software Dev Manager, Amazon EKS. Today, Amazon Elastic Kubernetes Service (Amazon EKS) announced support for clusters with up to 100,000 nodes. With Amazon EC2‚Äôs new generation accelerated computing instance types, this translates to 1. 6 million AWS Trainium chips or 800,000 NVIDIA GPUs in a single Kubernetes cluster. This unlocks ultra scale artificial intelligence (AI) and machine leaning (ML) workloads such as state-of-the-art model training, fine-tuning and agentic inference. Besides customers directly consuming Amazon EKS today, these improvements also extend to other AI/ML services like Amazon SageMaker HyperPod with EKS that leverage EKS as their compute layer, advancing AWS‚Äôs overall ultra scale computing capabilities. Our customers have made it clear that containerization of training jobs and operators such as Kubeflow, the ability to streamline resource provisioning and lifecycle through projects like Karpenter, support for pluggable scheduling strategies, and access to a vast ecosystem of cloud-native tools is critical for their success in the AI/ML domain. Kubernetes has emerged as a key enabler here due to its powerful and extensible API model along with robust container orchestration capabilities, allowing accelerated workloads to scale quickly and run reliably. Through multiple technical innovations, architectural improvements and open-source collaboration, Amazon EKS has built the next generation of its cluster control plane and data plane for ultra scale, with full Kubernetes conformance. At AWS, we recommend customers running general-purpose applications with low coupling and horizontal scalability to follow a cell-based architecture as the strategy to sustain growth.</description></item><item><title>Amazon EKS enables ultra scale AI/ML workloads with support for 100K nodes per cluster</title><link>https://kubermates.org/docs/2025-07-16-amazon-eks-enables-ultra-scale-ai-ml-workloads-with-support-for-100k-nodes-per-c/</link><pubDate>Wed, 16 Jul 2025 00:14:26 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-16-amazon-eks-enables-ultra-scale-ai-ml-workloads-with-support-for-100k-nodes-per-c/</guid><description>&lt;p&gt;Open the original post ‚Üó &lt;a href="https://aws.amazon.com/blogs/containers/amazon-eks-enables-ultra-scale-ai-ml-workloads-with-support-for-100k-nodes-per-cluster/"&gt;https://aws.amazon.com/blogs/containers/amazon-eks-enables-ultra-scale-ai-ml-workloads-with-support-for-100k-nodes-per-cluster/&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Kubernetes v1.33.3</title><link>https://kubermates.org/releases/2025-07-15-kubernetes-v1-33-3/</link><pubDate>Tue, 15 Jul 2025 22:19:31 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-15-kubernetes-v1-33-3/</guid><description>See kubernetes-announce@. Additional binary downloads are linked in the CHANGELOG. See the CHANGELOG for more details. There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>Kubernetes v1.32.7</title><link>https://kubermates.org/releases/2025-07-15-kubernetes-v1-32-7/</link><pubDate>Tue, 15 Jul 2025 22:18:27 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-15-kubernetes-v1-32-7/</guid><description>See kubernetes-announce@. Additional binary downloads are linked in the CHANGELOG. See the CHANGELOG for more details. There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>4.20.0-okd-scos.ec.8</title><link>https://kubermates.org/releases/2025-07-15-4-20-0-okd-scos-ec-8/</link><pubDate>Tue, 15 Jul 2025 20:59:22 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-15-4-20-0-okd-scos-ec-8/</guid><description>These archives contain the client tooling for OKD on CentOS Stream CoreOS. To verify the contents of this directory, use the &amp;lsquo;gpg&amp;rsquo; and &amp;lsquo;shasum&amp;rsquo; tools to ensure the archives you have downloaded match those published from this location. The openshift-install binary has been preconfigured to install the following release: There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>VPC CNI Multi-NIC feature for multi-homed pods</title><link>https://kubermates.org/releases/2025-07-15-vpc-cni-multi-nic-feature-for-multi-homed-pods/</link><pubDate>Tue, 15 Jul 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-15-vpc-cni-multi-nic-feature-for-multi-homed-pods/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. By default, the Amazon VPC CNI plugin assigns one IP address to each pod. This IP address is attached to an elastic network interface that handles all incoming and outgoing traffic for the pod. To increase the bandwidth and packet per second rate performance, you can use the Multi-NIC feature of the VPC CNI to configure a multi-homed pod. A multi-homed pod is a single Kubernetes pod that uses multiple network interfaces (and multiple IP addresses). By running a multi-homed pod, you can spread its application traffic across multiple network interfaces by using concurrent connections. This is especially useful for Artificial Intelligence (AI), Machine Learning (ML), and High Performance Computing (HPC) use cases. The following diagram shows a multi-homed pod running on a worker node with multiple network interface cards (NICs) in use. On Amazon EC2, an elastic network interface is a logical networking component in a VPC that represents a virtual network card. For many EC2 instance types, the network interfaces share a single network interface card (NIC) in hardware. This single NIC has a maximum bandwidth and packet per second rate. If the multi-NIC feature is enabled, the VPC CNI doesn√¢¬Ä¬ôt assign IP addresses in bulk, which it does by default.</description></item><item><title>Dry Run: Your Kubernetes network policies with Calico staged network policies</title><link>https://kubermates.org/docs/2025-07-15-dry-run-your-kubernetes-network-policies-with-calico-staged-network-policies/</link><pubDate>Tue, 15 Jul 2025 14:01:01 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-15-dry-run-your-kubernetes-network-policies-with-calico-staged-network-policies/</guid><description>Kubernetes Network Policies (KNP) are powerful resources that help secure and isolate workloads in a cluster. By defining what traffic is allowed to and from specific pods, KNPs provide the foundation for zero-trust networking and least-privilege access in cloud-native environments. But there‚Äôs a problem: KNPs are risky, and applying them without a clear game plan can be potentially disruptive. Without deep insight into existing traffic flows, applying a restrictive policy can instantly break connectivity killing live workloads, user sessions, or critical app dependencies. An even scarier scenario is when we implement policies that we think cover everything and workloads actually work, but after a restart or scaling operation we hit new problems. Kubernetes, with all of its features, has no built-in ‚Äúdry run‚Äù mode for policies, and no first-class observability to show what would be blocked or allowed which is the right decision since Kubernetes is an orchestrator not an implementer. This forces platform teams into a difficult choice, deploy permissive or no policies and weaken security, or Risk service disruption while debugging restrictive ones. As a result, many teams delay implementing network policies entirely only to regret it after a zero-day exploit like Log4Shell, XZ backdoor, or other vulnerabilities that can impact production. The fear of breaking something becomes the top reason why Kubernetes environments go unsegmented. You can‚Äôt enforce what you can‚Äôt test safely. For instance, let‚Äôs say you want to secure a workload deployed by another team. You don‚Äôt control how it was configured.</description></item><item><title>Nirmata Teams Dashboard Gets a Makeover: A Cleaner, Smarter Experience for Managing Kubernetes and Cloud Environments</title><link>https://kubermates.org/docs/2025-07-15-nirmata-teams-dashboard-gets-a-makeover-a-cleaner-smarter-experience-for-managin/</link><pubDate>Tue, 15 Jul 2025 08:00:09 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-15-nirmata-teams-dashboard-gets-a-makeover-a-cleaner-smarter-experience-for-managin/</guid><description>We‚Äôre excited to announce a refreshed Teams Dashboard in Nirmata Control Hub. This redesign delivers a cleaner, more intuitive interface that not only looks better, it works better too. Whether you‚Äôre managing a few teams or dozens, the new dashboard streamlines how you view and interact with team-related data across your Kubernetes and cloud environments. To improve visibility and team-level accountability, the dashboard now shows how infrastructure resources are allocated across teams. You can easily see which Kubernetes clusters, namespaces, and repositories belong to each team ‚Äì such as Security, Platform, or Development ‚Äì and who the team members are. This makes it simpler to understand team footprints, manage access, and align infrastructure usage with organizational structure. Team cards have been completely redesigned with a cleaner layout that makes key information immediately visible. Each card now includes: This update gives you a high-level snapshot of your teams‚Äô footprint across your infrastructure , helping you stay informed at a glance. We‚Äôve improved how policy report assignments are surfaced on each team card, offering better visibility and preparing the groundwork for even more powerful insights and statistics in future releases. Instead of listing all teams on one long scroll, we now paginate the dashboard. You‚Äôll see 12 teams at a time, making navigation faster and more manageable, especially for larger organizations. We‚Äôve also made significant improvements to the Team Details page, focusing on clarity and ease of use.</description></item><item><title>What Is Kubernetes? Finally, a Simple Explanation!</title><link>https://kubermates.org/docs/2025-07-15-what-is-kubernetes-finally-a-simple-explanation/</link><pubDate>Tue, 15 Jul 2025 05:26:35 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-15-what-is-kubernetes-finally-a-simple-explanation/</guid><description>If you&amp;rsquo;ve googled for &amp;ldquo;What Is Kubernetes?&amp;rdquo; you probably got the usual: Well, you&amp;rsquo;re about to read something different. Finally, a blog that will explain &amp;ldquo;What is Kubernetes?&amp;rdquo; in very simple words ; an explanation for normal human beings. But Kubernetes is a solution to a problem. And to understand the solution, you first have to look at the problem it solves. Containers, containers, containers‚Ä¶ in the voice of Steve Ballmer. You probably heard about Docker. But even if you didn&amp;rsquo;t, it&amp;rsquo;s no big deal to understand it. Docker is a set of tools that allow you to create, edit, and run containers. And a container is just an application in a small little box. What&amp;rsquo;s so cool about containers? Well, one thing is that the application has all that it needs (the so-called dependencies ) inside that little box. So you can take that container, run it on Windows, MacOS, Linux, whatever. It won&amp;rsquo;t complain that stuff is missing.</description></item><item><title>From Raw Data to Model Serving: A Blueprint for the AI/ML Lifecycle with Kubeflow</title><link>https://kubermates.org/docs/2025-07-15-from-raw-data-to-model-serving-a-blueprint-for-the-ai-ml-lifecycle-with-kubeflow/</link><pubDate>Tue, 15 Jul 2025 00:00:00 -0500</pubDate><guid>https://kubermates.org/docs/2025-07-15-from-raw-data-to-model-serving-a-blueprint-for-the-ai-ml-lifecycle-with-kubeflow/</guid><description>Jul 15, 2025 ‚Ä¢ Helber Belmiro ‚Ä¢ 22 min read mlops pipelines spark feast model-registry kserve Are you looking for a practical, reproducible way to take a machine learning project from raw data all the way to a deployed, production-ready model? This post is your blueprint for the AI/ML lifecycle: you‚Äôll learn how to use Kubeflow and open source tools such as Feast to build a workflow you can run on your laptop and adapt to your own projects. We‚Äôll walk through the entire ML lifecycle‚Äîfrom data preparation to live inference‚Äîleveraging the Kubeflow platform to create a cohesive, production-grade MLOps workflow. The project implements a complete MLOps workflow for a fraud detection use case. Fraud detection is a critical application in financial services, where organizations need to identify potentially fraudulent transactions in real-time while minimizing false positives that could disrupt legitimate customer activity. Our fraud detection system leverages machine learning to analyze large volumes of transaction data, learn patterns from historical behavior, and flag suspicious transactions that deviate from normal patterns. The model considers various features such as transaction amounts, location data, merchant information, and user behavior patterns to make predictions. This makes fraud detection an ideal use case for demonstrating MLOps concepts because it requires: The workflow ingests raw transaction data, proceeds through data preparation and feature engineering, then model training and registration, and finally deploys the model as a production-ready inference service that can evaluate transactions in real-time. The entire workflow is orchestrated as a Kubeflow Pipeline, which provides a powerful framework for defining, deploying, and managing complex machine learning pipelines on Kubernetes. Here is a high-level overview of the pipeline: The pipeline assumes that the initial datasets ( train. csv , test. csv , etc. ) are already available.</description></item><item><title>Kubernetes Monitoring backend 2.2: better cluster observability through new alert and recording rules</title><link>https://kubermates.org/docs/2025-07-15-kubernetes-monitoring-backend-2-2-better-cluster-observability-through-new-alert/</link><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-15-kubernetes-monitoring-backend-2-2-better-cluster-observability-through-new-alert/</guid><description>Pete Wall √Ç¬∑ 17 Jun 2025 √Ç¬∑ 5 min read Our latest Kubernetes Monitoring Helm chart offers easier, dynamic configuration with Alloy Operator. Read more.</description></item><item><title>Kubernetes Governance with Nirmata &amp; SUSE</title><link>https://kubermates.org/docs/2025-07-14-kubernetes-governance-with-nirmata-suse/</link><pubDate>Mon, 14 Jul 2025 08:05:54 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-14-kubernetes-governance-with-nirmata-suse/</guid><description>Happy Monday, cloud-native enthusiasts! We‚Äôre thrilled to share some exciting news that marks a significant milestone for Nirmata and the broader Kubernetes community. We are proud to announce that Nirmata is now officially approved and listed in the SUSE Partner Certification &amp;amp; Solutions Catalog! This achievement isn‚Äôt just a badge; it‚Äôs a testament to our commitment to providing robust, integrated solutions that simplify Kubernetes governance and management to enhance security for enterprises. For users of SUSE and Rancher, this partnership opens up new avenues for streamlined operations and greater control over their cloud-native environments. One of the most immediate benefits of this partnership is the seamless availability of Nirmata within the Rancher ecosystem. You can now easily access Nirmata directly from your Rancher Manager interface. Navigate to Apps ‚Üí Charts within your Rancher Manager, and you‚Äôll find Nirmata listed in the Rancher Partner Catalog. This direct integration makes it incredibly easy for Rancher users to discover, deploy, and leverage Nirmata‚Äôs capabilities for Kubernetes governance, policy management, governance, and security. This collaboration between Nirmata and SUSE/Rancher brings together two powerful platforms to address critical needs in the cloud-native landscape: We are incredibly excited about the possibilities this partnership unlocks. By combining Nirmata‚Äôs powerful policy-based management with SUSE and Rancher‚Äôs robust Kubernetes management platform, we are empowering organizations to achieve greater agility, security, and efficiency in their cloud-native journeys. We encourage all SUSE and Rancher users to explore Nirmata in the Rancher Partner Catalog and the Partner Certification and Solutions Catalog , and see firsthand how our integrated solution can transform your Kubernetes governance and security operations. Stay tuned for more updates as we continue to deepen our collaboration with SUSE/Rancher and contribute to a more secure and manageable cloud-native future! To see what all the buzz is about with Nirmata, check out the Nirmata Control Hub product brief while you‚Äôre here.</description></item><item><title>July 14, 2025</title><link>https://kubermates.org/releases/2025-07-14-july-14-2025/</link><pubDate>Mon, 14 Jul 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-07-14-july-14-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>July 11, 2025</title><link>https://kubermates.org/releases/2025-07-11-july-11-2025/</link><pubDate>Fri, 11 Jul 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-07-11-july-11-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>Powered by DigitalOcean Hatch: Why Uxify‚Äôs Founders Always Choose DigitalOcean</title><link>https://kubermates.org/docs/2025-07-10-powered-by-digitalocean-hatch-why-uxify-s-founders-always-choose-digitalocean/</link><pubDate>Thu, 10 Jul 2025 18:29:23 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-10-powered-by-digitalocean-hatch-why-uxify-s-founders-always-choose-digitalocean/</guid><description>By Martin Nguyen Hatch is DigitalOcean√¢¬Ä¬ôs global program for startups that provides select technology companies with cloud infrastructure credits and discounts, direct access to DigitalOcean√¢¬Ä¬ôs product experts for personalized guidance, priority technical support, and access to a thriving community of like-minded founders. By keeping infrastructure and cost management simple, the Hatch program enables startups to build tech solutions that make an impact. For Georgi Petrov and Mihail Stoychev, the co-founders and serial entrepreneurs behind Uxify , choosing DigitalOcean to host their startup√¢¬Ä¬ôs infrastructure was a no-brainer. They built their previous ventures NitroPack, SMSBump, and iSenseLabs on DigitalOcean and could trust the platform√¢¬Ä¬ôs simplicity, reliability, and user experience. When it came time to power Uxify√¢¬Ä¬ôs AI-enabled website performance solution and AI agent Uxi, they opted for DigitalOcean again With Hatch program benefits like cloud credits and GPU Droplet discounts, Uxify could focus on building and scaling their underlying AI infrastructure. Uxify√¢¬Ä¬ôs story begins over a decade ago with iSenseLabs, an agency that provided services but also built and sold its own products. One of those products was NitroPack, a performance optimization tool designed to help websites load faster. As NitroPack gained traction, the team began seeing possibilities beyond their current solution. Fast forward to 2024, the team behind NitroPack reached a pivotal moment. They began rethinking web performance and user√¢¬Ä¬ôs perception of site responsiveness√¢¬Ä¬¶ This new philosophy led them to develop Navigation AI, a performance solution focused on user experience rather than purely technical metrics. Around the same time, NitroPack was acquired by WP Engine. As part of that acquisition, the emerging Navigation AI project was spun out into a new company: Uxify.</description></item><item><title>Sharks of DigitalOcean: Laura Schaffer, VP, Growth</title><link>https://kubermates.org/docs/2025-07-10-sharks-of-digitalocean-laura-schaffer-vp-growth/</link><pubDate>Thu, 10 Jul 2025 15:25:33 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-10-sharks-of-digitalocean-laura-schaffer-vp-growth/</guid><description>By Sujatha R Technical Writer Laura Schaffer, Vice President of Growth at DigitalOcean, leads the team responsible for helping customers onboard and grow with DigitalOcean. In this edition of Shark Tales, she shares what excites her about building a self-serve experience, the energy that fuels her team, and why DigitalOcean stands out as a special place to work. There√¢¬Ä¬ôs a ton of opportunity here, and I√¢¬Ä¬ôm excited about everything we√¢¬Ä¬ôre building together. But what really sets DigitalOcean apart is how driven and united we are. We show up each day ready to collaborate, eager to deliver value to our customers, and motivated to make things happen. It√¢¬Ä¬ôs incredibly energizing to work in an environment where everyone shares that same hunger. You feel it in every conversation and project; it√¢¬Ä¬ôs inspiring, it√¢¬Ä¬ôs motivating, and it makes this a truly special place to be. √∞¬ü¬é¬• Have a look at Laura Schaffer√¢¬Ä¬ôs full conversation √¢¬¨¬á√Ø¬∏¬è I lead the Growth team, and our focus is on helping customers get started with DigitalOcean. It√¢¬Ä¬ôs such a fun space to be in because we get to witness that first spark, when someone signs up, explores what we offer, and has that √¢¬Ä¬úAha!√¢¬Ä¬ù moment. Our platform is simple and easy to use, and seeing new users discover that for themselves is incredibly rewarding. Every day, my team helps users reach that point of discovery, and that√¢¬Ä¬ôs a powerful thing to be a part of. As someone focused on growth and self-serve, I√¢¬Ä¬ôm especially excited about how our technology enables simplicity.</description></item><item><title>The Linux Foundation Announces Keynote Speakers for Open Source Summit India 2025</title><link>https://kubermates.org/events/2025-07-09-the-linux-foundation-announces-keynote-speakers-for-open-source-summit-india-202/</link><pubDate>Wed, 09 Jul 2025 22:57:56 +0000</pubDate><guid>https://kubermates.org/events/2025-07-09-the-linux-foundation-announces-keynote-speakers-for-open-source-summit-india-202/</guid><description>&lt;p&gt;Leaders from across the global open source ecosystem and India‚Äôs leading Digital Public Infrastructure executives to speak in ‚Ä¶&lt;/p&gt;</description></item><item><title>4.20.0-okd-scos.ec.7</title><link>https://kubermates.org/releases/2025-07-08-4-20-0-okd-scos-ec-7/</link><pubDate>Tue, 08 Jul 2025 21:00:58 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-08-4-20-0-okd-scos-ec-7/</guid><description>These archives contain the client tooling for OKD on CentOS Stream CoreOS. To verify the contents of this directory, use the &amp;lsquo;gpg&amp;rsquo; and &amp;lsquo;shasum&amp;rsquo; tools to ensure the archives you have downloaded match those published from this location. The openshift-install binary has been preconfigured to install the following release: There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>What's New on DigitalOcean Gradient‚Ñ¢ AI Platform</title><link>https://kubermates.org/docs/2025-07-08-what-s-new-on-digitalocean-gradient-ai-platform/</link><pubDate>Tue, 08 Jul 2025 18:59:38 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-08-what-s-new-on-digitalocean-gradient-ai-platform/</guid><description>By Grace Morgan DigitalOcean√¢¬Ä¬ôs GenAI Platform is now DigitalOcean Gradient√¢¬Ñ¬¢ AI Platform. Learn more about the GA release and features. Welcome to What√¢¬Ä¬ôs New on DigitalOcean Gradient√¢¬Ñ¬¢ AI Platform √¢¬Ä¬îyour weekly roundup of the latest updates for the Gradient AI Platform. Each week, we√¢¬Ä¬ôll share new feature releases, technical tutorials, or video walkthroughs to help you stay ahead of the curve and keep building. Check back regularly for new insights and inspiration, or get started now: build an agent or call a model with serverless inference. Product Update Add Dropbox folders as a data source On Gradient AI Platform, you can now add a Dropbox folder as a data source to your knowledge bases. This allows you to index and use files stored in your Dropbox account within your knowledge base. Product Update Log Stream Insights on Gradient AI Platform DigitalOcean GradientAI is a unified AI cloud platform that combines GPU infrastructure, intelligent agent development, and prebuilt applications to help developers seamlessly build, deploy, and scale AI solutions from prototype to production. Product Update Gradient AI Platform SDK Now Available in Public Preview The official DigitalOcean Platform SDK is now in Public Preview. You can use the SDK to manage Gradient AI Platform resources, including knowledge bases and generative AI agents, from Python applications. Product Update Introducing DigitalOcean Gradient√¢¬Ñ¬¢ AI DigitalOcean GradientAI is a unified AI cloud platform that combines GPU infrastructure, intelligent agent development, and prebuilt applications to help developers seamlessly build, deploy, and scale AI solutions from prototype to production. Product Update Gradient AI Platform is now GA Gradient AI Platform is now generally available, offering developers a fully managed environment to build, scale, and deploy AI-powered applications and agents√¢¬Ä¬îcomplete with integrated tools for data integration, evaluation, observability, and serverless access to top LLMs√¢¬Ä¬îall without managing infrastructure.</description></item><item><title>Introducing Gradient: DigitalOcean‚Äôs Unified AI Cloud</title><link>https://kubermates.org/docs/2025-07-08-introducing-gradient-digitalocean-s-unified-ai-cloud/</link><pubDate>Tue, 08 Jul 2025 18:36:43 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-08-introducing-gradient-digitalocean-s-unified-ai-cloud/</guid><description>By Bratin Saha At DigitalOcean, we√¢¬Ä¬ôve always focused on delivering simple, powerful infrastructure that helps digital native enterprises build and scale their applications. Over the past year, we√¢¬Ä¬ôve extended that commitment into the AI space by investing in compute, tooling, and workflows that make it easier to build with AI from the ground up. We believe that to truly unlock the value of AI, developers need more than just infrastructure, they need an integrated experience that spans the full development lifecycle. That√¢¬Ä¬ôs why we√¢¬Ä¬ôre bringing together GPU infrastructure, intelligent agent development, and pre-built AI applications into one cohesive platform. Today, we√¢¬Ä¬ôre taking the next step in that journey with the introduction of DigitalOcean Gradient, our unified AI cloud. It gives you everything you need to train, fine-tune, deploy, and scale AI workloads√¢¬Ä¬înot just as isolated components, but as a seamless, full-stack platform purpose-built for modern AI development. As our AI portfolio has grown, we√¢¬Ä¬ôve recognized the need for a dedicated identity that reflects the scale and ambition of what we√¢¬Ä¬ôre building. The name DigitalOcean Gradient captures both the technical roots of machine learning and the philosophy behind our platform. In model training, gradients guide each step of learning√¢¬Ä¬îadjusting weights, reducing error, and moving toward better outcomes. In the same way, DigitalOcean Gradient is designed to guide developers from idea to production, with the right tools, abstractions, and infrastructure at every stage. Whether you√¢¬Ä¬ôre experimenting with foundation models, building intelligent agents, or deploying production workloads at scale, DigitalOcean Gradient is your end-to-end AI cloud. With DigitalOcean Gradient, we√¢¬Ä¬ôre unifying our AI offerings under one umbrella to make them easier to discover, adopt, and scale: If you√¢¬Ä¬ôre already building with our AI tools, there√¢¬Ä¬ôs nothing you need to change.</description></item><item><title>DigitalOcean Gradient Platform is now Generally Available</title><link>https://kubermates.org/docs/2025-07-08-digitalocean-gradient-platform-is-now-generally-available/</link><pubDate>Tue, 08 Jul 2025 18:36:14 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-08-digitalocean-gradient-platform-is-now-generally-available/</guid><description>By Grace Morgan TL;DR: Gradient Platform (previously GenAI Platform) is now generally available, offering a fully managed way to build and deploy AI apps with agents and serverless inference. New features like external data integration, traceability, and evaluations make it easy to go from prototype to production√¢¬Ä¬îno infrastructure management required. -&amp;gt; Build an agent -&amp;gt; Call a model with serverless inference -&amp;gt; Check out our tutorial on how to use serverless inference and agents with the OpenAI SDK: Today, we√¢¬Ä¬ôre excited to announce the General Availability (GA) of DigitalOcean√¢¬Ä¬ôs Gradient Platform , a developer-first platform that makes it easy to build, scale, and ship AI-powered applications. Gradient Platform brings together everything you need to go from prototype to production with AI, all in one fully managed experience. You can create intelligent agents that reason over your data, integrate cutting-edge LLMs into your app with a single API, or experiment with prebuilt tools, like evaluations or versioning, to accelerate your AI development. Whether you√¢¬Ä¬ôre automating workflows, enhancing customer support, or building new AI products from the ground up, the Platform gives you all the power without the complexity of managing infrastructure. Thousands of developers explored the platform during its public preview (formerly known as the GenAI Platform). Their feedback helped shape the product launching today. Today√¢¬Ä¬ôs release brings you a comprehensive platform that makes it easier to build, debug, and scale AI applications. New features like external data integration, agent traceability, customer conversation logs, and agent evaluation give you deeper visibility into how your agents behave, more control over their data and logic, and better ways to collaborate across teams√¢¬Ä¬îall without adding infrastructure overhead. Since the initial release in January 2025 , we√¢¬Ä¬ôve launched a wide range of enhancements driven by real-world adoption and evolving use cases. From foundational infrastructure upgrades to powerful new developer tools, here√¢¬Ä¬ôs what√¢¬Ä¬ôs changed between public preview and General Availability: Not every AI use case requires a full agent.</description></item><item><title>Calico Whisker &amp; Staged Network Policies: Secure Kubernetes Workloads Without Downtime</title><link>https://kubermates.org/docs/2025-07-07-calico-whisker-staged-network-policies-secure-kubernetes-workloads-without-downt/</link><pubDate>Mon, 07 Jul 2025 20:00:17 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-07-calico-whisker-staged-network-policies-secure-kubernetes-workloads-without-downt/</guid><description>Rolling out network policies in a live Kubernetes cluster can feel like swapping wings mid-flight‚Äîone typo or overly broad rule and critical traffic is grounded. Calico‚Äôs Staged Network Policies remove the turbulence by letting you deploy policies in staged mode, so you can observe their impact before enforcing anything. Add Whisker , the open-source policy enforcement and testing tool (introduced as part of Calico Open Source 3. 30 ) that captures every flow and tags it with a policy verdict, and you‚Äôve got a safety harness that proves your change is sound long before you flip the switch. In this post, we‚Äôll walk you through how you can leverage these capabilities to tighten security, validate intent, and ship changes confidently‚Äîwithout a single packet of downtime. Calico for Policy is a CNI agnostic tool. Refer to the Calico Open Source docs for a list of supported CNIs. The git repository for this blog post can be found here. For this post, let‚Äôs deploy a simple AKS cluster with Azure CNI. Now that our cluster is deployed. Let‚Äôs provision a demo application. For this post, we will deploy a three-tier web application called ‚Äúyet-another-bank‚Äù (yaobank).</description></item><item><title>Introducing Kafka Schema Registry for DigitalOcean Managed Kafka</title><link>https://kubermates.org/docs/2025-07-07-introducing-kafka-schema-registry-for-digitalocean-managed-kafka/</link><pubDate>Mon, 07 Jul 2025 17:15:04 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-07-introducing-kafka-schema-registry-for-digitalocean-managed-kafka/</guid><description>By Nicole Ghalwash We√¢¬Ä¬ôre excited to announce support for Kafka Schema Registry in DigitalOcean√¢¬Ä¬ôs Managed Kafka service, giving developers a powerful way to manage and validate schemas in their event-driven applications. Kafka Schema Registry, also referred to as Karapace, is a centralized service for managing and validating schemas for Kafka messages. It helps to ensure that data produced to and consumed from Kafka topics adheres to a defined structure, preventing data compatibility issues. For a developer, Kafka Schema Registry provides robust schema governance and standardized HTTP access for Kafka services, improving data integrity, developer productivity, and system interoperability. This helps to empower DevOps teams to build reliable, evolving event-driven applications by ensuring data integrity through centralized schema management and validation. Schema Registry simplifies the Kafka integration across diverse systems in a more secure way. Ultimately, this tool brings structure to your Kafka topics. It lets you define, version, and validate message schemas so that producers and consumers can stay in sync, even as your data evolves. It helps prevent breaking changes, making debugging easier, and keeps your Kafka pipelines reliable at scale. Please note that Kafka Schema Registry is only available for Kafka customers with a dedicated CPU environment. To learn more about our CPU-optimized Droplets, visit our Droplets homepage or our [product documentation. ]( https://docs.</description></item><item><title>Quick Fixes for Common Kubernetes Issues</title><link>https://kubermates.org/docs/2025-07-06-quick-fixes-for-common-kubernetes-issues/</link><pubDate>Sun, 06 Jul 2025 18:26:31 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-06-quick-fixes-for-common-kubernetes-issues/</guid><description>If you‚Äôve ever used Kubernetes in a real-world project, you&amp;rsquo;ve probably hit an error that made no sense at first glance ‚Äî a Pod stuck restarting, a Service not routing traffic, or your app mysteriously vanishing from the internet. The good news? You‚Äôre not alone. This guide walks you through the most common Kubernetes problems developers and DevOps teams face ‚Äî and more importantly, how to fix them quickly. Whether you&amp;rsquo;re new to Kubernetes or scaling your first production app, consider this your essential cheat sheet. Symptoms: Your Pod starts, crashes, restarts, and repeats the loop. Why it happens: How to fix it: Pro Tip: If your app needs time to boot, adjust initialDelaySeconds in your probe. Symptoms: Your Pod stays in Pending status forever. Why it happens: How to fix it: Look for reasons like Insufficient CPU. Symptoms: You created a Service, but can‚Äôt reach your app. Why it happens: How to fix it: If it shows &lt;none&gt; , your Service isn‚Äôt routing to any Pods. Symptoms: Pod never gets created due to image pulling failure. Why it happens: How to fix it: Then reference it in your Pod spec.</description></item><item><title>Navigating Failures in Pods With Devices</title><link>https://kubermates.org/docs/2025-07-03-navigating-failures-in-pods-with-devices/</link><pubDate>Thu, 03 Jul 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-03-navigating-failures-in-pods-with-devices/</guid><description>Kubernetes is the de facto standard for container orchestration, but when it comes to handling specialized hardware like GPUs and other accelerators, things get a bit complicated. This blog post dives into the challenges of managing failure modes when operating pods with devices in Kubernetes, based on insights from Sergey Kanzhelev and Mrunal Patel&amp;rsquo;s talk at KubeCon NA 2024. You can follow the links to slides and recording. The rise of AI/ML workloads has brought new challenges to Kubernetes. These workloads often rely heavily on specialized hardware, and any device failure can significantly impact performance and lead to frustrating interruptions. As highlighted in the 2024 Llama paper , hardware issues, particularly GPU failures, are a major cause of disruption in AI/ML training. You can also learn how much effort NVIDIA spends on handling devices failures and maintenance in the KubeCon talk by Ryan Hallisey and Piotr Prokop All-Your-GPUs-Are-Belong-to-Us: An Inside Look at NVIDIA&amp;rsquo;s Self-Healing GeForce NOW Infrastructure ( recording ) as they see 19 remediation requests per 1000 nodes a day! We also see data centers offering spot consumption models and overcommit on power, making device failures commonplace and a part of the business model. However, Kubernetes‚Äôs view on resources is still very static. The resource is either there or not. And if it is there, the assumption is that it will stay there fully functional - Kubernetes lacks good support for handling full or partial hardware failures. These long-existing assumptions combined with the overall complexity of a setup lead to a variety of failure modes, which we discuss here. Generally, all AI/ML workloads require specialized hardware, have challenging scheduling requirements, and are expensive when idle.</description></item><item><title>A Detailed Look at Calico Cloud Free Tier</title><link>https://kubermates.org/docs/2025-07-02-a-detailed-look-at-calico-cloud-free-tier/</link><pubDate>Wed, 02 Jul 2025 20:23:35 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-02-a-detailed-look-at-calico-cloud-free-tier/</guid><description>As Kubernetes environments grow in scale and complexity, platform teams face increasing pressure to secure workloads without slowing down application delivery. But managing and enforcing network policies in Kubernetes is notoriously difficult‚Äîespecially when visibility into pod-to-pod communication is limited or nonexistent. Teams are often forced to rely on manual traffic inspection, standalone logs, or trial-and-error policy changes, increasing the risk of misconfiguration and service disruption. Safe policy management and microsegmentation becomes a daunting task without clear knowledge or insight into which services should communicate with each other. In this detailed look, we‚Äôll explore how Calico Cloud Free Tier builds upon Calico Open Source , and helps platform teams visualize traffic with a dynamic service graph, simplifies policy management, and even analyzes actual traffic to recommend policies. Calico Cloud Free Tier is a managed SaaS, no-cost offering that extends the capabilities of Calico Open Source 3. 30 and higher to help Kubernetes teams improve network visibility, simplify policy management, and improve security by simplifying microsegmentation. Designed for single-cluster environments, it provides platform engineers and operators with powerful observability and policy management tools. With a seamless onboarding experience for users already running Calico Open Source 3. 30 or higher, Calico Cloud Free Tier empowers teams to take control of their Kubernetes traffic‚Äîwithout additional cost or vendor lock-in. Let‚Äôs take a closer look at the key features that make Calico Cloud Free Tier a powerful solution for Kubernetes network security and observability: Calico‚Äôs primary observability solution is Dynamic Service Graph , a powerful visualization tool that maps real-time pod-to-pod communication across your cluster. This Service Graph, which is available in Calico Cloud Free Tier, gives you an immediate understanding of how workloads interact, making it far easier to identify unexpected traffic patterns or missing connections.</description></item><item><title>July 02, 2025</title><link>https://kubermates.org/releases/2025-07-02-july-02-2025/</link><pubDate>Wed, 02 Jul 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-07-02-july-02-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>4.20.0-okd-scos.ec.6</title><link>https://kubermates.org/releases/2025-07-01-4-20-0-okd-scos-ec-6/</link><pubDate>Tue, 01 Jul 2025 22:03:20 +0000</pubDate><guid>https://kubermates.org/releases/2025-07-01-4-20-0-okd-scos-ec-6/</guid><description>These archives contain the client tooling for OKD on CentOS Stream CoreOS. To verify the contents of this directory, use the &amp;lsquo;gpg&amp;rsquo; and &amp;lsquo;shasum&amp;rsquo; tools to ensure the archives you have downloaded match those published from this location. The openshift-install binary has been preconfigured to install the following release: There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>VPC CNI troubleshooting content update</title><link>https://kubermates.org/releases/2025-06-30-vpc-cni-troubleshooting-content-update/</link><pubDate>Mon, 30 Jun 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-06-30-vpc-cni-troubleshooting-content-update/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. This is the troubleshooting guide for network policy feature of the Amazon VPC CNI. This guide covers: Install information, CRD and RBAC permissions New policyendpoints CRD and permissions Logs to examine when diagnosing network policy problems Network policy logs Running the eBPF SDK collection of tools to troubleshoot Known issues and solutions Known issues and solutions Note that network policies are only applied to pods that are made by Kubernetes Deployments. For more limitations of the network policies in the VPC CNI, see Considerations. You can troubleshoot and investigate network connections that use network policies by reading the Network policy logs and by running tools from the eBPF SDK. CRD: policyendpoints. networking. k8s. aws Kubernetes API: apiservice called v1. networking. k8s. io Kubernetes resource: Kind: NetworkPolicy RBAC: ClusterRole called aws-node (VPC CNI), ClusterRole called eks:network-policy-controller (network policy controller in EKS cluster control plane) For network policy, the VPC CNI creates a new CustomResourceDefinition (CRD) called policyendpoints.</description></item><item><title>Expanding DigitalOcean‚Äôs Role-Based Access Controls with custom roles</title><link>https://kubermates.org/docs/2025-06-30-expanding-digitalocean-s-role-based-access-controls-with-custom-roles/</link><pubDate>Mon, 30 Jun 2025 17:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-30-expanding-digitalocean-s-role-based-access-controls-with-custom-roles/</guid><description>By Nicole Ghalwash Today, we are excited to announce our latest Role-Based Access Control (RBAC) feature, custom roles. With custom roles, teams can now assign permissions to individuals that are precisely aligned with their operational and security requirements, reinforcing the principle of least privilege. This allows for more precise permission management, which helps to enhance overall infrastructure security by reducing the risk of over-privileged accounts. Custom roles give you full control over who can do what on your projects, improving the overall security of your cloud resources. In this blog post, we will walk through what custom roles are, how they work, key features, when to use them, and how they can help your team. Custom roles are user-defined sets of permissions that allow organizations to tailor access control to their specific needs, beyond what√¢¬Ä¬ôs available in predefined roles. In other words, custom roles let you create your own set of permissions instead of relying only on default, predefined roles (like Viewer, Billing Viewer, etc. ) that may not work for you. Now, users can define more detailed custom permissions that target specific resources and needs. For example, a user may only need read access to Droplets, but write access to Kubernetes. Hear what a DigitalOcean customer had to say about using custom roles. This customer is a Co-founder of a revenue management company: √¢¬Ä¬úCustom roles helped me bring my team onto the platform without granting blanket access.</description></item><item><title>Is Your Kyverno Healthy? Now You Can Know for Sure.</title><link>https://kubermates.org/docs/2025-06-30-is-your-kyverno-healthy-now-you-can-know-for-sure/</link><pubDate>Mon, 30 Jun 2025 08:00:13 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-30-is-your-kyverno-healthy-now-you-can-know-for-sure/</guid><description>Running Kyverno is essential for enforcing Kubernetes governance and security policies. But is your Kyverno setup secure, scalable, and resilient? With the latest enhancement in Nirmata Control Hub , you no longer have to guess. We‚Äôre excited to introduce the Kyverno Health Check ‚Äì a new capability that provides a clear, actionable view of Kyverno‚Äôs configuration in your clusters. Whether you‚Äôre a platform engineer managing dozens of clusters or a security lead enforcing policy compliance, this feature helps to ensure that Kyverno is running optimally at all times. Kyverno by Nirmata enforces critical security, compliance, and operational policies in Kubernetes. But like any controller, its effectiveness depends on its own configuration and health. A misconfigured or unhealthy Kyverno deployment can : Kyverno Health Check ensures none of this happens by continuously and vigilantly evaluating your deployments for best practices and known risks. The new Kyverno Health Check feature in Nirmata Control Hub evaluates Kyverno deployments based on four critical categories: Each category is scored and flagged as: These results are aggregated into a Kyverno Health Grade (A to F), allowing you to view the overall state of Kyverno at a glance and drill down to address the areas that matter most. In many clusters, Kyverno components may restart frequently or be terminated due to out-of-memory (OOM) conditions. These issues often go unnoticed until policy failures occur. That‚Äôs a risk. Nirmata Control Hub detects this and gives you: You or your team can apply the fix, rescan the cluster, and watch your health grade improve.</description></item><item><title>Cut Through the DevSecOps Noise: Smart Violation Prioritization in Nirmata Control Hub</title><link>https://kubermates.org/docs/2025-06-27-cut-through-the-devsecops-noise-smart-violation-prioritization-in-nirmata-contro/</link><pubDate>Fri, 27 Jun 2025 14:54:55 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-27-cut-through-the-devsecops-noise-smart-violation-prioritization-in-nirmata-contro/</guid><description>When managing security and compliance in Kubernetes, it‚Äôs easy to get overwhelmed. As a platform engineer or administrator, you might look at your cluster or namespace and find hundreds or even thousands of violations. But not all violations are created equal. Some are critical security risks that demand immediate attention, while others are best practices that can be addressed later. The problem? Without clear guidance, most teams end up doing nothing. Not because they don‚Äôt care ‚Äì but because they don‚Äôt know where to start. That‚Äôs exactly the problem we solve with Violation Summarization and Prioritization in Nirmata Control Hub. With Nirmata Control Hub, you don‚Äôt have to dig through pages of findings and all the time spent therein. With a timely glance, you get a summary report for your entire cluster or specific namespaces, showing: Nirmata Control Hub won‚Äôt just tell you what‚Äôs wrong. We prioritize violations into clear buckets: This intelligent triage lets your team focus on what matters most ‚Äì without getting distracted by noise. Every prioritized finding comes with clear remediation suggestions. For example: This built-in guidance significantly reduces the mean time to remediate (MTTR) ‚Äì especially for large or complex environments.</description></item><item><title>June 27, 2025</title><link>https://kubermates.org/releases/2025-06-27-june-27-2025/</link><pubDate>Fri, 27 Jun 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-06-27-june-27-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>AWS managed policy updates</title><link>https://kubermates.org/releases/2025-06-26-aws-managed-policy-updates/</link><pubDate>Thu, 26 Jun 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-06-26-aws-managed-policy-updates/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. An AWS managed policy is a standalone policy that is created and administered by AWS. AWS managed policies are designed to provide permissions for many common use cases so that you can start assigning permissions to users, groups, and roles. Keep in mind that AWS managed policies might not grant least-privilege permissions for your specific use cases because they√¢¬Ä¬ôre available for all AWS customers to use. We recommend that you reduce permissions further by defining customer managed policies that are specific to your use cases. You cannot change the permissions defined in AWS managed policies. If AWS updates the permissions defined in an AWS managed policy, the update affects all principal identities (users, groups, and roles) that the policy is attached to. AWS is most likely to update an AWS managed policy when a new AWS service is launched or new API operations become available for existing services. For more information, see AWS managed policies in the IAM User Guide. You can attach the AmazonEKS_CNI_Policy to your IAM entities. Before you create an Amazon EC2 node group, this policy must be attached to either the node IAM role , or to an IAM role that√¢¬Ä¬ôs used specifically by the Amazon VPC CNI plugin for Kubernetes. This is so that it can perform actions on your behalf.</description></item><item><title>More resilient, flexible networking for the cloud workloads that matter</title><link>https://kubermates.org/docs/2025-06-26-more-resilient-flexible-networking-for-the-cloud-workloads-that-matter/</link><pubDate>Thu, 26 Jun 2025 16:50:42 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-26-more-resilient-flexible-networking-for-the-cloud-workloads-that-matter/</guid><description>By Anantha Ramachandran and Udhay Ravindran Building and scaling cloud infrastructure often means navigating fragile network configurations, unpredictable IP behavior, and rigid networking provider limitations. Whether it√¢¬Ä¬ôs dealing with flaky multi-cloud connections, scrambling to update IP allow-lists after a restart, or hesitating to migrate workloads due to reputation-bound IPs, these challenges add unnecessary risk and complexity to modern deployments. We√¢¬Ä¬ôre excited to announce three new updates designed to simplify cloud networking and enhance deployment flexibility: Partner Network Connect supports high availability and DOKS, general availability of Reserved IPv6 on Droplets, and public preview of Bring Your Own IP (BYOIP). These improvements address real-world infrastructure challenges and are built with simplicity, scalability, and reliability in mind. Multi-cloud architectures are great, until the private links between them become a single point of failure. For teams routing traffic between multiple cloud providers like DigitalOcean, AWS, Microsoft Azure, or Google Cloud Platform, even brief disruptions can stall ML pipelines, gaming platforms, or analytics services. Traditional setups offer limited redundancy, and recovering from outages usually involves manual intervention or rerouting traffic over public networks. High Availability for Partner Network Connect helps solve this by letting you provision redundant links across two separate DigitalOcean gateway routers. Traffic automatically fails over if one link goes down, with no manual steps required and no impact to your customers. With fully managed failover, this update removes operational overhead while helping meet the needs of latency-sensitive or distributed systems. Key benefits include: Automatic failover: Eliminate downtime from single link failures. Reliable for demanding workloads: Especially useful for platforms in streaming, AI, or multiplayer applications.</description></item><item><title>Switching to eBPF One Step at a Time with Calico DNS Inline Policy</title><link>https://kubermates.org/docs/2025-06-25-switching-to-ebpf-one-step-at-a-time-with-calico-dns-inline-policy/</link><pubDate>Wed, 25 Jun 2025 16:05:49 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-25-switching-to-ebpf-one-step-at-a-time-with-calico-dns-inline-policy/</guid><description>Calico Enterprise lets users write network policies using domain names instead of IP addresses. This is done by dynamically mapping domain names to IP addresses and matching the egress traffic against these IPs. We have discussed this feature in detail when we introduced the Inline mode for the eBPF data plane in Calico Enterprise 3. 20 release! It addresses the latency and performance issues of the various modes used by Calico in iptables/nftables data planes. It is a shame that Calico users who are not yet ready to switch completely to eBPF would miss out on this big DNS policy improvement. Don‚Äôt worry! We found a way to port it to iptables to enhance our users‚Äô experience without forcing users to make a huge leap. In Calico Enterprise v3. 21, we have extended the Inline DNS policy mode to iptables. In this mode, DNS policies are updated in real time as DNS responses are parsed by eBPF within the data plane, thus improving the performance. In all the existing modes in the iptables data plane, the DNS response packets are sent to Felix ‚Äì Calico‚Äôs userspace agent. It parses the packets and updates the data plane since advanced packet parsing is not feasible with standard iptables rules. However, iptables has an xt_bpf extension which lets us process and match the packets by an eBPF program the same way we do that in the eBPF data plane! An iptables rule that allows it may look something like this: The iptables rule calls the eBPF DNS parser program on the response packet and updates the data plane inline.</description></item><item><title>June 25, 2025</title><link>https://kubermates.org/releases/2025-06-25-june-25-2025/</link><pubDate>Wed, 25 Jun 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-06-25-june-25-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>Image Compatibility In Cloud Native Environments</title><link>https://kubermates.org/docs/2025-06-25-image-compatibility-in-cloud-native-environments/</link><pubDate>Wed, 25 Jun 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-25-image-compatibility-in-cloud-native-environments/</guid><description>In industries where systems must run very reliably and meet strict performance criteria such as telecommunication, high-performance or AI computing, containerized applications often need specific operating system configuration or hardware presence. It is common practice to require the use of specific versions of the kernel, its configuration, device drivers, or system components. Despite the existence of the Open Container Initiative (OCI) , a governing community to define standards and specifications for container images, there has been a gap in expression of such compatibility requirements. The need to address this issue has led to different proposals and, ultimately, an implementation in Kubernetes&amp;rsquo; Node Feature Discovery (NFD). NFD is an open source Kubernetes project that automatically detects and reports hardware and system features of cluster nodes. This information helps users to schedule workloads on nodes that meet specific system requirements, which is especially useful for applications with strict hardware or operating system dependencies. A container image is built on a base image, which provides a minimal runtime environment, often a stripped-down Linux userland, completely empty or distroless. When an application requires certain features from the host OS, compatibility issues arise. These dependencies can manifest in several ways: While containers in Kubernetes are the most likely unit of abstraction for these needs, the definition of compatibility can extend further to include other container technologies such as Singularity and other OCI artifacts such as binaries from a spack binary cache. Containerized applications are deployed across various Kubernetes distributions and cloud providers, where different host operating systems introduce compatibility challenges. Often those have to be pre-configured before workload deployment or are immutable. For instance, different cloud providers will include different operating systems like: Each OS comes with unique kernel versions, configurations, and drivers, making compatibility a non-trivial issue for applications requiring specific features.</description></item><item><title>4.20.0-okd-scos.ec.5</title><link>https://kubermates.org/releases/2025-06-24-4-20-0-okd-scos-ec-5/</link><pubDate>Tue, 24 Jun 2025 21:02:12 +0000</pubDate><guid>https://kubermates.org/releases/2025-06-24-4-20-0-okd-scos-ec-5/</guid><description>These archives contain the client tooling for OKD on CentOS Stream CoreOS. To verify the contents of this directory, use the &amp;lsquo;gpg&amp;rsquo; and &amp;lsquo;shasum&amp;rsquo; tools to ensure the archives you have downloaded match those published from this location. The openshift-install binary has been preconfigured to install the following release: There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>See More, Worry Less: Managed Database Observability, Monitoring, and Hardening Advancements</title><link>https://kubermates.org/docs/2025-06-24-see-more-worry-less-managed-database-observability-monitoring-and-hardening-adva/</link><pubDate>Tue, 24 Jun 2025 17:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-24-see-more-worry-less-managed-database-observability-monitoring-and-hardening-adva/</guid><description>By Nicole Ghalwash As your applications scale, keeping a close eye on them becomes more important than ever√¢¬Ä¬ìit also becomes harder the more data you have. That√¢¬Ä¬ôs why we√¢¬Ä¬ôre pleased to announce that we have made several new observability, monitoring, and hardening advancements to DigitalOcean Managed Databases√¢¬Ä¬ìto give you better visibility, understanding, and peace of mind. To be exact, we have made three advancements that will improve your Managed Database observability experience here at DigitalOcean. Let√¢¬Ä¬ôs walk through all three of them. DigitalOcean Managed Databases now supports log forwarding to Datadog , enabling seamless integration to your existing Datadog observability dashboard. You can now send, view, and analyze your Managed Database logs directly within Datadog. This feature will enhance your overall observability and monitoring. Key features of this integration include: Direct log forwarding: This feature automatically pushes your database service logs to Datadog√¢¬Ä¬ôs log intake endpoints, eliminating the need to manage a separate log collection agent. By automating log forwarding, you no longer have to worry about deploying, managing, or scaling additional logging agents, which helps to reduce operational complexity. UI-based configuration: The integration is fully configurable through the management console, allowing you to set it up for most database types directly within the log forwarding section. UI-based configuration simplifies the setup process and consolidates your database logs with application and infrastructure telemetry, improving visibility and monitoring√¢¬Ä¬îall without the need for managing separate agents. Customizable log templates: The Datadog log template can be customized per database, enabling specific log details or filtering to suit your needs.</description></item><item><title>June 24, 2025</title><link>https://kubermates.org/releases/2025-06-24-june-24-2025/</link><pubDate>Tue, 24 Jun 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-06-24-june-24-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>4.20.0-okd-scos.ec.4</title><link>https://kubermates.org/releases/2025-06-19-4-20-0-okd-scos-ec-4/</link><pubDate>Thu, 19 Jun 2025 17:52:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-06-19-4-20-0-okd-scos-ec-4/</guid><description>These archives contain the client tooling for OKD on CentOS Stream CoreOS. To verify the contents of this directory, use the &amp;lsquo;gpg&amp;rsquo; and &amp;lsquo;shasum&amp;rsquo; tools to ensure the archives you have downloaded match those published from this location. The openshift-install binary has been preconfigured to install the following release: There was an error while loading. Please reload this page. There was an error while loading. Please reload this page.</description></item><item><title>New Spaces features make it easier to stay secure, compliant, and in control</title><link>https://kubermates.org/docs/2025-06-18-new-spaces-features-make-it-easier-to-stay-secure-compliant-and-in-control/</link><pubDate>Wed, 18 Jun 2025 19:05:05 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-18-new-spaces-features-make-it-easier-to-stay-secure-compliant-and-in-control/</guid><description>By Anantha Ramachandran and Keshav Attrey As teams grow and their cloud storage needs scale, seemingly small issues can snowball into bigger problems. We√¢¬Ä¬ôve seen common pain points emerge, like struggling to track who accessed files through CDN endpoints, or wrestling with tricky configurations when using third-party tools like the AWS CLI or s3cmd, plus missed audit trails, prolonged troubleshooting, or accidental misconfigurations that can lead to data loss. That√¢¬Ä¬ôs why we√¢¬Ä¬ôve made two important updates to help simplify and strengthen the way you manage object storage with DigitalOcean Spaces , our S3-compatible object storage solution, built for simplicity, scalability, and affordability. Whether you√¢¬Ä¬ôre serving up images on a website, storing large media files, managing data backups, or powering developer workflows, Spaces helps you move fast without managing infrastructure. These new features√¢¬Ä¬îthe general availability of Spaces access logs and the introduction of the connection wizard√¢¬Ä¬îare designed to enhance your visibility, minimize risk, and empower you to work with greater confidence. With access logs, you can get detailed, time-stamped records of all file read and write requests, including traffic that flows through both Spaces origin and CDN endpoints. Previously, there was no easy way to monitor who was accessing your files over the CDN. That made it difficult to troubleshoot, detect suspicious behavior, or meet compliance and audit requirements. What√¢¬Ä¬ôs new Logs now capture both direct and CDN traffic, so you have a complete view of object activity. Spaces Origin access logs are compatible with Amazon S3 Server Access Logs format, while Spaces CDN access logs are compatible with Amazon CloudFront log formats, making it easy to integrate with tools you already use. Logs are stored in your existing Spaces buckets, so they fit directly within your existing services and workflows. Benefits of using Spaces access logs If you serve public files such as images, videos, downloadable content especially at scale or over a CDN, Access Logs can help you: Monitor object storage usage trends Detect unusual patterns or spikes Audit access to meet compliance needs Real-world use cases include AdTech and media platforms with high public traffic that need to track downloads and API activity across their CDN endpoints for better performance insights and usage monitoring.</description></item><item><title>June 18, 2025</title><link>https://kubermates.org/releases/2025-06-18-june-18-2025/</link><pubDate>Wed, 18 Jun 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-06-18-june-18-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>Securing Kubernetes Traffic with Calico Ingress Gateway</title><link>https://kubermates.org/docs/2025-06-17-securing-kubernetes-traffic-with-calico-ingress-gateway/</link><pubDate>Tue, 17 Jun 2025 16:26:04 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-17-securing-kubernetes-traffic-with-calico-ingress-gateway/</guid><description>If you‚Äôve managed traffic in Kubernetes, you‚Äôve likely navigated the world of Ingress controllers. For years, Ingress has been the standard way of getting our HTTP/S services exposed. But let‚Äôs be honest, it often felt like a compromise. We wrestled with controller-specific annotations to unlock critical features, blurred the lines between infrastructure and application concerns, and sometimes wished for richer protocol support or a more standardized approach. This ‚Äúpile of vendor annotations,‚Äù while functional, highlighted the limitations of a standard that struggled to keep pace with the complex demands of modern, multi-team environments and even led to security vulnerabilities. Yes, and it‚Äôs a crucial one. The Kubernetes Gateway API isn‚Äôt just an Ingress v2; it‚Äôs a fundamental redesign, the ‚Äúfuture‚Äù of Kubernetes ingress, built by the community to address these very challenges head-on. There are three main points that I came across while evaluating GatewayAPI and Ingress controllers: The Ingress controller landscape is a mishmash of vendors with cool ideas. While they all can route HTTP/S traffic into your cluster, expanding your services to include other protocols puts you at the mercy of that vendor and the capabilities that they implement. On top of that, if you try to migrate from your old Ingress Controller to a new one at some point, there is that sweet conversation of vendor lock-in which ties your hands. If you are wondering how vendor lock-in plays a role here, then take a closer look at your Ingress resources, don‚Äôt they all share some sort of annotation? That ‚Äúpile of vendor annotations,‚Äù while functional, is specific to that one great solution you are currently using, highlighting the limitations of a standard that struggled to keep pace and even led to security vulnerabilities. While Ingress isn‚Äôt disappearing tomorrow, the direction is clear.</description></item><item><title>June 16, 2025</title><link>https://kubermates.org/releases/2025-06-16-june-16-2025/</link><pubDate>Mon, 16 Jun 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-06-16-june-16-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>The Definitive Guide to Microservices</title><link>https://kubermates.org/docs/2025-06-16-the-definitive-guide-to-microservices/</link><pubDate>Mon, 16 Jun 2025 04:23:11 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-16-the-definitive-guide-to-microservices/</guid><description>Microservices are a way to design applications as a collection of small, independent services, each focusing on specific business functions. They ensure scalability, flexibility, and ease of maintenance by allowing individual services to operate and be updated autonomously. In this guide, we will break down what microservices are, how they operate, their advantages, key components, and best practices, offering a comprehensive overview for anyone looking to understand this modern architectural approach. Microservices are an architectural style that organizes an application as a collection of small, independently deployable services. Unlike traditional monolithic architectures, where a single service encapsulates all functionalities, microservices based applications break down applications into individual services, each focusing on a specific business capability. This modularity allows for independent deployment and scaling, making microservice architecture a popular choice for modern applications, particularly when considering internal microservices. One of the key characteristics of microservices is their cloud-native approach. Each service operates independently but is part of a larger application framework, often utilizing cloud resources to ensure scalability and fault tolerance. This approach not only enhances the flexibility of the system but also promotes the use of a diverse technology stack for different services, allowing teams to choose the best tools for each specific function, including individual microservices. Microservices are designed around bounded contexts, where a consistent domain model applies to a specific area of the application. This ensures that responsibilities are well defined, and each service can focus on a single function within the application. Small, cross-functional teams can manage and develop these services independently, fostering agility and rapid development cycles in a microservices architecture.</description></item><item><title>Blog: Changes to Kubernetes Slack</title><link>https://kubermates.org/docs/2025-06-16-blog-changes-to-kubernetes-slack/</link><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-16-blog-changes-to-kubernetes-slack/</guid><description>UPDATE : We‚Äôve received notice from Salesforce that our Slack workspace WILL NOT BE DOWNGRADED on June 20th. Stand by for more details, but for now, there is no urgency to back up private channels or direct messages. Kubernetes Slack will lose its special status and will be changing into a standard free Slack on June 20, 2025. Sometime later this year, our community may move to a new platform. If you are responsible for a channel or private channel, or a member of a User Group, you will need to take some actions as soon as you can. For the last decade, Slack has supported our project with a free customized enterprise account. They have let us know that they can no longer do so, particularly since our Slack is one of the largest and more active ones on the platform. As such, they will be downgrading it to a standard free Slack while we decide on, and implement, other options. On Friday, June 20, we will be subject to the feature limitations of free Slack. The primary ones which will affect us will be only retaining 90 days of history, and having to disable several apps and workflows which we are currently using. The Slack Admin team will do their best to manage these limitations. Responsible channel owners, members of private channels, and members of User Groups should take some actions to prepare for the upgrade and preserve information as soon as possible.</description></item><item><title>Changes to Kubernetes Slack</title><link>https://kubermates.org/docs/2025-06-16-changes-to-kubernetes-slack/</link><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-16-changes-to-kubernetes-slack/</guid><description>UPDATE : We‚Äôve received notice from Salesforce that our Slack workspace WILL NOT BE DOWNGRADED on June 20th. Stand by for more details, but for now, there is no urgency to back up private channels or direct messages. Kubernetes Slack will lose its special status and will be changing into a standard free Slack on June 20, 2025. Sometime later this year, our community may move to a new platform. If you are responsible for a channel or private channel, or a member of a User Group, you will need to take some actions as soon as you can. For the last decade, Slack has supported our project with a free customized enterprise account. They have let us know that they can no longer do so, particularly since our Slack is one of the largest and more active ones on the platform. As such, they will be downgrading it to a standard free Slack while we decide on, and implement, other options. On Friday, June 20, we will be subject to the feature limitations of free Slack. The primary ones which will affect us will be only retaining 90 days of history, and having to disable several apps and workflows which we are currently using. The Slack Admin team will do their best to manage these limitations. Responsible channel owners, members of private channels, and members of User Groups should take some actions to prepare for the upgrade and preserve information as soon as possible.</description></item><item><title>Kubernetes HPA: Mastering Horizontal Pod Autoscaler Basics and Best Practices</title><link>https://kubermates.org/docs/2025-06-15-kubernetes-hpa-mastering-horizontal-pod-autoscaler-basics-and-best-practices/</link><pubDate>Sun, 15 Jun 2025 04:51:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-15-kubernetes-hpa-mastering-horizontal-pod-autoscaler-basics-and-best-practices/</guid><description>Kubernetes Horizontal Pod Autoscaler (HPA) automatically adjusts the number of pod replicas in your Kubernetes cluster based on CPU or memory usage metrics. This capability ensures your application remains responsive and performs well under varying traffic loads. In this article, we will cover the basics of Kubernetes HPA, how it works, best practices, and a hands-on example to help you master this critical Kubernetes feature. At its core, the Kubernetes Horizontal Pod Autoscaler (HPA) is a Kubernetes resource that automatically adjusts the number of pod replicas based on observed CPU and memory usage metrics. This dynamic adjustment ensures your application maintains stable performance even as traffic fluctuates, making it a critical component for production workloads. Horizontal pod autoscaling HPA operates by continuously monitoring specified metrics and making scaling decisions to match the demand. The HPA utilizes resource metrics like CPU and memory, as well as custom metrics and cpu metrics, to determine the appropriate number of replicas needed. For example, if the average CPU utilization exceeds a predefined threshold, HPA will increase the number of replicas to distribute the load. Conversely, when the demand decreases, HPA reduces the number of replicas, optimizing resource usage and reducing costs. Additionally, the custom metrics api can be leveraged to enhance monitoring capabilities. This automatic scaling mechanism helps maintain application performance and reliability without manual intervention and automatically scales. Understanding how the Horizontal Pod Autoscaler (HPA) operates is crucial for leveraging its full potential.</description></item><item><title>kubectl logs: How to Get Pod Logs in Kubernetes (With Examples)</title><link>https://kubermates.org/docs/2025-06-14-kubectl-logs-how-to-get-pod-logs-in-kubernetes-with-examples/</link><pubDate>Sat, 14 Jun 2025 14:09:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-14-kubectl-logs-how-to-get-pod-logs-in-kubernetes-with-examples/</guid><description>Kubernetes is a container orchestration tool used to deploy and manage containerized applications. Like any software, these applications can sometimes fail or not perform as expected due to various reasons. When such failures occur, it‚Äôs important to identify and rectify the issue quickly. One key aspect of troubleshooting involves analyzing the application logs, which can provide valuable information about the root cause of the problem. Logs are essentially records of events happening within your application. By examining these logs, we can often gain insights into what went wrong. In this blog post, we‚Äôll learn how to access Pod logs in Kubernetes using the kubectl logs command. Note that when we say Pod logs, we‚Äôre generally referring to the logs of the applications running in containers inside the Pod. To easily follow along with the examples in this post, we recommend using KodeKloud‚Äôs Kubernetes playground. This playground will provide you instant access to a running Kubernetes cluster with kubectl already installed. No need for you to install any software. With just one click, you&amp;rsquo;ll be ready to run the example code snippets and start experimenting right away.</description></item><item><title>Amazon EKS Auto Mode update to NodeClass</title><link>https://kubermates.org/releases/2025-06-13-amazon-eks-auto-mode-update-to-nodeclass/</link><pubDate>Fri, 13 Jun 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-06-13-amazon-eks-auto-mode-update-to-nodeclass/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Amazon EKS Node Classes are templates that offer granular control over the configuration of your EKS Auto Mode managed nodes. A Node Class defines infrastructure-level settings that apply to groups of nodes in your EKS cluster, including network configuration, storage settings, and resource tagging. This topic explains how to create and configure a Node Class to meet your specific operational requirements. When you need to customize how EKS Auto Mode provisions and configures EC2 instances beyond the default settings, creating a Node Class gives you precise control over critical infrastructure parameters. For example, you can specify private subnet placement for enhanced security, configure instance ephemeral storage for performance-sensitive workloads, or apply custom tagging for cost allocation. To create a NodeClass , follow these steps: Create a YAML file (for example, nodeclass. yaml ) with your Node Class configuration Apply the configuration to your cluster using kubectl Reference the Node Class in your Node Pool configuration. For more information, see Create a Node Pool for EKS Auto Mode. You need kubectl installed and configured. For more information, see Set up to use Amazon EKS. Here√¢¬Ä¬ôs an example Node Class: This NodeClass increases the amount of ephemeral storage on the node.</description></item><item><title>Introducing AMD Instinct‚Ñ¢ MI300X GPU Droplets</title><link>https://kubermates.org/docs/2025-06-12-introducing-amd-instinct-mi300x-gpu-droplets/</link><pubDate>Thu, 12 Jun 2025 13:30:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-12-introducing-amd-instinct-mi300x-gpu-droplets/</guid><description>By Waverly Swinton GPU Droplets are now DigitalOcean GradientAI GPU Droplets. Learn more about DigitalOcean GradientAI , our suite of AI products. At DigitalOcean, we√¢¬Ä¬ôre committed to giving you even more options to power your AI/ML workloads. Today, we√¢¬Ä¬ôre excited to announce that DigitalOcean customers now have access to AMD Instinct√¢¬Ñ¬¢ MI300X as DigitalOcean GPU Droplets. AMD Instinct√¢¬Ñ¬¢ MI300X GPUs deliver leadership performance for accelerated high-performance computing (HPC) applications and the newly exploding demands of generative AI. Use-cases for these GPUs include large model training, fine-tuning, inference, and HPC. With the AMD ROCm√¢¬Ñ¬¢ software platform, you can develop powerful HPC and AI production-ready systems faster than ever before. AMD Instinct√¢¬Ñ¬¢ MI300X√¢¬Ä¬ôs large memory capacity allows it to hold models with hundreds of billions of parameters entirely in memory, reducing the need for model splitting across multiple GPUs. By combining powerful AMD AI compute engines and DigitalOcean√¢¬Ä¬ôs cloud technologies, we aim to empower the massive community of developers like you to integrate AI into your applications and support your most demanding AI workloads at scale. Memory performance : High memory bandwidth (3. 35 TB/s) and capacity (192 GB of HBM3) to efficiently handle larger models and datasets. Value: Offered at a competitive price point ($1.</description></item><item><title>June 12, 2025</title><link>https://kubermates.org/releases/2025-06-12-june-12-2025/</link><pubDate>Thu, 12 Jun 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-06-12-june-12-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>Choosing the Right GPU Droplet for your AI/ML Workload</title><link>https://kubermates.org/docs/2025-06-11-choosing-the-right-gpu-droplet-for-your-ai-ml-workload/</link><pubDate>Wed, 11 Jun 2025 21:54:17 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-11-choosing-the-right-gpu-droplet-for-your-ai-ml-workload/</guid><description>By Waverly Swinton GPU Droplets are now DigitalOcean GradientAI GPU Droplets. Learn more about DigitalOcean GradientAI , our suite of AI products. Whether you√¢¬Ä¬ôre new to AI and machine learning (ML) or a seasoned expert, looking to train a large language model (LLM) or run cost-effective inference, DigitalOcean has a GPU Droplet for you. We currently offer seven different GPU Droplet types from industry-leading brands - AMD and Nvidia - with more GPU Droplet types to come. Read on to learn more about how to choose the right GPU Droplet for your workload. AMD Instinct√¢¬Ñ¬¢ MI325X Use cases: Large model training, fine-tuning, inference, and HPC Why choose: AMD Instinct√¢¬Ñ¬¢ MI325X√¢¬Ä¬ôs large memory capacity allows it to hold models with hundreds of billions of parameters entirely in memory, reducing the need for model splitting across multiple GPUs. Key benefits: Memory performance: High memory capacity to hold models with hundreds of billions of parameters, reducing the need for model splitting across multiple GPUs Value: Offered at a competitive price point ($1. 69/GPU/hr/contract) for a HPC GPU. Contact us to reserve capacity. Key performance benchmark: With 256 GB of HBM3E memory (vs. MI300X√¢¬Ä¬ôs 192 GB), MI325X can handle significantly larger models and datasets entirely on a single GPU AMD Instinct√¢¬Ñ¬¢ MI300X Use cases: Generative AI LLM training, fine-tuning, inference, and HPC Why choose: AMD Instinct√¢¬Ñ¬¢ MI300X√¢¬Ä¬ôs large memory capacity allows it to hold models with hundreds of billions of parameters entirely in memory, reducing the need for model splitting across multiple GPUs. Key benefits: Memory performance: High memory bandwidth (up to 5.</description></item><item><title>Target secondary and cross-account roles with EKS Pod Identities</title><link>https://kubermates.org/releases/2025-06-11-target-secondary-and-cross-account-roles-with-eks-pod-identities/</link><pubDate>Wed, 11 Jun 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-06-11-target-secondary-and-cross-account-roles-with-eks-pod-identities/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. When running applications on Amazon Elastic Kubernetes Service (Amazon EKS), you might need to access AWS resources that exist in different AWS accounts. This guide shows you how to set up cross account access using EKS Pod Identity, which enables your Kubernetes pods to access other AWS resources using target roles. Before you begin, ensure you have completed the following steps: Set up the Amazon EKS Pod Identity Agent Create an EKS Pod Identity role Pod Identity enables applications in your EKS cluster to access AWS resources across accounts through a process called role chaining. When creating a Pod Identity association, you can provide two IAM roles: an EKS Pod Identity role in the same account as your EKS cluster and a Target IAM Role from the account containing your AWS resources you wish to access, (like S3 buckets or RDS Databases). The EKS Pod Identity role must be in your EKS cluster√¢¬Ä¬ôs account due to IAM PassRole requirements, while the Target IAM Role can be in any AWS account. PassRole enables an AWS entity to delegate role assumption to another service. EKS Pod Identity uses PassRole to connect a role to a Kubernetes service account, requiring both the role and the identity passing it to be in the same AWS account as the EKS cluster. When your application pod needs to access AWS resources, it requests credentials from Pod Identity. Pod Identity then automatically performs two role assumptions in sequence: first assuming the EKS Pod Identity role , then using those credentials to assume the Target IAM Role. This process provides your pod with temporary credentials that have the permissions defined in the target role, allowing secure access to resources in other AWS accounts. Due to caching mechanisms, updates to an IAM role in an existing Pod Identity association may not take effect immediately in the pods running on your EKS cluster.</description></item><item><title>Secure and Scalable Kubernetes for Multi-Cluster Management</title><link>https://kubermates.org/docs/2025-06-10-secure-and-scalable-kubernetes-for-multi-cluster-management/</link><pubDate>Tue, 10 Jun 2025 20:03:12 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-10-secure-and-scalable-kubernetes-for-multi-cluster-management/</guid><description>This story is becoming more and more common in the Kubernetes world. What starts as a manageable cluster or two can quickly balloon into a sprawling, multi-cluster architecture spanning public clouds, private data centers, or a bit of both. And with that growth comes a whole new set of headaches. How do you keep tabs on compliance across wildly different configurations? When a service goes down across multiple clusters, how do you pinpoint the cause amidst the chaos? And what about those hard-to-diagnose latency issues that seem to crop up between regions? The truth is, achieving secure and scalable multi-cluster Kubernetes isn‚Äôt about throwing more tools at the problem. It‚Äôs about having the right tools and adopting the right best practices. This is where a solution like Calico Cluster Mesh shines, offering those essential capabilities for a seamless multi-cluster experience without the complexity or overhead that you expect with traditional service meshes. So, why are so many organizations finding themselves in this multi-cluster maze? Often, it‚Äôs driven by solid business reasons: While these motivations are sound, the challenges that emerge in these multi-cloud environments are remarkably consistent: This is where Calico Cluster Mesh comes into play. It provides seamless connectivity between clusters deployed across different regions, VPCs, or VNETs, allowing you to interact with services as if they were local. Calico handles the networking with a VXLAN overlay, which means less reliance on cloud vendor-specific networking and fewer unnecessary network hops. Securing Kubernetes isn‚Äôt just about drawing lines around your network. It‚Äôs about truly understanding how your workloads interact and then enforcing granular policies, even across distributed clusters. A consistent and scalable policy model is absolutely vital in multi-cluster deployments.</description></item><item><title>K3s vs K8s: What are the Differences &amp; Use Cases</title><link>https://kubermates.org/docs/2025-06-10-k3s-vs-k8s-what-are-the-differences-use-cases/</link><pubDate>Tue, 10 Jun 2025 07:48:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-10-k3s-vs-k8s-what-are-the-differences-use-cases/</guid><description>When it comes to container orchestration, K8s (Kubernetes) has become a de facto standard for managing applications and infrastructure at scale across on-premise data centres and public clouds. But as organizations look to deploy containerized workloads to devices at the edge of their network or for Internet of Things (IoT) applications, the full Kubernetes distribution can be overkill. This is where K3s comes in. Developed by Rancher Labs, K3s is a lightweight Kubernetes distribution designed specifically for resource-constrained edge and IoT environments. In this article, we&amp;rsquo;ll explain the key differences between K3s and the upstream Kubernetes project to help you understand when each makes the most sense for your application architecture and deployment needs. Try the Kubernetes Deployments Lab for free. Kubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications. It allows you to define your application&amp;rsquo;s desired state and ensures that it runs consistently in a cluster of machines. Kubernetes automates tasks such as load balancing, self-healing, and scaling, making it easier to manage and maintain container-based applications. It has become the industry standard for container orchestration, simplifying the management of complex, distributed applications. To learn more about how it works, check out this blog: Kubernetes Architecture Explained: Overview for DevOps Enthusiasts. At its core, K3s shares the same API and fundamental concepts as Kubernetes.</description></item><item><title>June 10, 2025</title><link>https://kubermates.org/releases/2025-06-10-june-10-2025/</link><pubDate>Tue, 10 Jun 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-06-10-june-10-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>Enhancing Kubernetes Event Management with Custom Aggregation</title><link>https://kubermates.org/docs/2025-06-10-enhancing-kubernetes-event-management-with-custom-aggregation/</link><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-10-enhancing-kubernetes-event-management-with-custom-aggregation/</guid><description>Kubernetes Events provide crucial insights into cluster operations, but as clusters grow, managing and analyzing these events becomes increasingly challenging. This blog post explores how to build custom event aggregation systems that help engineering teams better understand cluster behavior and troubleshoot issues more effectively. In a Kubernetes cluster, events are generated for various operations - from pod scheduling and container starts to volume mounts and network configurations. While these events are invaluable for debugging and monitoring, several challenges emerge in production environments: To learn more about Events in Kubernetes, read the Event API reference. Consider a production environment with tens of microservices where the users report intermittent transaction failures: Traditional event aggregation process: Engineers are wasting hours sifting through thousands of standalone events spread across namespaces. By the time they look into it, the older events have long since purged, and correlating pod restarts to node-level issues is practically impossible. With its event aggregation in its custom events: The system groups events across resources, instantly surfacing correlation patterns such as volume mount timeouts before pod restarts. History indicates it occurred during past record traffic spikes, highlighting a storage scalability issue in minutes rather than hours. The beneÔ¨Åt of this approach is that organizations that implement it commonly cut down their troubleshooting time significantly along with increasing the reliability of systems by detecting patterns early. This post explores how to build a custom event aggregation system that addresses these challenges, aligned to Kubernetes best practices. I&amp;rsquo;ve picked the Go programming language for my example. This event aggregation system consists of three main components: Here&amp;rsquo;s a sketch for how to implement the event watcher: The event processor enriches events with additional context and classification: One of the key features you could implement is a way of correlating related Events.</description></item><item><title>Introducing Serverless Inference on the GenAI Platform</title><link>https://kubermates.org/docs/2025-06-09-introducing-serverless-inference-on-the-genai-platform/</link><pubDate>Mon, 09 Jun 2025 17:48:18 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-09-introducing-serverless-inference-on-the-genai-platform/</guid><description>By Grace Morgan DigitalOcean√¢¬Ä¬ôs GenAI Platform is now DigitalOcean Gradient Platform. Learn more about the GA release and features. In order to scale AI applications, developers often end up spending more time wrangling infrastructure, scaling for unpredictable traffic, or juggling multiple model providers than actually building. Don√¢¬Ä¬ôt even get us started on fragmented billing. Serverless inference, now available on the DigitalOcean GenAI Platform , removes all of that complexity. It gives you a fast, low-friction way to integrate powerful models from providers like OpenAI, Anthropic, and Meta, without provisioning infrastructure or managing multiple keys and accounts. Serverless inference is one of the simplest ways to integrate AI models into your application. No infrastructure, no setup, no hassle. Whether you√¢¬Ä¬ôre building a recommendation engine, chatbot, or another AI-powered feature, you get direct access to powerful models through a single API. It√¢¬Ä¬ôs built for simplicity and scalability: nothing to provision, no clusters to manage, and automatic scaling to handle unpredictable workloads. You stay focused on building, while we handle the rest. With the newest feature, you get: It√¢¬Ä¬ôs a low-friction, cost-efficient way to embed AI features into your product, ideal for teams who want full control over the experience and integration.</description></item><item><title>Should I Use Kubernetes?</title><link>https://kubermates.org/docs/2025-06-07-should-i-use-kubernetes/</link><pubDate>Sat, 07 Jun 2025 07:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-07-should-i-use-kubernetes/</guid><description>In the DevOps world, container orchestration is often synonymous with Kubernetes, a platform for deploying and managing container-based workloads in production. Since its debut in 2014, Kubernetes has seen a meteoric rise in its adoption and popularity. Despite being a relatively new technology, it is now used by a vast number of organizations, from small start-ups to large enterprises, so much so that it has become a standard in the industry. Kubernetes benefits from strong community support and boasts a rich ecosystem of tools and extensions. It is continuously being developed and improved, and judging by the current trends, Kubernetes is only going to get stronger in the future. In this blog post, we‚Äôll discuss why you should consider using Kubernetes , scenarios where Kubernetes might not be the ideal solution, and explore some of its alternatives. Let‚Äôs get started! Kubernetes offers several key features that cater to a wide range of application deployment and management needs. Here are the top three features of Kubernetes that make it particularly suitable for modern software development: In recent years, the microservices approach ( where software systems are developed as a collection of small, independent services ) has become increasingly popular in software development. Kubernetes provides many abstractions and APIs that are particularly well-suited to the requirements and characteristics of a microservices architecture. Here&amp;rsquo;s how Kubernetes simplifies the deployment and management of microservices: One of the standout features of Kubernetes is its ability to auto-scale applications in response to fluctuating traffic demands. When production systems experience spikes or drops in traffic, Kubernetes seamlessly steps in to adjust resources. It achieves this through two key scaling strategies: In addition to horizontal and vertical scaling, Kubernetes excels in cloud-based scalability, using a feature known as the Cluster Autoscaler.</description></item><item><title>Amazon EKS AWS Region expansion</title><link>https://kubermates.org/releases/2025-06-06-amazon-eks-aws-region-expansion/</link><pubDate>Fri, 06 Jun 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-06-06-amazon-eks-aws-region-expansion/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. The following table describes some of the major updates and new features for the Amazon EKS User Guide. To receive notifications when this table gets a new entry, you can subscribe to the following URL with an RSS reader: Refresh cluster insights You can now manually refresh cluster insights. August 27, 2025 AWS managed policy updates Added permission to AmazonEKSServiceRolePolicy. This role can attach new access policy AmazonEKSEventPolicy. Restricted permissions for ec2:DeleteLaunchTemplate and ec2:TerminateInstances. August 26, 2025 Cross-service confused deputy prevention Added a topic with an example trust policy that you can apply for Cross-service confused deputy prevention. Amazon EKS accepts the aws:SourceArn and aws:SourceAccount conditions in the trust policy of an EKS cluster role. August 19, 2025 Amazon EKS platform version update This is a new platform version with security fixes and enhancements. This includes new patch versions of Kubernetes 1. 33. 2 , 1.</description></item><item><title>Sharks of DigitalOcean: Ali Munir, Staff Technical Account Manager</title><link>https://kubermates.org/docs/2025-06-06-sharks-of-digitalocean-ali-munir-staff-technical-account-manager/</link><pubDate>Fri, 06 Jun 2025 04:14:12 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-06-sharks-of-digitalocean-ali-munir-staff-technical-account-manager/</guid><description>By Sujatha R Technical Writer Ali Munir, a Staff Technical Account Manager at DigitalOcean, is driven by one core mission: helping customers succeed. From day one, he√¢¬Ä¬ôs embraced a culture rooted in honesty, collaboration, and a deep commitment to putting customers first. DigitalOcean stood out to me in a crowded cloud industry of enterprise giants. Here, we lead with simplicity, transparency, and a genuine commitment to customer experience. I was most impressed with the company√¢¬Ä¬ôs ability to offer powerful technology without overwhelming users with complexity. When I researched DO, I immediately knew it was the right place for me. It√¢¬Ä¬ôs a place where your voice matters, where you√¢¬Ä¬ôre empowered to act, and where collaboration feels seamless. If you have an idea that can make a difference, people here listen, align with you, and help bring it to life. That√¢¬Ä¬ôs powerful. To me, DigitalOcean isn√¢¬Ä¬ôt just a workplace, it√¢¬Ä¬ôs a launchpad for innovation. We√¢¬Ä¬ôre not just building tools; we√¢¬Ä¬ôre helping builders around the world solve real problems and bring meaningful software to life. What I love most is our customer-centric mindset.</description></item><item><title>IPv6 access control for dual-stack public endpoints for new IPv6 clusters</title><link>https://kubermates.org/releases/2025-06-05-ipv6-access-control-for-dual-stack-public-endpoints-for-new-ipv6-clusters/</link><pubDate>Thu, 05 Jun 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-06-05-ipv6-access-control-for-dual-stack-public-endpoints-for-new-ipv6-clusters/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. This topic helps you to enable private access for your Amazon EKS cluster√¢¬Ä¬ôs Kubernetes API server endpoint and limit, or completely disable, public access from the internet. When you create a new cluster, Amazon EKS creates an endpoint for the managed Kubernetes API server that you use to communicate with your cluster (using Kubernetes management tools such as kubectl ). By default, this API server endpoint is public to the internet, and access to the API server is secured using a combination of AWS Identity and Access Management (IAM) and native Kubernetes Role Based Access Control (RBAC). This endpoint is known as the cluster public endpoint. Also there is a cluster private endpoint. For more information about the cluster private endpoint, see the following section Cluster private endpoint. EKS creates a unique dual-stack endpoint in the following format for new IPv6 clusters that are made after October 2024. An IPv6 cluster is a cluster that you select IPv6 in the IP family ( ipFamily ) setting of the cluster. EKS cluster public/private endpoint: eks-cluster. region. api.</description></item><item><title>June 05, 2025</title><link>https://kubermates.org/releases/2025-06-05-june-05-2025/</link><pubDate>Thu, 05 Jun 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-06-05-june-05-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>Introducing Gateway API Inference Extension</title><link>https://kubermates.org/docs/2025-06-05-introducing-gateway-api-inference-extension/</link><pubDate>Thu, 05 Jun 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-05-introducing-gateway-api-inference-extension/</guid><description>Modern generative AI and large language model (LLM) services create unique traffic-routing challenges on Kubernetes. Unlike typical short-lived, stateless web requests, LLM inference sessions are often long-running, resource-intensive, and partially stateful. For example, a single GPU-backed model server may keep multiple inference sessions active and maintain in-memory token caches. Traditional load balancers focused on HTTP path or round-robin lack the specialized capabilities needed for these workloads. They also don‚Äôt account for model identity or request criticality (e. g. , interactive chat vs. batch jobs). Organizations often patch together ad-hoc solutions, but a standardized approach is missing. Gateway API Inference Extension was created to address this gap by building on the existing Gateway API , adding inference-specific routing capabilities while retaining the familiar model of Gateways and HTTPRoutes. By adding an inference extension to your existing gateway, you effectively transform it into an Inference Gateway , enabling you to self-host GenAI/LLMs with a ‚Äúmodel-as-a-service‚Äù mindset. The project‚Äôs goal is to improve and standardize routing to inference workloads across the ecosystem.</description></item><item><title>Is It Time to Migrate? A Practical Look at Kubernetes Ingress vs. Gateway API</title><link>https://kubermates.org/docs/2025-06-04-is-it-time-to-migrate-a-practical-look-at-kubernetes-ingress-vs-gateway-api/</link><pubDate>Wed, 04 Jun 2025 14:10:48 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-04-is-it-time-to-migrate-a-practical-look-at-kubernetes-ingress-vs-gateway-api/</guid><description>If you‚Äôve managed traffic in Kubernetes, you‚Äôve likely navigated the world of Ingress controllers. For years, Ingress has been the standard way of getting HTTP/S services exposed. But let‚Äôs be honest, it often felt like a compromise. We wrestled with controller-specific annotations to unlock critical features, blurred the lines between infrastructure and application concerns, this complexity didn‚Äôt just make portability more difficult, it sometimes led to security vulnerabilities and other complications. As part of Calico Open Source v3. 30 , we have released a free and open source Calico Ingress Gateway that implements a custom built Envoy proxy with the Kubernetes Gateway API standard to help you navigate Ingress complexities with style. This blog post is designed to get you up to speed on why such a change might be the missing link in your environment. The challenge with traditional Ingress wasn‚Äôt a lack of effort, since the landscape is full of innovative solutions. However, the problem was the lack of a unified, expressive, and role-aware standard. Existing ingress controllers were capable, implemented advanced features, however at the same time tied you to a specific project/vendor. This meant: This ‚ÄúIngress rut‚Äù had tangible consequences. Platform teams struggled to enforce security standards and provide a consistent ingress experience.</description></item><item><title>Introducing ATL1: DigitalOcean‚Äôs new AI-optimized data center in Atlanta</title><link>https://kubermates.org/docs/2025-06-03-introducing-atl1-digitalocean-s-new-ai-optimized-data-center-in-atlanta/</link><pubDate>Tue, 03 Jun 2025 12:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-03-introducing-atl1-digitalocean-s-new-ai-optimized-data-center-in-atlanta/</guid><description>By Grace Morgan We√¢¬Ä¬ôre excited to announce the launch of ATL1, DigitalOcean√¢¬Ä¬ôs newest and largest data center, opening in Atlanta. The new data center is purpose-built to deliver high-density GPU infrastructure optimized for the future of AI/ML and is a major step forward in our mission to simplify cloud computing and AI for growing tech companies. Starting now, you will be able to deploy DigitalOcean products, including Premium Droplets, Spaces Object Storage, Load Balancers, Kubernetes clusters, managed databases, and Bare Metal GPUs, from our Atlanta data center, offering you even greater performance, scalability, and flexibility. ATL1 expands DigitalOcean√¢¬Ä¬ôs footprint to 17 data centers across 10 global regions and marks a major investment into supporting the next generation of AI, machine learning, and high-performance applications. ATL1 isn√¢¬Ä¬ôt just another data center√¢¬Ä¬îit√¢¬Ä¬ôs built to power the next generation of AI/ML, providing the infrastructure needed for the world√¢¬Ä¬ôs most advanced workloads. Key highlights about the new data center: Massive capacity Cutting-edge hardware DigitalOcean infrastructure services With ATL1, DigitalOcean is expanding our footprint to better serve developers, startups, and digital native enterprises across the Southern U. S. region. By adding local cloud and AI infrastructure in Atlanta, we√¢¬Ä¬ôre making it easier for customers to access the high-performance compute, storage, and GPU resources they need, with lower latency, greater reliability, and the same simplicity and cost-effectiveness they expect from DigitalOcean. Located at Flexential√¢¬Ä¬ôs Atlanta-Douglasville campus , ATL1 benefits from a high-density, enterprise-grade facility optimized for AI/ML workloads, further enhancing performance, scalability, and security for our customers. Whether you√¢¬Ä¬ôre building AI applications, training machine learning models, scaling your cloud-native apps, or growing your next big idea, ATL1 brings the infrastructure closer to you. Share Read more Read more Read more.</description></item><item><title>Start Sidecar First: How To Avoid Snags</title><link>https://kubermates.org/docs/2025-06-03-start-sidecar-first-how-to-avoid-snags/</link><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-03-start-sidecar-first-how-to-avoid-snags/</guid><description>From the Kubernetes Multicontainer Pods: An Overview blog post you know what their job is, what are the main architectural patterns, and how they are implemented in Kubernetes. The main thing I‚Äôll cover in this article is how to ensure that your sidecar containers start before the main app. It‚Äôs more complicated than you might think! I&amp;rsquo;d just like to remind readers that the v1. 29. 0 release of Kubernetes added native support for sidecar containers , which can now be defined within the. spec. initContainers field, but with restartPolicy: Always. You can see that illustrated in the following example Pod manifest snippet: What are the specifics of defining sidecars with a. spec. initContainers block, rather than as a legacy multi-container pod with multiple. spec. containers ? Well, all.</description></item><item><title>Gateway API v1.3.0: Advancements in Request Mirroring, CORS, Gateway Merging, and Retry Budgets</title><link>https://kubermates.org/docs/2025-06-02-gateway-api-v1-3-0-advancements-in-request-mirroring-cors-gateway-merging-and-re/</link><pubDate>Mon, 02 Jun 2025 09:00:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-06-02-gateway-api-v1-3-0-advancements-in-request-mirroring-cors-gateway-merging-and-re/</guid><description>Join us in the Kubernetes SIG Network community in celebrating the general availability of Gateway API v1. 3. 0! We are also pleased to announce that there are already a number of conformant implementations to try, made possible by postponing this blog announcement. Version 1. 3. 0 of the API was released about a month ago on April 24, 2025. Gateway API v1. 3. 0 brings a new feature to the Standard channel (Gateway API&amp;rsquo;s GA release channel): percentage-based request mirroring , and introduces three new experimental features: cross-origin resource sharing (CORS) filters, a standardized mechanism for listener and gateway merging, and retry budgets. Also see the full release notes and applaud the v1. 3. 0 release team next time you see them.</description></item><item><title>May 30, 2025</title><link>https://kubermates.org/releases/2025-05-30-may-30-2025/</link><pubDate>Fri, 30 May 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-05-30-may-30-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>Kubernetes version 1.33</title><link>https://kubermates.org/releases/2025-05-29-kubernetes-version-1-33/</link><pubDate>Thu, 29 May 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-05-29-kubernetes-version-1-33/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. This topic gives important changes to be aware of for each Kubernetes version in standard support. When upgrading, carefully review the changes that have occurred between the old and new versions for your cluster. Kubernetes 1. 33 is now available in Amazon EKS. For more information about Kubernetes 1. 33 , see the official release announcement. The Dynamic Resource Allocation beta Kubernetes API is enabled. This beta API improves the experience of scheduling and monitoring workloads that require resources such as GPUs. The beta API is defined by the Kubernetes community, and might change in future versions of Kubernetes. Carefully review Feature stages in the Kubernetes documentation to understand the implications of using beta APIs. AWS is not releasing an EKS-optimized Amazon Linux 2 AMI for Kubernetes 1.</description></item><item><title>New cluster insights for EKS Hybrid Nodes</title><link>https://kubermates.org/releases/2025-05-29-new-cluster-insights-for-eks-hybrid-nodes/</link><pubDate>Thu, 29 May 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-05-29-new-cluster-insights-for-eks-hybrid-nodes/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Amazon EKS cluster insights provide detection of issues and recommendations to resolve them to help you manage your cluster. Every Amazon EKS cluster undergoes automatic, recurring checks against an Amazon EKS curated list of insights. These insight checks are fully managed by Amazon EKS and offer recommendations on how to address any findings. Configuration insights : Identifies misconfigurations in your EKS Hybrid Nodes setup that could impair functionality of your cluster or workloads. Upgrade insights : Identifies issues that could impact your ability to upgrade to new versions of Kubernetes. Frequency : Amazon EKS refreshes cluster insights every 24 hours, or you can manually refresh them to see the latest status. For example, you can manually refresh cluster insights after addressing an issue to see if the issue was resolved. Permissions : Amazon EKS automatically creates a cluster access entry for cluster insights in every EKS cluster. This entry gives EKS permission to view information about your cluster. Amazon EKS uses this information to generate the insights. For more information, see AmazonEKSClusterInsightsPolicy.</description></item><item><title>Leveling Up Policy Enforcement in Kubernetes: A Look at Kyverno 1.14 and CEL</title><link>https://kubermates.org/docs/2025-05-29-leveling-up-policy-enforcement-in-kubernetes-a-look-at-kyverno-1-14-and-cel/</link><pubDate>Thu, 29 May 2025 17:37:06 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-29-leveling-up-policy-enforcement-in-kubernetes-a-look-at-kyverno-1-14-and-cel/</guid><description>Recently, Cloud Native Live featured a session diving into the powerful integration of Common Expression Language (CEL) in Kyverno 1. 14. Hosted by Eric Durmishi and presented by Kyverno maintainers Mariam Fahmy and Charles Edward, the session introduced exciting new capabilities that enhance Kyverno‚Äôs flexibility and consistency as a policy engine for Kubernetes. At its core, Kyverno is a policy engine for Kubernetes. It enables you to define policies for validating, mutating, generating, or verifying images for resources within your cluster. When an API request, like creating a Pod, is sent to the Kubernetes API server, it passes through several stages, including authentication and authorization. Kyverno integrates as an admission webhook. The API request is forwarded to the Kyverno server, which fetches relevant policies, applies them to the resource, and then either accepts or rejects the request based on whether the resource violates the policy. Kyverno also includes components such as the Reports Controller, which generates policy reports to visualize policy results, and a Cleanup Controller, which manages unused resources based on cleanup policies. Traditionally, Kyverno used a single policy type with different rule types nested within it, such as validate, mutate, generate, and verifyImages. Policies could define rules to match or exclude specific resources. For validation, you could use patterns, deny rules, Pod Security subrules, or assertion trees.</description></item><item><title>No More kubectl Commands ‚Äî Just Talk to Your Kubernetes Cluster in Natural Language</title><link>https://kubermates.org/docs/2025-05-28-no-more-kubectl-commands-just-talk-to-your-kubernetes-cluster-in-natural-languag/</link><pubDate>Wed, 28 May 2025 17:52:01 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-28-no-more-kubectl-commands-just-talk-to-your-kubernetes-cluster-in-natural-languag/</guid><description>What if you could just ask: &amp;raquo;&amp;gt; how&amp;rsquo;s my nginx app doing? ‚Ä¶and your terminal figured out the rest? Thanks to kubectl-ai , Kubernetes can now understand natural language. You type what you want in plain English, and it responds by executing real, context-aware kubectl commands ‚Äî without needing to memorize flags, write YAML, or guess namespaces. Let‚Äôs explore how this works in action. Before you can chat with your cluster, make sure you have the basics in place: The fastest way (Linux &amp;amp; macOS): Skip the manual steps and install in one line: curl -sSL https://raw. githubusercontent. com/GoogleCloudPlatform/kubectl-ai/main/install. sh | bash You‚Äôll be ready to talk to your cluster in under a minute. Prefer to do it yourself? Manual install (Linux, macOS, Windows): Already a kubectl power user? Install with Krew (Linux/macOS/Windows): If you use Krew (the plugin manager for kubectl), installation is a breeze: Now, just invoke it as a kubectl plugin: No matter which method you choose, you‚Äôll be ready to type natural language commands and let kubectl-ai handle the rest. No more memorizing flags, no more YAML wrangling‚Äîjust pure, conversational Kubernetes. Ready to see kubectl-ai in action? Now that setup is out of the way, it‚Äôs time to experience the magic of conversational Kubernetes firsthand. Let‚Äôs dive in: Instead of juggling flags and memorizing resource names, just type your question‚Äîkubectl-ai will handle the rest. Note: The following examples are run on a Windows system using the OpenAI GPT-4o model, but kubectl-ai works just as smoothly across platforms and with other LLM providers.</description></item><item><title>May 27, 2025</title><link>https://kubermates.org/releases/2025-05-27-may-27-2025/</link><pubDate>Tue, 27 May 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-05-27-may-27-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>Why we need a unified approach to Kubernetes environments</title><link>https://kubermates.org/docs/2025-05-26-why-we-need-a-unified-approach-to-kubernetes-environments/</link><pubDate>Mon, 26 May 2025 16:46:48 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-26-why-we-need-a-unified-approach-to-kubernetes-environments/</guid><description>Today, organizations struggle managing disparate technologies for their Kubernetes networking and network security needs. Leveraging multiple technologies for networking and security for in-cluster, ingress, egress, and traffic across clusters creates challenges, including operational complexities and increased costs. For example, to manage ingress traffic for Kubernetes clusters, users cobble together multiple solutions from different providers such as ingress controllers or gateways and load balancers for routing traffic, as well as Web Application Firewalls (WAFs) for enhanced security. Despite the challenges it brings, deploying disparate technologies has been a ‚Äúnecessary evil‚Äù for organizations to get all the capabilities needed for holistic Kubernetes networking. Here, we‚Äôll explore challenges this proliferation of tooling introduces, and provide actionable tips for today‚Äôs platform and security teams to overcome these issues. The fragmented approach to networking and network security in Kubernetes leads to challenges and inefficiencies, including: Take managing ingress traffic, and everything that goes with it. It is typical in a Kubernetes environment, that a user might need to manage multiple tools and services, such as cloud provider load balancers, application gateways, and ingress controllers like NGINX or others, to enable traffic flow and security. This can lead to complexity and fragmentation when integrating these components across your cloud infrastructure and Kubernetes clusters. The user is then required to learn about these individual tools, how they work, what their API is, how to manage them, deploy them, and troubleshoot them. And when it comes to troubleshooting, different sources for logging leads to issues identifying the source of an issue‚Äîand, in turn, challenges remediating said issue. Organizations can address these challenges by adopting a unified approach to Kubernetes networking. Deploying a single, unified solution for Kubernetes networking from the application to the networking layer eliminates the need for separate tools to manage ingress, egress, in-cluster, and cross-cluster traffic, significantly simplifying operations and reducing costs without compromising performance or security.</description></item><item><title>Add-on support for Amazon FSx CSI driver</title><link>https://kubermates.org/releases/2025-05-23-add-on-support-for-amazon-fsx-csi-driver/</link><pubDate>Fri, 23 May 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-05-23-add-on-support-for-amazon-fsx-csi-driver/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. The Amazon FSx for Lustre Container Storage Interface (CSI) driver provides a CSI interface that allows Amazon EKS clusters to manage the lifecycle of Amazon FSx for Lustre file systems. For more information, see the Amazon FSx for Lustre User Guide. For details on how to deploy the Amazon FSx for Lustre CSI driver to your Amazon EKS cluster and verify that it works, see Deploy the FSx for Lustre driver. Thanks for letting us know we&amp;rsquo;re doing a good job! If you&amp;rsquo;ve got a moment, please tell us what we did right so we can do more of it. Thanks for letting us know this page needs work. We&amp;rsquo;re sorry we let you down. If you&amp;rsquo;ve got a moment, please tell us how we can make the documentation better.</description></item><item><title>May 23, 2025</title><link>https://kubermates.org/releases/2025-05-23-may-23-2025/</link><pubDate>Fri, 23 May 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-05-23-may-23-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>Edit Prometheus scrapers in the console</title><link>https://kubermates.org/releases/2025-05-22-edit-prometheus-scrapers-in-the-console/</link><pubDate>Thu, 22 May 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-05-22-edit-prometheus-scrapers-in-the-console/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Prometheus is a monitoring and time series database that scrapes endpoints. It provides the ability to query, aggregate, and store collected data. You can also use it for alerting and alert aggregation. This topic explains how to set up Prometheus as either a managed or open source option. Monitoring Amazon EKS control plane metrics is a common use case. Amazon Managed Service for Prometheus is a Prometheus-compatible monitoring and alerting service that makes it easy to monitor containerized applications and infrastructure at scale. It is a fully-managed service that automatically scales the ingestion, storage, querying, and alerting of your metrics. It also integrates with AWS security services to enable fast and secure access to your data. You can use the open-source PromQL query language to query your metrics and alert on them. Also, you can use alert manager in Amazon Managed Service for Prometheus to set up alerting rules for critical alerts. You can then send these critical alerts as notifications to an Amazon SNS topic.</description></item><item><title>May 22, 2025</title><link>https://kubermates.org/releases/2025-05-22-may-22-2025/</link><pubDate>Thu, 22 May 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-05-22-may-22-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>What‚Äôs New in Calico: Spring 2025</title><link>https://kubermates.org/docs/2025-05-21-what-s-new-in-calico-spring-2025/</link><pubDate>Wed, 21 May 2025 21:52:07 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-21-what-s-new-in-calico-spring-2025/</guid><description>Calico provides a unified platform for all your Kubernetes networking, network security, and observability requirements. From ingress/egress management and east-west policy enforcement to multi-cluster connectivity, Calico delivers comprehensive capabilities. It is distribution-agnostic, preventing vendor lock-in and offering a consistent experience across popular Kubernetes distributions and managed services. Calico eliminates silos, providing seamless networking and observability for containers, VMs, and bare metal servers, and extends effortlessly to multi-cluster environments, in the cloud, on-premises, and at the edge. With the recent release of Calico Open Source 3. 30 , we added: And to expand on our robust Calico Open Source 3. 30 offering, we‚Äôre excited to introduce Calico Cloud Free Tier. This new product edition further expands our open source offerings by providing enhanced observability and policy management capabilities to help visualize and troubleshoot workload communication, and simplify network security enforcement and microsegmentation. Already using Calico Open Source 3. 30 or higher? Get started in less than five minutes. All you need to do is connect your Calico Open Source 3. 30 cluster to Calico Cloud Free Tier and see the difference! Calico Cloud Free Tier is designed to expand on the capabilities of Calico Open Source and address the many challenges faced by Kubernetes operators and platform engineers, including: Gain deep insights into workload communication with Calico Dynamic Service Graph Calico Cloud Free Tier directly addresses these challenges by offering enhanced capabilities, including: Calico Policy Board simplifies microsegmentation by making it easier to create, stage, and safely deploy network policies.</description></item><item><title>May 20, 2025</title><link>https://kubermates.org/releases/2025-05-20-may-20-2025/</link><pubDate>Tue, 20 May 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-05-20-may-20-2025/</guid><description>This page includes release notes for all channels and releases. The following table lists the current versions for each release channel. To learn more about the designations in this table, see What versions are available in a channel. For general information on versioning and upgrades, see GKE versioning and support and About GKE cluster upgrades. For information on the current minor versions rollout and support schedule, see the GKE release schedule. To find all the patch versions available in a channel, check available and default versions. This table also lists the Container-Optimized OS version that corresponds to the channel&amp;rsquo;s default patch version. To upgrade a cluster to a specific image version, see Map Container-Optimized OS node image versions to GKE patch versions. For more detailed information about security-related known issues, see the security bulletin page. To view release notes for versions prior to 2020, see the Release notes archive. You can see the latest product updates for all of Google Cloud on the Google Cloud page, browse and filter all release notes in the Google Cloud console , or programmatically access release notes in BigQuery. To get the latest product updates delivered to you, add the URL of this page to your feed reader , or add the feed URL directly.</description></item><item><title>Agentic Cloud: Reinventing the Cloud with AI Agents</title><link>https://kubermates.org/docs/2025-05-19-agentic-cloud-reinventing-the-cloud-with-ai-agents/</link><pubDate>Mon, 19 May 2025 20:22:04 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-19-agentic-cloud-reinventing-the-cloud-with-ai-agents/</guid><description>By Bratin Saha, Chief Product &amp;amp; Technology Officer AI is shaping up to be one of the most transformational technologies of our lifetimes and will change how we live and work just like the PC, the internet, and the smartphone did. The launch of ChatGPT was a seminal moment in the evolution of AI, and since then, AI√¢¬Ä¬ôs adoption has accelerated. According to √¢¬Ä¬ú The rapid adoption of Generative AI. Bick, Blandin, &amp;amp; Deming (2024 )√¢¬Ä¬ù, AI is being adopted at twice the rate of the PC and internet at a similar point in their evolution. Not just adoption, the rate of innovation in AI has been faster than anything we have seen before. So where is AI headed? To answer this question, let us start by looking at how other transformational technologies have evolved throughout history. Take the Internet for example √¢¬Ä¬ì it started with the infrastructure - with routers and network switches. But the true inflection point came with the emergence of platform capabilities, such as web browsers and HTML, which allowed people to build and use applications like search engines, e-commerce, and social media. If you look at the PC, microprocessors and memory and other infrastructure components came first. But what triggered the massive adoption was the creation of a platform √¢¬Ä¬ì WinTel, which provided a standardized environment for developing and running PC applications. Same with the smartphones; the infrastructure started first with modems and cell towers, but it really took off when a platform enabled the creation and use of applications. AI will follow a similar IPA ( I nfrastructure -&amp;gt; P latform -&amp;gt; A pplication) model, with the innovation moving from Infrastructure (e.</description></item><item><title>Kubernetes v1.33: In-Place Pod Resize Graduated to Beta</title><link>https://kubermates.org/docs/2025-05-16-kubernetes-v1-33-in-place-pod-resize-graduated-to-beta/</link><pubDate>Fri, 16 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-16-kubernetes-v1-33-in-place-pod-resize-graduated-to-beta/</guid><description>On behalf of the Kubernetes project, I am excited to announce that the in-place Pod resize feature (also known as In-Place Pod Vertical Scaling), first introduced as alpha in Kubernetes v1. 27, has graduated to Beta and will be enabled by default in the Kubernetes v1. 33 release! This marks a significant milestone in making resource management for Kubernetes workloads more flexible and less disruptive. Traditionally, changing the CPU or memory resources allocated to a container required restarting the Pod. While acceptable for many stateless applications, this could be disruptive for stateful services, batch jobs, or any workloads sensitive to restarts. In-place Pod resizing allows you to change the CPU and memory requests and limits assigned to containers within a running Pod, often without requiring a container restart. Here&amp;rsquo;s the core idea: You can try it out on a v1. 33 Kubernetes cluster by using kubectl to edit a Pod (requires kubectl v1. 32+): For detailed usage instructions and examples, please refer to the official Kubernetes documentation: Resize CPU and Memory Resources assigned to Containers. Kubernetes still excels at scaling workloads horizontally (adding or removing replicas), but in-place Pod resizing unlocks several key benefits for vertical scaling: Since the alpha release in v1. 27, significant work has gone into maturing the feature, improving its stability, and refining the user experience based on feedback and further development. Here are the key changes: Graduating to Beta means the feature is ready for broader adoption, but development doesn&amp;rsquo;t stop here! Here&amp;rsquo;s what the community is focusing on next: With the InPlacePodVerticalScaling feature gate enabled by default in v1.</description></item><item><title>May 16, 2025</title><link>https://kubermates.org/releases/2025-05-16-may-16-2025/</link><pubDate>Fri, 16 May 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-05-16-may-16-2025/</guid><description>&lt;p&gt;Open the original post ‚Üó &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#May_16_2025"&gt;https://cloud.google.com/kubernetes-engine/docs/release-notes#May_16_2025&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Announcing etcd v3.6.0</title><link>https://kubermates.org/docs/2025-05-15-announcing-etcd-v3-6-0/</link><pubDate>Thu, 15 May 2025 16:00:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-15-announcing-etcd-v3-6-0/</guid><description>This announcement originally appeared on the etcd blog. Today, we are releasing etcd v3. 6. 0 , the first minor release since etcd v3. 5. 0 on June 15, 2021. This release introduces several new features, makes significant progress on long-standing efforts like downgrade support and migration to v3store, and addresses numerous critical &amp;amp; major issues. It also includes major optimizations in memory usage, improving efficiency and performance. In addition to the features of v3. 6. 0, etcd has joined Kubernetes as a SIG (sig-etcd), enabling us to improve project sustainability. We&amp;rsquo;ve introduced systematic robustness testing to ensure correctness and reliability.</description></item><item><title>Kubernetes 1.33: Job's SuccessPolicy Goes GA</title><link>https://kubermates.org/docs/2025-05-15-kubernetes-1-33-job-s-successpolicy-goes-ga/</link><pubDate>Thu, 15 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-15-kubernetes-1-33-job-s-successpolicy-goes-ga/</guid><description>On behalf of the Kubernetes project, I&amp;rsquo;m pleased to announce that Job success policy has graduated to General Availability (GA) as part of the v1. 33 release. In batch workloads, you might want to use leader-follower patterns like MPI , in which the leader controls the execution, including the followers&amp;rsquo; lifecycle. In this case, you might want to mark it as succeeded even if some of the indexes failed. Unfortunately, a leader-follower Kubernetes Job that didn&amp;rsquo;t use a success policy, in most cases, would have to require all Pods to finish successfully for that Job to reach an overall succeeded state. For Kubernetes Jobs, the API allows you to specify the early exit criteria using the. spec. successPolicy field (you can only use the. spec. successPolicy field for an indexed Job ). Which describes a set of rules either using a list of succeeded indexes for a job, or defining a minimal required size of succeeded indexes. This newly stable field is especially valuable for scientific simulation, AI/ML and High-Performance Computing (HPC) batch workloads.</description></item><item><title>Kubernetes v1.33: Updates to Container Lifecycle</title><link>https://kubermates.org/docs/2025-05-14-kubernetes-v1-33-updates-to-container-lifecycle/</link><pubDate>Wed, 14 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-14-kubernetes-v1-33-updates-to-container-lifecycle/</guid><description>Kubernetes v1. 33 introduces a few updates to the lifecycle of containers. The Sleep action for container lifecycle hooks now supports a zero sleep duration (feature enabled by default). There is also alpha support for customizing the stop signal sent to containers when they are being terminated. This blog post goes into the details of these new aspects of the container lifecycle, and how you can use them. Kubernetes v1. 29 introduced the Sleep action for container PreStop and PostStart Lifecycle hooks. The Sleep action lets your containers pause for a specified duration after the container is started or before it is terminated. This was needed to provide a straightforward way to manage graceful shutdowns. Before the Sleep action, folks used to run the sleep command using the exec action in their container lifecycle hooks. If you wanted to do this you&amp;rsquo;d need to have the binary for the sleep command in your container image. This is difficult if you&amp;rsquo;re using third party images.</description></item><item><title>Automate Policy Violation Tracking with Jira in Nirmata Control Hub</title><link>https://kubermates.org/docs/2025-05-14-automate-policy-violation-tracking-with-jira-in-nirmata-control-hub/</link><pubDate>Wed, 14 May 2025 10:08:17 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-14-automate-policy-violation-tracking-with-jira-in-nirmata-control-hub/</guid><description>Keeping track of Kubernetes policy violations across namespaces and clusters can be tedious, especially when teams rely on disconnected tools and manual workflows. With Nirmata Control Hub‚Äôs Jira integration, you can automatically or manually create Jira tickets for policy violations directly from the platform. This ensures accountability, reduces MTTR (mean time to resolution), and improves team productivity by embedding policy compliance into your teams‚Äô tools. Every misconfiguration or policy violation is a potential security or compliance risk. However, identifying issues is only the first step. The real value is in closing the loop‚Äîfrom detection to resolution. Traditionally, teams rely on Slack alerts, dashboards, or Excel exports to track violations. But these quickly fall out of sync with what‚Äôs happening in the cluster or Git. Hira is where real work happens, and that‚Äôs where Nirmata Control Hub (NCH) now meets your team. From any Policy Reports page, users can instantly generate a Jira issue for violations. Simply click ‚ÄúCreate Jira Issue‚Äù and fill in the ticket details, such as project, title, issue type, priority, and assignee. This creates traceability right at the source of the ossie, whether it;s a misconfigured deployment, a missing securityContext, or a policy deviation.</description></item><item><title>Why Policy as Code is a Game Changer for Platform Engineers</title><link>https://kubermates.org/docs/2025-05-13-why-policy-as-code-is-a-game-changer-for-platform-engineers/</link><pubDate>Tue, 13 May 2025 20:07:21 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-13-why-policy-as-code-is-a-game-changer-for-platform-engineers/</guid><description>Platform engineers, let‚Äôs talk about a fundamental shift that‚Äôs revolutionizing how we build and manage internal developer platforms: Policy as Code (PaC). This isn‚Äôt just another buzzword; it‚Äôs the key to creating scalable, secure, and efficient operations that empower developers. Let‚Äôs dive in! Platform engineers, let‚Äôs talk about a fundamental shift that‚Äôs revolutionizing how we build and manage internal developer platforms: Policy as Code (PaC). This isn‚Äôt just another buzzword; it‚Äôs the key to creating scalable, secure, and efficient operations that empower developers. Let‚Äôs dive in! Policy as Code drastically improves developer experience. By implementing policy checks early in the development process (‚Äúshifting left‚Äù), developers get instant feedback on potential issues. This prevents problems from reaching production and reduces mental load, allowing them to focus on innovation, not compliance headaches. Platform engineering is about giving developers self-service capabilities. Infrastructure as Code (IaC) provides automation, but Policy as Code (PaC) provides the critical guardrails. Developers can provision resources and deploy applications within defined ‚Äúgolden paths,‚Äù ensuring agility without compromising security or compliance. Policies written as code are stored in version control, creating a clear history of changes. This auditability is essential for troubleshooting and demonstrating compliance.</description></item><item><title>Kubernetes v1.33: Job's Backoff Limit Per Index Goes GA</title><link>https://kubermates.org/docs/2025-05-13-kubernetes-v1-33-job-s-backoff-limit-per-index-goes-ga/</link><pubDate>Tue, 13 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-13-kubernetes-v1-33-job-s-backoff-limit-per-index-goes-ga/</guid><description>In Kubernetes v1. 33, the Backoff Limit Per Index feature reaches general availability (GA). This blog describes the Backoff Limit Per Index feature and its benefits. When you run workloads on Kubernetes, you must consider scenarios where Pod failures can affect the completion of your workloads. Ideally, your workload should tolerate transient failures and continue running. To achieve failure tolerance in a Kubernetes Job, you can set the spec. backoffLimit field. This field specifies the total number of tolerated failures. However, for workloads where every index is considered independent, like embarassingly parallel workloads - the spec. backoffLimit field is often not flexible enough. For example, you may choose to run multiple suites of integration tests by representing each suite as an index within an Indexed Job. In that setup, a fast-failing index (test suite) is likely to consume your entire budget for tolerating Pod failures, and you might not be able to run the other indexes.</description></item><item><title>May 13, 2025</title><link>https://kubermates.org/releases/2025-05-13-may-13-2025/</link><pubDate>Tue, 13 May 2025 00:00:00 -0700</pubDate><guid>https://kubermates.org/releases/2025-05-13-may-13-2025/</guid><description>&lt;p&gt;Open the original post ‚Üó &lt;a href="https://cloud.google.com/kubernetes-engine/docs/release-notes#May_13_2025"&gt;https://cloud.google.com/kubernetes-engine/docs/release-notes#May_13_2025&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Kubernetes v1.33: Image Pull Policy the way you always thought it worked!</title><link>https://kubermates.org/docs/2025-05-12-kubernetes-v1-33-image-pull-policy-the-way-you-always-thought-it-worked/</link><pubDate>Mon, 12 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-12-kubernetes-v1-33-image-pull-policy-the-way-you-always-thought-it-worked/</guid><description>Some things in Kubernetes are surprising, and the way imagePullPolicy behaves might be one of them. Given Kubernetes is all about running pods, it may be peculiar to learn that there has been a caveat to restricting pod access to authenticated images for over 10 years in the form of issue 18787 ! It is an exciting release when you can resolve a ten-year-old issue. The gist of the problem is that the imagePullPolicy: IfNotPresent strategy has done precisely what it says, and nothing more. Let&amp;rsquo;s set up a scenario. To begin, Pod A in Namespace X is scheduled to Node 1 and requires image Foo from a private repository. For it&amp;rsquo;s image pull authentication material, the pod references Secret 1 in its imagePullSecrets. Secret 1 contains the necessary credentials to pull from the private repository. The Kubelet will utilize the credentials from Secret 1 as supplied by Pod A and it will pull container image Foo from the registry. This is the intended (and secure) behavior. But now things get curious. If Pod B in Namespace Y happens to also be scheduled to Node 1 , unexpected (and potentially insecure) things happen. Pod B may reference the same private image, specifying the IfNotPresent image pull policy.</description></item><item><title>How to optimize your cloud architecture for business growth</title><link>https://kubermates.org/docs/2025-05-09-how-to-optimize-your-cloud-architecture-for-business-growth/</link><pubDate>Fri, 09 May 2025 23:25:48 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-09-how-to-optimize-your-cloud-architecture-for-business-growth/</guid><description>By Anantha Ramachandran Principal Product Marketing Manager Your cloud setup is like a ship on the ocean. To ensure a safe journey, it needs regular maintenance and care. As your business grows and your tech stack becomes more complex, it√¢¬Ä¬ôs easy for small inefficiencies to creep into your cloud environment which can drive up costs, waste resources, and make it harder to effectively scale. DigitalOcean is proud to offer hands-on support to our customers to help them optimize their cloud setup, which includes free architecture reviews with expert solutions engineers, and tips to avoid inefficiencies in your cloud setup. If you√¢¬Ä¬ôre a DigitalOcean customer, you can request your own architecture review now. To explore actionable insights on your own, watch our recent Sail to Success webinar, where we shared practical tips on improving your cloud architecture, including ways to boost performance, enhance security, and save on costs. Regular cloud architecture reviews are beneficial to businesses of all sizes. Here are a few reasons we recommend examining your cloud setup regularly: They support business growth: As your business evolves, your cloud infrastructure must evolve with it. A cloud architecture review helps you align your current environment with your business goals, helping to ensure your cloud is positioned to support where you are today, where you√¢¬Ä¬ôre heading, and the outcomes you need to achieve. They improve performance and security: Regular reviews can find hidden problems that can slow you down or make you less secure. Downtime isn√¢¬Ä¬ôt just inconvenient, it√¢¬Ä¬ôs costly. A strong, resilient cloud architecture helps you avoid downtime and keeps your business running smoothly, no matter what comes your way.</description></item><item><title>Kubernetes v1.33: Streaming List responses</title><link>https://kubermates.org/docs/2025-05-09-kubernetes-v1-33-streaming-list-responses/</link><pubDate>Fri, 09 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-09-kubernetes-v1-33-streaming-list-responses/</guid><description>Managing Kubernetes cluster stability becomes increasingly critical as your infrastructure grows. One of the most challenging aspects of operating large-scale clusters has been handling List requests that fetch substantial datasets - a common operation that could unexpectedly impact your cluster&amp;rsquo;s stability. Today, the Kubernetes community is excited to announce a significant architectural improvement: streaming encoding for List responses. Current API response encoders just serialize an entire response into a single contiguous memory and perform one ResponseWriter. Write call to transmit data to the client. Despite HTTP/2&amp;rsquo;s capability to split responses into smaller frames for transmission, the underlying HTTP server continues to hold the complete response data as a single buffer. Even as individual frames are transmitted to the client, the memory associated with these frames cannot be freed incrementally. When cluster size grows, the single response body can be substantial - like hundreds of megabytes in size. At large scale, the current approach becomes particularly inefficient, as it prevents incremental memory release during transmission. Imagining that when network congestion occurs, that large response body‚Äôs memory block stays active for tens of seconds or even minutes. This limitation leads to unnecessarily high and prolonged memory consumption in the kube-apiserver process. If multiple large List requests occur simultaneously, the cumulative memory consumption can escalate rapidly, potentially leading to an Out-of-Memory (OOM) situation that compromises cluster stability.</description></item><item><title>Kubernetes 1.33: Volume Populators Graduate to GA</title><link>https://kubermates.org/docs/2025-05-08-kubernetes-1-33-volume-populators-graduate-to-ga/</link><pubDate>Thu, 08 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-08-kubernetes-1-33-volume-populators-graduate-to-ga/</guid><description>Kubernetes volume populators are now generally available (GA)! The AnyVolumeDataSource feature gate is treated as always enabled for Kubernetes v1. 33, which means that users can specify any appropriate custom resource as the data source of a PersistentVolumeClaim (PVC). An example of how to use dataSourceRef in PVC: There are four major enhancements from beta. During the beta phase, contributors to Kubernetes identified potential resource leaks with PersistentVolumeClaim (PVC) deletion while volume population was in progress; these leaks happened due to limitations in finalizer handling. Ahead of the graduation to general availability, the Kubernetes project added support to delete temporary resources (PVC prime, etc. ) if the original PVC is deleted. To accommodate this, we&amp;rsquo;ve introduced three new plugin-based functions: A provider example is added in lib-volume-populator/example. For GA, the CSI volume populator controller code gained a MutatorConfig , allowing the specification of mutator functions to modify Kubernetes resources. For example, if the PVC prime is not an exact copy of the PVC and you need provider-specific information for the driver, you can include this information in the optional MutatorConfig. This allows you to customize the Kubernetes objects in the volume populator. Our beta phase highlighted a new requirement: the need to aggregate metrics not just from lib-volume-populator, but also from other components within the provider&amp;rsquo;s codebase. To address this, SIG Storage introduced a provider metric manager.</description></item><item><title>Expanding our GPU Droplet portfolio - NVIDIA RTX 4000 Ada Generation, NVIDIA RTX 6000 Ada Generation, and NVIDIA L40S</title><link>https://kubermates.org/docs/2025-05-08-expanding-our-gpu-droplet-portfolio-nvidia-rtx-4000-ada-generation-nvidia-rtx-60/</link><pubDate>Thu, 08 May 2025 08:05:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-08-expanding-our-gpu-droplet-portfolio-nvidia-rtx-4000-ada-generation-nvidia-rtx-60/</guid><description>By Waverly Swinton GPU Droplets are now DigitalOcean GradientAI GPU Droplets. Learn more about DigitalOcean GradientAI , our suite of AI products. At DigitalOcean, we√¢¬Ä¬ôre committed to bringing you the latest technology to power your AI projects. We√¢¬Ä¬ôre excited to announce that NVIDIA RTX 4000 Ada Generation, NVIDIA RTX 6000 Ada Generation, and NVIDIA L40S GPUs are now available as DigitalOcean GPU Droplets. NVIDIA√¢¬Ä¬ôs RTX 4000 Ada Generation, 6000 Ada Generation, and L40S deliver on AI inference, training, and graphical workloads. These GPU Droplets are available in budget-friendly, single-node configurations. Read on for more on how DigitalOcean and NVIDIA can help make your AI journey simple, scalable, and cost-effective. What are NVIDIA RTX 4000 Ada Generation, 6000 Ada Generation and L40S? These Nvidia GPU Droplets combine powerful AI compute with best-in-class graphics and media acceleration and deliver end-to-end acceleration for the next generation of AI-enabled applications√¢¬Ä¬îfrom generative AI, large-language models (LLM) inference, small-model training and fine-tuning to 3D graphics, rendering, and video applications√¢¬Ä¬¶ Key features of these newly introduced machines include: NVIDIA RTX 4000 Ada Generation GPU is a powerful single-slot GPU. Key use cases include content creation, 3D modeling, rendering, video, and inference workflows with exceptional performance and efficiency. NVIDIA RTX 6000 Ada Generation GPU is built on the NVIDIA Ada Lovelace GPU architecture. RTX 6000 Ada Generation combines third-generation RT Cores, fourth-generation Tensor Cores, and Ada generation CUDA cores with 48GB of graphics memory. Use cases include rendering, virtual workstations, AI, graphics, and compute performance.</description></item><item><title>Powered by DigitalOcean Hatch: Ontra Mobility is Building Smarter Cities</title><link>https://kubermates.org/docs/2025-05-07-powered-by-digitalocean-hatch-ontra-mobility-is-building-smarter-cities/</link><pubDate>Wed, 07 May 2025 21:30:29 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-07-powered-by-digitalocean-hatch-ontra-mobility-is-building-smarter-cities/</guid><description>By Martin Nguyen Hatch is DigitalOcean√¢¬Ä¬ôs global program for startups, which provides selected growing technology companies with credits and discounts on computing resources so they can build and scale with less worry about costs. By keeping infrastructure and cost management simple, the Hatch program helps enable startups to build solutions that move society forward. Ontra Mobility , a Y Combinator startup, transforms communities by developing data-driven tools that help cities and transit agencies plan and operate more efficient, equitable, and sustainable transportation systems, focusing on optimizing networks and improving first-and-last mile access through on-demand ridesharing. We sat down with CTO Connor Riley, who chose DigitalOcean for its simple, managed infrastructure so they can focus on making public transit more accessible. The Hatch program provided credits and resources that helped support their mission. During their PhD programs at Georgia Institute of Technology, Riley and his co-founder Anthony Trasatti worked with their academic advisor, Pascal Van Hentenryck, to optimize public transit, which serves as a vital lifeline for many. Working closely with MARTA in Atlanta, GA, they developed techniques for network design optimization, special events scheduling, and dispatching algorithms for on-demand ridesharing. These projects powered on-demand ridesharing pilots MARTA Reach and CAT SMART in Savannah, Georgia. Getting adoption for a product that serves public agencies is as much of a battle as building the product itself. Since many residents rely on public transportation, establishing trust with transit agencies is essential before they√¢¬Ä¬ôll risk adopting new products that could impact municipal residents. Despite these hurdles, they√¢¬Ä¬ôre undeterred: they√¢¬Ä¬ôve already gained traction with a number of customers, including conducting a microtransit planning project in Washington D. C.</description></item><item><title>Kubernetes v1.33: From Secrets to Service Accounts: Kubernetes Image Pulls Evolved</title><link>https://kubermates.org/docs/2025-05-07-kubernetes-v1-33-from-secrets-to-service-accounts-kubernetes-image-pulls-evolved/</link><pubDate>Wed, 07 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-07-kubernetes-v1-33-from-secrets-to-service-accounts-kubernetes-image-pulls-evolved/</guid><description>Kubernetes has steadily evolved to reduce reliance on long-lived credentials stored in the API. A prime example of this shift is the transition of Kubernetes Service Account (KSA) tokens from long-lived, static tokens to ephemeral, automatically rotated tokens with OpenID Connect (OIDC)-compliant semantics. This advancement enables workloads to securely authenticate with external services without needing persistent secrets. However, one major gap remains: image pull authentication. Today, Kubernetes clusters rely on image pull secrets stored in the API, which are long-lived and difficult to rotate, or on node-level kubelet credential providers, which allow any pod running on a node to access the same credentials. This presents security and operational challenges. To address this, Kubernetes is introducing Service Account Token Integration for Kubelet Credential Providers , now available in alpha. This enhancement allows credential providers to use pod-specific service account tokens to obtain registry credentials, which kubelet can then use for image pulls ‚Äî eliminating the need for long-lived image pull secrets. Currently, Kubernetes administrators have two primary options for handling private container image pulls: Image pull secrets stored in the Kubernetes API Kubelet credential providers Neither approach aligns with the principles of least privilege or ephemeral authentication , leaving Kubernetes with a security gap. This new enhancement enables kubelet credential providers to use workload identity when fetching image registry credentials. Instead of relying on long-lived secrets, credential providers can use service account tokens to request short-lived credentials tied to a specific pod‚Äôs identity. This approach provides: Kubelet generates short-lived, automatically rotated tokens for service accounts if the credential provider it communicates with has opted into receiving a service account token for image pulls.</description></item><item><title>Do You Need a Degree to Be a DevOps Engineer? A 2025 Guide</title><link>https://kubermates.org/docs/2025-05-07-do-you-need-a-degree-to-be-a-devops-engineer-a-2025-guide/</link><pubDate>Wed, 07 May 2025 17:41:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-07-do-you-need-a-degree-to-be-a-devops-engineer-a-2025-guide/</guid><description>When you think of a DevOps Engineer, you might picture someone with a shiny Computer Science degree from a top-tier university. That was the old rulebook. In today‚Äôs cloud-powered, open-source-fueled, community-driven tech world‚Ä¶ the rules have changed. üëâ The truth? A degree is nice to have, but it‚Äôs NOT a mandatory ticket to join the DevOps revolution. üëâ The better question: ‚ÄúDo you have the skills and mindset of a DevOps engineer?‚Äù Top companies (even tech giants) now hire for skills, not just degrees. They want someone who can: In short: They want a problem-solver. A degree shows you can learn. Hands-on skills show you can do. The second matters way more in DevOps. A degree CAN help you (but it‚Äôs optional). Most common degrees: A degree may: üëâ But it will NOT teach you DevOps ‚Äúout-of-the-box‚Äù. You will still need to build those skills yourself.</description></item><item><title>Introducing Role-Based Access Control to DigitalOcean Managed MongoDB with Predefined Roles</title><link>https://kubermates.org/docs/2025-05-07-introducing-role-based-access-control-to-digitalocean-managed-mongodb-with-prede/</link><pubDate>Wed, 07 May 2025 16:46:32 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-07-introducing-role-based-access-control-to-digitalocean-managed-mongodb-with-prede/</guid><description>By Nicole Ghalwash We are excited to announce that role-based access control (RBAC) is now available for DigitalOcean Managed MongoDB, starting with predefined roles! These new roles include the pre-defined roles of √¢¬Ä¬ú readOnly √¢¬Ä¬ù, √¢¬Ä¬ú readWrite √¢¬Ä¬ù and √¢¬Ä¬ú dbAdmin √¢¬Ä¬ù. Learn more about it here. This update brings greater security, efficiency, and compliance to your MongoDB clusters. With predefined roles, you can now easily manage access control, reduce security risks, and streamline database administration across your MongoDB clusters on DigitalOcean. Managing database access can be a challenge, especially as teams grow and security requirements become more complex. Without a structured approach, organizations risk unauthorized access, operational inefficiencies, and compliance gaps. With RBAC now applicable to your MongoDB environment, you can enforce clear, predefined access policies√¢¬Ä¬îhelping to ensure secure, efficient, and scalable database management. Here√¢¬Ä¬ôs how RBAC can benefit your business: Enhanced security: RBAC protects against unauthorized access by only allowing verified users to interact with sensitive database resources. This reduces the risk of data breaches and strengthens overall security posture. Operational efficiency: With predefined roles, administrators can streamline user provisioning and de-provisioning, minimizing the manual workload and reducing errors. Improved compliance: RBAC helps organizations meet industry standards and regulatory requirements by enforcing structured access controls, making audits and reporting more straightforward. Cost savings: By automating user access management, RBAC reduces administrative overhead and lowers the risk of security breaches, which can be costly to remediate.</description></item><item><title>Kubernetes v1.33: Fine-grained SupplementalGroups Control Graduates to Beta</title><link>https://kubermates.org/docs/2025-05-06-kubernetes-v1-33-fine-grained-supplementalgroups-control-graduates-to-beta/</link><pubDate>Tue, 06 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-06-kubernetes-v1-33-fine-grained-supplementalgroups-control-graduates-to-beta/</guid><description>The new field, supplementalGroupsPolicy , was introduced as an opt-in alpha feature for Kubernetes v1. 31 and has graduated to beta in v1. 33; the corresponding feature gate ( SupplementalGroupsPolicy ) is now enabled by default. This feature enables to implement more precise control over supplemental groups in containers that can strengthen the security posture, particularly in accessing volumes. Moreover, it also enhances the transparency of UID/GID details in containers, offering improved security oversight. Please be aware that this beta release contains some behavioral breaking change. See The Behavioral Changes Introduced In Beta and Upgrade Considerations sections for details. Although the majority of Kubernetes cluster admins/users may not be aware, kubernetes, by default, merges group information from the Pod with information defined in /etc/group in the container image. Let&amp;rsquo;s see an example, below Pod manifest specifies runAsUser=1000 , runAsGroup=3000 and supplementalGroups=4000 in the Pod&amp;rsquo;s security context. What is the result of id command in the ctr container? The output should be similar to this: Where does group ID 50000 in supplementary groups ( groups field) come from, even though 50000 is not defined in the Pod&amp;rsquo;s manifest at all? The answer is /etc/group file in the container image. Checking the contents of /etc/group in the container image should show below: This shows that the container&amp;rsquo;s primary user 1000 belongs to the group 50000 in the last entry. Thus, the group membership defined in /etc/group in the container image for the container&amp;rsquo;s primary user is implicitly merged to the information from the Pod.</description></item><item><title>Nirmata‚Äôs AI-Powered Remediations: A Smarter Way to Fix Policy Violations</title><link>https://kubermates.org/docs/2025-05-06-nirmata-s-ai-powered-remediations-a-smarter-way-to-fix-policy-violations/</link><pubDate>Tue, 06 May 2025 18:25:24 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-06-nirmata-s-ai-powered-remediations-a-smarter-way-to-fix-policy-violations/</guid><description>Every modern enterprise strives for faster software delivery without compromising on security and compliance. As cloud-native environments grow in scale and complexity, so does the burden of identifying and fixing misconfigurations across clusters, pipelines, and cloud infrastructure. Today, we‚Äôre excited to announce a major leap forward in our mission to simplify cloud governance: AI-Powered Remediations , which is now available in preview in Nirmata Control Hub (NCH). Security and platform teams often face a growing backlog of policy violations‚Äîfrom missing labels to insecure container configurations to overly permissive network rules. Manually investigating each violation, understanding its root cause, and crafting a compliant fix takes time‚Äîand that time adds up. With AI Remediations, we‚Äôre dramatically reducing the Mean Time to Remediation (MTTR ). Instead of spending hours chasing down documentation or writing YAML from scratch, teams can now review and apply context-aware fixes in minutes. And the benefits go beyond speed: Whether it‚Äôs a Deployment, ConfigMap, NetworkPolicy, or any Kubernetes resource, if it violates a rule enforced by your Kyverno policies in NCH, we can generate a fix. Here‚Äôs how: Transparency by design: We don‚Äôt just tell you what to change ‚Äì we explain why. You stay in control, with the ability to test and validate before deployment. AI Remediations aren‚Äôt about removing humans from the loop‚Äîthey‚Äôre about giving your teams a smarter starting point. Think of them as policy-aware code suggestions that plug right into your existing workflows.</description></item><item><title>Mainframes Are the New AI Infrastructure. Protect it with Secure AI</title><link>https://kubermates.org/docs/2025-05-06-mainframes-are-the-new-ai-infrastructure-protect-it-with-secure-ai/</link><pubDate>Tue, 06 May 2025 10:13:27 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-06-mainframes-are-the-new-ai-infrastructure-protect-it-with-secure-ai/</guid><description>If your AI workloads run in containers, then securing those containers is the first and most important step in protecting your AI. And as enterprises begin to deploy containerized AI workloads on Red Hat OpenShift for mainframe environments, that priority becomes even more urgent. IBM Z and IBM LinuxONE, long trusted to power the world‚Äôs most critical business systems are now evolving into innovation hubs, supporting advanced, containerized applications. With this transformation comes a new challenge: securing the infrastructure behind your most sensitive and high-stakes workloads The rise of generative AI and large language models (LLMs) has changed how organizations build and deliver value. From real-time fraud detection to intelligent customer support, AI is becoming embedded in nearly every business function. According to a recent McKinsey study , ‚Äú 78 percent of respondents say their organizations use AI in at least one business function, up from 72 percent in early 2024. ‚Äù These AI workloads are built and deployed in containers. Why? Containers offer portability, scalability, and efficiency, which makes them ideal for AI training, inference, and everything in between. IDC projects that 1 billion new logical applications will be created by 2028, resulting in more than 10 billion container instances across enterprise environments. These billions of containers won‚Äôt just run in a general purpose cloud. They will be deployed on purpose built hardware. For regulatory, proprietary, or even cost reasons, many of these models will be deployed on-prem.</description></item><item><title>Kubernetes v1.33: Prevent PersistentVolume Leaks When Deleting out of Order graduates to GA</title><link>https://kubermates.org/docs/2025-05-05-kubernetes-v1-33-prevent-persistentvolume-leaks-when-deleting-out-of-order-gradu/</link><pubDate>Mon, 05 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-05-kubernetes-v1-33-prevent-persistentvolume-leaks-when-deleting-out-of-order-gradu/</guid><description>I am thrilled to announce that the feature to prevent PersistentVolume (or PVs for short) leaks when deleting out of order has graduated to General Availability (GA) in Kubernetes v1. 33! This improvement, initially introduced as a beta feature in Kubernetes v1. 31, ensures that your storage resources are properly reclaimed, preventing unwanted leaks. PersistentVolumeClaim (or PVC for short) is a user&amp;rsquo;s request for storage. A PV and PVC are considered Bound if a newly created PV or a matching PV is found. The PVs themselves are backed by volumes allocated by the storage backend. Normally, if the volume is to be deleted, then the expectation is to delete the PVC for a bound PV-PVC pair. However, there are no restrictions on deleting a PV before deleting a PVC. For a Bound PV-PVC pair, the ordering of PV-PVC deletion determines whether the PV reclaim policy is honored. The reclaim policy is honored if the PVC is deleted first; however, if the PV is deleted prior to deleting the PVC, then the reclaim policy is not exercised. As a result of this behavior, the associated storage asset in the external infrastructure is not removed. With the graduation to GA in Kubernetes v1.</description></item><item><title>Nirmata Enterprise for Kyverno (N4K) Now Available on AWS Marketplace!</title><link>https://kubermates.org/docs/2025-05-05-nirmata-enterprise-for-kyverno-n4k-now-available-on-aws-marketplace/</link><pubDate>Mon, 05 May 2025 17:55:11 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-05-nirmata-enterprise-for-kyverno-n4k-now-available-on-aws-marketplace/</guid><description>We are thrilled to announce that Nirmata Enterprise for Kyverno (N4K) is now available on AWS Marketplace ! This enterprise-grade distribution of Kyverno brings the power of Kubernetes policy management with enhanced features, security, and support to meet the needs of the most demanding production environments. Nirmata Enterprise for Kyverno (N4K) is the enterprise-grade distribution of the widely-used Kyverno policy engine. N4K offers the same powerful policy enforcement capabilities as Kyverno but with key enhancements that cater to large-scale, security-conscious organizations. This includes zero CVEs , 24√ó7 support with SLA , and priority fixes and features to ensure your Kubernetes environments are always secure, compliant, and running at peak performance. By making Nirmata Enterprise for Kyverno (N4K) available on AWS Marketplace , we provide AWS customers with a streamlined way to purchase, deploy, and integrate Nirmata into their existing cloud-native environments. AWS Marketplace offers: Nirmata Enterprise for Kyverno (N4K) is available as a pay-as-you-go option through AWS Marketplace, making it flexible for businesses of all sizes. For organizations seeking annual or long-term contracts , we offer custom pricing options. Contact us directly for more information and to discuss a plan that fits your specific needs. Nirmata Enterprise for Kyverno (N4K) is available now on AWS Marketplace! Ready to elevate your Kubernetes security and governance? Visit the AWS Marketplace today to install the N4K add-on and discover how Nirmata can help you manage Kubernetes policy enforcement at scale. Read the complete documentation here. For more information or if you‚Äôre interested in annual or long-term contract options, contact us today. If you‚Äôd like to schedule a demo to see what all the buzz is about, you can do that here.</description></item><item><title>Kubernetes 1.33: Top 5 Features of ‚ÄúOctarine</title><link>https://kubermates.org/docs/2025-05-05-kubernetes-1-33-top-5-features-of-octarine/</link><pubDate>Mon, 05 May 2025 17:20:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-05-kubernetes-1-33-top-5-features-of-octarine/</guid><description>The wait is over‚Äî Kubernetes 1. 33 has officially arrived, and it‚Äôs packed with magical new improvements for developers and operators alike. Code-named ‚ÄúOctarine‚Äù , a nod to the mythical ‚Äúcolor of magic‚Äù from Terry Pratchett‚Äôs Discworld novels, this release continues Kubernetes‚Äô steady momentum of delivering a scalable, secure, and developer-friendly container orchestration platform. According to the official Kubernetes announcement , Kubernetes 1. 33 is all about pushing boundaries while making life easier for everyone working with Kubernetes in production environments. Whether you‚Äôre running massive enterprise workloads or experimenting in a dev cluster, this version has something for you. Kubernetes 1. 33 includes a total of 64 enhancements , breaking down as follows: This broad set of updates signals ongoing investments across the project‚Äôs key pillars: performance, scalability, security, extensibility, and usability. With such a packed release, it‚Äôs easy to get lost in the full changelog. That‚Äôs why we‚Äôve focused this overview on the five most important and popular features that developers and operators are most excited about. From long-awaited capabilities that boost cluster scaling and tighten security , to much-needed quality-of-life improvements for everyday Kubernetes usage, the Kubernetes 1. 33 ‚ÄúOctarine‚Äù release delivers on all fronts.</description></item><item><title>Sharks of DigitalOcean: Jason Dobry, Staff IT Project Specialist</title><link>https://kubermates.org/docs/2025-05-05-sharks-of-digitalocean-jason-dobry-staff-it-project-specialist/</link><pubDate>Mon, 05 May 2025 13:05:52 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-05-sharks-of-digitalocean-jason-dobry-staff-it-project-specialist/</guid><description>By Sujatha R Technical Writer For Jason Dobry, joining DigitalOcean was about finding genuine belonging, not just accepting a new position. Today, as a Staff IT Project Specialist, Jason coordinates IT project work across global support teams, helping admins and agents across APAC (Asia-Pacific) and EMEA (Europe, Middle East, and Africa) stay aligned and effective. He leads the Atlassian modernization team and collaborates closely with the IT support teams and system admins. Whether it√¢¬Ä¬ôs optimizing internal dashboards to track team performance, coordinating with vendors on new initiatives, or uncovering trends in operational data, Jason acts as a multi-tool within the IT organization. He√¢¬Ä¬ôs passionate about creating opportunities for his colleagues to grow, learn, and find joy in their work. Read on to discover Jason√¢¬Ä¬ôs journey, his passion for purpose-driven work, and how he√¢¬Ä¬ôs now shaping the very culture he once sought out at DO. When I joined DigitalOcean, I was looking for more than just a job√¢¬Ä¬îI wanted a place that felt like home. DO gave me that from day one. The way the company supported me, especially outside of work, allowed me to show up as my best self every day. Through big transitions like the pandemic, they always put my family first. That support helped me stay grounded and, in turn, empowered me to help others do their best work for our customers. √∞¬ü¬é¬• Have a look at Jason Dobry√¢¬Ä¬ôs full conversation √¢¬¨¬á√Ø¬∏¬è Our willingness to experiment.</description></item><item><title>Day 7: Your Kubernetes Learning Roadmap ‚Äî What‚Äôs Next After the Basics?</title><link>https://kubermates.org/docs/2025-05-03-day-7-your-kubernetes-learning-roadmap-what-s-next-after-the-basics/</link><pubDate>Sat, 03 May 2025 13:23:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-03-day-7-your-kubernetes-learning-roadmap-what-s-next-after-the-basics/</guid><description>You‚Äôve made it through: You‚Äôve just unlocked the foundation of Kubernetes. But Kubernetes is big. It‚Äôs normal to now think: This post will give you the answer. Here‚Äôs a simple path we recommend: What to learn next: Tools: üëâ KodeKloud Kubernetes Free Labs üëâ Kubernetes Playground Goal: Be confident to deploy small apps on Kubernetes. What to explore: Tools: üëâ KodeKloud Kubernetes Learning Path Goal: Run small microservices apps in Kubernetes with persistence &amp;amp; scaling. What to master: Certification Tip: Start preparing for CKA (Certified Kubernetes Administrator). Tools: üëâ KodeKloud CKA Course üëâ Official Kubernetes Docs Goal: Be job-ready for DevOps &amp;amp; Cloud Native roles. We recommend keeping this simple checklist as your personal tracker : [ ] I understand Containers &amp;amp; Docker [ ] I know how to use kubectl [ ] I can create and manage Pods [ ] I know how Deployments &amp;amp; ReplicaSets work [ ] I can expose apps using Services [ ] I can manage ConfigMaps &amp;amp; Secrets [ ] I understand Clusters, Nodes, and the Control Plane [ ] I ‚Äôve tried Kubernetes labs or playgrounds [ ] I‚Äôve deployed a real-world app in Kubernetes [ ] I ‚Äôm exploring intermediate concepts (Volumes, Ingress, Monitoring) üëâ Pro tip: Print it or keep it in Notion/GitHub and check items off as you progress! Kubernetes is NOT something you master overnight. The key is consistent practice + real projects. Even learning these basics puts you ahead of most beginner engineers. You‚Äôve started your Cloud Native journey ‚Äî keep going. We‚Äôll be here to help every step of the way.</description></item><item><title>Kubernetes v1.33: Mutable CSI Node Allocatable Count</title><link>https://kubermates.org/docs/2025-05-02-kubernetes-v1-33-mutable-csi-node-allocatable-count/</link><pubDate>Fri, 02 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-02-kubernetes-v1-33-mutable-csi-node-allocatable-count/</guid><description>Scheduling stateful applications reliably depends heavily on accurate information about resource availability on nodes. Kubernetes v1. 33 introduces an alpha feature called mutable CSI node allocatable count , allowing Container Storage Interface (CSI) drivers to dynamically update the reported maximum number of volumes that a node can handle. This capability significantly enhances the accuracy of pod scheduling decisions and reduces scheduling failures caused by outdated volume capacity information. Traditionally, Kubernetes CSI drivers report a static maximum volume attachment limit when initializing. However, actual attachment capacities can change during a node&amp;rsquo;s lifecycle for various reasons, such as: Static reporting can cause Kubernetes to schedule pods onto nodes that appear to have capacity but don&amp;rsquo;t, leading to pods stuck in a ContainerCreating state. With the new feature gate MutableCSINodeAllocatableCount , Kubernetes enables CSI drivers to dynamically adjust and report node attachment capacities at runtime. This ensures that the scheduler has the most accurate, up-to-date view of node capacity. When this feature is enabled, Kubernetes supports two mechanisms for updating the reported node volume limits: To use this alpha feature, you must enable the MutableCSINodeAllocatableCount feature gate in these components: Below is an example of configuring a CSI driver to enable periodic updates every 60 seconds: This configuration directs Kubelet to periodically call the CSI driver&amp;rsquo;s NodeGetInfo method every 60 seconds, updating the node‚Äôs allocatable volume count. Kubernetes enforces a minimum update interval of 10 seconds to balance accuracy and resource usage. In addition to periodic updates, Kubernetes now reacts to attachment failures. Specifically, if a volume attachment fails with a ResourceExhausted error (gRPC code 8 ), an immediate update is triggered to correct the allocatable count promptly.</description></item><item><title>Kubernetes v1.33: New features in DRA</title><link>https://kubermates.org/docs/2025-05-01-kubernetes-v1-33-new-features-in-dra/</link><pubDate>Thu, 01 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-01-kubernetes-v1-33-new-features-in-dra/</guid><description>Kubernetes Dynamic Resource Allocation (DRA) was originally introduced as an alpha feature in the v1. 26 release, and then went through a significant redesign for Kubernetes v1. 31. The main DRA feature went to beta in v1. 32, and the project hopes it will be generally available in Kubernetes v1. 34. The basic feature set of DRA provides a far more powerful and flexible API for requesting devices than Device Plugin. And while DRA remains a beta feature for v1. 33, the DRA team has been hard at work implementing a number of new features and UX improvements. One feature has been promoted to beta, while a number of new features have been added in alpha. The team has also made progress towards getting DRA ready for GA. Driver-owned Resource Claim Status was promoted to beta.</description></item><item><title>Day 6: ConfigMaps &amp; Secrets ‚Äî Managing App Settings and Sensitive Data in Kubernetes</title><link>https://kubermates.org/docs/2025-05-01-day-6-configmaps-secrets-managing-app-settings-and-sensitive-data-in-kubernetes/</link><pubDate>Thu, 01 May 2025 18:11:50 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-01-day-6-configmaps-secrets-managing-app-settings-and-sensitive-data-in-kubernetes/</guid><description>In traditional apps ‚Äî whether Node. js, Python, Java, or others ‚Äî you‚Äôve probably done this before: And sometimes‚Ä¶ you (accidentally or lazily) hardcode passwords into the code itself. üò¨ But in Kubernetes, there‚Äôs a better and safer way to do this. Enter: ConfigMaps and Secrets It keeps your app configs separate from your app code and container image ‚Äî which is great for flexibility and security. Secrets are base64-encoded and can be managed with tighter access controls in Kubernetes. You: And a Secret: Then, in your Pod spec (simplified YAML): üëâ Use the KodeKloud Kubernetes Playground 1 - Create a Secret: 2 - Create a Pod using that secret: Apply it with: You‚Äôll see the password securely injected. Imagine you‚Äôre deploying an app on a shared team server. üìÖ Day 7: Your Kubernetes Learning Roadmap ‚Äî What‚Äôs Next After the Basics? You‚Äôll get: New here? Start from Day 1 and catch up on the series: Day 1: What Is Kubernetes &amp;amp; Why Should You Care? Discover why Kubernetes matters and how it changes the game. Day 2: What Are Pods in Kubernetes? Understand the smallest deployable unit in Kubernetes. Day 3: Understanding Nodes, Clusters &amp;amp; the Kubernetes Control Plane See how all the pieces connect behind the scenes. Day 4: Deployments &amp;amp; ReplicaSets ‚Äî How Kubernetes Runs and Manages Your App ‚öôLearn how Kubernetes keeps your apps running smoothly. Day 5: Kubernetes Services ‚Äî How Your App Gets a Stable IP or URL Discover how Services expose and connect your app reliably.</description></item><item><title>Kubernetes v1.33: Storage Capacity Scoring of Nodes for Dynamic Provisioning (alpha)</title><link>https://kubermates.org/docs/2025-04-30-kubernetes-v1-33-storage-capacity-scoring-of-nodes-for-dynamic-provisioning-alph/</link><pubDate>Wed, 30 Apr 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-04-30-kubernetes-v1-33-storage-capacity-scoring-of-nodes-for-dynamic-provisioning-alph/</guid><description>Kubernetes v1. 33 introduces a new alpha feature called StorageCapacityScoring. This feature adds a scoring method for pod scheduling with the topology-aware volume provisioning. This feature eases to schedule pods on nodes with either the most or least available storage capacity. This feature extends the kube-scheduler&amp;rsquo;s VolumeBinding plugin to perform scoring using node storage capacity information obtained from Storage Capacity. Currently, you can only filter out nodes with insufficient storage capacity. So, you have to use a scheduler extender to achieve storage-capacity-based pod scheduling. This feature is useful for provisioning node-local PVs, which have size limits based on the node&amp;rsquo;s storage capacity. By using this feature, you can assign the PVs to the nodes with the most available storage space so that you can expand the PVs later as much as possible. In another use case, you might want to reduce the number of nodes as much as possible for low operation costs in cloud environments by choosing the least storage capacity node. This feature helps maximize resource utilization by filling up nodes more sequentially, starting with the most utilized nodes first that still have enough storage capacity for the requested volume size. In the alpha phase, StorageCapacityScoring is disabled by default.</description></item><item><title>AI agent development just got easier on GenAI Platform</title><link>https://kubermates.org/docs/2025-04-30-ai-agent-development-just-got-easier-on-genai-platform/</link><pubDate>Wed, 30 Apr 2025 15:30:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-30-ai-agent-development-just-got-easier-on-genai-platform/</guid><description>By Grace Morgan DigitalOcean√¢¬Ä¬ôs GenAI Platform is now DigitalOcean Gradient Platform. Learn more about the GA release and features. As AI agents become increasingly embedded in business operations, user experiences, and regulated workflows, the need for greater transparency, control, and performance visibility is more important than ever. That√¢¬Ä¬ôs why we√¢¬Ä¬ôre introducing a set of major updates to GenAI Platform, now in public preview, including enhanced agent development features and improved data sourcing through knowledge base citations. These new capabilities make it easier than ever to build, deploy, and refine your AI agents. We√¢¬Ä¬ôre rolling out powerful new features that give teams more control, transparency, and efficiency when working with AI agents. Knowledge base citations for better transparency Understanding the source of AI-generated responses is critical for trust and reliability. With the new knowledge base citations feature , AI agents can now provide clear source attribution for retrieved information, helping users trace responses back to their original context. This feature is especially valuable for business intelligence applications and educational platforms, where accurate and verifiable information is paramount. Agent insights for performance monitoring Optimizing an AI agent requires clear visibility into how it processes requests and responds to users. Agent insights provide key performance metrics to help you refine your agents for speed, efficiency, and cost-effectiveness. Agent insights give you the power to proactively monitor and refine your AI agents, leading to a better end-user experience.</description></item><item><title>Bottlerocket for hybrid nodes</title><link>https://kubermates.org/releases/2025-04-29-bottlerocket-for-hybrid-nodes/</link><pubDate>Tue, 29 Apr 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-04-29-bottlerocket-for-hybrid-nodes/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. This topic describes how to connect hybrid nodes running Bottlerocket to an Amazon EKS cluster. Bottlerocket is an open source Linux distribution that is sponsored and supported by AWS. Bottlerocket is purpose-built for hosting container workloads. With Bottlerocket, you can improve the availability of containerized deployments and reduce operational costs by automating updates to your container infrastructure. Bottlerocket includes only the essential software to run containers, which improves resource usage, reduces security threats, and lowers management overhead. Only VMware variants of Bottlerocket version v1. 37. 0 and above are supported with EKS Hybrid Nodes. VMware variants of Bottlerocket are available for Kubernetes versions v1. 28 and above. The OS images for these variants include the kubelet, containerd, aws-iam-authenticator and other software prerequisites for EKS Hybrid Nodes.</description></item><item><title>Kubernetes v1.33: Image Volumes graduate to beta!</title><link>https://kubermates.org/docs/2025-04-29-kubernetes-v1-33-image-volumes-graduate-to-beta/</link><pubDate>Tue, 29 Apr 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-04-29-kubernetes-v1-33-image-volumes-graduate-to-beta/</guid><description>Image Volumes were introduced as an Alpha feature with the Kubernetes v1. 31 release as part of KEP-4639. In Kubernetes v1. 33, this feature graduates to beta. Please note that the feature is still disabled by default, because not all container runtimes have full support for it. CRI-O supports the initial feature since version v1. 31 and will add support for Image Volumes as beta in v1. 33. containerd merged support for the alpha feature which will be part of the v2. 1. 0 release and is working on beta support as part of PR #11578. The major change for the beta graduation of Image Volumes is the support for subPath and subPathExpr mounts for containers via spec.</description></item><item><title>Day 5: Kubernetes Services ‚Äî How Your App Gets a Stable IP or URL</title><link>https://kubermates.org/docs/2025-04-29-day-5-kubernetes-services-how-your-app-gets-a-stable-ip-or-url/</link><pubDate>Tue, 29 Apr 2025 17:26:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-29-day-5-kubernetes-services-how-your-app-gets-a-stable-ip-or-url/</guid><description>By now, you‚Äôve launched a container, wrapped it in a Pod, deployed it using a Deployment, and scaled it up to multiple Pods. But here‚Äôs the next problem: You try running: And you see something like: Great ‚Äî but: That‚Äôs where Services come in. You can think of it as: So even if Pods restart, the Service endpoint never changes. Let‚Äôs say you run 3 Pods of a web app with the label: You create a Service like this: Now, when someone accesses the Service: Imagine your app‚Äôs Pods are like food trucks moving around a festival ground. A Service is like the signpost that says: You don‚Äôt care where the trucks are parked ‚Äî you just follow the sign. Kubernetes routes you to the right place, even if trucks come and go. Use the Kubernetes Playground: üëâ KodeKloud Kubernetes Playground Create a deployment: Now run: Look for the NodePort (e. g. , 30123 ) and try opening: You‚Äôre now accessing your app through a Kubernetes Service! üìÖ Day 6: ConfigMaps &amp;amp; Secrets ‚Äî Managing App Settings and Sensitive Data You‚Äôll learn: New here? Start from Day 1 and catch up on the series: Day 1: What Is Kubernetes &amp;amp; Why Should You Care? Discover why Kubernetes matters and how it changes the game. Day 2: What Are Pods in Kubernetes? Understand the smallest deployable unit in Kubernetes. Day 3: Understanding Nodes, Clusters &amp;amp; the Kubernetes Control Plane See how all the pieces connect behind the scenes. Day 4: Deployments &amp;amp; ReplicaSets ‚Äî How Kubernetes Runs and Manages Your App Learn how Kubernetes keeps your apps running smoothly.</description></item><item><title>Shadow Roles: AWS Defaults Can Open the Door to Service Takeover</title><link>https://kubermates.org/docs/2025-04-29-shadow-roles-aws-defaults-can-open-the-door-to-service-takeover/</link><pubDate>Tue, 29 Apr 2025 16:16:07 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-29-shadow-roles-aws-defaults-can-open-the-door-to-service-takeover/</guid><description>What if the biggest risk to your cloud environment wasn‚Äôt a misconfiguration you made, but one baked into the defaults? Our research uncovered security concerns in the deployment of resources within a few AWS services, specifically in the default AWS service roles. These roles, often created automatically or recommended during setup, grant overly broad permissions, such as full S3 access. These default roles silently introduce attack paths that allow privilege escalation, cross-service access, and even potential account compromise. Aqua Security is the largest pure-play cloud native security company, providing customers the freedom to innovate and accelerate their digital transformations. The Aqua Platform is the leading Cloud Native Application Protection Platform (CNAPP) and provides prevention, detection, and response automation across the entire application lifecycle to secure the supply chain, secure cloud infrastructure and secure running workloads wherever they are deployed. Aqua customers are among the world‚Äôs largest enterprises in financial services, software, media, manufacturing and retail, with implementations across a broad range of cloud providers and modern technology stacks spanning containers, serverless functions and cloud VMs.</description></item><item><title>Kubernetes v1.33: HorizontalPodAutoscaler Configurable Tolerance</title><link>https://kubermates.org/docs/2025-04-28-kubernetes-v1-33-horizontalpodautoscaler-configurable-tolerance/</link><pubDate>Mon, 28 Apr 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-04-28-kubernetes-v1-33-horizontalpodautoscaler-configurable-tolerance/</guid><description>This post describes configurable tolerance for horizontal Pod autoscaling , a new alpha feature first available in Kubernetes 1. 33. Horizontal Pod Autoscaling is a well-known Kubernetes feature that allows your workload to automatically resize by adding or removing replicas based on resource utilization. Let&amp;rsquo;s say you have a web application running in a Kubernetes cluster with 50 replicas. You configure the HorizontalPodAutoscaler (HPA) to scale based on CPU utilization, with a target of 75% utilization. Now, imagine that the current CPU utilization across all replicas is 90%, which is higher than the desired 75%. The HPA will calculate the required number of replicas using the formula: In this example: So, the HPA will increase the number of replicas from 50 to 60 to reduce the load on each pod. Similarly, if the CPU utilization were to drop below 75%, the HPA would scale down the number of replicas accordingly. The Kubernetes documentation provides a detailed description of the scaling algorithm. In order to avoid replicas being created or deleted whenever a small metric fluctuation occurs, Kubernetes applies a form of hysteresis: it only changes the number of replicas when the current and desired metric values differ by more than 10%. In the example above, since the ratio between the current and desired metric values is (90/75), or 20% above target, exceeding the 10% tolerance, the scale-up action will proceed. This default tolerance of 10% is cluster-wide; in older Kubernetes releases, it could not be fine-tuned.</description></item><item><title>Kubernetes v1.33: User Namespaces enabled by default!</title><link>https://kubermates.org/docs/2025-04-25-kubernetes-v1-33-user-namespaces-enabled-by-default/</link><pubDate>Fri, 25 Apr 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-04-25-kubernetes-v1-33-user-namespaces-enabled-by-default/</guid><description>In Kubernetes v1. 33 support for user namespaces is enabled by default. This means that, when the stack requirements are met, pods can opt-in to use user namespaces. To use the feature there is no need to enable any Kubernetes feature flag anymore! In this blog post we answer some common questions about user namespaces. But, before we dive into that, let&amp;rsquo;s recap what user namespaces are and why they are important. Note: Linux user namespaces are a different concept from Kubernetes namespaces. The former is a Linux kernel feature; the latter is a Kubernetes feature. Linux provides different namespaces to isolate processes from each other. For example, a typical Kubernetes pod runs within a network namespace to isolate the network identity and a PID namespace to isolate the processes. One Linux namespace that was left behind is the user namespace. It isolates the UIDs and GIDs of the containers from the ones on the host. The identifiers in a container can be mapped to identifiers on the host in a way where host and container(s) never end up in overlapping UID/GIDs.</description></item><item><title>Introducing DigitalOcean Managed Caching for Valkey, The New Evolution of Managed Caching</title><link>https://kubermates.org/docs/2025-04-24-introducing-digitalocean-managed-caching-for-valkey-the-new-evolution-of-managed/</link><pubDate>Thu, 24 Apr 2025 20:16:49 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-24-introducing-digitalocean-managed-caching-for-valkey-the-new-evolution-of-managed/</guid><description>By Nicole Ghalwash Today, we√¢¬Ä¬ôre excited to announce the launch of DigitalOcean√¢¬Ä¬ôs Managed Caching for Valkey, our new Managed Database service that seamlessly replaces Managed Caching (previously Managed Redis√Ç¬Æ). Managed Caching for Valkey builds on the capabilities you√¢¬Ä¬ôve come to rely on while also offering enhanced tools to support your development needs. DigitalOcean Managed Caching for Valkey is a fully managed, high-performance, in-memory key-value datastore designed for caching, message queues, and primary database use. Fully compatible with Valkey 8. 0 and Redis 7. 2. 4√Ç¬Æ as a database, it serves as a drop-in replacement for our Managed Caching database service while offering enhanced functionality for fast and efficient data storage. Built for developers and growing businesses, Managed Caching for Valkey helps reduce database load, improve response times, and optimize resource usage√¢¬Ä¬îdelivering an easy-to-use, cost-effective alternative to self-managed caching solutions. As for our current customers with Managed Caching environments, Managed Caching will have an end-of-availability (EOA) date of April 30, meaning you cannot create new clusters after this date, and April 29th is when new Redis creates will be disabled. However, existing clusters will remain operational, but we encourage you to explore our product documentation to learn how to convert your Caching workloads to Valkey, step-by-step. DigitalOcean Managed Caching for Valkey offers a wealth of benefits to the modern-day developer. Amongst them all, these are the ones we think will be of the most value to you: A Redis√Ç¬Æ-compatible database: Valkey is a one-in replacement for Managed Caching, our previous Redis√Ç¬Æ Managed Database service.</description></item><item><title>Kubernetes v1.33: Continuing the transition from Endpoints to EndpointSlices</title><link>https://kubermates.org/docs/2025-04-24-kubernetes-v1-33-continuing-the-transition-from-endpoints-to-endpointslices/</link><pubDate>Thu, 24 Apr 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-04-24-kubernetes-v1-33-continuing-the-transition-from-endpoints-to-endpointslices/</guid><description>Since the addition of EndpointSlices ( KEP-752 ) as alpha in v1. 15 and later GA in v1. 21, the Endpoints API in Kubernetes has been gathering dust. New Service features like dual-stack networking and traffic distribution are only supported via the EndpointSlice API, so all service proxies, Gateway API implementations, and similar controllers have had to be ported from using Endpoints to using EndpointSlices. At this point, the Endpoints API is really only there to avoid breaking end user workloads and scripts that still make use of it. As of Kubernetes 1. 33, the Endpoints API is now officially deprecated, and the API server will return warnings to users who read or write Endpoints resources rather than using EndpointSlices. Eventually, the plan (as documented in KEP-4974 ) is to change the Kubernetes Conformance criteria to no longer require that clusters run the Endpoints controller (which generates Endpoints objects based on Services and Pods), to avoid doing work that is unneeded in most modern-day clusters. Thus, while the Kubernetes deprecation policy means that the Endpoints type itself will probably never completely go away, users who still have workloads or scripts that use the Endpoints API should start migrating them to EndpointSlices. For end users, the biggest change between the Endpoints API and the EndpointSlice API is that while every Service with a selector has exactly 1 Endpoints object (with the same name as the Service), a Service may have any number of EndpointSlices associated with it: In this case, because the service is dual stack, it has 2 EndpointSlices: 1 for IPv4 addresses and 1 for IPv6 addresses. (The Endpoints API does not support dual stack, so the Endpoints object shows only the addresses in the cluster&amp;rsquo;s primary address family. ) Although any Service with multiple endpoints can have multiple EndpointSlices, there are three main cases where you will see this: An EndpointSlice can only represent endpoints of a single IP family, so dual-stack Services will have separate EndpointSlices for IPv4 and IPv6.</description></item><item><title>DigitalOcean Customers Eligible to Process DORA Workloads</title><link>https://kubermates.org/docs/2025-04-24-digitalocean-customers-eligible-to-process-dora-workloads/</link><pubDate>Thu, 24 Apr 2025 12:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-24-digitalocean-customers-eligible-to-process-dora-workloads/</guid><description>By David Lopez Staff Trust &amp;amp; Governance Advisor, Trust and Governance Today, we√¢¬Ä¬ôre excited to announce that DigitalOcean will begin signing Digital Operational Resilience Act (DORA) Addendums with eligible customers. Additionally, we engaged Schellman &amp;amp; Company to facilitate an audit of our environment, so that customers can deploy on DigitalOcean with trust in the organization√¢¬Ä¬ôs risk management, security controls, operational resilience, incident management and reporting, and outsourcing and third party risk management. Effective January 17, 2025, financial entities operating in the European Union (EU) and Information and Communications Technology (ICT) third-party service providers are subject to the EU√¢¬Ä¬ôs DORA. The regulation standardizes how financial entities report major ICT-related incidents, test their digital operational resilience, and manage ICT third-party risk across the financial services sector and EU member states. While DigitalOcean has not yet been designated a critical Information and Communications Technology (ICT) third-party service provider by European Supervisory Authorities, we recognize the need to enable select customers to address their DORA obligations relative to ICT third-party service providers, which can include; but, are not limited to, cloud service providers. Customers who require the DORA Addendum can request the document by contacting Sales. As part of our continued commitment to you, we engaged Schellman Compliance, LLC to facilitate an audit of our environment against DORA√¢¬Ä¬ôs core regulatory framework, relevant Regulatory Technical Standards (RTS) and Implementation Technical Standards (ITS). The assessment returned zero findings. A summary letter can be viewed within our Trust Platform. Share Read more Read more Narasimha Badrinath Read more.</description></item><item><title>Recap: KubeCon + CloudNativeCon Europe 2025</title><link>https://kubermates.org/docs/2025-04-23-recap-kubecon-cloudnativecon-europe-2025/</link><pubDate>Wed, 23 Apr 2025 20:32:36 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-23-recap-kubecon-cloudnativecon-europe-2025/</guid><description>When I got the assignment to attend KubeCon 1st of April I thought it was an April prank, but as the date got closer I realized‚Äîthis is for real and I‚Äôll be on the ground in London at the tenth anniversary of cloud native computing. I‚Äôve seen a lot of tech events during my years in the industry while trying not to get replaced by AI and I have to say this one stands out! Image source: CNCF YouTube Channel Here is my recap of KubeCon + CloudNativeCon Europe 2025. CalicoCon is an event that happens twice every year, as a co-located event during KubeCon NA and EU. It‚Äôs a free event that allows you to learn about Tigera‚Äôs vision for the future of networking and security in the cloud. There‚Äôs also an after-party to celebrate our community and people like you who are on this journey with us! This year our main focus was on Calico v3. 30 , our upcoming release that will add a lot of anticipated features to Calico, unlocking things like observability, staged network policy, and gateway api. CalicoCon brought together cloud-native enthusiasts to explore the latest advancements in Calico and Kubernetes networking. The following is a brief summary of this year‚Äôs CalicoCon sessions. Note: The CalicoCon playlist with session recordings is now available on YouTube. Peter Kelly, VP of Engineering at Tigera, highlighted the new features in Calico v3. 30. A key focus was on the new observability features, including the ‚ÄúWhisker‚Äù dashboard, designed to provide deeper insights into network behavior.</description></item><item><title>Kubernetes v1.33: Octarine</title><link>https://kubermates.org/docs/2025-04-23-kubernetes-v1-33-octarine/</link><pubDate>Wed, 23 Apr 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-04-23-kubernetes-v1-33-octarine/</guid><description>Editors: Agustina Barbetta, Aakanksha Bhende, Udi Hofesh, Ryota Sawada, Sneha Yadav Similar to previous releases, the release of Kubernetes v1. 33 introduces new stable, beta, and alpha features. The consistent delivery of high-quality releases underscores the strength of our development cycle and the vibrant support from our community. This release consists of 64 enhancements. Of those enhancements, 18 have graduated to Stable, 20 are entering Beta, 24 have entered Alpha, and 2 are deprecated or withdrawn. There are also several notable deprecations and removals in this release; make sure to read about those if you already run an older version of Kubernetes. The theme for Kubernetes v1. 33 is Octarine: The Color of Magic 1 , inspired by Terry Pratchett‚Äôs Discworld series. This release highlights the open source magic 2 that Kubernetes enables across the ecosystem. If you‚Äôre familiar with the world of Discworld, you might recognize a small swamp dragon perched atop the tower of the Unseen University, gazing up at the Kubernetes moon above the city of Ankh-Morpork with 64 stars 3 in the background. As Kubernetes moves into its second decade, we celebrate both the wizardry of its maintainers, the curiosity of new contributors, and the collaborative spirit that fuels the project. The v1.</description></item><item><title>What‚Äôs Really Happening in Your Containers? Aqua‚Äôs Risk Assessment Has the Answer</title><link>https://kubermates.org/docs/2025-04-23-what-s-really-happening-in-your-containers-aqua-s-risk-assessment-has-the-answer/</link><pubDate>Wed, 23 Apr 2025 12:12:36 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-23-what-s-really-happening-in-your-containers-aqua-s-risk-assessment-has-the-answer/</guid><description>Containers may be mainstream, but securing them in production remains a moving target. As AI adoption scales and environments grow more complex, so too do the risks, especially at runtime, where traditional tools struggle to provide meaningful visibility. These are not legacy exploits like port scans or brute force attempts. Attackers are targeting what happens inside your environment, at runtime, where misconfigurations, unexpected behaviors, and subtle anomalies can quietly introduce business risk. Yet most security teams are flying blind in production. They have tools that check for vulnerabilities before deployment, scan code for secrets, or assess infrastructure misconfigurations. What they don‚Äôt have is visibility into what‚Äôs actually happening right now, in the workloads already running in their environment. That‚Äôs the gap Aqua is closing with the launch of the Container Security Risk Assessment (CSRA). Security tooling has largely focused on pre-production controls like image scanning, CI/CD pipeline checks, and IaC validation. It is logical, and even easier than runtime. But once containers are running, things change. Configurations drift.</description></item><item><title>Kubernetes Multicontainer Pods: An Overview</title><link>https://kubermates.org/docs/2025-04-22-kubernetes-multicontainer-pods-an-overview/</link><pubDate>Tue, 22 Apr 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-22-kubernetes-multicontainer-pods-an-overview/</guid><description>As cloud-native architectures continue to evolve, Kubernetes has become the go-to platform for deploying complex, distributed systems. One of the most powerful yet nuanced design patterns in this ecosystem is the sidecar pattern‚Äîa technique that allows developers to extend application functionality without diving deep into source code. Think of a sidecar like a trusty companion motorcycle attachment. Historically, IT infrastructures have always used auxiliary services to handle critical tasks. Before containers, we relied on background processes and helper daemons to manage logging, monitoring, and networking. The microservices revolution transformed this approach, making sidecars a structured and intentional architectural choice. With the rise of microservices, the sidecar pattern became more clearly defined, allowing developers to offload specific responsibilities from the main service without altering its code. Service meshes like Istio and Linkerd have popularized sidecar proxies, demonstrating how these companion containers can elegantly handle observability, security, and traffic management in distributed systems. In Kubernetes, sidecar containers operate within the same Pod as the main application, enabling communication and resource sharing. Does this sound just like defining multiple containers along each other inside the Pod? It actually does, and this is how sidecar containers had to be implemented before Kubernetes v1. 29. 0, which introduced native support for sidecars.</description></item><item><title>New concepts pages for hybrid networking</title><link>https://kubermates.org/releases/2025-04-18-new-concepts-pages-for-hybrid-networking/</link><pubDate>Fri, 18 Apr 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-04-18-new-concepts-pages-for-hybrid-networking/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. With Amazon EKS Hybrid Nodes , you join physical or virtual machines running in on-premises or edge environments to Amazon EKS clusters running in the AWS Cloud. This approach brings many benefits, but also introduces new networking concepts and architectures for those familiar with running Kubernetes clusters in a single network environment. The following sections dive deep into the Kubernetes and networking concepts for EKS Hybrid Nodes and details how traffic flows through the hybrid architecture. These sections require that you are familiar with basic Kubernetes networking knowledge, such as the concepts of pods, nodes, services, Kubernetes control plane, kubelet and kube-proxy. We recommend reading these pages in order, starting with the Networking concepts for hybrid nodes , then the Kubernetes concepts for hybrid nodes , and finally the Network traffic flows for hybrid nodes. Networking concepts for hybrid nodes Kubernetes concepts for hybrid nodes Network traffic flows for hybrid nodes Thanks for letting us know we&amp;rsquo;re doing a good job! If you&amp;rsquo;ve got a moment, please tell us what we did right so we can do more of it. Thanks for letting us know this page needs work. We&amp;rsquo;re sorry we let you down. If you&amp;rsquo;ve got a moment, please tell us how we can make the documentation better.</description></item><item><title>üöÄüåê Elevating Infrastructure: From Terraform/Terragrunt Foundations to Platform Engineering üòä</title><link>https://kubermates.org/blog/elevating-infrastructure-from-terraformterragrunt-foundations-to-platform-engineering-41fc/</link><pubDate>Fri, 18 Apr 2025 10:24:24 +0000</pubDate><guid>https://kubermates.org/blog/elevating-infrastructure-from-terraformterragrunt-foundations-to-platform-engineering-41fc/</guid><description>&lt;p&gt;Hey there, cloud adventurers! üöÄ Let‚Äôs chat about why keeping &lt;strong&gt;Terraform&lt;/strong&gt; (or &lt;strong&gt;OpenTofu&lt;/strong&gt;) and &lt;strong&gt;Terragrunt&lt;/strong&gt; in their own lanes is absolutely essential‚Äîand how using &lt;strong&gt;Terraform JSON tfvars&lt;/strong&gt; makes life easier when you‚Äôre building nifty tools on top. Ready? Let‚Äôs dive in! üòÑ&lt;/p&gt;
&lt;h3 id="why-separation-is-a-must-not-an-option-"&gt;Why Separation Is a Must, Not an Option üôÖ‚Äç‚ôÇÔ∏èüôÖ‚Äç‚ôÄÔ∏è&lt;/h3&gt;
&lt;p&gt;It might be tempting to mix Terraform and Terragrunt into one big file‚Äîafter all, they work together, right? But trust me, keeping them decoupled is a game‚Äëchanger:&lt;/p&gt;</description></item><item><title>Smarter Knowledge Bases for Smarter AI Agents</title><link>https://kubermates.org/docs/2025-04-16-smarter-knowledge-bases-for-smarter-ai-agents/</link><pubDate>Wed, 16 Apr 2025 22:09:43 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-16-smarter-knowledge-bases-for-smarter-ai-agents/</guid><description>By Grace Morgan We√¢¬Ä¬ôre rolling out new features to the GenAI Platform that make it easier to build, manage, and improve the knowledge bases behind your AI agents. With web crawling, custom crawling rules, and one-click reindexing, you can keep your agents up to date with relevant, real-world information, without manual data collection or external storage. Combined with recent enhancements to our Retrieval-Augmented Generation (RAG) system, these updates can help your AI agents deliver faster, more accurate, and more context-aware responses from richer, better-organized data sources. The quality of your AI agent output depends on the data it can access. With the new web crawling feature, you can crawl publicly available websites and index content into your knowledge base , reducing the need for manual data collection. This is especially valuable for AI agents that rely on public web content to drive insights and actions. With web crawling, your AI agent can stay informed by pulling in real-time information. To help your agents make the most of that knowledge, we√¢¬Ä¬ôve significantly upgraded our Retrieval-Augmented Generation (RAG) system. These RAG enhancements help make your AI agents not just smarter, but also more precise, transparent, and useful across a wide range of real-world applications. We√¢¬Ä¬ôve also added Claude 3. 7 Sonnet to the GenAI Platform, giving you access to Anthropic√¢¬Ä¬ôs latest and most advanced reasoning-focused model. With an extended thinking mode for deeper analysis and more accurate answers to complex queries, Claude 3.</description></item><item><title>Meet our new AI-powered product documentation chatbot</title><link>https://kubermates.org/docs/2025-04-16-meet-our-new-ai-powered-product-documentation-chatbot/</link><pubDate>Wed, 16 Apr 2025 11:45:01 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-16-meet-our-new-ai-powered-product-documentation-chatbot/</guid><description>By Grace Morgan Searching for the right information in product documentation can be time consuming, especially when you√¢¬Ä¬ôre deep in the middle of development. That√¢¬Ä¬ôs why we√¢¬Ä¬ôre excited to introduce DigitalOcean√¢¬Ä¬ôs new product documentation chatbot , a new tool designed to help you quickly find accurate answers to your pressing product questions sourced directly from DigitalOcean√¢¬Ä¬ôs official documentation. Let√¢¬Ä¬ôs be honest, while documentation is essential, finding the exact answer you need often means clicking through multiple pages, refining search terms, or scrolling through long guides. Our new product documentation chatbot changes that by allowing you to ask natural-language questions and get instant, documentation-backed responses, helping you stay focused on building instead of searching. Using the chatbot is as simple as asking a question. Instead of digging through pages of documentation, just type what you need, like you would with a teammate, and get instant, sourced answers. No guesswork, no wasted time. Here√¢¬Ä¬ôs why it makes a difference: This isn√¢¬Ä¬ôt just another AI tool, it√¢¬Ä¬ôs built specifically to make your workflow smoother and your development process faster. Whether you√¢¬Ä¬ôre troubleshooting, configuring a Droplet, or exploring new features, the chatbot helps you get back to coding with confidence. The new product documentation chatbot is now live on docs. digitalocean. com.</description></item><item><title>How to get started with Calico Observability features</title><link>https://kubermates.org/docs/2025-04-15-how-to-get-started-with-calico-observability-features/</link><pubDate>Tue, 15 Apr 2025 15:37:28 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-15-how-to-get-started-with-calico-observability-features/</guid><description>Kubernetes, by default, adopts a permissive networking model where all pods can freely communicate unless explicitly restricted using network policies. While this simplifies application deployment, it introduces significant security risks. Unrestricted network traffic allows workloads to interact with unauthorized destinations, increasing the potential for cyberattacks such as Remote Code Execution (RCE), DNS spoofing, and privilege escalation. To better understand these problems, let‚Äôs examine a sample Kubernetes application: ANP Demo App. This application comprises a deployment that spawns pods and a service that exposes them to external users in a similar situation like any real word workload which you will encounter in your environment. If you open the application service before implementing any policies, the application reports the following messages: In this blog post we are going through a scenario to secure our cluster by preventing our workloads from accessing the external resources. Number 1 and 3 from the previous list. In Kubernetes, the default permissive networking model, where all pods can freely communicate, poses a significant security challenge. While network policies are crucial for enforcing a Zero Trust security model, identifying the necessary network flows for an application to function correctly can be difficult. Using CLI tools to inspect network traffic and deduce the required policies can be a complex and time-consuming task. It often involves sifting through large amounts of data and requires a deep understanding of network protocols and Kubernetes internals. Even for experienced administrators, accurately capturing all necessary flows without disrupting application functionality is a challenge.</description></item><item><title>What's New on the GenAI Platform</title><link>https://kubermates.org/docs/2025-04-14-what-s-new-on-the-genai-platform/</link><pubDate>Mon, 14 Apr 2025 19:02:28 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-14-what-s-new-on-the-genai-platform/</guid><description>By Grace Morgan DigitalOcean√¢¬Ä¬ôs GenAI Platform is now DigitalOcean Gradient Platform. To learn more about what√¢¬Ä¬ôs new, please check out our new blog - What√¢¬Ä¬ôs New on DigitalOcean Gradient Platform. Share Read more Read more Read more.</description></item><item><title>AWS managed policy updates</title><link>https://kubermates.org/releases/2025-04-14-aws-managed-policy-updates/</link><pubDate>Mon, 14 Apr 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-04-14-aws-managed-policy-updates/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. An AWS managed policy is a standalone policy that is created and administered by AWS. AWS managed policies are designed to provide permissions for many common use cases so that you can start assigning permissions to users, groups, and roles. Keep in mind that AWS managed policies might not grant least-privilege permissions for your specific use cases because they√¢¬Ä¬ôre available for all AWS customers to use. We recommend that you reduce permissions further by defining customer managed policies that are specific to your use cases. You cannot change the permissions defined in AWS managed policies. If AWS updates the permissions defined in an AWS managed policy, the update affects all principal identities (users, groups, and roles) that the policy is attached to. AWS is most likely to update an AWS managed policy when a new AWS service is launched or new API operations become available for existing services. For more information, see AWS managed policies in the IAM User Guide. You can attach the AmazonEKS_CNI_Policy to your IAM entities. Before you create an Amazon EC2 node group, this policy must be attached to either the node IAM role , or to an IAM role that√¢¬Ä¬ôs used specifically by the Amazon VPC CNI plugin for Kubernetes. This is so that it can perform actions on your behalf.</description></item><item><title>Expanding the GenAI Platform: Now supporting OpenAI models</title><link>https://kubermates.org/docs/2025-04-11-expanding-the-genai-platform-now-supporting-openai-models/</link><pubDate>Fri, 11 Apr 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-11-expanding-the-genai-platform-now-supporting-openai-models/</guid><description>By Grace Morgan DigitalOcean√¢¬Ä¬ôs GenAI Platform is now DigitalOcean Gradient Platform. Learn more about the GA release and features. We√¢¬Ä¬ôre excited to announce a major update to DigitalOcean√¢¬Ä¬ôs GenAI Platform: support for OpenAI models is now live ! You can now bring your own OpenAI API key to use models like GPT-4o, o1, and o3-mini with GenAI agents on DigitalOcean. Whether you√¢¬Ä¬ôre testing ideas in the model playground or deploying an agent into production, this gives you more flexibility when choosing the right model for your use case. GenAI Platform now supports OpenAI√¢¬Ä¬ôs industry-leading models, allowing you to bring your own OpenAI API key to integrate GPT-4o and reasoning models like o1 and o3-mini into your GenAI agents. Users choose OpenAI√¢¬Ä¬ôs models for their quality and comparatively advanced capabilities, making them a strong option for a wide range of AI applications. OpenAI models available: By supporting these models, DigitalOcean provides more flexibility to build AI-powered applications using OpenAI√¢¬Ä¬ôs technology while seamlessly integrating into our GenAI Platform. OpenAI on DigitalOcean√¢¬Ä¬ôs GenAI Platform gives developers a powerful, streamlined path to production-ready AI, no infrastructure headaches, no re-architecting. OpenAI is one of the most popular names in AI today, with models known for their performance, reliability, and capabilities. By integrating OpenAI models with DigitalOcean√¢¬Ä¬ôs GenAI Platform, you can: Don√¢¬Ä¬ôt have a key yet? No problem. You can now try out OpenAI models in the model playground , no API key required. Get a feel for how each model performs across your use cases before committing to them in production.</description></item><item><title>Announcing enhancements to per-bucket access keys and public preview of Spaces access logs</title><link>https://kubermates.org/docs/2025-04-08-announcing-enhancements-to-per-bucket-access-keys-and-public-preview-of-spaces-a/</link><pubDate>Tue, 08 Apr 2025 17:40:20 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-08-announcing-enhancements-to-per-bucket-access-keys-and-public-preview-of-spaces-a/</guid><description>By Anantha Ramachandran and Keshav Attrey At DigitalOcean, we continuously enhance our cloud storage solutions to empower developers and growing businesses. Today, we√¢¬Ä¬ôre excited to announce the general availability of DO API support for per-bucket access keys, mixed permissions support for per-bucket access keys, and the public preview of Spaces access logs, delivering greater automation, visibility, and security to DigitalOcean Spaces object storage. Building on the success of per-bucket access keys, we√¢¬Ä¬ôre introducing two major upgrades that are now generally available to all customers to streamline storage access management: DO API support for managing access keys √¢¬Ä¬ì Manage Spaces access keys programmatically using the DigitalOcean API, enabling automation through the DigitalOcean Terraform Provider, doctl CLI, DigitalOcean Go API Client (godo), and DigitalOcean√¢¬Ä¬ôs Python library (PyDo). More granular access control √¢¬Ä¬ì A single access key can now be configured with permissions that vary by bucket. This lets you grant read-only permissions for some buckets and read-write permissions for other buckets to a single person or application. These enhancements simplify storage management for customers handling large-scale deployments, automated backups, and security-driven workflows. Explore the documentation to use DO API and manage mixed-permissions access keys. You can start using these new features right now. Spaces access logs are now available in public preview to provide detailed records of read and write requests to your Spaces buckets, helping you to better understand usage and enhance security. Access logging √¢¬Ä¬ì Generate detailed records of reads, writes, and deletions of objects in your Spaces buckets, whether using Spaces origin endpoints or Spaces CDN endpoints. Detailed metadata √¢¬Ä¬ì Capture object paths, client IPs, and more. S3-compatible √¢¬Ä¬ì Logs are compatible with Amazon S3 server access log format, and logging is enabled using the S3-compatible PutBucketLogging API.</description></item><item><title>Aqua Security Achieves FedRAMP¬Æ High Authorization</title><link>https://kubermates.org/docs/2025-04-08-aqua-security-achieves-fedramp-high-authorization/</link><pubDate>Tue, 08 Apr 2025 12:35:09 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-08-aqua-security-achieves-fedramp-high-authorization/</guid><description>Aqua Security‚Äôs Cloud Native Application Protection Platform (CNAPP) has achieved FedRAMP¬Æ High Impact Authorization, making Aqua one of the few CNAPP providers authorized at the highest level of federal cloud security compliance. This milestone opens the door for U. S. federal agencies, commercial organizations that require FedRAMP High, and cloud service providers operating in FedRAMP-authorized environments to confidently use Aqua‚Äôs platform for securing their cloud native applications. We met more than 400 rigorous security controls, giving federal agencies and commercial organizations the peace of mind that they can adopt cloud native technologies while meeting the highest security standards. FedRAMP (Federal Risk and Authorization Management Program) is a United States government-wide program that standardizes security assessments for cloud products and services. FedRAMP High is the most stringent level, designed for cloud services that manage highly sensitive data related to national security, public health, and other sensitive government functions. Achieving this level of authorization involves an extensive review of security controls, practices, and monitoring processes. The result: confidence that the platform is built to withstand today‚Äôs complex cyber threats. However, FedRAMP isn‚Äôt just about security; it also plays a critical role in compliance. It helps agencies meet a wide range of federal mandates and executive directives, such as NIST 800-53 standards, Executive Order 14028 on improving the nation‚Äôs cybersecurity, and OMB memoranda like M-22-09. By aligning cloud solutions with these policies, FedRAMP High Authorization ensures that agencies not only deploy secure technology but also do so in a manner consistent with federal oversight and accountability frameworks.</description></item><item><title>Introducing kube-scheduler-simulator</title><link>https://kubermates.org/docs/2025-04-07-introducing-kube-scheduler-simulator/</link><pubDate>Mon, 07 Apr 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-07-introducing-kube-scheduler-simulator/</guid><description>The Kubernetes Scheduler is a crucial control plane component that determines which node a Pod will run on. Thus, anyone utilizing Kubernetes relies on a scheduler. kube-scheduler-simulator is a simulator for the Kubernetes scheduler, that started as a Google Summer of Code 2021 project developed by me (Kensei Nakada) and later received a lot of contributions. This tool allows users to closely examine the scheduler‚Äôs behavior and decisions. It is useful for casual users who employ scheduling constraints (for example, inter-Pod affinity ) and experts who extend the scheduler with custom plugins. The scheduler often appears as a black box, composed of many plugins that each contribute to the scheduling decision-making process from their unique perspectives. Understanding its behavior can be challenging due to the multitude of factors it considers. Even if a Pod appears to be scheduled correctly in a simple test cluster, it might have been scheduled based on different calculations than expected. This discrepancy could lead to unexpected scheduling outcomes when deployed in a large production environment. Also, testing a scheduler is a complex challenge. There are countless patterns of operations executed within a real cluster, making it unfeasible to anticipate every scenario with a finite number of tests. More often than not, bugs are discovered only when the scheduler is deployed in an actual cluster.</description></item><item><title>The Linux Foundation Announces Schedule for Open Source Summit North America 2025</title><link>https://kubermates.org/events/2025-04-03-the-linux-foundation-announces-schedule-for-open-source-summit-north-america-202/</link><pubDate>Thu, 03 Apr 2025 14:29:50 +0000</pubDate><guid>https://kubermates.org/events/2025-04-03-the-linux-foundation-announces-schedule-for-open-source-summit-north-america-202/</guid><description>&lt;p&gt;Kicking off in Denver this year, one of the largest open source events in North America will bring ‚Ä¶&lt;/p&gt;</description></item><item><title>Sharks of DigitalOcean: Makeissah Robinson, Senior Director, Customer Support</title><link>https://kubermates.org/docs/2025-04-03-sharks-of-digitalocean-makeissah-robinson-senior-director-customer-support/</link><pubDate>Thu, 03 Apr 2025 04:15:54 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-03-sharks-of-digitalocean-makeissah-robinson-senior-director-customer-support/</guid><description>By Sujatha R Technical Writer At DigitalOcean, customers are at the heart of what we do, and Makeissah Robinson, our Senior Director of Customer Support, embodies this commitment. She leads her team with a focus on customer success and continuous improvement. Let√¢¬Ä¬ôs dive into Makeissah√¢¬Ä¬ôs leadership journey√¢¬Ä¬îone that blends innovation, cultural inclusivity, and hands-on customer advocacy. I lead the global customer support and service team. We wake up and go to bed thinking about the customer. Whether you√¢¬Ä¬ôre a student new to the cloud, a developer creating the next great application or a growing tech business looking to build on a platform that provides white-glove service, we√¢¬Ä¬ôre here to make sure our customers and developers are successful on the DigitalOcean platform. We√¢¬Ä¬ôre also here to listen to and echo their needs within the organization. I love the diversity in our working environment and the diversity in our employees. It√¢¬Ä¬ôs really great to have different perspectives, and, as a global company, I get to learn a lot about other people√¢¬Ä¬ôs cultures, which is also awesome. I also appreciate that our values promote creativity and innovation√¢¬Ä¬îthinking big and bold. There√¢¬Ä¬ôs a strong focus on continuous learning and investing in yourself and your growth. I√¢¬Ä¬ôm always learning something new, whether it√¢¬Ä¬ôs meeting new people, learning new technology, or discovering how our customers are using our products and services.</description></item><item><title>Introducing DigitalOcean Partner Network Connect: Secure, High-Performance Multi-Cloud Connectivity</title><link>https://kubermates.org/docs/2025-04-02-introducing-digitalocean-partner-network-connect-secure-high-performance-multi-c/</link><pubDate>Wed, 02 Apr 2025 12:30:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-02-introducing-digitalocean-partner-network-connect-secure-high-performance-multi-c/</guid><description>By Anantha Ramachandran and Udhay Ravindran Growing businesses increasingly rely on multiple cloud providers and on-premise infrastructure to scale their applications. However, connecting different cloud environments securely and efficiently is a major headache. Many businesses struggle with unpredictable latency and complex VPN setups to keep their distributed workloads running smoothly. Without a seamless solution, managing cloud connectivity becomes a costly and time-consuming burden. At DigitalOcean, we√¢¬Ä¬ôre committed to helping to eliminate these challenges by making cloud networking simpler, more scalable, and more accessible. Today, we√¢¬Ä¬ôre excited to introduce the general availability of DigitalOcean Partner Network Connect, a cross-cloud connectivity solution that enables more secure, high-performance networking between DigitalOcean and other cloud providers or on-premise data centers. With Partner Network Connect, growing businesses can establish direct, private connections between DigitalOcean and other cloud providers or on-premise data centers or co-location facilities√¢¬Ä¬îbypassing the public internet to help improve security, minimize latency, and increase network performance. By integrating with Megaport, a leading Network-as-a-Service (NaaS) provider, we√¢¬Ä¬ôre helping our customers connect their distributed workloads across multiple clouds easily, reliably, and affordably. As businesses scale, they often deploy workloads across multiple cloud providers or maintain hybrid cloud environments. Until now, cloud customers had to rely on VPNs or self-managed solutions to connect these environments, which can be complex to set up, costly, and difficult to maintain. DigitalOcean Partner Network Connect helps eliminate these challenges for cloud-to-cloud and on-premise connectivity. Key benefits include: Improved latency: Low latency, high-bandwidth private connectivity (1 Gbps to 10 Gbps) for faster, more reliable performance across cloud and on-premise environments.</description></item><item><title>Tomcat in the Crosshairs: New Research Reveals Ongoing Attacks</title><link>https://kubermates.org/docs/2025-04-02-tomcat-in-the-crosshairs-new-research-reveals-ongoing-attacks/</link><pubDate>Wed, 02 Apr 2025 12:00:49 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-02-tomcat-in-the-crosshairs-new-research-reveals-ongoing-attacks/</guid><description>News headlines reported that it took just 30 hours for attackers to exploit a newly discovered vulnerability in Apache Tomcat servers. But what does this mean for workloads relying on Tomcat? Aqua Nautilus researchers discovered a new attack campaign targeting Apache Tomcat. In this blog, we shed light on newly discovered malware that targets Tomcat servers to hijack resources. After gaining initial access, the attackers uploads encrypted and encoded payloads that establish backdoors and persistence mechanisms. They then deploy two binaries disguised as kernel processes to exploit the server. The attack infrastructure appears to be relatively new, and code snippets suggest possible links to a Chinese-speaking threat actor. In this blog, we break down how the attack works‚Äîand how to stop it. The campaign targets Apache Tomcat servers and deploys encrypted payloads designed to run on both Windows and Linux systems. Once executed, the attack disguises itself, steals SSH credentials to spread laterally, and ultimately hijacks resources for cryptocurrency mining. It all starts with a brute-force attempt from a remote server using a Python script, which tests commonly used usernames and weak passwords on the Tomcat management console (e. g. , username ‚ÄúTomcat‚Äù and password ‚Äú123456‚Äù).</description></item><item><title>Introducing AMD Instinct MI300X GPUs to the DigitalOcean Bare Metal fleet</title><link>https://kubermates.org/docs/2025-04-01-introducing-amd-instinct-mi300x-gpus-to-the-digitalocean-bare-metal-fleet/</link><pubDate>Tue, 01 Apr 2025 21:42:26 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-01-introducing-amd-instinct-mi300x-gpus-to-the-digitalocean-bare-metal-fleet/</guid><description>By Waverly Swinton Bare Metal GPUs are now DigitalOcean GradientAI Bare Metal GPUs. Learn more about DigitalOcean GradientAI , our suite of AI products. We√¢¬Ä¬ôre excited to announce that DigitalOcean customers now have access to AMD Instinct MI300X GPUs with ROCm Software to power their AI workloads. At DigitalOcean, we√¢¬Ä¬ôre committed to bringing you even more options to power your projects. One of the highest-bandwith GPUs (5. 3 TB/s of HBM3 memory), AMD Instinct MI300X offers significant benefits like faster deployments and much more for AI, machine learning, and high-performance computing (HPC) workloads. Along with expanding access to GPUs, we continue to improve on our existing offerings for developers like you. Whether you√¢¬Ä¬ôre deploying popular AI models with our zero-configuration setup that reduces deployment time from weeks to minutes or managing your AI budget with our simple pricing structure , DigitalOcean can help make your AI journey simple, scalable, and cost-effective. AMD Instinct MI300X GPUs are designed to boost AI and HPC capabilities in a new compact, efficient package. These next-generation GPUs are now available on DigitalOcean in single-tenant Bare Metal configurations for customers seeking increased control and computing power. MI300X offers raw acceleration power with eight GPUs per node, leading compute unit counts, and HBM3 capacity. With MI300X, you can accelerate your deployment efforts with ROCm developer resources that support multiple AI and HPC frameworks, leading software platforms and models, and networking infrastructure within an open, proven software ecosystem.</description></item><item><title>The Next Evolution of DigitalOcean Kubernetes: Introducing Features that Unlock Superior Scalability for Growing Businesses</title><link>https://kubermates.org/docs/2025-03-31-the-next-evolution-of-digitalocean-kubernetes-introducing-features-that-unlock-s/</link><pubDate>Mon, 31 Mar 2025 19:59:14 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-31-the-next-evolution-of-digitalocean-kubernetes-introducing-features-that-unlock-s/</guid><description>By Nicole Ghalwash Kubernetes is the foundation of many modern applications, providing the scalability and resilience needed for today√¢¬Ä¬ôs dynamic workloads. However, as applications grow, so do their infrastructure and scaling requirements. Managing multiple clusters to handle large-scale workloads introduces operational complexity, increased network management, and challenges for the DevOps teams. These challenges can include greater resource fragmentation, increased latency due to inter-cluster communication, and the manual burden of maintaining security and compliance across multiple clusters. These challenges are particularly pronounced for data-intensive workloads like video streaming, large-scale data analytics, and security operations. That√¢¬Ä¬ôs why we√¢¬Ä¬ôre excited to announce a major evolution of DigitalOcean Kubernetes Service (DOKS), which consists of four features: Increased cluster capacity to 1,000 worker nodes per cluster Optimized networking with VPC-native Kubernetes Improved performance with eBPF-powered networking Managed Cilium for high-performance networking These updates help to empower businesses to run larger workloads within a single DOKS cluster, reducing the need for complex multi-cluster management and unlocking new levels of performance, reliability, and simplicity. NoBid, a Kubernetes customer who joined us at Deploy 2025, has seen significant improvements in scalability and security when deploying DOKS. √¢¬Ä¬úDigitalOcean Kubernetes provides NoBid with a powerful platform for our containerized workloads,&amp;quot; said Shawn Petersen, CIO at NoBid. √¢¬Ä¬úAdditionally, the ability to rapidly scale based on business demand, handle 1. 3 PB/month in data egress, along with better security controls were all key differentiators for us to partner with DigitalOcean. &amp;quot; If you want to learn more about NoBid√¢¬Ä¬ôs use case, check out their case study. Also, you can learn more about their migration from AWS to DigitalOcean and their subsequent 30% cost-savings by watching the testimonial they gave at Deploy 2025.</description></item><item><title>EKS Hybrid Nodes for existing clusters</title><link>https://kubermates.org/releases/2025-03-31-eks-hybrid-nodes-for-existing-clusters/</link><pubDate>Mon, 31 Mar 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-03-31-eks-hybrid-nodes-for-existing-clusters/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. This topic provides an overview of the available options and describes what to consider when you add, change, or remove the hybrid nodes configuration for an Amazon EKS cluster. To enable an Amazon EKS cluster to use hybrid nodes, add the IP address CIDR ranges of your on-premises node and optionally pod network in the RemoteNetworkConfig configuration. EKS uses this list of CIDRs to enable connectivity between the cluster and your on-premises networks. For a full list of options when updating your cluster configuration, see the UpdateClusterConfig in the Amazon EKS API Reference. You can do any of the following actions to the EKS Hybrid Nodes networking configuration in a cluster: Add remote network configuration to enable EKS Hybrid Nodes in an existing cluster. Add, change, or remove the remote node networks or the remote pod networks in an existing cluster. Remove all remote node network CIDR ranges to disable EKS Hybrid Nodes in an existing cluster. Before enabling your Amazon EKS cluster for hybrid nodes, ensure your environment meets the requirements outlined at Prerequisite setup for hybrid nodes , and detailed at Prepare networking for hybrid nodes , Prepare operating system for hybrid nodes , and Prepare credentials for hybrid nodes. Your cluster must use IPv4 address family. Your cluster must use either API or API_AND_CONFIG_MAP for the cluster authentication mode. The process for modifying the cluster authentication mode is described at Change authentication mode to use access entries.</description></item><item><title>Node health for EKS Hybrid Nodes</title><link>https://kubermates.org/releases/2025-03-31-node-health-for-eks-hybrid-nodes/</link><pubDate>Mon, 31 Mar 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-03-31-node-health-for-eks-hybrid-nodes/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Node health refers to the operational status and capability of a node to effectively run workloads. A healthy node maintains expected connectivity, has sufficient resources, and can successfully run Pods without disruption. For information on getting details about your nodes, see View the health status of your nodes and Retrieve node logs for a managed node using kubectl and S3. To help with maintaining healthy nodes, Amazon EKS offers the node monitoring agent and node auto repair. The node monitoring agent and node auto repair are only available on Linux. These features aren√¢¬Ä¬ôt available on Windows. The node monitoring agent automatically reads node logs to detect certain health issues. It parses through node logs to detect failures and surfaces various status information about worker nodes. A dedicated NodeCondition is applied on the worker nodes for each category of issues detected, such as storage and networking issues. Descriptions of detected health issues are made available in the observability dashboard. For more information, see Node health issues.</description></item><item><title>Rollback: Prevent accidental upgrades with cluster insights</title><link>https://kubermates.org/releases/2025-03-28-rollback-prevent-accidental-upgrades-with-cluster-insights/</link><pubDate>Fri, 28 Mar 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-03-28-rollback-prevent-accidental-upgrades-with-cluster-insights/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. When a new Kubernetes version is available in Amazon EKS, you can update your Amazon EKS cluster to the latest version. Once you upgrade a cluster, you can√¢¬Ä¬ôt downgrade to a previous version. Before you update to a new Kubernetes version, we recommend that you review the information in Understand the Kubernetes version lifecycle on EKS and the update steps in this topic. New Kubernetes versions sometimes introduce significant changes. Therefore, we recommend that you test the behavior of your applications against a new Kubernetes version before you update your production clusters. You can do this by building a continuous integration workflow to test your application behavior before moving to a new Kubernetes version. The update process consists of Amazon EKS launching new API server nodes with the updated Kubernetes version to replace the existing ones. Amazon EKS performs standard infrastructure and readiness health checks for network traffic on these new nodes to verify that they√¢¬Ä¬ôre working as expected. However, once you√¢¬Ä¬ôve started the cluster upgrade, you can√¢¬Ä¬ôt pause or stop it. If any of these checks fail, Amazon EKS reverts the infrastructure deployment, and your cluster remains on the prior Kubernetes version. Running applications aren√¢¬Ä¬ôt affected, and your cluster is never left in a non-deterministic or unrecoverable state.</description></item><item><title>Bottlerocket FIPS AMIs</title><link>https://kubermates.org/releases/2025-03-27-bottlerocket-fips-amis/</link><pubDate>Thu, 27 Mar 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-03-27-bottlerocket-fips-amis/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. The Federal Information Processing Standard (FIPS) Publication 140-3 is a United States and Canadian government standard that specifies the security requirements for cryptographic modules that protect sensitive information. Bottlerocket makes it easier to adhere to FIPS by offering AMIs with a FIPS kernel. These AMIs are preconfigured to use FIPS 140-3 validated cryptographic modules. This includes the Amazon Linux 2023 Kernel Crypto API Cryptographic Module and the AWS-LC Cryptographic Module. Using Bottlerocket FIPS AMIs makes your worker nodes &amp;ldquo;FIPS ready&amp;rdquo; but not automatically &amp;ldquo;FIPS-compliant&amp;rdquo;. For more information, see Federal Information Processing Standard (FIPS) 140-3. If your cluster uses isolated subnets, the Amazon ECR FIPS endpoint may not be accessible. This can cause the node bootstrap to fail. Make sure that your network configuration allows access to the necessary FIPS endpoints. For more information, see Access a resource through a resource VPC endpoint in the AWS PrivateLink Guide. If your cluster uses a subnet with PrivateLink , image pulls will fail because Amazon ECR FIPS endpoints are not available through PrivateLink.</description></item><item><title>Cut Through Alert Noise and Fix Toxic Combinations First</title><link>https://kubermates.org/docs/2025-03-27-cut-through-alert-noise-and-fix-toxic-combinations-first/</link><pubDate>Thu, 27 Mar 2025 17:13:45 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-27-cut-through-alert-noise-and-fix-toxic-combinations-first/</guid><description>Not every security alert is a threat, but the right combination can bring down your cloud native and containerized applications. Security incidents rarely happen because of a single weak point. Instead, they stem from toxic combinations. A misconfigured workload might seem harmless on its own, but add exposed credentials and an unpatched vulnerability, and attackers have a direct path to exploitation. Traditional vulnerability scanners surface thousands of issues, yet many tools treat vulnerabilities, misconfigurations, malware, and exposed credentials as isolated problems rather than recognizing how they can combine to create real attack scenarios Without understanding the full attack surface and how risks interact, security teams end up chasing alerts instead of preventing breaches. Instead of drowning in alerts, you need context, a way to connect security findings across the entire cloud native application lifecycle. Aqua provides that visibility, correlating risks from the first line of code to runtime so security teams can focus on what is actually exploitable, not just what is flagged. But risk prioritization is not just about what exists, it is about what an attacker can actually access and exploit. A vulnerability might seem critical, but is it isolated in a test environment, restricted within an internal network, or exposed in a production system accessible from the internet? For example, a banking application with an unpatched critical vulnerability might seem like an urgent issue, but if it is running in a segmented test environment, the risk is far lower than if the same vulnerability exists in a publicly accessible production system. Aqua assesses network exposure, identifying whether a CVE is just a theoretical risk or if it is publicly accessible and exploitable. By linking misconfigurations, exposed credentials, and network exposure with known vulnerabilities, Aqua surfaces toxic combinations that create real world attack paths. Aqua also connects these risks across hybrid and multi-cloud environments, ensuring teams have a comprehensive view of their attack surface, no matter where workloads are running.</description></item><item><title>Kubernetes v1.33 sneak peek</title><link>https://kubermates.org/docs/2025-03-26-kubernetes-v1-33-sneak-peek/</link><pubDate>Wed, 26 Mar 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-03-26-kubernetes-v1-33-sneak-peek/</guid><description>As the release of Kubernetes v1. 33 approaches, the Kubernetes project continues to evolve. Features may be deprecated, removed, or replaced to improve the overall health of the project. This blog post outlines some planned changes for the v1. 33 release, which the release team believes you should be aware of to ensure the continued smooth operation of your Kubernetes environment and to keep you up-to-date with the latest developments. The information below is based on the current status of the v1. 33 release and is subject to change before the final release date. The Kubernetes project has a well-documented deprecation policy for features. This policy states that stable APIs may only be deprecated when a newer, stable version of that same API is available and that APIs have a minimum lifetime for each stability level. A deprecated API has been marked for removal in a future Kubernetes release. It will continue to function until removal (at least one year from the deprecation), but usage will result in a warning being displayed. Removed APIs are no longer available in the current version, at which point you must migrate to using the replacement.</description></item><item><title>IngressNightmare Vulnerabilities: All You Need to Know</title><link>https://kubermates.org/docs/2025-03-26-ingressnightmare-vulnerabilities-all-you-need-to-know/</link><pubDate>Wed, 26 Mar 2025 13:15:57 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-26-ingressnightmare-vulnerabilities-all-you-need-to-know/</guid><description>CVE-2025-1974 * CVE-2025-24514 * CVE-2025-1097 * CVE-2025-1098 * CVE-2025-24513 On March 24, 2025, a set of critical vulnerabilities (CVE-2025-1097, CVE-2025-1098, CVE-2025-24514, and CVE-2025-1974 ‚Äî collectively referred to as IngressNightmare was disclosed in the ingress-nginx Controller for Kubernetes. These vulnerabilities could lead to a complete cluster takeover by granting attackers unauthorized access to all secrets stored across all namespaces in the Kubernetes cluster. In Kubernetes, an Ingress Controller manages external access to services within a cluster, typically via HTTP or HTTPS. The ingress NGINX Controller, built on the NGINX web server , is widely used to route incoming traffic to the appropriate backend services based on defined rules. The disclosed vulnerabilities include: CVE-2025-1974 (CVSS Score 9. 8 Critical) Allows unauthenticated attackers with pod network access to execute arbitrary code in the ingress-nginx controller, potentially leading to full cluster takeover. CVE-2025-24514, CVE-2025-1097, CVE-2025-1098 (CVSS Score 8. 8 High) Involve improper handling of Ingress annotations that can lead to code execution or unauthorized data access. CVE-2025-24513 (CVSS Score 4. 8 Medium) Involves directory traversal that can lead to DoS or limited secret disclosure. The researchers who found these vulnerabilities indicated that over 40% of cloud environments were vulnerable to these remote code execution (RCE) risks. Their analysis discovered over 6,500 clusters, including those of Fortune 500 companies, that publicly expose the admission controllers of vulnerable Kubernetes ingress controllers to the public internet, placing them at immediate critical risk.</description></item><item><title>Kubeflow 1.10 Release Announcement</title><link>https://kubermates.org/docs/2025-03-26-kubeflow-1-10-release-announcement/</link><pubDate>Wed, 26 Mar 2025 00:00:00 -0500</pubDate><guid>https://kubermates.org/docs/2025-03-26-kubeflow-1-10-release-announcement/</guid><description>Mar 26, 2025 ‚Ä¢ Kubeflow 1. 10 Release Team, Dimitris Poulopoulos ‚Ä¢ 8 min read release Kubeflow 1. 10. 0 delivers essential updates that enhance the flexibility, efficiency, and scalability of machine learning workflows. The new features span across several components, improving both user experience and system performance. The Kubeflow Platform Working Group focuses on simplifying Kubeflow installation, operations, and security. See details below. Trivy CVE scans March 25 2025: Kubeflow Pipelines 2. 4. 1 introduces support for placeholders in resource limits , enhancing flexibility in pipeline execution. This update allows users to define dynamic resource limits using parameterized values, enabling more adaptable and reusable pipeline definitions. Kubeflow Pipelines 2.</description></item><item><title>Fresh Swap Features for Linux Users in Kubernetes 1.32</title><link>https://kubermates.org/docs/2025-03-25-fresh-swap-features-for-linux-users-in-kubernetes-1-32/</link><pubDate>Tue, 25 Mar 2025 10:00:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-03-25-fresh-swap-features-for-linux-users-in-kubernetes-1-32/</guid><description>Swap is a fundamental and an invaluable Linux feature. It offers numerous benefits, such as effectively increasing a node‚Äôs memory by swapping out unused data, shielding nodes from system-level memory spikes, preventing Pods from crashing when they hit their memory limits, and much more. As a result, the node special interest group within the Kubernetes project has invested significant effort into supporting swap on Linux nodes. The 1. 22 release introduced Alpha support for configuring swap memory usage for Kubernetes workloads running on Linux on a per-node basis. Later, in release 1. 28, support for swap on Linux nodes has graduated to Beta, along with many new improvements. In the following Kubernetes releases more improvements were made, paving the way to GA in the near future. Prior to version 1. 22, Kubernetes did not provide support for swap memory on Linux systems. This was due to the inherent difficulty in guaranteeing and accounting for pod memory utilization when swap memory was involved. As a result, swap support was deemed out of scope in the initial design of Kubernetes, and the default behavior of a kubelet was to fail to start if swap memory was detected on a node.</description></item><item><title>Ingress-nginx CVE-2025-1974: What You Need to Know</title><link>https://kubermates.org/docs/2025-03-24-ingress-nginx-cve-2025-1974-what-you-need-to-know/</link><pubDate>Mon, 24 Mar 2025 12:00:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-03-24-ingress-nginx-cve-2025-1974-what-you-need-to-know/</guid><description>Today, the ingress-nginx maintainers have released patches for a batch of critical vulnerabilities that could make it easy for attackers to take over your Kubernetes cluster: ingress-nginx v1. 12. 1 and ingress-nginx v1. 11. 5. If you are among the over 40% of Kubernetes administrators using ingress-nginx , you should take action immediately to protect your users and data. Ingress is the traditional Kubernetes feature for exposing your workload Pods to the world so that they can be useful. In an implementation-agnostic way, Kubernetes users can define how their applications should be made available on the network. Then, an ingress controller uses that definition to set up local or cloud resources as required for the user‚Äôs particular situation and needs. Many different ingress controllers are available, to suit users of different cloud providers or brands of load balancers. Ingress-nginx is a software-only ingress controller provided by the Kubernetes project. Because of its versatility and ease of use, ingress-nginx is quite popular: it is deployed in over 40% of Kubernetes clusters! Ingress-nginx translates the requirements from Ingress objects into configuration for nginx, a powerful open source webserver daemon.</description></item><item><title>How the Google-Wiz acquisition redefines cloud security</title><link>https://kubermates.org/docs/2025-03-24-how-the-google-wiz-acquisition-redefines-cloud-security/</link><pubDate>Mon, 24 Mar 2025 16:36:45 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-24-how-the-google-wiz-acquisition-redefines-cloud-security/</guid><description>Google‚Äôs acquisition of Wiz, announced last week, is a pivotal moment as it marks a strategic shift in how cyber security will evolve over the next few years. It instantly turns Google into a major player in security, adding Wiz to other building blocks Google has racked up in the past couple of years, most notably Mandiant and Google Chronicle. Google will be a new gorilla in the security market, just as Microsoft created a thriving, multi-billion dollar business out of the Defender product line. But while Microsoft has a long history of ‚Äúowning‚Äù operating systems, servers, and Office applications, and is leveraging that to expand its footprint, Google is new to the enterprise security business. In the first phase of public cloud adoption, the ‚Äúlift and shift‚Äù phase, CSPM (cloud security posture management) emerged to ensure proper configuration of cloud services, and CWPP (cloud workload protection platforms) to monitor and protect workloads running in the cloud. The second phase of cloud adoption, the cloud native phase, driven by technologies such as containers, serverless functions, CI/CD, and orchestration (Kubernetes), imparted an even more dramatic change in security ‚Äì the integration of multiple silos of application related security information. It introduced integrated shift left capabilities , providing a broader risk-based approach to vulnerability management, hardening, and incident management. Thus CNAPP (cloud native application protection platforms) was born. In the early days of cloud services, the cloud providers adopted the shared responsibility model in which some responsibilities were borne by them (such as infrastructure and physical security), others were clearly on the shoulders of customers (configuring of services, their data, their users, their applications), and there‚Äôs been a non-negligible area of ‚Äúshared responsibility‚Äù where the answer is often ‚Äúit depends‚Äù. I believe that with this move, Google is shifting the borders within this model and will be offering more of the posture management and visibility to customers as part of its infrastructure. After all this is what Wiz has become famous for ‚Äì agentless scanning of the cloud estate, providing broad visibility and risk assessment. And this is something a cloud provider can easily integrate into cloud operations.</description></item><item><title>Introducing JobSet</title><link>https://kubermates.org/docs/2025-03-23-introducing-jobset/</link><pubDate>Sun, 23 Mar 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-23-introducing-jobset/</guid><description>Authors : Daniel Vega-Myhre (Google), Abdullah Gharaibeh (Google), Kevin Hannon (Red Hat) In this article, we introduce JobSet , an open source API for representing distributed jobs. The goal of JobSet is to provide a unified API for distributed ML training and HPC workloads on Kubernetes. The Kubernetes community‚Äôs recent enhancements to the batch ecosystem on Kubernetes has attracted ML engineers who have found it to be a natural fit for the requirements of running distributed training workloads. Large ML models (particularly LLMs) which cannot fit into the memory of the GPU or TPU chips on a single host are often distributed across tens of thousands of accelerator chips, which in turn may span thousands of hosts. As such, the model training code is often containerized and executed simultaneously on all these hosts, performing distributed computations which often shard both the model parameters and/or the training dataset across the target accelerator chips, using communication collective primitives like all-gather and all-reduce to perform distributed computations and synchronize gradients between hosts. These workload characteristics make Kubernetes a great fit for this type of workload, as efficiently scheduling and managing the lifecycle of containerized applications across a cluster of compute resources is an area where it shines. It is also very extensible, allowing developers to define their own Kubernetes APIs, objects, and controllers which manage the behavior and life cycle of these objects, allowing engineers to develop custom distributed training orchestration solutions to fit their needs. However, as distributed ML training techniques continue to evolve, existing Kubernetes primitives do not adequately model them alone anymore. Furthermore, the landscape of Kubernetes distributed training orchestration APIs has become fragmented, and each of the existing solutions in this fragmented landscape has certain limitations that make it non-optimal for distributed ML training. For example, the KubeFlow training operator defines custom APIs for different frameworks (e. g. PyTorchJob, TFJob, MPIJob, etc.</description></item><item><title>Powering AI Innovation: DigitalOcean Bare Metal GPUs in EU Data Center</title><link>https://kubermates.org/docs/2025-03-20-powering-ai-innovation-digitalocean-bare-metal-gpus-in-eu-data-center/</link><pubDate>Thu, 20 Mar 2025 14:59:45 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-20-powering-ai-innovation-digitalocean-bare-metal-gpus-in-eu-data-center/</guid><description>By DigitalOcean Bare Metal GPUs are now DigitalOcean GradientAI Bare Metal GPUs. Learn more about DigitalOcean GradientAI , our suite of AI products. Whether you√¢¬Ä¬ôre building a generative video platform like Moonvalley or an advanced coding assistant like Supermaven , GPU computing is now a non-negotiable for engineering AI applications. Whether it√¢¬Ä¬ôs for training LLMs or powering real-time inference, these processors have become essential infrastructure for startups working on AI products. DigitalOcean is powering this development by providing a number of AI/ML offerings, including Bare Metal GPUs √¢¬Ä¬îdedicated machines with NVIDIA Hopper architecture for the most demanding AI workloads. These computing resources are available in our European data center based in Amsterdam, Netherlands, giving EU-based companies (or international businesses with EU customers) direct access to high-performance AI infrastructure right where they need it. Build your AI projects with DigitalOcean√¢¬Ä¬ôs Bare Metal GPUs in Amsterdam. Reserve capacity to experience the power of NVIDIA HGX H100 with up to 640 GB of GPU RAM and enhanced NVLink for multi-GPU scaling. Contact our experts to talk through your workload needs and join innovative companies who are already making use of our high-performance, dedicated infrastructure for their AI/ML applications. When it comes to AI applications, milliseconds matter. Placing your inference workloads physically closer to your end users reduces the time it takes for requests to travel to your servers and for responses to return. With DigitalOcean√¢¬Ä¬ôs Bare Metal GPUs in Amsterdam, companies serving European markets can cut latency compared to running the same workloads in data centers outside of Europe.</description></item><item><title>Scale into the stratosphere: Managed MySQL &amp; PostgreSQL now support up to 20TB and 30TB</title><link>https://kubermates.org/docs/2025-03-19-scale-into-the-stratosphere-managed-mysql-postgresql-now-support-up-to-20tb-and-/</link><pubDate>Wed, 19 Mar 2025 17:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-19-scale-into-the-stratosphere-managed-mysql-postgresql-now-support-up-to-20tb-and-/</guid><description>By Nicole Ghalwash Good news for growing businesses and folks with data-intensive applications: we√¢¬Ä¬ôre excited to introduce 100% larger storage plans for both Managed MySQL and PostgreSQL in select regions. Starting today, Managed MySQL storage has doubled to 20TB, and Managed PostgreSQL now supports up to 30TB, up from 15TB. These expanded storage options are available in Singapore (SGP1), San Francisco (SFO2), and Frankfurt (FRA1), with plans to roll out to more regions later this year. Now users have even more flexibility to scale seamlessly, whether growing existing workloads or migrating from self-managed databases. With DigitalOcean√¢¬Ä¬ôs fully managed service, you get the performance, security, and simplicity you need√¢¬Ä¬îwithout the operational burden. Scaling your databases is now easier than ever, helping to ensure your infrastructure keeps pace with your business growth. As your businesses grow, so do data storage requirements. Our expanded, scalable storage options allow you to: Big databases, no worries: Support large databases with up to 20TB and 30TB of scalable storage. Control both performance and costs: Scale storage independently of CPU and memory, giving you the flexibility to adapt as your workload evolves. This ensures you pay only for what you need while avoiding over- or under-provisioning resources. Migrate like magic: Move from self-managed databases on Droplets or other cloud providers to DigitalOcean Managed Databases without storage constraints. Every business and every developer managing large-scale applications, analytics platforms, e-commerce sites, and any other workload requiring lots of storage capacity.</description></item><item><title>GPU Droplets Achieve SOC 2 Compliance</title><link>https://kubermates.org/docs/2025-03-17-gpu-droplets-achieve-soc-2-compliance/</link><pubDate>Mon, 17 Mar 2025 22:08:32 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-17-gpu-droplets-achieve-soc-2-compliance/</guid><description>By David Lopez Staff Trust &amp;amp; Governance Advisor, Trust and Governance GPU Droplets are now DigitalOcean GradientAI GPU Droplets. Learn more about DigitalOcean GradientAI , our suite of AI products. Today, we√¢¬Ä¬ôre excited to announce that GPU Droplets join the suite of DigitalOcean√¢¬Ä¬ôs Service Organization Control (SOC) 2 Type II compliant products. With this achievement, customers can experiment, train, and scale AI projects on GPU Droplets with trust in DigitalOcean√¢¬Ä¬ôs ongoing commitment to protect customers√¢¬Ä¬ô sensitive information. Adhering to SOC 2 remains a critical part of DigitalOcean√¢¬Ä¬ôs operations given our commitment to security and simplicity. This means that when you build on DigitalOcean, you build on a foundation of proven security practices and controls. As an example, consider the following: Security: Based on the cloud product in use, customers inherit a portion of DigitalOcean√¢¬Ä¬ôs security posture under the Shared Responsibility Model. By using a SOC 2 compliant product, customers can be more confident that they can manage their risk footprint to the requirements of that standard. Simplicity: Many regulatory regimes have overlapping compliance requirements as the SOC 2, therefore helping you further enhance your own compliance objectives. SOC 2 is one of three SOC standards ( SOC 1, SOC 2, and SOC 3 ) developed by the American Institute of Certified Public Accountants (AICPA) which assess an organization√¢¬Ä¬ôs controls against the applicable Trust Services Criteria, which include some combination of the following: Security, availability, processing integrity, confidentiality, and privacy. For a deep dive into SOC 2 compliance, please see this article on The Wave. The SOC 3 Type II report summarizes our SOC 2 Type II report and is readily available for public review.</description></item><item><title>Supply Chain Security Risk: GitHub Action tj-actions/changed-files Compromised</title><link>https://kubermates.org/docs/2025-03-16-supply-chain-security-risk-github-action-tj-actions-changed-files-compromised/</link><pubDate>Sun, 16 Mar 2025 15:56:32 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-16-supply-chain-security-risk-github-action-tj-actions-changed-files-compromised/</guid><description>CVE-2025-30066 On March 14th, 2025, security researchers discovered a critical software supply chain vulnerability in the widely-used GitHub Action tj-actions/changed-files ( CVE-2025-30066 ). This vulnerability allows remote attackers to expose CI/CD secrets via the action‚Äôs build logs. The issue affects users who rely on the tj-actions/changed-files action in GitHub workflows to track changed files within a pull request. Due to the compromised action, sensitive CI/CD secrets are being inadvertently logged in the GitHub Actions build logs. If these logs are publicly accessible, such as in public repositories, unauthorized users could access and retrieve the clear text secrets. However, there is no evidence suggesting that the exposed secrets were transmitted to any external network. The tj-actions/changed-files action is widely used in GitHub CI/CD workflows to efficiently detect file changes within pull requests, streamlining development processes by conditionally triggering actions based on modified files. With over 23,000 active repositories and more than 1 million monthly downloads, its widespread adoption makes this compromise particularly impactful, exposing numerous organizations to potential supply chain attacks. According to the initial report by StepSecurity this incident was first discovered at 4PM UTC on March 14th, 2025, and isolated by 10:30 AM UTC on March 15th, 2025. But, we still warmly advise to avoid using this action until this matter is fully resolved. Initial investigation implies on a malicious commit ( hash:0e58ed8671d6b60d0890c21b07f8835ace038e67 ), and a retroactive compromise of multiple versions, possibly all versions. The attackers introduced malicious JavaScript code directly into the dist/index.</description></item><item><title>üöÄ Announcing the Kubeflow Spark Operator Benchmarking Results</title><link>https://kubermates.org/docs/2025-03-15-announcing-the-kubeflow-spark-operator-benchmarking-results/</link><pubDate>Sat, 15 Mar 2025 00:00:00 -0500</pubDate><guid>https://kubermates.org/docs/2025-03-15-announcing-the-kubeflow-spark-operator-benchmarking-results/</guid><description>Mar 15, 2025 ‚Ä¢ Vara Bonthu , Manabu McCloskey , Ratnopam Chakrabarti , Alan Halcyon ‚Ä¢ 5 min read operators benchmarking performance Kubernetes has become the go-to platform for running large-scale Apache Spark workloads. But as workloads scale, how do you ensure your Spark jobs run efficiently without hitting bottlenecks? Managing thousands of concurrent Spark jobs can introduce severe performance challenges ‚Äîfrom CPU saturation in the Spark Operator to Kubernetes API slowdowns and job scheduling inefficiencies. To address these challenges, we are excited to introduce the Kubeflow Spark Operator Benchmarking Results and Toolkit ‚Äîa comprehensive framework to analyze performance, pinpoint bottlenecks, and optimize your Spark on Kubernetes deployments. This benchmarking effort provides three key outcomes to help you take full control of your Spark on Kubernetes deployment: ‚úÖ Benchmarking Results ‚Äì A detailed evaluation of performance insights and tuning recommendations for large-scale Spark workloads. üõ† Benchmarking Test Toolkit ‚Äì A fully reproducible test suite to help users evaluate their own Spark Operator performance and validate improvements. üìä Open-Sourced Grafana Dashboard ‚Äì A battle-tested visualization tool designed specifically to track large-scale Spark Operator deployments, providing real-time monitoring of job processing efficiency, API latencies, and system health. Running thousands of Spark jobs on Kubernetes at scale uncovers several performance roadblocks that can cripple efficiency if left unresolved: üí° So, how do you fix these issues and optimize your Spark Operator deployment? That‚Äôs where our benchmarking results and toolkit come in. Based on our benchmarking findings, we provide clear, actionable recommendations for improving Spark Operator performance at scale. If you‚Äôre running thousands of concurrent Spark jobs , here‚Äôs what you need to do: üí° Why? A single Spark Operator instance struggles to keep up with high job submission rates. ‚úÖ Solution : When a single Spark Operator instance struggles with high job submission rates, leading to CPU saturation and slower job launches, deploying multiple instances can help. Distribute the workload by assigning different namespaces to each instance. For example, one instance can manage ` 20 namespaces while another handles a separate set of 20 namespaces.</description></item><item><title>DigitalOcean Managed MongoDB now supports MongoDB 8.0</title><link>https://kubermates.org/docs/2025-03-13-digitalocean-managed-mongodb-now-supports-mongodb-8-0/</link><pubDate>Thu, 13 Mar 2025 17:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-13-digitalocean-managed-mongodb-now-supports-mongodb-8-0/</guid><description>By Nicole Ghalwash MongoDB 8. 0 is here, bringing significant performance, scalability, and security enhancements to DigitalOcean Managed MongoDB. One of the most popular database engines available, MongoDB continues to evolve to meet the demands of cutting-edge applications. With MongoDB 8. 0, Managed Database customers running MongoDB get improved query efficiency, expanded encryption capabilities, and optimizations that make scaling large workloads easier than ever. MongoDB 8. 0 features several upgrades designed to enhance performance, security, and ease of use. Whether you√¢¬Ä¬ôre managing high-throughput applications or looking for better query optimization, these improvements make DigitalOcean Managed MongoDB even more powerful. Higher throughput and improved replication performance : MongoDB 8. 0 enhances concurrent writes during data replication, reducing bottlenecks and increasing overall update speeds. Better time-series handling : Store and manage time-series data more efficiently, enabling smoother analytics and reporting for applications that rely on time-sensitive information. Expanded client-side encryption : MongoDB 8.</description></item><item><title>Blog: Spotlight on SIG Apps</title><link>https://kubermates.org/docs/2025-03-12-blog-spotlight-on-sig-apps/</link><pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-12-blog-spotlight-on-sig-apps/</guid><description>In our ongoing SIG Spotlight series, we dive into the heart of the Kubernetes project by talking to the leaders of its various Special Interest Groups (SIGs). This time, we focus on SIG Apps , the group responsible for everything related to developing, deploying, and operating applications on Kubernetes. Sandipan Panda ( DevZero ) had the opportunity to interview Maciej Szulik ( Defense Unicorns ) and Janet Kuo ( Google ), the chairs and tech leads of SIG Apps. They shared their experiences, challenges, and visions for the future of application management within the Kubernetes ecosystem. Sandipan: Hello, could you start by telling us a bit about yourself, your role, and your journey within the Kubernetes community that led to your current roles in SIG Apps? Maciej : Hey, my name is Maciej, and I‚Äôm one of the leads for SIG Apps. Aside from this role, you can also find me helping SIG CLI and also being one of the Steering Committee members. I‚Äôve been contributing to Kubernetes since late 2014 in various areas, including controllers, apiserver, and kubectl. Janet : Certainly! I‚Äôm Janet, a Staff Software Engineer at Google, and I‚Äôve been deeply involved with the Kubernetes project since its early days, even before the 1. 0 launch in 2015. It‚Äôs been an amazing journey! My current role within the Kubernetes community is one of the chairs and tech leads of SIG Apps. My journey with SIG Apps started organically. I started with building the Deployment API and adding rolling update functionalities.</description></item><item><title>Spotlight on SIG Apps</title><link>https://kubermates.org/docs/2025-03-12-spotlight-on-sig-apps/</link><pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-12-spotlight-on-sig-apps/</guid><description>In our ongoing SIG Spotlight series, we dive into the heart of the Kubernetes project by talking to the leaders of its various Special Interest Groups (SIGs). This time, we focus on SIG Apps , the group responsible for everything related to developing, deploying, and operating applications on Kubernetes. Sandipan Panda ( DevZero ) had the opportunity to interview Maciej Szulik ( Defense Unicorns ) and Janet Kuo ( Google ), the chairs and tech leads of SIG Apps. They shared their experiences, challenges, and visions for the future of application management within the Kubernetes ecosystem. Sandipan: Hello, could you start by telling us a bit about yourself, your role, and your journey within the Kubernetes community that led to your current roles in SIG Apps? Maciej : Hey, my name is Maciej, and I‚Äôm one of the leads for SIG Apps. Aside from this role, you can also find me helping SIG CLI and also being one of the Steering Committee members. I‚Äôve been contributing to Kubernetes since late 2014 in various areas, including controllers, apiserver, and kubectl. Janet : Certainly! I&amp;rsquo;m Janet, a Staff Software Engineer at Google, and I&amp;rsquo;ve been deeply involved with the Kubernetes project since its early days, even before the 1. 0 launch in 2015. It&amp;rsquo;s been an amazing journey! My current role within the Kubernetes community is one of the chairs and tech leads of SIG Apps. My journey with SIG Apps started organically. I started with building the Deployment API and adding rolling update functionalities.</description></item><item><title>Stopping Sobolan Malware with Aqua Runtime Protection</title><link>https://kubermates.org/docs/2025-03-11-stopping-sobolan-malware-with-aqua-runtime-protection/</link><pubDate>Tue, 11 Mar 2025 14:01:42 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-11-stopping-sobolan-malware-with-aqua-runtime-protection/</guid><description>Aqua Nautilus researchers have discovered a new attack campaign targeting interactive computing environments such as Jupyter Notebooks. The attack consists of multiple stages, beginning with the download of a compressed file from a remote server. Once executed, the attacker deploys several malicious tools to exploit the server and establish persistence. This campaign poses a significant risk to cloud-native environments, as it enables unauthorized access and long-term control over compromised systems. In this blog, we will outline the attack stages, discuss the potential risks, and provide recommendations to strengthen security and prevent exploitation. Interactive Computing Environments or Notebook Interfaces are platforms designed for data scientists and programmers to write, execute, and analyze code interactively. There are many products available, including Jupyter Notebook , JupyterLab , Apache Zeppelin , Google Colab , Databricks Notebooks , and others. These environments are often connected to the internet and require authentication to access data or execute code. However, a simple misconfiguration can sometimes expose the server to malicious activity by hackers. Figure 1: Sobolan campaign attack flow The attackers gained initial access through an unauthenticated JupyterLab instance, allowing them to deploy malware and cryptominers. They first downloaded and extracted a compressed archive containing 13 malicious files, consisting of both binaries and shell scripts. Once executed, these scripts initiated multiple processes to establish persistence, hijack system resources for cryptomining, and evade detection (as shown in Figure 1).</description></item><item><title>Introducing a Managed Component for Maintaining Host Routes in Kubernetes</title><link>https://kubermates.org/docs/2025-03-10-introducing-a-managed-component-for-maintaining-host-routes-in-kubernetes/</link><pubDate>Mon, 10 Mar 2025 19:39:28 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-10-introducing-a-managed-component-for-maintaining-host-routes-in-kubernetes/</guid><description>By Marco Jantke Our new DOKS routing agent is a managed component for configuring static routes on Kubernetes worker nodes. It is a direct response to user feedback on its predecessor, the static route operator, and introduces new features to enhance routing flexibility. Despite being a managed component, the DOKS routing agent is included at no additional cost for users. The DOKS routing agent enables users to configure IP routes on their Kubernetes worker nodes using a dedicated Kubernetes Custom Resource. This is particularly useful for VPN setups or tunneling egress traffic through specific gateway nodes. The routing agent supports multiple gateways and automatically configures ECMP (Equal-Cost Multi-Path) routing to distribute traffic across them. How ECMP Works: The routing agent allows users to override default routes without disrupting cluster connectivity√¢¬Ä¬îone of the most requested features. To prevent issues with Kubernetes components, the routing agent ensures that essential control plane endpoints, metadata services, and DNS servers maintain direct connectivity through the worker node Droplet√¢¬Ä¬ôs default gateway. Routes can be applied to specific nodes using Kubernetes label selectors, allowing for fine-grained control over network configurations. The routing agent can be enabled or disabled using doctl or the DigitalOcean API. For API users, the field structure is: With the DOKS routing agent and a self-managed VPC gateway Droplet, users can configure static egress IPs, ensuring outbound traffic from Kubernetes workloads originates from a predictable IP address. We√¢¬Ä¬ôre also working on a fully managed NAT gateway , which will offer a simpler solution for achieving static egress IPs.</description></item><item><title>Blog: Spotlight on SIG etcd</title><link>https://kubermates.org/docs/2025-03-04-blog-spotlight-on-sig-etcd/</link><pubDate>Tue, 04 Mar 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-04-blog-spotlight-on-sig-etcd/</guid><description>In this SIG etcd spotlight we talked with James Blair , Marek Siarkowicz , Wenjia Zhang , and Benjamin Wang to learn a bit more about this Kubernetes Special Interest Group. Frederico: Hello, thank you for the time! Let‚Äôs start with some introductions, could you tell us a bit about yourself, your role and how you got involved in Kubernetes. Benjamin: Hello, I am Benjamin. I am a SIG etcd Tech Lead and one of the etcd maintainers. I work for VMware, which is part of the Broadcom group. I got involved in Kubernetes &amp;amp; etcd &amp;amp; CSI ( Container Storage Interface ) because of work and also a big passion for open source. I have been working on Kubernetes &amp;amp; etcd (and also CSI) since 2020. James: Hey team, I‚Äôm James, a co-chair for SIG etcd and etcd maintainer. I work at Red Hat as a Specialist Architect helping people adopt cloud native technology. I got involved with the Kubernetes ecosystem in 2019. Around the end of 2022 I noticed how the etcd community and project needed help so started contributing as often as I could. There is a saying in our community that ‚Äúyou come for the technology, and stay for the people‚Äù: for me this is absolutely real, it‚Äôs been a wonderful journey so far and I‚Äôm excited to support our community moving forward.</description></item><item><title>Spotlight on SIG etcd</title><link>https://kubermates.org/docs/2025-03-04-spotlight-on-sig-etcd/</link><pubDate>Tue, 04 Mar 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-04-spotlight-on-sig-etcd/</guid><description>In this SIG etcd spotlight we talked with James Blair , Marek Siarkowicz , Wenjia Zhang , and Benjamin Wang to learn a bit more about this Kubernetes Special Interest Group. Frederico: Hello, thank you for the time! Let‚Äôs start with some introductions, could you tell us a bit about yourself, your role and how you got involved in Kubernetes. Benjamin: Hello, I am Benjamin. I am a SIG etcd Tech Lead and one of the etcd maintainers. I work for VMware, which is part of the Broadcom group. I got involved in Kubernetes &amp;amp; etcd &amp;amp; CSI ( Container Storage Interface ) because of work and also a big passion for open source. I have been working on Kubernetes &amp;amp; etcd (and also CSI) since 2020. James: Hey team, I‚Äôm James, a co-chair for SIG etcd and etcd maintainer. I work at Red Hat as a Specialist Architect helping people adopt cloud native technology. I got involved with the Kubernetes ecosystem in 2019. Around the end of 2022 I noticed how the etcd community and project needed help so started contributing as often as I could. There is a saying in our community that &amp;ldquo;you come for the technology, and stay for the people&amp;rdquo;: for me this is absolutely real, it‚Äôs been a wonderful journey so far and I‚Äôm excited to support our community moving forward.</description></item><item><title>NFTables mode for kube-proxy</title><link>https://kubermates.org/docs/2025-02-28-nftables-mode-for-kube-proxy/</link><pubDate>Fri, 28 Feb 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-02-28-nftables-mode-for-kube-proxy/</guid><description>A new nftables mode for kube-proxy was introduced as an alpha feature in Kubernetes 1. 29. Currently in beta, it is expected to be GA as of 1. 33. The new mode fixes long-standing performance problems with the iptables mode and all users running on systems with reasonably-recent kernels are encouraged to try it out. (For compatibility reasons, even once nftables becomes GA, iptables will still be the default. ) The iptables API was designed for implementing simple firewalls, and has problems scaling up to support Service proxying in a large Kubernetes cluster with tens of thousands of Services. In general, the ruleset generated by kube-proxy in iptables mode has a number of iptables rules proportional to the sum of the number of Services and the total number of endpoints. In particular, at the top level of the ruleset, there is one rule to test each possible Service IP (and port) that a packet might be addressed to: This means that when a packet comes in, the time it takes the kernel to check it against all of the Service rules is O(n) in the number of Services. As the number of Services increases, both the average and the worst-case latency for the first packet of a new connection increases (with the difference between best-case, average, and worst-case being mostly determined by whether a given Service IP address appears earlier or later in the KUBE-SERVICES chain). By contrast, with nftables, the normal way to write a ruleset like this is to have a single rule, using a &amp;ldquo;verdict map&amp;rdquo; to do the dispatch: Since there&amp;rsquo;s only a single rule, with a roughly O(1) map lookup, packet processing time is more or less constant regardless of cluster size, and the best/average/worst cases are very similar: But note the huge difference in the vertical scale between the iptables and nftables graphs! In the clusters with 5000 and 10,000 Services, the p50 (average) latency for nftables is about the same as the p01 (approximately best-case) latency for iptables. In the 30,000 Service cluster, the p99 (approximately worst-case) latency for nftables manages to beat out the p01 latency for iptables by a few microseconds! Here&amp;rsquo;s both sets of data together, but you may have to squint to see the nftables results!: While the improvements to data plane latency in large clusters are great, there&amp;rsquo;s another problem with iptables kube-proxy that often keeps users from even being able to grow their clusters to that size: the time it takes kube-proxy to program new iptables rules when Services and their endpoints change.</description></item><item><title>üîê Secure Secret Management with SOPS in Helm üöÄ</title><link>https://kubermates.org/blog/secure-secret-management-with-sops-in-helm-1940/</link><pubDate>Thu, 27 Feb 2025 08:15:10 +0000</pubDate><guid>https://kubermates.org/blog/secure-secret-management-with-sops-in-helm-1940/</guid><description>&lt;p&gt;When managing applications deployed on Kubernetes, keeping secrets safe while still making them accessible to Helm charts is a challenge. Storing secrets in plaintext is a &lt;strong&gt;security risk&lt;/strong&gt; üö® ‚Äî and that‚Äôs where &lt;strong&gt;SOPS&lt;/strong&gt; (Secrets OPerationS) and the &lt;strong&gt;Helm Secrets plugin&lt;/strong&gt; come in!&lt;/p&gt;
&lt;p&gt;In this guide, we‚Äôll cover:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;‚úÖ How to use &lt;strong&gt;SOPS&lt;/strong&gt; with &lt;strong&gt;age&lt;/strong&gt; and &lt;strong&gt;GPG&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;‚úÖ How to configure &lt;strong&gt;SOPS with &lt;code&gt;sops.yaml&lt;/code&gt;&lt;/strong&gt; for better management&lt;/li&gt;
&lt;li&gt;‚úÖ How to use &lt;strong&gt;Helm Secrets Plugin&lt;/strong&gt; to manage encrypted secrets directly in your Helm charts&lt;/li&gt;
&lt;li&gt;‚úÖ A &lt;strong&gt;GitHub Actions workflow&lt;/strong&gt; to securely deploy Helm charts using encrypted secrets&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="-why-use-sops-with-helm"&gt;üìå Why Use SOPS with Helm?&lt;/h2&gt;
&lt;p&gt;SOPS is an open-source tool from Mozilla that lets you &lt;strong&gt;encrypt and decrypt&lt;/strong&gt; secrets with ease. When combined with the Helm Secrets plugin, you can safely store your sensitive data in Git repositories and automatically decrypt them during Helm deployments. Here‚Äôs why it‚Äôs awesome:&lt;/p&gt;</description></item><item><title>üîê Secure Secret Management with SOPS in Terraform &amp; Terragrunt</title><link>https://kubermates.org/blog/secure-secret-management-with-sops-in-terraform-terragrunt-231a/</link><pubDate>Wed, 26 Feb 2025 14:44:59 +0000</pubDate><guid>https://kubermates.org/blog/secure-secret-management-with-sops-in-terraform-terragrunt-231a/</guid><description>&lt;p&gt;When managing infrastructure as code (IaC), keeping secrets &lt;strong&gt;safe&lt;/strong&gt; while still making them accessible to Terraform/Terragrunt is a challenge. Storing secrets in plaintext is a &lt;strong&gt;security risk&lt;/strong&gt; üö®‚Äîand that‚Äôs where &lt;strong&gt;SOPS&lt;/strong&gt; (Secrets OPerationS) comes in!&lt;/p&gt;
&lt;p&gt;In this guide, we‚Äôll cover:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;‚úÖ How to use &lt;strong&gt;SOPS&lt;/strong&gt; with &lt;strong&gt;age&lt;/strong&gt; and &lt;strong&gt;GPG&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;‚úÖ How to configure &lt;strong&gt;SOPS with &lt;code&gt;sops.yaml&lt;/code&gt;&lt;/strong&gt; for better management&lt;/li&gt;
&lt;li&gt;‚úÖ How to use &lt;strong&gt;Terragrunt‚Äôs built-in SOPS decryption&lt;/strong&gt; (without &lt;code&gt;run_cmd&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;‚úÖ A &lt;strong&gt;GitHub Actions workflow&lt;/strong&gt; to securely use secrets in CI/CD&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="-why-use-sops"&gt;üìå Why Use SOPS?&lt;/h2&gt;
&lt;p&gt;SOPS is an open-source tool from Mozilla that lets you &lt;strong&gt;encrypt and decrypt&lt;/strong&gt; secrets easily. It supports multiple encryption methods, including &lt;strong&gt;GPG&lt;/strong&gt;, &lt;strong&gt;AWS KMS&lt;/strong&gt;, &lt;strong&gt;Azure Key Vault&lt;/strong&gt;, &lt;strong&gt;Google Cloud KMS&lt;/strong&gt;, and &lt;strong&gt;age&lt;/strong&gt;.&lt;/p&gt;</description></item><item><title>Optimizing RAG Pipelines with Katib: Hyperparameter Tuning for Better Retrieval &amp;amp; Generation</title><link>https://kubermates.org/docs/2025-02-21-optimizing-rag-pipelines-with-katib-hyperparameter-tuning-for-better-retrieval-a/</link><pubDate>Fri, 21 Feb 2025 00:00:00 -0600</pubDate><guid>https://kubermates.org/docs/2025-02-21-optimizing-rag-pipelines-with-katib-hyperparameter-tuning-for-better-retrieval-a/</guid><description>Leveraging Katib for efficient RAG optimization. Feb 21, 2025 ‚Ä¢ Varsha Prasad Narsing (@varshaprasad96) ‚Ä¢ 7 min read katib As artificial intelligence and machine learning models become more sophisticated, optimising their performance remains a critical challenge. Kubeflow provides a robust component, Katib , designed for hyperparameter optimization and neural architecture search. As a part of the Kubeflow ecosystem, Katib enables scalable, automated tuning of underlying machine learning models, reducing the manual effort required for parameter selection while improving model performance across diverse ML workflows. With Retrieval-Augmented Generation ( RAG ) becoming an increasingly popular approach for improving search and retrieval quality, optimizing its parameters is essential to achieving high-quality results. RAG pipelines involve multiple hyperparameters that influence retrieval accuracy, hallucination reduction, and language generation quality. In this blog, we will explore how Katib can be leveraged to fine-tune a RAG pipeline, ensuring optimal performance by systematically adjusting key hyperparameters. Since compute resources are scarcer than a perfectly labeled dataset :), we‚Äôll use a lightweight Kind cluster (Kubernetes in Docker) cluster to run this example locally. Rest assured, this setup can seamlessly scale to larger clusters by increasing the dataset size and the number of hyperparameters to tune. To get started, we‚Äôll first install the Katib control plane in our cluster by following the steps outlined in the documentation. In this implementation, we use a retriever model , which encodes queries and documents into vector representations to find the most relevant matches, to fetch relevant documents based on a query and a generator model to produce coherent text responses. To run Katib, we will use the Katib SDK , which provides a programmatic interface for defining and running hyperparameter tuning experiments in Kubeflow.</description></item><item><title>Synthetic Data Generation with Kubeflow Pipelines</title><link>https://kubermates.org/docs/2025-02-16-synthetic-data-generation-with-kubeflow-pipelines/</link><pubDate>Sun, 16 Feb 2025 00:00:00 -0600</pubDate><guid>https://kubermates.org/docs/2025-02-16-synthetic-data-generation-with-kubeflow-pipelines/</guid><description>Feb 16, 2025 ‚Ä¢ √Öke Edlund , Tarek Abouzeid ‚Ä¢ 11 min read kfp When creating insights, decisions, and actions from data, the best results come from real data. But accessing real data often requires lengthy security and legal processes. The data may also be incomplete, biased, or too small, and during early exploration, we may not even know if it‚Äôs worth pursuing. While real data is essential for proper evaluation, gaps or limited access frequently hinder progress until the formal process is complete. To address these challenges, synthetic data provides an alternative. It mimics real data‚Äôs statistical properties while preserving privacy and accessibility. Synthetic data generators (synthesizers) are models trained on real data to generate new datasets that follow the same statistical distributions and relationships but do not contain real records. This allows for accelerated development, improved data availability, and enhanced privacy. Depending on the technique used, synthetic data not only mirrors statistical base properties of real data but also preserves correlations between features. These synthesizers ‚Äî such as those based on Gaussian Copulas, Generative Adversarial Networks (GANs), and Variational Autoencoders (VAEs) ‚Äî enable the creation of high-fidelity synthetic datasets. See more description of these techniques below. While the above focuses on speed of development in general, and augmentation of data to improve performance of analytical modes, there are more motivations for creating (synthetic) data: Enhanced Privacy and Security Mimics real datasets without containing sensitive or personally identifiable information, mitigating privacy risks and ensuring compliance with regulations like GDPR.</description></item><item><title>The Cloud Controller Manager Chicken and Egg Problem</title><link>https://kubermates.org/docs/2025-02-14-the-cloud-controller-manager-chicken-and-egg-problem/</link><pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-02-14-the-cloud-controller-manager-chicken-and-egg-problem/</guid><description>Kubernetes 1. 31 completed the largest migration in Kubernetes history , removing the in-tree cloud provider. While the component migration is now done, this leaves some additional complexity for users and installer projects (for example, kOps or Cluster API). We will go over those additional steps and failure points and make recommendations for cluster owners. This migration was complex and some logic had to be extracted from the core components, building four new subsystems. The cloud controller manager is part of the control plane. It is a critical component that replaces some functionality that existed previously in the kube-controller-manager and the kubelet. Components of Kubernetes One of the most critical functionalities of the cloud controller manager is the node controller, which is responsible for the initialization of the nodes. As you can see in the following diagram, when the kubelet starts, it registers the Node object with the apiserver, Tainting the node so it can be processed first by the cloud-controller-manager. The initial Node is missing the cloud-provider specific information, like the Node Addresses and the Labels with the cloud provider specific information like the Node, Region and Instance type information. Chicken and egg problem sequence diagram This new initialization process adds some latency to the node readiness. Previously, the kubelet was able to initialize the node at the same time it created the node.</description></item><item><title>Update strategies for managed node groups</title><link>https://kubermates.org/releases/2025-01-27-update-strategies-for-managed-node-groups/</link><pubDate>Mon, 27 Jan 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-01-27-update-strategies-for-managed-node-groups/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. The Amazon EKS managed worker node upgrade strategy has four different phases described in the following sections. The setup phase has these steps: It creates a new Amazon EC2 launch template version for the Auto Scaling Group that√¢¬Ä¬ôs associated with your node group. The new launch template version uses the target AMI or a custom launch template version for the update. It updates the Auto Scaling Group to use the latest launch template version. It determines the maximum quantity of nodes to upgrade in parallel using the updateConfig property for the node group. The maximum unavailable has a quota of 100 nodes. The default value is one node. For more information, see the updateConfig property in the Amazon EKS API Reference. When upgrading the nodes in a managed node group, the upgraded nodes are launched in the same Availability Zone as those that are being upgraded. To guarantee this placement, we use Amazon EC2√¢¬Ä¬ôs Availability Zone Rebalancing. For more information, see Availability Zone Rebalancing in the Amazon EC2 Auto Scaling User Guide.</description></item><item><title>Kubernetes version 1.32</title><link>https://kubermates.org/releases/2025-01-23-kubernetes-version-1-32/</link><pubDate>Thu, 23 Jan 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2025-01-23-kubernetes-version-1-32/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Kubernetes rapidly evolves with new features, design updates, and bug fixes. The community releases new Kubernetes minor versions (such as 1. 33 ) on average once every four months. Amazon EKS follows the upstream release and deprecation cycle for minor versions. As new Kubernetes versions become available in Amazon EKS, we recommend that you proactively update your clusters to use the latest available version. A minor version is under standard support in Amazon EKS for the first 14 months after it√¢¬Ä¬ôs released. Once a version is past the end of standard support date, it enters extended support for the next 12 months. Extended support allows you to stay at a specific Kubernetes version for longer at an additional cost per cluster hour. If you haven√¢¬Ä¬ôt updated your cluster before the extended support period ends, your cluster is auto-upgraded to the oldest currently supported extended version. Extended support is enabled by default. To disable, see Disable EKS extended support.</description></item><item><title>Helm Chart Essentials &amp; Writing Effective Charts üöÄ</title><link>https://kubermates.org/blog/helm-chart-essentials-writing-effective-charts-11ca/</link><pubDate>Thu, 23 Jan 2025 10:50:53 +0000</pubDate><guid>https://kubermates.org/blog/helm-chart-essentials-writing-effective-charts-11ca/</guid><description>&lt;p&gt;Helm charts are a powerful way to define, install, and upgrade Kubernetes applications. By packaging all the Kubernetes manifests and parameters in a neat, reproducible format, Helm simplifies the deployment process for engineers and DevOps teams. In this article, we‚Äôll explore some best practices for writing effective Helm charts, introduce the &lt;strong&gt;Helm Schema plugin&lt;/strong&gt; for validation, show how to include tests to ensure reliability, discuss &lt;strong&gt;helm-docs&lt;/strong&gt; for automated documentation generation, and share an additional resource for testing and linting. Let‚Äôs get started! üéâ&lt;/p&gt;</description></item><item><title>Blog: Spotlight on SIG Architecture: Enhancements</title><link>https://kubermates.org/docs/2025-01-21-blog-spotlight-on-sig-architecture-enhancements/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-01-21-blog-spotlight-on-sig-architecture-enhancements/</guid><description>This is the fourth interview of a SIG Architecture Spotlight series that will cover the different subprojects, and we will be covering SIG Architecture: Enhancements. In this SIG Architecture spotlight we talked with Kirsten Garrison , lead of the Enhancements subproject. Frederico (FSM): Hi Kirsten, very happy to have the opportunity to talk about the Enhancements subproject. Let‚Äôs start with some quick information about yourself and your role. Kirsten Garrison (KG) : I‚Äôm a lead of the Enhancements subproject of SIG-Architecture and currently work at Google. I first got involved by contributing to the service-catalog project with the help of Carolyn Van Slyck. With time, I joined the Release team , eventually becoming the Enhancements Lead and a Release Lead shadow. While on the release team, I worked on some ideas to make the process better for the SIGs and Enhancements team (the opt-in process) based on my team‚Äôs experiences. Eventually, I started attending Subproject meetings and contributing to the Subproject‚Äôs work. FSM: You mentioned the Enhancements subproject: how would you describe its main goals and areas of intervention? KG : The Enhancements Subproject primarily concerns itself with the Kubernetes Enhancement Proposal ( KEP for short)‚Äîthe ‚Äúdesign‚Äù documents required for all features and significant changes to the Kubernetes project. FSM: The improvement of the KEP process was (and is) one in which SIG Architecture was heavily involved. Could you explain the process to those that aren‚Äôt aware of it? KG : Every release , the SIGs let the Release Team know which features they intend to work on to be put into the release.</description></item><item><title>Demystifying the OpenTelemetry Operator: Observing Kubernetes applications without writing code</title><link>https://kubermates.org/docs/2025-01-21-demystifying-the-opentelemetry-operator-observing-kubernetes-applications-withou/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-01-21-demystifying-the-opentelemetry-operator-observing-kubernetes-applications-withou/</guid><description>Colin Steele √Ç¬∑ 22 Aug 2025 √Ç¬∑ 5 min read Learn how Beyla, eBPF, and OpenTelemetry combine to make homelab observability easy with this recap of a recent GrafanaCON 2025 session. Read more.</description></item><item><title>Spotlight on SIG Architecture: Enhancements</title><link>https://kubermates.org/docs/2025-01-21-spotlight-on-sig-architecture-enhancements/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-01-21-spotlight-on-sig-architecture-enhancements/</guid><description>This is the fourth interview of a SIG Architecture Spotlight series that will cover the different subprojects, and we will be covering SIG Architecture: Enhancements. In this SIG Architecture spotlight we talked with Kirsten Garrison , lead of the Enhancements subproject. Frederico (FSM): Hi Kirsten, very happy to have the opportunity to talk about the Enhancements subproject. Let&amp;rsquo;s start with some quick information about yourself and your role. Kirsten Garrison (KG) : I‚Äôm a lead of the Enhancements subproject of SIG-Architecture and currently work at Google. I first got involved by contributing to the service-catalog project with the help of Carolyn Van Slyck. With time, I joined the Release team , eventually becoming the Enhancements Lead and a Release Lead shadow. While on the release team, I worked on some ideas to make the process better for the SIGs and Enhancements team (the opt-in process) based on my team‚Äôs experiences. Eventually, I started attending Subproject meetings and contributing to the Subproject‚Äôs work. FSM: You mentioned the Enhancements subproject: how would you describe its main goals and areas of intervention? KG : The Enhancements Subproject primarily concerns itself with the Kubernetes Enhancement Proposal ( KEP for short)‚Äîthe &amp;ldquo;design&amp;rdquo; documents required for all features and significant changes to the Kubernetes project. FSM: The improvement of the KEP process was (and is) one in which SIG Architecture was heavily involved. Could you explain the process to those that aren‚Äôt aware of it? KG : Every release , the SIGs let the Release Team know which features they intend to work on to be put into the release.</description></item><item><title>Unlocking Secrets with External Secrets Operator üîê‚ú®</title><link>https://kubermates.org/blog/unlocking-secrets-with-external-secrets-operator-2f89/</link><pubDate>Fri, 03 Jan 2025 20:00:01 +0000</pubDate><guid>https://kubermates.org/blog/unlocking-secrets-with-external-secrets-operator-2f89/</guid><description>&lt;p&gt;In modern cloud-native applications, securely managing sensitive data like API keys, database credentials, and certificates is a top priority. Two powerful tools stand out when integrating secrets into Kubernetes: &lt;strong&gt;External Secrets Operator&lt;/strong&gt; and &lt;strong&gt;SecretStoreProviders plugin&lt;/strong&gt; (for Azure and AWS). Let‚Äôs dive into how to use them, their differences, and when to pick one over the other. üöÄ&lt;/p&gt;
&lt;h2 id="what-is-external-secrets-operator-"&gt;&lt;strong&gt;What Is External Secrets Operator?&lt;/strong&gt; ü§î&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;External Secrets Operator&lt;/strong&gt; (ESO) simplifies secret management in Kubernetes by integrating external secret stores directly into your cluster. Instead of manually creating Kubernetes Secrets, ESO syncs secrets from providers like AWS Secrets Manager, Azure Key Vault, or HashiCorp Vault.&lt;/p&gt;</description></item><item><title>Automating DNS in Azure Private DNS with External-DNS ‚òÅÔ∏èüîê</title><link>https://kubermates.org/blog/automating-dns-in-azure-private-dns-with-external-dns-3knk/</link><pubDate>Sun, 29 Dec 2024 17:07:48 +0000</pubDate><guid>https://kubermates.org/blog/automating-dns-in-azure-private-dns-with-external-dns-3knk/</guid><description>&lt;p&gt;&lt;em&gt;When running Kubernetes in Azure, one of the biggest time-savers you can implement is automatic DNS record management‚Äîespecially for internal (private) services. By integrating &lt;a href="https://github.com/kubernetes-sigs/external-dns"&gt;External-DNS&lt;/a&gt; with Azure Private DNS, you can say goodbye to manual record updates. Better yet, you can skip traditional service principals and use Azure Workload Identity to make your cluster more secure and secrets-free!&lt;/em&gt; üöÄ&lt;/p&gt;
&lt;p&gt;In this article, we‚Äôll show you how to configure External-DNS for Azure Private DNS using &lt;strong&gt;Azure Workload Identity&lt;/strong&gt;, leveraging a snippet of a &lt;code&gt;values.yaml&lt;/code&gt; that highlights the relevant settings.&lt;/p&gt;</description></item><item><title>Using Nginx Ingress Controller and Cert-Manager for HTTPS with Let‚Äôs Encrypt ‚ö°</title><link>https://kubermates.org/blog/using-nginx-ingress-controller-and-cert-manager-for-https-with-lets-encrypt-2flh/</link><pubDate>Thu, 26 Dec 2024 09:39:11 +0000</pubDate><guid>https://kubermates.org/blog/using-nginx-ingress-controller-and-cert-manager-for-https-with-lets-encrypt-2flh/</guid><description>&lt;p&gt;Hey there! In today‚Äôs world, serving your web apps over HTTPS is a must. Luckily, combining the power of &lt;strong&gt;Nginx Ingress Controller&lt;/strong&gt; with &lt;strong&gt;Cert-Manager&lt;/strong&gt; helps you easily request, issue, and renew TLS certificates from &lt;strong&gt;Let‚Äôs Encrypt&lt;/strong&gt;. In this friendly guide, we‚Äôll walk you through:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Installing the &lt;strong&gt;Nginx Ingress Controller&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Installing &lt;strong&gt;Cert-Manager&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Creating a &lt;strong&gt;ClusterIssuer&lt;/strong&gt; to fetch certificates from Let‚Äôs Encrypt&lt;/li&gt;
&lt;li&gt;Configuring an example &lt;strong&gt;Ingress&lt;/strong&gt; to serve traffic via HTTPS&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let‚Äôs get started! üöÄ&lt;/p&gt;</description></item><item><title>Kubernetes 1.32: Moving Volume Group Snapshots to Beta</title><link>https://kubermates.org/docs/2024-12-18-kubernetes-1-32-moving-volume-group-snapshots-to-beta/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-12-18-kubernetes-1-32-moving-volume-group-snapshots-to-beta/</guid><description>Volume group snapshots were introduced as an Alpha feature with the Kubernetes 1. 27 release. The recent release of Kubernetes v1. 32 moved that support to beta. The support for volume group snapshots relies on a set of extension APIs for group snapshots. These APIs allow users to take crash consistent snapshots for a set of volumes. Behind the scenes, Kubernetes uses a label selector to group multiple PersistentVolumeClaims for snapshotting. A key aim is to allow you restore that set of snapshots to new volumes and recover your workload based on a crash consistent recovery point. This new feature is only supported for CSI volume drivers. Some storage systems provide the ability to create a crash consistent snapshot of multiple volumes. A group snapshot represents copies made from multiple volumes, that are taken at the same point-in-time. A group snapshot can be used either to rehydrate new volumes (pre-populated with the snapshot data) or to restore existing volumes to a previous state (represented by the snapshots).</description></item><item><title>Enhancing Kubernetes API Server Efficiency with API Streaming</title><link>https://kubermates.org/docs/2024-12-17-enhancing-kubernetes-api-server-efficiency-with-api-streaming/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-12-17-enhancing-kubernetes-api-server-efficiency-with-api-streaming/</guid><description>Managing Kubernetes clusters efficiently is critical, especially as their size is growing. A significant challenge with large clusters is the memory overhead caused by list requests. In the existing implementation, the kube-apiserver processes list requests by assembling the entire response in-memory before transmitting any data to the client. But what if the response body is substantial, say hundreds of megabytes? Additionally, imagine a scenario where multiple list requests flood in simultaneously, perhaps after a brief network outage. While API Priority and Fairness has proven to reasonably protect kube-apiserver from CPU overload, its impact is visibly smaller for memory protection. This can be explained by the differing nature of resource consumption by a single API request - the CPU usage at any given time is capped by a constant, whereas memory, being uncompressible, can grow proportionally with the number of processed objects and is unbounded. This situation poses a genuine risk, potentially overwhelming and crashing any kube-apiserver within seconds due to out-of-memory (OOM) conditions. To better visualize the issue, let&amp;rsquo;s consider the below graph. The graph shows the memory usage of a kube-apiserver during a synthetic test. (see the synthetic test section for more details). The results clearly show that increasing the number of informers significantly boosts the server&amp;rsquo;s memory consumption. Notably, at approximately 16:40, the server crashed when serving only 16 informers.</description></item><item><title>Kubernetes v1.32 Adds A New CPU Manager Static Policy Option For Strict CPU Reservation</title><link>https://kubermates.org/docs/2024-12-16-kubernetes-v1-32-adds-a-new-cpu-manager-static-policy-option-for-strict-cpu-rese/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-12-16-kubernetes-v1-32-adds-a-new-cpu-manager-static-policy-option-for-strict-cpu-rese/</guid><description>In Kubernetes v1. 32, after years of community discussion, we are excited to introduce a strict-cpu-reservation option for the CPU Manager static policy. This feature is currently in alpha, with the associated policy hidden by default. You can only use the policy if you explicitly enable the alpha behavior in your cluster. The CPU Manager static policy is used to reduce latency or improve performance. The reservedSystemCPUs defines an explicit CPU set for OS system daemons and kubernetes system daemons. This option is designed for Telco/NFV type use cases where uncontrolled interrupts/timers may impact the workload performance. you can use this option to define the explicit cpuset for the system/kubernetes daemons as well as the interrupts/timers, so the rest CPUs on the system can be used exclusively for workloads, with less impact from uncontrolled interrupts/timers. More details of this parameter can be found on the Explicitly Reserved CPU List page. If you want to protect your system daemons and interrupt processing, the obvious way is to use the reservedSystemCPUs option. However, until the Kubernetes v1. 32 release, this isolation was only implemented for guaranteed pods that made requests for a whole number of CPUs.</description></item><item><title>Kubernetes v1.32: Memory Manager Goes GA</title><link>https://kubermates.org/docs/2024-12-13-kubernetes-v1-32-memory-manager-goes-ga/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-12-13-kubernetes-v1-32-memory-manager-goes-ga/</guid><description>With Kubernetes 1. 32, the memory manager has officially graduated to General Availability (GA), marking a significant milestone in the journey toward efficient and predictable memory allocation for containerized applications. Since Kubernetes v1. 22, where it graduated to beta, the memory manager has proved itself reliable, stable and a good complementary feature for the CPU Manager. As part of kubelet&amp;rsquo;s workload admission process, the memory manager provides topology hints to optimize memory allocation and alignment. This enables users to allocate exclusive memory for Pods in the Guaranteed QoS class. More details about the process can be found in the memory manager goes to beta blog. Most of the changes introduced since the Beta are bug fixes, internal refactoring and observability improvements, such as metrics and better logging. As part of the effort to increase the observability of memory manager, new metrics have been added to provide some statistics on memory allocation patterns. memory_manager_pinning_requests_total - tracks the number of times the pod spec required the memory manager to pin memory pages. memory_manager_pinning_errors_total - tracks the number of times the pod spec required the memory manager to pin memory pages, but the allocation failed. The kubelet does not guarantee pod ordering when admitting pods after a restart or reboot.</description></item><item><title>Kubernetes v1.32: QueueingHint Brings a New Possibility to Optimize Pod Scheduling</title><link>https://kubermates.org/docs/2024-12-12-kubernetes-v1-32-queueinghint-brings-a-new-possibility-to-optimize-pod-schedulin/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-12-12-kubernetes-v1-32-queueinghint-brings-a-new-possibility-to-optimize-pod-schedulin/</guid><description>The Kubernetes scheduler is the core component that selects the nodes on which new Pods run. The scheduler processes these new Pods one by one. Therefore, the larger your clusters, the more important the throughput of the scheduler becomes. Over the years, Kubernetes SIG Scheduling has improved the throughput of the scheduler in multiple enhancements. This blog post describes a major improvement to the scheduler in Kubernetes v1. 32: a scheduling context element named QueueingHint. This page provides background knowledge of the scheduler and explains how QueueingHint improves scheduling throughput. The scheduler stores all unscheduled Pods in an internal component called the scheduling queue. The scheduling queue consists of the following data structures: The Kubernetes scheduler is implemented following the Kubernetes scheduling framework. And, all scheduling features are implemented as plugins (e. g. , Pod affinity is implemented in the InterPodAffinity plugin.</description></item><item><title>Kubernetes v1.32: Penelope</title><link>https://kubermates.org/docs/2024-12-11-kubernetes-v1-32-penelope/</link><pubDate>Wed, 11 Dec 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-12-11-kubernetes-v1-32-penelope/</guid><description>Editors: Matteo Bianchi, Edith Puclla, William Rizzo, Ryota Sawada, Rashan Smith Announcing the release of Kubernetes v1. 32: Penelope! In line with previous releases, the release of Kubernetes v1. 32 introduces new stable, beta, and alpha features. The consistent delivery of high-quality releases underscores the strength of our development cycle and the vibrant support from our community. This release consists of 44 enhancements in total. Of those enhancements, 13 have graduated to Stable, 12 are entering Beta, and 19 have entered in Alpha. The Kubernetes v1. 32 Release Theme is &amp;ldquo;Penelope&amp;rdquo;. If Kubernetes is Ancient Greek for &amp;ldquo;pilot&amp;rdquo;, in this release we start from that origin and reflect on the last 10 years of Kubernetes and our accomplishments: each release cycle is a journey, and just like Penelope, in &amp;ldquo;The Odyssey&amp;rdquo;, weaved for 10 years &amp;ndash; each night removing parts of what she had done during the day &amp;ndash; so does each release add new features and removes others, albeit here with a much clearer purpose of constantly improving Kubernetes. With v1. 32 being the last release in the year Kubernetes marks its first decade anniversary, we wanted to honour all of those that have been part of the global Kubernetes crew that roams the cloud-native seas through perils and challanges: may we continue to weave the future of Kubernetes together. In this release, like the previous one, the Kubernetes project continues proposing a number of enhancements to the Dynamic Resource Allocation (DRA), a key component of the Kubernetes resource management system.</description></item><item><title>Amazon EKS Auto Mode</title><link>https://kubermates.org/releases/2024-12-01-amazon-eks-auto-mode/</link><pubDate>Sun, 01 Dec 2024 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2024-12-01-amazon-eks-auto-mode/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. EKS Auto Mode extends AWS management of Kubernetes clusters beyond the cluster itself, to allow AWS to also set up and manage the infrastructure that enables the smooth operation of your workloads. You can delegate key infrastructure decisions and leverage the expertise of AWS for day-to-day operations. Cluster infrastructure managed by AWS includes many Kubernetes capabilities as core components, as opposed to add-ons, such as compute autoscaling, pod and service networking, application load balancing, cluster DNS, block storage, and GPU support. To get started, you can deploy a new EKS Auto Mode cluster or enable EKS Auto Mode on an existing cluster. You can deploy, upgrade, or modify your EKS Auto Mode clusters using eksctl, the AWS CLI, the AWS Management Console, EKS APIs, or your preferred infrastructure-as-code tools. With EKS Auto Mode, you can continue using your preferred Kubernetes-compatible tools. EKS Auto Mode integrates with AWS services like Amazon EC2, Amazon EBS, and ELB, leveraging AWS cloud resources that follow best practices. These resources are automatically scaled, cost-optimized, and regularly updated to help minimize operational costs and overhead. EKS Auto Mode provides the following high-level features: Streamline Kubernetes Cluster Management : EKS Auto Mode streamlines EKS management by providing production-ready clusters with minimal operational overhead. With EKS Auto Mode, you can run demanding, dynamic workloads confidently, without requiring deep EKS expertise. Application Availability : EKS Auto Mode dynamically adds or removes nodes in your EKS cluster based on the demands of your Kubernetes applications.</description></item><item><title>Amazon EKS Hybrid Nodes</title><link>https://kubermates.org/releases/2024-12-01-amazon-eks-hybrid-nodes/</link><pubDate>Sun, 01 Dec 2024 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2024-12-01-amazon-eks-hybrid-nodes/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. With Amazon EKS Hybrid Nodes , you can use your on-premises and edge infrastructure as nodes in Amazon EKS clusters. AWS manages the AWS-hosted Kubernetes control plane of the Amazon EKS cluster, and you manage the hybrid nodes that run in your on-premises or edge environments. This unifies Kubernetes management across your environments and offloads Kubernetes control plane management to AWS for your on-premises and edge applications. Amazon EKS Hybrid Nodes works with any on-premises hardware or virtual machines, bringing the efficiency, scalability, and availability of Amazon EKS to wherever your applications need to run. You can use a wide range of Amazon EKS features with Amazon EKS Hybrid Nodes including Amazon EKS add-ons, Amazon EKS Pod Identity, cluster access entries, cluster insights, and extended Kubernetes version support. Amazon EKS Hybrid Nodes natively integrates with AWS services including AWS Systems Manager, AWS IAM Roles Anywhere, Amazon Managed Service for Prometheus, and Amazon CloudWatch for centralized monitoring, logging, and identity management. With Amazon EKS Hybrid Nodes, there are no upfront commitments or minimum fees, and you are charged per hour for the vCPU resources of your hybrid nodes when they are attached to your Amazon EKS clusters. For more pricing information, see Amazon EKS Pricing. EKS Hybrid Nodes has the following high-level features: Managed Kubernetes control plane : AWS manages the AWS-hosted Kubernetes control plane of the EKS cluster, and you manage the hybrid nodes that run in your on-premises or edge environments. This unifies Kubernetes management across your environments and offloads Kubernetes control plane management to AWS for your on-premises and edge applications. By moving the Kubernetes control plane to AWS, you can conserve on-premises capacity for your applications and trust that the Kubernetes control plane scales with your workloads.</description></item><item><title>Kubernetes version 1.30 is now available for local clusters on AWS Outposts</title><link>https://kubermates.org/releases/2024-11-21-kubernetes-version-1-30-is-now-available-for-local-clusters-on-aws-outposts/</link><pubDate>Thu, 21 Nov 2024 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2024-11-21-kubernetes-version-1-30-is-now-available-for-local-clusters-on-aws-outposts/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Local cluster platform versions represent the capabilities of the Amazon EKS cluster on AWS Outposts. The versions include the components that run on the Kubernetes control plane, which Kubernetes API server flags are enabled. They also include the current Kubernetes patch version. Each Kubernetes minor version has one or more associated platform versions. The platform versions for different Kubernetes minor versions are independent. The platform versions for local clusters and Amazon EKS clusters in the cloud are independent. When a new Kubernetes minor version is available for local clusters, such as 1. 31 , the initial platform version for that Kubernetes minor version starts at eks-local-outposts. 1. However, Amazon EKS releases new platform versions periodically to enable new Kubernetes control plane settings and to provide security fixes. When new local cluster platform versions become available for a minor version: The platform version number is incremented ( eks-local-outposts.</description></item><item><title>Gateway API v1.2: WebSockets, Timeouts, Retries, and More</title><link>https://kubermates.org/docs/2024-11-21-gateway-api-v1-2-websockets-timeouts-retries-and-more/</link><pubDate>Thu, 21 Nov 2024 09:00:00 -0800</pubDate><guid>https://kubermates.org/docs/2024-11-21-gateway-api-v1-2-websockets-timeouts-retries-and-more/</guid><description>Kubernetes SIG Network is delighted to announce the general availability of Gateway API v1. 2! This version of the API was released on October 3, and we&amp;rsquo;re delighted to report that we now have a number of conformant implementations of it for you to try out. Gateway API v1. 2 brings a number of new features to the Standard channel (Gateway API&amp;rsquo;s GA release channel), introduces some new experimental features, and inaugurates our new release process ‚Äî but it also brings two breaking changes that you&amp;rsquo;ll want to be careful of. Now that the v1 versions of GRPCRoute and ReferenceGrant have graduated to Standard, the old v1alpha2 versions have been removed from both the Standard and Experimental channels, in order to ease the maintenance burden that perpetually supporting the old versions would place on the Gateway API community. Before upgrading to Gateway API v1. 2, you&amp;rsquo;ll want to confirm that any implementations of Gateway API have been upgraded to support the v1 API version of these resources instead of the v1alpha2 API version. Note that even if you&amp;rsquo;ve been using v1 in your YAML manifests, a controller may still be using v1alpha2 which would cause it to fail during this upgrade. Additionally, Kubernetes itself goes to some effort to stop you from removing a CRD version that it thinks you&amp;rsquo;re using: check out the release notes for more information about what you need to do to safely upgrade. A much smaller breaking change:. status. supportedFeatures in a Gateway is now a list of objects instead of a list of strings.</description></item><item><title>How we built a dynamic Kubernetes API Server for the API Aggregation Layer in Cozystack</title><link>https://kubermates.org/docs/2024-11-21-how-we-built-a-dynamic-kubernetes-api-server-for-the-api-aggregation-layer-in-co/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-11-21-how-we-built-a-dynamic-kubernetes-api-server-for-the-api-aggregation-layer-in-co/</guid><description>Hi there! I&amp;rsquo;m Andrei Kvapil, but you might know me as @kvaps in communities dedicated to Kubernetes and cloud-native tools. In this article, I want to share how we implemented our own extension api-server in the open-source PaaS platform, Cozystack. Kubernetes truly amazes me with its powerful extensibility features. You&amp;rsquo;re probably already familiar with the controller concept and frameworks like kubebuilder and operator-sdk that help you implement it. In a nutshell, they allow you to extend your Kubernetes cluster by defining custom resources (CRDs) and writing additional controllers that handle your business logic for reconciling and managing these kinds of resources. This approach is well-documented, with a wealth of information available online on how to develop your own operators. However, this is not the only way to extend the Kubernetes API. For more complex scenarios such as implementing imperative logic, managing subresources, and dynamically generating responses‚Äîthe Kubernetes API aggregation layer provides an effective alternative. Through the aggregation layer, you can develop a custom extension API server and seamlessly integrate it within the broader Kubernetes API framework. In this article, I will explore the API aggregation layer, the types of challenges it is well-suited to address, cases where it may be less appropriate, and how we utilized this model to implement our own extension API server in Cozystack. First, let&amp;rsquo;s get definitions straight to avoid any confusion down the road. The API aggregation layer is a feature in Kubernetes, while an extension api-server is a specific implementation of an API server for the aggregation layer.</description></item><item><title>Bottlerocket AMIs that use FIPS 140-3</title><link>https://kubermates.org/releases/2024-11-20-bottlerocket-amis-that-use-fips-140-3/</link><pubDate>Wed, 20 Nov 2024 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2024-11-20-bottlerocket-amis-that-use-fips-140-3/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. When deploying nodes, you can specify an ID for a pre-built Amazon EKS optimized Amazon Machine Image (AMI). To retrieve an AMI ID that fits your desired configuration, query the AWS Systems Manager Parameter Store API. Using this API eliminates the need to manually look up Amazon EKS optimized AMI IDs. For more information, see GetParameter. The IAM principal that you use must have the ssm:GetParameter IAM permission to retrieve the Amazon EKS optimized AMI metadata. You can retrieve the image ID of the latest recommended Amazon EKS optimized Bottlerocket AMI with the following AWS CLI command, which uses the sub-parameter image_id. Make the following modifications to the command as needed and then run the modified command: Replace kubernetes-version with a supported platform-version. Replace -flavor with one of the following options. Remove -flavor for variants without a GPU. Use -nvidia for GPU-enabled variants. Use -fips for FIPS-enabled variants.</description></item><item><title>Observability dashboard</title><link>https://kubermates.org/releases/2024-11-18-observability-dashboard/</link><pubDate>Mon, 18 Nov 2024 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2024-11-18-observability-dashboard/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. The Amazon EKS console includes an observability dashboard that gives visibility into the performance of your cluster. The information it provides helps you to quickly detect, troubleshoot, and remediate issues. You can open the applicable section of the observability dashboard by choosing an item in the Health and performance summary. This summary is included in several places, including the Observability tab. The observability dashboard is split into several tabs. The Health and performance summary lists the quantity of items in various categories. Each number acts as a hyperlink to a location in the observability dashboard with a list for that category. Cluster health provides important notifications to be aware of, some of which you may need to take action on as soon as possible. With this list, you can see descriptions and the affected resources. Cluster health includes two tables: Health issues and Configuration insights. To refresh the status of Health issues , choose the refresh button ( √¢¬Ü¬ª ).</description></item><item><title>New role creation in console for add-ons that support EKS Pod Identities</title><link>https://kubermates.org/releases/2024-11-15-new-role-creation-in-console-for-add-ons-that-support-eks-pod-identities/</link><pubDate>Fri, 15 Nov 2024 19:00:00 +0000</pubDate><guid>https://kubermates.org/releases/2024-11-15-new-role-creation-in-console-for-add-ons-that-support-eks-pod-identities/</guid><description>Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Amazon EKS add-ons are add-on software for Amazon EKS clusters. All Amazon EKS add-ons: Include the latest security patches and bug fixes. Are validated by AWS to work with Amazon EKS. Reduce the amount of work required to manage the add-on software. You can create an Amazon EKS add-on using eksctl , the AWS Management Console, or the AWS CLI. If the add-on requires an IAM role, see the details for the specific add-on in Amazon EKS add-ons for details about creating the role. Complete the following before you create an add-on: The cluster must exist before you create an add-on for it. For more information, see Create an Amazon EKS cluster. Check if your add-on requires an IAM role. For more information, see Verify Amazon EKS add-on version compatibility with a cluster. Verify that the Amazon EKS add-on version is compatabile with your cluster.</description></item><item><title>Kubernetes v1.32 sneak peek</title><link>https://kubermates.org/docs/2024-11-08-kubernetes-v1-32-sneak-peek/</link><pubDate>Fri, 08 Nov 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-11-08-kubernetes-v1-32-sneak-peek/</guid><description>As we get closer to the release date for Kubernetes v1. 32, the project develops and matures. Features may be deprecated, removed, or replaced with better ones for the project&amp;rsquo;s overall health. This blog outlines some of the planned changes for the Kubernetes v1. 32 release, that the release team feels you should be aware of, for the continued maintenance of your Kubernetes environment and keeping up to date with the latest changes. Information listed below is based on the current status of the v1. 32 release and may change before the actual release date. The Kubernetes project has a well-documented deprecation policy for features. This policy states that stable APIs may only be deprecated when a newer, stable version of that API is available and that APIs have a minimum lifetime for each stability level. A deprecated API has been marked for removal in a future Kubernetes release will continue to function until removal (at least one year from the deprecation). Its usage will result in a warning being displayed. Removed APIs are no longer available in the current version, so you must migrate to use the replacement instead.</description></item><item><title>Creating alerts from panels in Kubernetes Monitoring: an overlooked, powerhouse feature</title><link>https://kubermates.org/docs/2024-11-04-creating-alerts-from-panels-in-kubernetes-monitoring-an-overlooked-powerhouse-fe/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-11-04-creating-alerts-from-panels-in-kubernetes-monitoring-an-overlooked-powerhouse-fe/</guid><description>Alejandro Fraenkel √Ç¬∑ 5 Aug 2025 √Ç¬∑ 9 min read We built a brand new alert rules list page. Find out how this new design will make your on-call life a little easier and get insights into how we. Read more.</description></item><item><title>Blog: Spotlight on Kubernetes Upstream Training in Japan</title><link>https://kubermates.org/docs/2024-10-28-blog-spotlight-on-kubernetes-upstream-training-in-japan/</link><pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-10-28-blog-spotlight-on-kubernetes-upstream-training-in-japan/</guid><description>We are organizers of Kubernetes Upstream Training in Japan. Our team is composed of members who actively contribute to Kubernetes, including individuals who hold roles such as member, reviewer, approver, and chair. Our goal is to increase the number of Kubernetes contributors and foster the growth of the community. While Kubernetes community is friendly and collaborative, newcomers may find the first step of contributing to be a bit challenging. Our training program aims to lower that barrier and create an environment where even beginners can participate smoothly. Our training started in 2019 and is held 1 to 2 times a year. Initially, Kubernetes Upstream Training was conducted as a co-located event of KubeCon (Kubernetes Contributor Summit), but we launched Kubernetes Upstream Training in Japan with the aim of increasing Japanese contributors by hosting a similar event in Japan. Before the pandemic, the training was held in person, but since 2020, it has been conducted online. The training offers the following content for those who have not yet contributed to Kubernetes: At the beginning of the program, we explain why contributing to Kubernetes is important and who can contribute. We emphasize that contributing to Kubernetes allows you to make a global impact and that Kubernetes community is looking forward to your contributions! We also explain Kubernetes community, SIGs, and Working Groups. Next, we explain the roles and responsibilities of Member, Reviewer, Approver, Tech Lead, and Chair. Additionally, we introduce the communication tools we primarily use, such as Slack, GitHub, and mailing lists.</description></item><item><title>Monitoring Kubernetes: Why traditional techniques aren't enough</title><link>https://kubermates.org/docs/2024-10-18-monitoring-kubernetes-why-traditional-techniques-aren-t-enough/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-10-18-monitoring-kubernetes-why-traditional-techniques-aren-t-enough/</guid><description>Serena Kei √Ç¬∑ 15 Jul 2025 √Ç¬∑ 4 min read The latest backend update to our Kubernetes Monitoring app in Grafana Cloud features significant improvements to alert rules and recording rules that. Read more.</description></item><item><title>Blog: Announcing the 2024 Steering Committee Election Results</title><link>https://kubermates.org/docs/2024-10-02-blog-announcing-the-2024-steering-committee-election-results/</link><pubDate>Wed, 02 Oct 2024 15:10:00 -0500</pubDate><guid>https://kubermates.org/docs/2024-10-02-blog-announcing-the-2024-steering-committee-election-results/</guid><description>The 2024 Steering Committee Election is now complete. The Kubernetes Steering Committee consists of 7 seats, 3 of which were up for election in 2024. Incoming committee members serve a term of 2 years, and all members are elected by the Kubernetes Community. This community body is significant since it oversees the governance of the entire Kubernetes project. With that great power comes great responsibility. You can learn more about the steering committee‚Äôs role in their charter. Thank you to everyone who voted in the election; your participation helps support the community‚Äôs continued health and success. Congratulations to the elected committee members whose two year terms begin immediately (listed in alphabetical order by GitHub handle): They join continuing members: Benjamin Elder is a returning Steering Committee Member. Thank you and congratulations on a successful election to this round‚Äôs election officers: Thanks to the Emeritus Steering Committee Members. Your service is appreciated by the community: And thank you to all the candidates who came forward to run for election. This governing body, like all of Kubernetes, is open to all. You can follow along with Steering Committee meeting notes and weigh in by filing an issue or creating a PR against their repo.</description></item><item><title>Container monitoring with Grafana: Helpful resources to get started</title><link>https://kubermates.org/docs/2024-10-02-container-monitoring-with-grafana-helpful-resources-to-get-started/</link><pubDate>Wed, 02 Oct 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-10-02-container-monitoring-with-grafana-helpful-resources-to-get-started/</guid><description>Trevor Jones √Ç¬∑ 12 Aug 2025 √Ç¬∑ 9 min read Check out the highlights from our first survey about observability practices in Brazil, including analysis on adoption, maturity, challenges, and. Read more.</description></item><item><title>Ensuring Effective Helm Charts with Linting, Testing, and Diff Checks üöÄ</title><link>https://kubermates.org/blog/ensuring-effective-helm-charts-with-linting-testing-and-diff-checks-ni0/</link><pubDate>Tue, 01 Oct 2024 16:25:51 +0000</pubDate><guid>https://kubermates.org/blog/ensuring-effective-helm-charts-with-linting-testing-and-diff-checks-ni0/</guid><description>&lt;p&gt;When deploying applications to Kubernetes, using Helm charts is a great way to simplify the process. But how do you make sure your Helm charts are high-quality and won‚Äôt cause issues down the line? Don‚Äôt worry! In this guide, we‚Äôll show you how to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use &lt;strong&gt;Helm Chart-Testing&lt;/strong&gt; for linting and validation üïµÔ∏è‚Äç‚ôÄÔ∏è&lt;/li&gt;
&lt;li&gt;Perform &lt;strong&gt;Unit Testing&lt;/strong&gt; with the Helm Unit Test plugin üîß&lt;/li&gt;
&lt;li&gt;Use &lt;strong&gt;Helm Diff&lt;/strong&gt; to check changes before installing or upgrading üö¶&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By following these steps, you‚Äôll catch potential issues early and ensure smooth deployments. We‚Äôll also build a fully tested &lt;strong&gt;NGINX Helm chart&lt;/strong&gt; at the end!&lt;/p&gt;</description></item><item><title>Blog: Spotlight on CNCF Deaf and Hard-of-hearing Working Group (DHHWG)</title><link>https://kubermates.org/docs/2024-09-30-blog-spotlight-on-cncf-deaf-and-hard-of-hearing-working-group-dhhwg/</link><pubDate>Mon, 30 Sep 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-09-30-blog-spotlight-on-cncf-deaf-and-hard-of-hearing-working-group-dhhwg/</guid><description>In recognition of Deaf Awareness Month and the importance of inclusivity in the tech community, we are spotlighting Catherine Paganini , facilitator and one of the founding members of CNCF Deaf and Hard-of-Hearing Working Group (DHHWG). In this interview, Sandeep Kanabar , a deaf member of the DHHWG and part of the Kubernetes SIG ContribEx Communications team , sits down with Catherine to explore the impact of the DHHWG on cloud native projects like Kubernetes. Sandeep‚Äôs journey is a testament to the power of inclusion. Through his involvement in the DHHWG, he connected with members of the Kubernetes community who encouraged him to join SIG ContribEx - the group responsible for sustaining the Kubernetes contributor experience. In an ecosystem where open-source projects are actively seeking contributors and maintainers, this story highlights how important it is to create pathways for underrepresented groups, including those with disabilities, to contribute their unique perspectives and skills. In this interview, we delve into Catherine‚Äôs journey, the challenges and triumphs of establishing the DHHWG, and the vision for a more inclusive future in cloud native. We invite Kubernetes contributors, maintainers, and community members to reflect on the significance of empathy, advocacy, and community in fostering a truly inclusive environment for all, and to think about how they can support efforts to increase diversity and accessibility within their own projects. Sandeep Kanabar (SK): Hello Catherine, could you please introduce yourself, share your professional background, and explain your connection to the Kubernetes ecosystem? Catherine Paganini (CP) : I‚Äôm the Head of Marketing at Buoyant , the creator of Linkerd , the CNCF-graduated service mesh, and 5th CNCF project. Four years ago, I started contributing to open source. The initial motivation was to make cloud native concepts more accessible to newbies and non-technical people. Without a technical background, it was hard for me to understand what Kubernetes, containers, service meshes, etc. mean.</description></item><item><title>Kubeflow and Me: A Story Started with Push-based Metrics Collection</title><link>https://kubermates.org/docs/2024-09-28-kubeflow-and-me-a-story-started-with-push-based-metrics-collection/</link><pubDate>Sat, 28 Sep 2024 00:00:00 -0500</pubDate><guid>https://kubermates.org/docs/2024-09-28-kubeflow-and-me-a-story-started-with-push-based-metrics-collection/</guid><description>Sep 28, 2024 ‚Ä¢ Shao Wang(Electronic-Waste) ‚Ä¢ 4 min read gsoc This summer, I gained a precious opportunity to participate in the Google Summer of Code(GSoC), in which I would contribute to Katib and fulfill a project named ‚ÄúPush-based Metrics Collection in Katib‚Äù within 12 weeks. Firstly, I got to know about GSoC and Kubeflow with the recommendation from the former active maintainer Ce Gao(gaocegege)‚Äôs personal blog. And I was deeply impressed by the idea of cloud native AI toolkits, I decided to dive into this area and learn some skills to enhance my career and future. In the blog, I‚Äôll provide my personal insight into Katib, for those who are interested in cloud native, AI, and hyperparameters tuning. The project aims to provide a Python SDK API interface for users to push metrics to Katib DB directly. The current implementation of Metrics Collector is pull-based, raising design problems such as determining the frequency at which we scrape the metrics, performance issues like the overhead caused by too many sidecar containers, and restrictions on developing environments that must support sidecar containers and admission webhooks. And also, for data scientists, they need to pay attention to the format of metrics printed in the training scripts, which is error prone and may be hard to recognize. We decided to implement a new API for Katib Python SDK to offer users a push-based way to store metrics directly into the Kaitb DB and resolve those issues raised by pull-based metrics collection. In the new design, users just need to set metrics_collector_config={&amp;ldquo;kind&amp;rdquo;: &amp;ldquo;Push&amp;rdquo;} in the tune() function and call the report_metrics() API in their objective function to push metrics to Katib DB directly. There are no sidecar containers and restricted metric log formats any more. After that, Trial Controller will continuously collect metrics from Katib DB and update the status of Trial, which is the same as pull-based metrics collection. If you are interested in it, please refer to this doc and example for more details.</description></item><item><title>Capsule: How to Use It and First Steps to Set It Up on an AKS Cluster üöÄ</title><link>https://kubermates.org/blog/capsule-how-to-use-it-and-first-steps-to-set-it-up-on-an-aks-cluster-5hl5/</link><pubDate>Tue, 24 Sep 2024 19:24:03 +0000</pubDate><guid>https://kubermates.org/blog/capsule-how-to-use-it-and-first-steps-to-set-it-up-on-an-aks-cluster-5hl5/</guid><description>&lt;p&gt;Capsule is an awesome, open-source solution that helps you manage multiple tenants in Kubernetes clusters, making it super easy to handle multi-tenancy. Whether you‚Äôre running Kubernetes for a big company or providing services for others, Capsule ensures your clusters stay organized and secure!&lt;/p&gt;
&lt;p&gt;In this guide, we‚Äôll dive into what Capsule is, why it‚Äôs a great choice, and show you how to set it up on an Azure Kubernetes Service (AKS) cluster. Ready? Let‚Äôs go! üéâ&lt;/p&gt;</description></item><item><title>Blog: Spotlight on SIG Scheduling</title><link>https://kubermates.org/docs/2024-09-24-blog-spotlight-on-sig-scheduling/</link><pubDate>Tue, 24 Sep 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-09-24-blog-spotlight-on-sig-scheduling/</guid><description>In this SIG Scheduling spotlight we talked with Kensei Nakada , an approver in SIG Scheduling. Arvind: Hello, thank you for the opportunity to learn more about SIG Scheduling! Would you like to introduce yourself and tell us a bit about your role, and how you got involved with Kubernetes? Kensei : Hi, thanks for the opportunity! I‚Äôm Kensei Nakada ( @sanposhiho ), a software engineer at Tetrate. io. I have been contributing to Kubernetes in my free time for more than 3 years, and now I‚Äôm an approver of SIG Scheduling in Kubernetes. Also, I‚Äôm a founder/owner of two SIG subprojects, kube-scheduler-simulator and kube-scheduler-wasm-extension. AP: That‚Äôs awesome! You‚Äôve been involved with the project since a long time. Can you provide a brief overview of SIG Scheduling and explain its role within the Kubernetes ecosystem? KN : As the name implies, our responsibility is to enhance scheduling within Kubernetes. Specifically, we develop the components that determine which Node is the best place for each Pod. In Kubernetes, our main focus is on maintaining the kube-scheduler , along with other scheduling-related components as part of our SIG subprojects. AP: I see, got it! That makes me curious‚Äìwhat recent innovations or developments has SIG Scheduling introduced to Kubernetes scheduling? KN : From a feature perspective, there have been several enhancements to PodTopologySpread recently. PodTopologySpread is a relatively new feature in the scheduler, and we are still in the process of gathering feedback and making improvements. Most recently, we have been focusing on a new internal enhancement called QueueingHint which aims to enhance scheduling throughput.</description></item><item><title>Understanding AKS NAP: Azure Kubernetes Service Node Auto-Provisioning (Powered by Karpenter) üöÄ</title><link>https://kubermates.org/blog/understanding-aks-nap-azure-kubernetes-service-node-auto-provisioning-powered-by-karpenter-3djh/</link><pubDate>Fri, 20 Sep 2024 06:50:50 +0000</pubDate><guid>https://kubermates.org/blog/understanding-aks-nap-azure-kubernetes-service-node-auto-provisioning-powered-by-karpenter-3djh/</guid><description>&lt;p&gt;As more organizations embrace cloud-native technologies like &lt;strong&gt;Kubernetes&lt;/strong&gt;, keeping your infrastructure lean and scalable is key to success. Thankfully, Azure Kubernetes Service (AKS) offers a powerful feature‚Äî&lt;strong&gt;Node Auto-Provisioning (NAP)&lt;/strong&gt;. NAP is powered by &lt;strong&gt;Karpenter&lt;/strong&gt;, a Kubernetes-native tool, and allows automatic node provisioning based on demand, ensuring that your cluster‚Äôs resources are always just right. In this article, we‚Äôll explore how AKS NAP works, how it&amp;rsquo;s powered by &lt;strong&gt;Karpenter&lt;/strong&gt;, and why it can make your life a lot easier! üåü&lt;/p&gt;</description></item><item><title>LLM Hyperparameter Optimization API: My Google Summer of Code Journey with Kubeflow</title><link>https://kubermates.org/docs/2024-09-19-llm-hyperparameter-optimization-api-my-google-summer-of-code-journey-with-kubefl/</link><pubDate>Thu, 19 Sep 2024 00:00:00 -0500</pubDate><guid>https://kubermates.org/docs/2024-09-19-llm-hyperparameter-optimization-api-my-google-summer-of-code-journey-with-kubefl/</guid><description>Sep 19, 2024 ‚Ä¢ Hezhi(Helen) Xie ‚Ä¢ 4 min read gsoc This summer, I had the opportunity to participate in the Google Summer of Code (GSoC) program, where I contributed to Kubeflow, an open-source machine learning toolkit. My project focused on developing a high-level API for optimizing hyperparameters in Large Language Models (LLMs) within Katib, Kubeflow‚Äôs automated hyperparameter tuning system. I‚Äôd like to share insights from this experience with others interested in Kubeflow, GSoC, or optimizing LLMs. The rapid advancements and rising popularity of LLMs, such as GPT and BERT, have created a growing demand for efficient LLMOps in Kubernetes. To address this, we have developed a train API within the Training Python SDK, simplifying the process of fine-tuning LLMs using distributed PyTorchJob workers. However, hyperparameter optimization remains a crucial yet labor-intensive task for enhancing model performance. Hyperparameter optimization is essential but time-consuming, especially for LLMs with billions of parameters. This API simplifies the process by handling Kubernetes infrastructure, allowing data scientists to focus on model performance rather than system configuration. With this API, users can import pretrained models and datasets from Hugging Face and Amazon S3, define parameters including the hyperparameter search space, optimization objective, and resource configuration. The API then automates the creation of Experiment, which contains multiple Trials with different hyperparameter settings using PyTorch distributed training. It then collects and analyzes the metrics from each Trial to identify the optimal hyperparameter configuration. For detailed instruction on using the API, please refer to this guide My work on the project can be broadly divided into four stages: In addition, I addressed several critical bugs in previous Katib and Training Operator releases and contributed new features, such as writing end-to-end tests for the train API.</description></item><item><title>Admission Controllers in Kubernetes: OPA GateKeeper, Kyverno, and Azure Policy Add-on for AKS‚ÄîWhich One Wins? üèÜ</title><link>https://kubermates.org/blog/admission-controllers-in-kubernetes-opa-gatekeeper-kyverno-and-azure-policy-add-on-for-aks-which-one-wins-237d/</link><pubDate>Tue, 17 Sep 2024 07:25:52 +0000</pubDate><guid>https://kubermates.org/blog/admission-controllers-in-kubernetes-opa-gatekeeper-kyverno-and-azure-policy-add-on-for-aks-which-one-wins-237d/</guid><description>&lt;p&gt;When managing a Kubernetes cluster, controlling what gets deployed and ensuring resources comply with security, governance, and operational policies is essential. Admission controllers act as &amp;ldquo;gatekeepers&amp;rdquo; for your cluster, ensuring only compliant resources get through. üõ°Ô∏è&lt;/p&gt;
&lt;p&gt;Three popular options for extending Kubernetes&amp;rsquo; admission control functionality are &lt;strong&gt;OPA GateKeeper&lt;/strong&gt;, &lt;strong&gt;Kyverno&lt;/strong&gt;, and the &lt;strong&gt;Azure Policy Add-on for AKS&lt;/strong&gt; (which incorporates OPA GateKeeper&amp;rsquo;s engine). In this article, we‚Äôll compare these solutions and show why &lt;strong&gt;Kyverno&lt;/strong&gt; is still the most user-friendly and versatile option for Kubernetes users. üåä&lt;/p&gt;</description></item><item><title>Upgrading AKS: In-Place, Blue-Green, and Canary Upgrades Explained üöÄ</title><link>https://kubermates.org/blog/upgrading-aks-in-place-blue-green-and-canary-upgrades-explained-3aap/</link><pubDate>Sun, 15 Sep 2024 10:09:30 +0000</pubDate><guid>https://kubermates.org/blog/upgrading-aks-in-place-blue-green-and-canary-upgrades-explained-3aap/</guid><description>&lt;p&gt;Keeping your Azure Kubernetes Service (AKS) cluster up to date is crucial for security, performance, and accessing new features. AKS offers different strategies for upgrading, and in this guide, we‚Äôll walk through the main methods: &lt;strong&gt;In-place upgrades&lt;/strong&gt;, &lt;strong&gt;Blue-Green deployments&lt;/strong&gt;, and &lt;strong&gt;Canary upgrades&lt;/strong&gt;, complete with real-world examples to help you understand the process!&lt;/p&gt;
&lt;h2 id="types-of-aks-upgrades-"&gt;Types of AKS Upgrades üöß&lt;/h2&gt;
&lt;h3 id="1-in-place-upgrades-"&gt;1. &lt;strong&gt;In-Place Upgrades&lt;/strong&gt; üîÑ&lt;/h3&gt;
&lt;p&gt;An &lt;strong&gt;In-place upgrade&lt;/strong&gt; is the most straightforward method, where the upgrade happens directly on your current AKS cluster. It updates the control plane and worker nodes without creating new resources.&lt;/p&gt;</description></item><item><title>Monitor and Optimize Multi-Cluster AKS Costs üí∞</title><link>https://kubermates.org/blog/monitor-and-optimize-multi-cluster-aks-costs-4627/</link><pubDate>Thu, 12 Sep 2024 07:42:06 +0000</pubDate><guid>https://kubermates.org/blog/monitor-and-optimize-multi-cluster-aks-costs-4627/</guid><description>&lt;p&gt;As businesses scale their Kubernetes workloads across multiple Azure Kubernetes Service (AKS) clusters, managing and optimizing cloud costs becomes critical. Deploying and managing observability tools such as KubeCost and OpenTelemetry (OTel) across multiple clusters can be simplified using &lt;a href="https://github.com/Azure/fleet/blob/main/docs/concepts/README.md"&gt;AKS Fleet Manager&lt;/a&gt;, Microsoft Managed Prometheus, and Grafana.&lt;/p&gt;
&lt;p&gt;This guide will explain how to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Deploy &lt;strong&gt;KubeCost&lt;/strong&gt; and &lt;strong&gt;OpenTelemetry&lt;/strong&gt; across multiple AKS clusters using &lt;strong&gt;AKS Fleet Manager&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Expose metrics through OpenTelemetry.&lt;/li&gt;
&lt;li&gt;Centralize monitoring via &lt;strong&gt;Managed Prometheus&lt;/strong&gt; and &lt;strong&gt;Grafana&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You‚Äôll gain a single-pane-of-glass view into your multi-cluster environment, enabling more efficient resource utilization and cost management.&lt;/p&gt;</description></item><item><title>üöÄ Enhancing Container Security: The Complete Guide to Secure and Clean Kubernetes Clusters üõ°Ô∏èüßº</title><link>https://kubermates.org/blog/enhancing-container-security-the-complete-guide-to-secure-and-clean-kubernetes-clusters-1ida/</link><pubDate>Wed, 11 Sep 2024 07:19:50 +0000</pubDate><guid>https://kubermates.org/blog/enhancing-container-security-the-complete-guide-to-secure-and-clean-kubernetes-clusters-1ida/</guid><description>&lt;p&gt;As Kubernetes continues to grow in popularity, ensuring the security and cleanliness of your container images is crucial. In this guide, we‚Äôll cover two key strategies: &lt;strong&gt;image signing using Notary&lt;/strong&gt; üñãÔ∏è and the &lt;strong&gt;AKS Image Cleaner (Eraser)&lt;/strong&gt; add-on üßº. Together, they form a robust, secure, and efficient container management workflow.&lt;/p&gt;
&lt;p&gt;By the end, you&amp;rsquo;ll know how to ensure that your AKS cluster pulls only verified, trusted images and stays free of unused images that could pose a risk to your environment.&lt;/p&gt;</description></item><item><title>Karmada: Deep Dive into Managing Multiple AKS Clusters üöÄ</title><link>https://kubermates.org/blog/karmada-deep-dive-into-managing-multiple-aks-clusters-1j08/</link><pubDate>Mon, 09 Sep 2024 07:00:23 +0000</pubDate><guid>https://kubermates.org/blog/karmada-deep-dive-into-managing-multiple-aks-clusters-1j08/</guid><description>&lt;p&gt;In today‚Äôs cloud-driven world, Kubernetes has become the go-to platform for running containerized applications. If you&amp;rsquo;re using Microsoft Azure Kubernetes Service (AKS), you know how powerful it can be. But what if you‚Äôre managing not just one, but multiple AKS clusters across different environments? Sounds a bit overwhelming, right? üòÖ&lt;/p&gt;
&lt;p&gt;That‚Äôs where &lt;strong&gt;Karmada&lt;/strong&gt; (Kubernetes Armada) comes to the rescue! Karmada is like your multi-cluster superhero, helping you deploy and manage applications across multiple AKS clusters as if they were one big happy family. This deep dive will take you through Karmada‚Äôs architecture, installation process, advanced deployment scenarios, best strategies, and how to integrate Karmada into your CI/CD pipelines with practical examples. üåü&lt;/p&gt;</description></item><item><title>Understanding eBPF and Its Application in Modern Cloud Environments üöÄ</title><link>https://kubermates.org/blog/understanding-ebpf-and-its-application-in-modern-cloud-environments-3f99/</link><pubDate>Sun, 08 Sep 2024 06:53:32 +0000</pubDate><guid>https://kubermates.org/blog/understanding-ebpf-and-its-application-in-modern-cloud-environments-3f99/</guid><description>&lt;h2 id="what-is-ebpf-"&gt;What is eBPF? ü§î&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;eBPF&lt;/strong&gt; (Extended Berkeley Packet Filter) is like magic for the Linux kernel! ü™Ñ It lets developers run custom code directly within the kernel, safely and efficiently, without needing to modify the kernel&amp;rsquo;s source code or load new modules. Originally, eBPF was created to help with network packet filtering, but it has evolved into a Swiss Army knife üõ†Ô∏è for all sorts of tasks, from observability to security and system performance monitoring.&lt;/p&gt;</description></item><item><title>üåê Securing Kubernetes Secrets in AKS: Using Azure Key Vault with Managed and User Assigned Identities üöÄ</title><link>https://kubermates.org/blog/securing-kubernetes-secrets-in-aks-using-azure-key-vault-with-managed-and-user-assigned-identities-569k/</link><pubDate>Wed, 04 Sep 2024 11:35:22 +0000</pubDate><guid>https://kubermates.org/blog/securing-kubernetes-secrets-in-aks-using-azure-key-vault-with-managed-and-user-assigned-identities-569k/</guid><description>&lt;p&gt;Hello Kubernetes enthusiast! üëã Let‚Äôs dive into a critical aspect of securing your applications running in Azure Kubernetes Service (AKS): managing secrets. While Kubernetes Secrets provide a way to manage sensitive information like passwords and API keys, they aren‚Äôt encrypted by default and can be vulnerable if not handled correctly. In this guide, we&amp;rsquo;ll explore how to securely manage secrets by integrating Azure Key Vault with AKS using both &lt;strong&gt;VM Managed Identities&lt;/strong&gt; and &lt;strong&gt;User Assigned Identities&lt;/strong&gt;. Plus, we&amp;rsquo;ll show you how to enable the Secret Store CSI Driver directly in AKS.&lt;/p&gt;</description></item><item><title>üöÄ Automating Image Vulnerability Patching in Kubernetes with Trivy Operator, Copacetic, and GitHub Actions</title><link>https://kubermates.org/blog/automating-image-vulnerability-patching-in-kubernetes-with-trivy-operator-copacetic-and-github-actions-13l/</link><pubDate>Tue, 03 Sep 2024 14:52:24 +0000</pubDate><guid>https://kubermates.org/blog/automating-image-vulnerability-patching-in-kubernetes-with-trivy-operator-copacetic-and-github-actions-13l/</guid><description>&lt;h1 id="-automating-image-vulnerability-patching-in-kubernetes-with-trivy-operator-copacetic-and-github-actions"&gt;üöÄ Automating Image Vulnerability Patching in Kubernetes with Trivy Operator, Copacetic, and GitHub Actions&lt;/h1&gt;
&lt;h2 id="1-installing-and-using-copacetic-copa-cli"&gt;1. Installing and Using Copacetic (Copa) CLI&lt;/h2&gt;
&lt;p&gt;Copacetic is a CLI tool (&lt;code&gt;copa&lt;/code&gt;) designed to help automate the patching of vulnerabilities in your container images. Here‚Äôs how to install it:&lt;/p&gt;
&lt;h3 id="11-clone-the-copacetic-repository"&gt;1.1. Clone the Copacetic Repository&lt;/h3&gt;
&lt;p&gt;Start by cloning the Copacetic repository to your local machine:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;git clone https://github.com/project-copacetic/copacetic
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; copacetic
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="12-build-copacetic"&gt;1.2. Build Copacetic&lt;/h3&gt;
&lt;p&gt;Inside the cloned directory, build the Copacetic CLI:&lt;/p&gt;</description></item><item><title>Autoscaling in Kubernetes: KEDA, Karpenter, and Native Autoscalers</title><link>https://kubermates.org/blog/autoscaling-in-kubernetes-keda-karpenter-and-native-autoscalers-1gpo/</link><pubDate>Mon, 02 Sep 2024 11:41:41 +0000</pubDate><guid>https://kubermates.org/blog/autoscaling-in-kubernetes-keda-karpenter-and-native-autoscalers-1gpo/</guid><description>&lt;p&gt;Autoscaling is a critical component of any robust Kubernetes environment, ensuring your applications and infrastructure can dynamically adjust to meet demand. In this guide, we&amp;rsquo;ll explore three powerful autoscaling tools: &lt;strong&gt;KEDA&lt;/strong&gt; for event-driven pod autoscaling, &lt;strong&gt;Karpenter&lt;/strong&gt; for dynamic node scaling, and Kubernetes&amp;rsquo; native autoscalers (HPA and VPA). We&amp;rsquo;ll dive into how to use them effectively, with plenty of examples to get you started. üöÄ&lt;/p&gt;
&lt;h2 id="introduction-to-keda-"&gt;Introduction to KEDA üöÄ&lt;/h2&gt;
&lt;p&gt;KEDA (Kubernetes-based Event Driven Autoscaling) allows you to scale applications based on custom event metrics, not just CPU or memory usage. It‚Äôs ideal for scenarios where workloads are triggered by external events, such as message queues, databases, or HTTP requests. Whether you&amp;rsquo;re processing incoming orders, reacting to sensor data, or scaling based on custom Prometheus metrics, KEDA has you covered! üí•&lt;/p&gt;</description></item><item><title>Insights on Securing Your Kubernetes Cluster with Falco üöÄüîí</title><link>https://kubermates.org/blog/insights-on-securing-your-kubernetes-cluster-with-falco-4b9g/</link><pubDate>Sun, 01 Sep 2024 12:11:29 +0000</pubDate><guid>https://kubermates.org/blog/insights-on-securing-your-kubernetes-cluster-with-falco-4b9g/</guid><description>&lt;p&gt;Falco is a powerful open-source security tool designed to monitor your Kubernetes cluster in real-time, detecting suspicious activities based on customizable rules. Implementing Falco effectively can significantly enhance your cluster‚Äôs security. In this comprehensive guide, we‚Äôll cover everything from installing Falco to best practices for implementing rules, and how to defend against potential bypasses.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="-1-installing-falco-in-kubernetes"&gt;üåü 1. Installing Falco in Kubernetes&lt;/h2&gt;
&lt;p&gt;Getting started with Falco is straightforward. Here‚Äôs how you can install it using Helm, a popular package manager for Kubernetes.&lt;/p&gt;</description></item><item><title>üöÄ Building a Kubernetes Operator with an NGINX CRD</title><link>https://kubermates.org/blog/building-a-kubernetes-operator-with-an-nginx-crd-3lil/</link><pubDate>Thu, 29 Aug 2024 08:25:36 +0000</pubDate><guid>https://kubermates.org/blog/building-a-kubernetes-operator-with-an-nginx-crd-3lil/</guid><description>&lt;p&gt;Kubernetes is a powerful platform that automates the deployment, scaling, and management of containerized applications. One of the coolest features of Kubernetes is its ability to be extended with &lt;strong&gt;Custom Resource Definitions (CRDs)&lt;/strong&gt; and &lt;strong&gt;Operators&lt;/strong&gt;. In this guide, we&amp;rsquo;ll build a simple Kubernetes operator using an NGINX CRD to manage NGINX instances in your cluster.&lt;/p&gt;
&lt;h3 id="-understanding-kubernetes-controllers-operators-and-crds"&gt;ü§ñ Understanding Kubernetes Controllers, Operators, and CRDs&lt;/h3&gt;
&lt;h4 id="what-is-a-kubernetes-controller"&gt;What is a Kubernetes Controller?&lt;/h4&gt;
&lt;p&gt;A Kubernetes controller is like a robot ü§ñ that continuously monitors your cluster. It checks whether the actual state of the resources matches the desired state (what you want) and makes adjustments to align them.&lt;/p&gt;</description></item><item><title>üé® Hacking the Helm Operator with Flux: Creating Self-Installable Services for Easier App Deployment</title><link>https://kubermates.org/blog/hacking-the-helm-operator-with-flux-creating-self-installable-services-for-easier-app-deployment-5a8l/</link><pubDate>Wed, 28 Aug 2024 08:01:26 +0000</pubDate><guid>https://kubermates.org/blog/hacking-the-helm-operator-with-flux-creating-self-installable-services-for-easier-app-deployment-5a8l/</guid><description>&lt;p&gt;Managing applications in Kubernetes can be tricky, but with tools like Helm, operators, and Flux, you can make the process smoother and even fun! In this guide, we&amp;rsquo;ll walk you through how to hack the Helm Operator using the Operator SDK and Flux to create powerful, self-installable services that make deploying apps like NGINX, Apache Tomcat, and even Redis a breeze. üå¨Ô∏è&lt;/p&gt;
&lt;p&gt;By the end, you&amp;rsquo;ll have your very own GitOps-powered system, making deployments as simple as pushing to a Git repository. Let&amp;rsquo;s dive in!&lt;/p&gt;</description></item><item><title>Chaos Engineering Let's Break Everything! üòà</title><link>https://kubermates.org/blog/chaos-engineering-lets-break-everything-io0/</link><pubDate>Tue, 27 Aug 2024 10:51:45 +0000</pubDate><guid>https://kubermates.org/blog/chaos-engineering-lets-break-everything-io0/</guid><description>&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Hey there! üëã If you&amp;rsquo;re running your applications on Kubernetes, you might already know that things can go wrong in unexpected ways. That&amp;rsquo;s where &lt;strong&gt;chaos engineering&lt;/strong&gt; comes in! Chaos engineering is all about intentionally injecting failures into your system to see how it behaves under stress. The idea is to discover weaknesses and fix them before they can cause real problems.&lt;/p&gt;
&lt;p&gt;Today, we&amp;rsquo;re diving into &lt;strong&gt;Chaos Mesh&lt;/strong&gt;, an awesome tool that makes chaos engineering in Kubernetes super easy and fun (well, as fun as breaking things can be!). We&amp;rsquo;ll go step-by-step through setting up Chaos Mesh and show you how to run some cool chaos experiments to test your app&amp;rsquo;s resilience.&lt;/p&gt;</description></item><item><title>A Guide to Modern Kubernetes Service Networking üöÄ</title><link>https://kubermates.org/blog/a-guide-to-modern-kubernetes-service-networking-4017/</link><pubDate>Mon, 26 Aug 2024 19:32:17 +0000</pubDate><guid>https://kubermates.org/blog/a-guide-to-modern-kubernetes-service-networking-4017/</guid><description>&lt;p&gt;As Kubernetes becomes the go-to platform for managing cloud-native applications, how we handle network traffic has evolved. If you‚Äôre already using Kubernetes, you might have heard about service meshes like Istio and Linkerd, which have been popular choices for managing service-to-service communication. But now, there‚Äôs a new player in town‚Äîthe Kubernetes Gateway API! Let‚Äôs dive into what the Gateway API is, how it compares to Istio and Linkerd, and when you might want to use each. üåü&lt;/p&gt;</description></item><item><title>üõ°Ô∏è Effective Vulnerability Monitoring in Kubernetes</title><link>https://kubermates.org/blog/effective-vulnerability-monitoring-in-kubernetes-1mge/</link><pubDate>Mon, 26 Aug 2024 10:32:34 +0000</pubDate><guid>https://kubermates.org/blog/effective-vulnerability-monitoring-in-kubernetes-1mge/</guid><description>&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Hey there, Kubernetes explorer! üåü As your Kubernetes environment grows, keeping it secure becomes more challenging, especially when dealing with multiple clusters. Imagine managing several clusters (spokes) and needing a single source of truth for all your security metrics‚Äîsounds like a big task, right? ü§î That&amp;rsquo;s where &lt;strong&gt;Trivy&lt;/strong&gt;, &lt;strong&gt;Trivy Operator&lt;/strong&gt;, &lt;strong&gt;OpenTelemetry&lt;/strong&gt;, &lt;strong&gt;Prometheus&lt;/strong&gt;, and &lt;strong&gt;Grafana&lt;/strong&gt; come to the rescue.&lt;/p&gt;
&lt;p&gt;In this guide, I‚Äôll show you how to set up Trivy and Trivy Operator in a federated Kubernetes environment, collect vulnerability data using OpenTelemetry, and centralize it using either an in-cluster Prometheus setup or managed services like &lt;strong&gt;Azure Monitor (for Prometheus)&lt;/strong&gt; and &lt;strong&gt;Grafana Cloud&lt;/strong&gt; or &lt;strong&gt;Azure Managed Grafana&lt;/strong&gt;. By the end of this, you‚Äôll have a system that monitors vulnerabilities across all your clusters from one place. Let‚Äôs dive in! üèä‚Äç‚ôÇÔ∏è&lt;/p&gt;</description></item><item><title>Kubernetes Multi-Cluster Management üì¶</title><link>https://kubermates.org/blog/kubernetes-multi-cluster-management-1nek/</link><pubDate>Fri, 23 Aug 2024 15:10:22 +0000</pubDate><guid>https://kubermates.org/blog/kubernetes-multi-cluster-management-1nek/</guid><description>&lt;p&gt;Managing Kubernetes deployments across multiple clusters is a complex yet crucial task for scaling modern applications. Whether ensuring consistency across environments or automating deployments for high availability, choosing the right tools and approach is essential. In this guide, we‚Äôll explore five powerful tools‚Äî&lt;strong&gt;Helmfile&lt;/strong&gt;, &lt;strong&gt;FluxCD&lt;/strong&gt;, &lt;strong&gt;ArgoCD&lt;/strong&gt;, &lt;strong&gt;ClusterAPI&lt;/strong&gt;, and &lt;strong&gt;Karmada&lt;/strong&gt;‚Äîand how they can help you efficiently manage multi-cluster Kubernetes environments. Let‚Äôs dive into the details and discover which strategy suits your needs best! üåê&lt;/p&gt;</description></item><item><title>How to Make Your Docker Images Go on a Diet üèä‚Äç‚ôÇÔ∏è</title><link>https://kubermates.org/blog/how-to-make-your-docker-images-go-on-a-diet-88l/</link><pubDate>Wed, 21 Aug 2024 07:50:27 +0000</pubDate><guid>https://kubermates.org/blog/how-to-make-your-docker-images-go-on-a-diet-88l/</guid><description>&lt;p&gt;Hey there, Docker enthusiast! üåü Are your Docker images feeling a little bloated lately? Don&amp;rsquo;t worry, you&amp;rsquo;re not alone. As our applications grow, so do our Docker images, but with a few nifty tricks, you can slim them down and keep everything running smooth as butter. üßà In this guide, we&amp;rsquo;ll explore how to optimize your Docker images using Alpine, Distroless, Scratch images, and multi-stage builds. Let&amp;rsquo;s dive in! üèä‚Äç‚ôÇÔ∏è&lt;/p&gt;</description></item><item><title>A quick navigation through Service Mesh in Kubernetes üëÄ</title><link>https://kubermates.org/blog/a-quick-navigation-through-service-mesh-in-kubernetes-5dea/</link><pubDate>Tue, 20 Aug 2024 10:52:45 +0000</pubDate><guid>https://kubermates.org/blog/a-quick-navigation-through-service-mesh-in-kubernetes-5dea/</guid><description>&lt;p&gt;If you&amp;rsquo;re working with Kubernetes, you know that managing communication between microservices can get complicated as your application grows. Enter &lt;strong&gt;Linkerd&lt;/strong&gt;, a powerful yet user-friendly service mesh that simplifies this process by handling traffic management, security, and observability for your microservices. In this article, we&amp;rsquo;ll walk you through what Linkerd is, how to set it up, and how to use it to manage your services, including examples of Blue-Green and Canary deployments. We‚Äôll also compare Linkerd with Istio, another popular service mesh, and provide references to official documentation to help you along the way.&lt;/p&gt;</description></item><item><title>üöÄ Kubernetes RBAC and Role Aggregation Made Easy</title><link>https://kubermates.org/blog/kubernetes-rbac-and-role-aggregation-made-easy-3j4o/</link><pubDate>Mon, 19 Aug 2024 21:03:46 +0000</pubDate><guid>https://kubermates.org/blog/kubernetes-rbac-and-role-aggregation-made-easy-3j4o/</guid><description>&lt;h2 id="what-is-kubernetes-rbac-"&gt;What is Kubernetes RBAC? ü§î&lt;/h2&gt;
&lt;p&gt;Kubernetes, the platform that helps you automate, scale, and manage your containerized applications, comes with a cool feature called &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/"&gt;Role-Based Access Control (RBAC)&lt;/a&gt;. Think of RBAC as a gatekeeper that controls who can do what within your Kubernetes cluster. It‚Äôs super important because it ensures that everyone and everything (like users, applications, and services) only have the permissions they need‚Äînothing more, nothing less.&lt;/p&gt;
&lt;h2 id="the-four-pillars-of-kubernetes-rbac-"&gt;The Four Pillars of Kubernetes RBAC üèõÔ∏è&lt;/h2&gt;
&lt;p&gt;Kubernetes RBAC revolves around four main building blocks:&lt;/p&gt;</description></item><item><title>Essential Tips for Setting Resource Limits in Kubernetes üìà</title><link>https://kubermates.org/blog/essential-tips-for-setting-resource-limits-in-kubernetes-3b54/</link><pubDate>Mon, 19 Aug 2024 07:31:56 +0000</pubDate><guid>https://kubermates.org/blog/essential-tips-for-setting-resource-limits-in-kubernetes-3b54/</guid><description>&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Kubernetes is like the ultimate conductor üé∂ in an orchestra, ensuring that all your applications (the musicians) play in harmony without hogging too many resources (the instruments). But if you don&amp;rsquo;t set the right limits, things can get out of tune quickly! üéª Setting the right resource limitations in Kubernetes helps keep everything running smoothly, preventing any one application from using too much CPU or memory and leaving the rest high and dry.&lt;/p&gt;</description></item><item><title>üåê Kubernetes DNS: How to Use In-Cluster and Azure Private DNS Together</title><link>https://kubermates.org/blog/kubernetes-dns-how-to-use-in-cluster-and-azure-private-dns-together-48h3/</link><pubDate>Sat, 17 Aug 2024 17:51:44 +0000</pubDate><guid>https://kubermates.org/blog/kubernetes-dns-how-to-use-in-cluster-and-azure-private-dns-together-48h3/</guid><description>&lt;p&gt;Kubernetes is a powerful platform for managing containerized applications, providing robust tools for networking, service discovery, and DNS resolution. This guide will explore how Kubernetes handles DNS resolution within the cluster and how you can integrate it with Azure Private DNS to securely resolve external, private resources.&lt;/p&gt;
&lt;h2 id="-understanding-kubernetes-services-and-dns-resolution"&gt;üîç Understanding Kubernetes Services and DNS Resolution&lt;/h2&gt;
&lt;p&gt;Kubernetes Services provide a stable network identity for a set of Pods, allowing other components to communicate with them reliably. This is crucial since Pods in Kubernetes are ephemeral and their IP addresses can change over time.&lt;/p&gt;</description></item><item><title>How to Manage Kubernetes App Storage Like a Pro üìÅ</title><link>https://kubermates.org/blog/how-to-manage-kubernetes-app-storage-like-a-pro-o33/</link><pubDate>Fri, 16 Aug 2024 20:25:33 +0000</pubDate><guid>https://kubermates.org/blog/how-to-manage-kubernetes-app-storage-like-a-pro-o33/</guid><description>&lt;p&gt;Managing storage in Kubernetes might seem a bit tricky at first, but don‚Äôt worry‚Äîwe&amp;rsquo;re here to help! This guide will walk you through everything you need to know about Kubernetes volumes, how they work, and how to use them effectively, especially if you&amp;rsquo;re using Azure.&lt;/p&gt;
&lt;h2 id="-what-are-kubernetes-volumes"&gt;üìÇ What Are Kubernetes Volumes?&lt;/h2&gt;
&lt;p&gt;Think of &lt;strong&gt;volumes&lt;/strong&gt; as a way to store data in your Kubernetes pods that doesn‚Äôt disappear when the pod shuts down. This is super important for things like saving files, databases, or anything else that needs to stick around.&lt;/p&gt;</description></item><item><title>Understanding Pod Topology Spread Constraints and Node Affinity in Kubernetes</title><link>https://kubermates.org/blog/understanding-pod-topology-spread-constraints-and-node-affinity-in-kubernetes-49a2/</link><pubDate>Thu, 15 Aug 2024 14:26:38 +0000</pubDate><guid>https://kubermates.org/blog/understanding-pod-topology-spread-constraints-and-node-affinity-in-kubernetes-49a2/</guid><description>&lt;p&gt;When you&amp;rsquo;re running applications in Kubernetes, it&amp;rsquo;s important to think about where your Pods (the units that make up your application) are placed in your cluster. Getting this right helps keep your application available, resilient, and running smoothly. Two tools that can help you do this are &lt;strong&gt;Pod Topology Spread Constraints&lt;/strong&gt; and &lt;strong&gt;Node Affinity&lt;/strong&gt;. Let‚Äôs break these down with some easy-to-understand examples.&lt;/p&gt;
&lt;h2 id="1-pod-topology-spread-constraints"&gt;1. Pod Topology Spread Constraints&lt;/h2&gt;
&lt;p&gt;Think of &lt;strong&gt;Pod Topology Spread Constraints&lt;/strong&gt; as a way to tell Kubernetes, &amp;ldquo;Hey, I want my Pods spread out evenly across different parts of my cluster.&amp;rdquo; This helps prevent all your Pods from ending up in the same spot, which could be a problem if that spot has an issue.&lt;/p&gt;</description></item><item><title>How to Test the Latest Kubernetes Changes in Version 1.31 "Elli"</title><link>https://kubermates.org/blog/how-to-test-the-latest-kubernetes-changes-in-version-131-elli-39ec/</link><pubDate>Wed, 14 Aug 2024 21:38:00 +0000</pubDate><guid>https://kubermates.org/blog/how-to-test-the-latest-kubernetes-changes-in-version-131-elli-39ec/</guid><description>&lt;p&gt;Testing Kubernetes 1.31 &amp;ldquo;Elli&amp;rdquo; involves setting up a dedicated environment, verifying new features, validating API changes, running automated tests, and closely monitoring your cluster. Here‚Äôs a detailed guide with examples for each step.&lt;/p&gt;
&lt;h2 id="1-set-up-a-testing-environment"&gt;1. Set Up a Testing Environment&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Create a Kubernetes Cluster&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Use Minikube to create a local cluster. Run:
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;minikube start --kubernetes-version&lt;span class="o"&gt;=&lt;/span&gt;v1.31.0
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;This command sets up a Kubernetes 1.31 cluster locally, allowing you to test the new features and changes in a controlled environment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cloud-Based Testing&lt;/strong&gt;: For cloud environments, use a tool like &lt;code&gt;eksctl&lt;/code&gt; for Amazon EKS:
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;eksctl create cluster --version 1.31 --name test-cluster
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;This command creates an Amazon EKS cluster with Kubernetes 1.31, suitable for more extensive testing scenarios.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Isolate the Environment&lt;/strong&gt;:&lt;/p&gt;</description></item><item><title>Observe deleted Kubernetes components in Grafana Cloud to boost troubleshooting and resource management</title><link>https://kubermates.org/docs/2024-08-08-observe-deleted-kubernetes-components-in-grafana-cloud-to-boost-troubleshooting-/</link><pubDate>Thu, 08 Aug 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-08-08-observe-deleted-kubernetes-components-in-grafana-cloud-to-boost-troubleshooting-/</guid><description>Elizabeth Burkly √Ç¬∑ 26 Aug 2025 √Ç¬∑ 9 min read With our Voice of Customer program, we can tighten the loop between what our users ask for and what we prioritize in R&amp;amp;D. Here√¢¬Ä¬ôs a look at some recent. Read more.</description></item><item><title>Blog: Spotlight on SIG API Machinery</title><link>https://kubermates.org/docs/2024-08-07-blog-spotlight-on-sig-api-machinery/</link><pubDate>Wed, 07 Aug 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-08-07-blog-spotlight-on-sig-api-machinery/</guid><description>We recently talked with Federico Bongiovanni (Google) and David Eads (Red Hat), Chairs of SIG API Machinery, to know a bit more about this Kubernetes Special Interest Group. Frederico (FSM): Hello, and thank your for your time. To start with, could you tell us about yourselves and how you got involved in Kubernetes? David : I started working on OpenShift (the Red Hat distribution of Kubernetes) in the fall of 2014 and got involved pretty quickly in API Machinery. My first PRs were fixing kube-apiserver error messages and from there I branched out to kubectl ( kubeconfigs are my fault!), auth ( RBAC and *Review APIs are ports from OpenShift), apps ( workqueues and sharedinformers for example). Don‚Äôt tell the others, but API Machinery is still my favorite :) Federico : I was not as early in Kubernetes as David, but now it‚Äôs been more than six years. At my previous company we were starting to use Kubernetes for our own products, and when I came across the opportunity to work directly with Kubernetes I left everything and boarded the ship (no pun intended). I joined Google and Kubernetes in early 2018, and have been involved since. FSM: It only takes a quick look at the SIG API Machinery charter to see that it has quite a significant scope, nothing less than the Kubernetes control plane. Could you describe this scope in your own words? David : We own the kube-apiserver and how to efficiently use it. On the backend, that includes its contract with backend storage and how it allows API schema evolution over time. On the frontend, that includes schema best practices, serialization, client patterns, and controller patterns on top of all of it. Federico : Kubernetes has a lot of different components, but the control plane has a really critical mission: it‚Äôs your communication layer with the cluster and also owns all the extensibility mechanisms that make Kubernetes so powerful.</description></item><item><title>Monitor these Kubernetes signals to help rightsize your fleet</title><link>https://kubermates.org/docs/2024-07-29-monitor-these-kubernetes-signals-to-help-rightsize-your-fleet/</link><pubDate>Mon, 29 Jul 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-07-29-monitor-these-kubernetes-signals-to-help-rightsize-your-fleet/</guid><description>Serena Kei √Ç¬∑ 15 Jul 2025 √Ç¬∑ 4 min read The latest backend update to our Kubernetes Monitoring app in Grafana Cloud features significant improvements to alert rules and recording rules that. Read more.</description></item><item><title>Kubeflow 1.9: New Tools for Model Management and Training Optimization</title><link>https://kubermates.org/docs/2024-07-22-kubeflow-1-9-new-tools-for-model-management-and-training-optimization/</link><pubDate>Mon, 22 Jul 2024 00:00:00 -0500</pubDate><guid>https://kubermates.org/docs/2024-07-22-kubeflow-1-9-new-tools-for-model-management-and-training-optimization/</guid><description>Jul 22, 2024 ‚Ä¢ Kubeflow 1. 9 Release Team, Stefano Fioravanzo ‚Ä¢ 11 min read release Kubeflow 1. 9 significantly simplifies the development, tuning and management of secure machine learning models and LLMs. Highlights include: These updates aim to simplify workflows, improve integration dependencies, and provide Kubernetes-native operational efficiencies for enterprise scale, security, and isolation. A model registry provides a central catalog for ML model developers to index and manage models, versions, and ML artifacts metadata. It fills a gap between model experimentation and production activities. It provides a central interface for all stakeholders in the ML lifecycle to collaborate on ML models. Model registry has been asked by the community for a long time and we are delighted to introduce it to the Kubeflow ecosystem. This initial release includes REST APIs and a Python SDK to track model artifacts and model metadata with a standardized format that can be reused across Kubeflow components, such as to deploy Inference Servers. You can get started by following the Model Registry tutorial on the Kubeflow website , or see a short demo video of the Model Registry in action. We are just getting started. This is an Alpha version and we look forward to feedback.</description></item><item><title>Blog: Spotlight on SIG Node</title><link>https://kubermates.org/docs/2024-06-20-blog-spotlight-on-sig-node/</link><pubDate>Thu, 20 Jun 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-06-20-blog-spotlight-on-sig-node/</guid><description>In the world of container orchestration, Kubernetes reigns supreme, powering some of the most complex and dynamic applications across the globe. Behind the scenes, a network of Special Interest Groups (SIGs) drives Kubernetes‚Äô innovation and stability. Today, I have the privilege of speaking with Matthias Bertschy , Gunju Kim , and Sergey Kanzhelev , members of SIG Node , who will shed some light on their roles, challenges, and the exciting developments within SIG Node. Answers given collectively by all interviewees will be marked by their initials. Arpit: Thank you for joining us today. Could you please introduce yourselves and provide a brief overview of your roles within SIG Node? Matthias: My name is Matthias Bertschy, I am French and live next to Lake Geneva, near the French Alps. I have been a Kubernetes contributor since 2017, a reviewer for SIG Node and a maintainer of Prow. I work as a Senior Kubernetes Developer for a security startup named ARMO , which donated Kubescape to the CNCF. Gunju: My name is Gunju Kim. I am a software engineer at NAVER , where I focus on developing a cloud platform for search services. I have been contributing to the Kubernetes project in my free time since 2021. Sergey: My name is Sergey Kanzhelev.</description></item><item><title>The Linux Foundation Announces Schedule for Open Source Summit Europe 2024</title><link>https://kubermates.org/events/2024-06-13-the-linux-foundation-announces-schedule-for-open-source-summit-europe-2024/</link><pubDate>Thu, 13 Jun 2024 19:44:29 +0000</pubDate><guid>https://kubermates.org/events/2024-06-13-the-linux-foundation-announces-schedule-for-open-source-summit-europe-2024/</guid><description>&lt;p&gt;The premier event in Europe for open source code and community contributors offers over 250 sessions across 17 ‚Ä¶&lt;/p&gt;</description></item><item><title>Blog: Introducing Hydrophone</title><link>https://kubermates.org/docs/2024-05-23-blog-introducing-hydrophone/</link><pubDate>Thu, 23 May 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-05-23-blog-introducing-hydrophone/</guid><description>In the ever-changing landscape of Kubernetes, ensuring that clusters operate as intended is essential. This is where conformance testing becomes crucial, verifying that a Kubernetes cluster meets the required standards set by the community. Today, we‚Äôre thrilled to introduce Hydrophone , a lightweight runner designed to streamline Kubernetes tests using the official conformance images released by the Kubernetes release team. Hydrophone‚Äôs design philosophy centers around ease of use. By starting the conformance image as a pod within the conformance namespace, Hydrophone waits for the tests to conclude, then prints and exports the results. This approach offers a hassle-free method for running either individual tests or the entire Conformance Test Suite. In the Kubernetes world, where providers like EKS, Rancher, and k3s offer diverse environments, ensuring consistent experiences is vital. This consistency is anchored in conformance testing, which validates whether these environments adhere to Kubernetes community standards. Historically, this validation has either been cumbersome or requires third-party tools. Hydrophone offers a simple, single binary tool that streamlines running these essential conformance tests. It‚Äôs designed to be user-friendly, allowing for straightforward validation of Kubernetes clusters against community benchmarks, ensuring providers can offer a certified, consistent service. Hydrophone doesn‚Äôt aim to replace the myriad of Kubernetes testing frameworks out there but rather to complement them.</description></item><item><title>Blog: Migrate to Google Artifact Registry</title><link>https://kubermates.org/docs/2024-05-06-blog-migrate-to-google-artifact-registry/</link><pubDate>Mon, 06 May 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-05-06-blog-migrate-to-google-artifact-registry/</guid><description>Google has announced that container registry will be shut down some time after March 18, 2025. For GKE clusters created with version 1. 12. 0 or later of terraform-google-jx it‚Äôs unlikely that anything needs to be done, but for older clusters you should upgrade your cluster while considering our advice regarding migration from container registry to artifact registry. If you are using a Google Service Account to run terraform you need to add the role requirement roles/artifactregistry. admin. See our guide regarding Google Service Account for details.</description></item><item><title>WasmCon 2024 Moving to November 11 &amp; 12, co-locating with KubeCon + CloudNativeCon North America in Salt Lake.</title><link>https://kubermates.org/events/2024-05-01-wasmcon-2024-moving-to-november-11-12-co-locating-with-kubecon-cloudnativecon-no/</link><pubDate>Wed, 01 May 2024 20:06:09 +0000</pubDate><guid>https://kubermates.org/events/2024-05-01-wasmcon-2024-moving-to-november-11-12-co-locating-with-kubecon-cloudnativecon-no/</guid><description>&lt;p&gt;After careful consideration, the Linux Foundation and the WasmCon Program Committee have decided to reschedule WasmCon 2024. Originally ‚Ä¶&lt;/p&gt;</description></item><item><title>How to use the Grafana Operator: Managing a Grafana Cloud stack in Kubernetes</title><link>https://kubermates.org/docs/2024-04-24-how-to-use-the-grafana-operator-managing-a-grafana-cloud-stack-in-kubernetes/</link><pubDate>Wed, 24 Apr 2024 10:22:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-04-24-how-to-use-the-grafana-operator-managing-a-grafana-cloud-stack-in-kubernetes/</guid><description>Elizabeth Burkly √Ç¬∑ 26 Aug 2025 √Ç¬∑ 9 min read With our Voice of Customer program, we can tighten the loop between what our users ask for and what we prioritize in R&amp;amp;D. Here√¢¬Ä¬ôs a look at some recent. Read more.</description></item><item><title>Announcing the Kubeflow Spark Operator: Building a Stronger Spark on Kubernetes Community</title><link>https://kubermates.org/docs/2024-04-15-announcing-the-kubeflow-spark-operator-building-a-stronger-spark-on-kubernetes-c/</link><pubDate>Mon, 15 Apr 2024 00:00:00 -0500</pubDate><guid>https://kubermates.org/docs/2024-04-15-announcing-the-kubeflow-spark-operator-building-a-stronger-spark-on-kubernetes-c/</guid><description>Apr 15, 2024 ‚Ä¢ Vara Bonthu , Chaoran Yu , Andrey Velichkevich , Marcin Wielgus ‚Ä¢ 4 min read operators We‚Äôre excited to announce the migration of Google‚Äôs Spark Operator to the Kubeflow Spark Operator , marking the launch of a significant addition to the Kubeflow ecosystem. The Kubeflow Spark Operator simplifies the deployment and management of Apache Spark applications on Kubernetes. This announcement isn‚Äôt just about a new piece of technology, it‚Äôs about building a stronger, open-governed, and more collaborative community around Spark on Kubernetes. The journey of the Kubeflow Spark Operator began with Google Cloud Platform‚Äôs Spark on Kubernetes Operator (https://cloud. google. com/blog/products/data-analytics/data-analytics-meet-containers-kubernetes-operator-for-apache-spark-now-in-beta). With over 2. 3k stars and 1. 3k forks on GitHub, this project laid the foundation for a robust Spark on Kubernetes experience, enabling users to deploy Spark workloads seamlessly across Kubernetes clusters. Growth and innovation require not just code but also community. Acknowledging the resource and time limitations faced by Google Cloud‚Äôs original maintainers, Kubeflow has taken up the mantle. This transition is not merely administrative but a strategic move towards fostering a vibrant, diverse, and more actively engaged community.</description></item><item><title>Blog: Spotlight on SIG Architecture: Code Organization</title><link>https://kubermates.org/docs/2024-04-11-blog-spotlight-on-sig-architecture-code-organization/</link><pubDate>Thu, 11 Apr 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-04-11-blog-spotlight-on-sig-architecture-code-organization/</guid><description>This is the third interview of a SIG Architecture Spotlight series that will cover the different subprojects. We will cover SIG Architecture: Code Organization. In this SIG Architecture spotlight I talked with Madhav Jivrajani (VMware), a member of the Code Organization subproject. Frederico (FSM) : Hello Madhav, thank you for your availability. Could you start by telling us a bit about yourself, your role and how you got involved in Kubernetes? Madhav Jivrajani (MJ) : Hello! My name is Madhav Jivrajani, I serve as a technical lead for SIG Contributor Experience and a GitHub Admin for the Kubernetes project. Apart from that I also contribute to SIG API Machinery and SIG Etcd, but more recently, I‚Äôve been helping out with the work that is needed to help Kubernetes stay on supported versions of Go , and it is through this that I am involved with the Code Organization subproject of SIG Architecture. FSM : A project the size of Kubernetes must have unique challenges in terms of code organization ‚Äì is this a fair assumption? If so, what would you pick as some of the main challenges that are specific to Kubernetes? MJ : That‚Äôs a fair assumption! The first interesting challenge comes from the sheer size of the Kubernetes codebase. We have ‚âÖ2. 2 million lines of Go code (which is steadily decreasing thanks to dims and other folks in this sub-project!), and a little over 240 dependencies that we rely on either directly or indirectly, which is why having a sub-project dedicated to helping out with dependency management is crucial: we need to know what dependencies we‚Äôre pulling in, what versions these dependencies are at, and tooling to help make sure we are managing these dependencies across different parts of the codebase in a consistent manner. Another interesting challenge with Kubernetes is that we publish a lot of Go modules as part of the Kubernetes release cycles, one example of this is client-go. However, we as a project would also like the benefits of having everything in one repository to get the advantages of using a monorepo, like atomic commits‚Ä¶ so, because of this, code organization works with other SIGs (like SIG Release) to automate the process of publishing code from the monorepo to downstream individual repositories which are much easier to consume, and this way you won‚Äôt have to import the entire Kubernetes codebase! FSM : For someone just starting contributing to Kubernetes code-wise, what are the main things they should consider in terms of code organization? How would you sum up the key concepts? MJ : I think one of the key things to keep in mind at least as you‚Äôre starting off is the concept of staging directories. In the kubernetes/kubernetes repository, you will come across a directory called staging/.</description></item><item><title>Blog: Using Go workspaces in Kubernetes</title><link>https://kubermates.org/docs/2024-03-19-blog-using-go-workspaces-in-kubernetes/</link><pubDate>Tue, 19 Mar 2024 08:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2024-03-19-blog-using-go-workspaces-in-kubernetes/</guid><description>The Go programming language has played a huge role in the success of Kubernetes. As Kubernetes has grown, matured, and pushed the bounds of what ‚Äúregular‚Äù projects do, the Go project team has also grown and evolved the language and tools. In recent releases, Go introduced a feature called ‚Äúworkspaces‚Äù which was aimed at making projects like Kubernetes easier to manage. We‚Äôve just completed a major effort to adopt workspaces in Kubernetes, and the results are great. Our codebase is simpler and less error-prone, and we‚Äôre no longer off on our own technology island. Kubernetes is one of the most visible open source projects written in Go. The earliest versions of Kubernetes, dating back to 2014, were built with Go 1. 3. Today, 10 years later, Go is up to version 1. 22 ‚Äî and let‚Äôs just say that a whole lot has changed. In 2014, Go development was entirely based on GOPATH. As a Go project, Kubernetes lived by the rules of GOPATH.</description></item><item><title>Grafana Labs at KubeCon: eBPF, sustainability, Prometheus, and more</title><link>https://kubermates.org/docs/2024-03-15-grafana-labs-at-kubecon-ebpf-sustainability-prometheus-and-more/</link><pubDate>Fri, 15 Mar 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-03-15-grafana-labs-at-kubecon-ebpf-sustainability-prometheus-and-more/</guid><description>Colin Steele √Ç¬∑ 22 Aug 2025 √Ç¬∑ 5 min read Learn how Beyla, eBPF, and OpenTelemetry combine to make homelab observability easy with this recap of a recent GrafanaCON 2025 session. Read more.</description></item><item><title>Blog: Spotlight on SIG Cloud Provider</title><link>https://kubermates.org/docs/2024-03-01-blog-spotlight-on-sig-cloud-provider/</link><pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-03-01-blog-spotlight-on-sig-cloud-provider/</guid><description>One of the most popular ways developers use Kubernetes-related services is via cloud providers, but have you ever wondered how cloud providers can do that? How does this whole process of integration of Kubernetes to various cloud providers happen? To answer that, let‚Äôs put the spotlight on SIG Cloud Provider. SIG Cloud Provider works to create seamless integrations between Kubernetes and various cloud providers. Their mission? Keeping the Kubernetes ecosystem fair and open for all. By setting clear standards and requirements, they ensure every cloud provider plays nicely with Kubernetes. It is their responsibility to configure cluster components to enable cloud provider integrations. In this blog of the SIG Spotlight series, Arujjwal Negi interviews Michael McCune (Red Hat), also known as elmiko , co-chair of SIG Cloud Provider, to give us an insight into the workings of this group. Arujjwal : Let‚Äôs start by getting to know you. Can you give us a small intro about yourself and how you got into Kubernetes? Michael : Hi, I‚Äôm Michael McCune, most people around the community call me by my handle, elmiko. I‚Äôve been a software developer for a long time now (Windows 3. 1 was popular when I started!), and I‚Äôve been involved with open-source software for most of my career. I first got involved with Kubernetes as a developer of machine learning and data science applications; the team I was on at the time was creating tutorials and examples to demonstrate the use of technologies like Apache Spark on Kubernetes. That said, I‚Äôve been interested in distributed systems for many years and when an opportunity arose to join a team working directly on Kubernetes, I jumped at it! Arujjwal : Can you give us an insight into what SIG Cloud Provider does and how it functions? Michael : SIG Cloud Provider was formed to help ensure that Kubernetes provides a neutral integration point for all infrastructure providers.</description></item><item><title>Blog: A look into the Kubernetes Book Club</title><link>https://kubermates.org/docs/2024-02-22-blog-a-look-into-the-kubernetes-book-club/</link><pubDate>Thu, 22 Feb 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-02-22-blog-a-look-into-the-kubernetes-book-club/</guid><description>Learning Kubernetes and the entire ecosystem of technologies around it is not without its challenges. In this interview, we will talk with Carlos Santana (AWS) to learn a bit more about how he created the Kubernetes Book Club , how it works, and how anyone can join in to take advantage of a community-based learning experience. Frederico Mu√±oz (FSM) : Hello Carlos, thank you so much for your availability. To start with, could you tell us a bit about yourself? Carlos Santana (CS) : Of course. My experience in deploying Kubernetes in production six years ago opened the door for me to join Knative and then contribute to Kubernetes through the Release Team. Working on upstream Kubernetes has been one of the best experiences I‚Äôve had in open-source. Over the past two years, in my role as a Senior Specialist Solutions Architect at AWS, I have been assisting large enterprises build their internal developer platforms (IDP) on top of Kubernetes. Going forward, my open source contributions are directed towards CNOE and CNCF projects like Argo , Crossplane , and Backstage. FSM : So your path led you to Kubernetes, and at that point what was the motivating factor for starting the Book Club? CS : The idea for the Kubernetes Book Club sprang from a casual suggestion during a TGIK livestream. For me, it was more than just about reading a book; it was about creating a learning community. This platform has not only been a source of knowledge but also a support system, especially during the challenging times of the pandemic. It‚Äôs gratifying to see how this initiative has helped members cope and grow.</description></item><item><title>The Linux Foundation Releases Conference Schedule for Open Source Summit North America 2024</title><link>https://kubermates.org/events/2024-02-20-the-linux-foundation-releases-conference-schedule-for-open-source-summit-north-a/</link><pubDate>Tue, 20 Feb 2024 20:50:13 +0000</pubDate><guid>https://kubermates.org/events/2024-02-20-the-linux-foundation-releases-conference-schedule-for-open-source-summit-north-a/</guid><description>&lt;p&gt;The leading gathering for the global open source community offers 200+ sessions presented through 16 microconferences, exploring the ‚Ä¶&lt;/p&gt;</description></item><item><title>The Linux Foundation Announces the Embedded Open Source Summit 2024 Schedule</title><link>https://kubermates.org/events/2024-02-20-the-linux-foundation-announces-the-embedded-open-source-summit-2024-schedule/</link><pubDate>Tue, 20 Feb 2024 20:06:52 +0000</pubDate><guid>https://kubermates.org/events/2024-02-20-the-linux-foundation-announces-the-embedded-open-source-summit-2024-schedule/</guid><description>&lt;p&gt;130+ sessions spotlighting recent advancements and the trajectory of embedded technologies provide exclusive access to leaders propelling innovation ‚Ä¶&lt;/p&gt;</description></item><item><title>Kubernetes alerting: Simplify anomaly detection in Kubernetes clusters with Grafana Cloud</title><link>https://kubermates.org/docs/2024-02-15-kubernetes-alerting-simplify-anomaly-detection-in-kubernetes-clusters-with-grafa/</link><pubDate>Thu, 15 Feb 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-02-15-kubernetes-alerting-simplify-anomaly-detection-in-kubernetes-clusters-with-grafa/</guid><description>Alejandro Fraenkel √Ç¬∑ 5 Aug 2025 √Ç¬∑ 9 min read We built a brand new alert rules list page. Find out how this new design will make your on-call life a little easier and get insights into how we. Read more.</description></item><item><title>The Linux Foundation Announces Registration is Open for Open Source Summit North America and More 2024 Events</title><link>https://kubermates.org/events/2024-02-01-the-linux-foundation-announces-registration-is-open-for-open-source-summit-north/</link><pubDate>Thu, 01 Feb 2024 20:00:08 +0000</pubDate><guid>https://kubermates.org/events/2024-02-01-the-linux-foundation-announces-registration-is-open-for-open-source-summit-north/</guid><description>&lt;p&gt;Linux Foundation Events are the meeting place of choice for open source maintainers, developers, architects, infrastructure managers, and ‚Ä¶&lt;/p&gt;</description></item><item><title>Statement on Anti-Equal Rights Legislation &amp; Event Locations</title><link>https://kubermates.org/events/2024-01-30-statement-on-anti-equal-rights-legislation-event-locations/</link><pubDate>Tue, 30 Jan 2024 18:45:00 +0000</pubDate><guid>https://kubermates.org/events/2024-01-30-statement-on-anti-equal-rights-legislation-event-locations/</guid><description>&lt;p&gt;Originally published January 30, 2024; Updated on May 9, 2024 We were saddened to see another state‚Äôs legislators ‚Ä¶&lt;/p&gt;</description></item><item><title>Blog: Spotlight on SIG Release (Release Team Subproject)</title><link>https://kubermates.org/docs/2024-01-15-blog-spotlight-on-sig-release-release-team-subproject/</link><pubDate>Mon, 15 Jan 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-01-15-blog-spotlight-on-sig-release-release-team-subproject/</guid><description>The Release Special Interest Group (SIG Release), where Kubernetes sharpens its blade with cutting-edge features and bug fixes every 4 months. Have you ever considered how such a big project like Kubernetes manages its timeline so efficiently to release its new version, or how the internal workings of the Release Team look like? If you‚Äôre curious about these questions or want to know more and get involved with the work SIG Release does, read on! SIG Release plays a crucial role in the development and evolution of Kubernetes. Its primary responsibility is to manage the release process of new versions of Kubernetes. It operates on a regular release cycle, typically every three to four months. During this cycle, the Kubernetes Release Team works closely with other SIGs and contributors to ensure a smooth and well-coordinated release. This includes planning the release schedule, setting deadlines for code freeze and testing phases, as well as creating release artefacts like binaries, documentation, and release notes. Before you read further, it is important to note that there are two subprojects under SIG Release - Release Engineering and Release Team. In this blog post, Nitish Kumar interviews Ver√≥nica L√≥pez (PlanetScale), Technical Lead of SIG Release, with the spotlight on the Release Team subproject, how the release process looks like, and ways to get involved. What is the typical release process for a new version of Kubernetes, from initial planning to the final release? Are there any specific methodologies and tools that you use to ensure a smooth release? The release process for a new Kubernetes version is a well-structured and community-driven effort. There are no specific methodologies or tools as such that we follow, except a calendar with a series of steps to keep things organised. The complete release process looks like this: Release Team Onboarding: We start with the formation of a Release Team, which includes volunteers from the Kubernetes community who will be responsible for managing different components of the new release. This is typically done before the previous release is about to wrap up.</description></item><item><title>Blog: Blixt - A load-balancer written in Rust, using eBPF, born from Gateway API</title><link>https://kubermates.org/docs/2024-01-08-blog-blixt-a-load-balancer-written-in-rust-using-ebpf-born-from-gateway-api/</link><pubDate>Mon, 08 Jan 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-01-08-blog-blixt-a-load-balancer-written-in-rust-using-ebpf-born-from-gateway-api/</guid><description>In SIG Network we now have a layer 4 (‚ÄúL4‚Äù) load balancer named Blixt. This project started as a fun experiment using emerging technologies and is intended to become a utility for CI and testing to help facilitate the continued development of Gateway API. Are you interested in developing networking tools in Rust and eBPF ? Or perhaps you‚Äôre specifically interested in Gateway API? We‚Äôll tell you a bit about the project and how it might benefit you. Blixt originated at Kong as an experiment to test load-balancing ingress traffic for Kubernetes clusters using eBPF for the dataplane. Around the time of Kubecon Detroit (2022) we (the Gateway API maintainers) realized it had significant potential to help us move our TCPRoute and UDPRoute support forward, which had been sort of ‚Äústuck in alpha‚Äù at the time due to a lack of conformance tests being developed for them. At the same time, various others in the SIG Network community developed an interest in the project due to the rapid growth of eBPFs use on Kubernetes. Given the potential for benefit to the Kubernetes ecosystem and the growing interest, Kong decided it would be helpful to donate the project to Kubernetes SIGs to benefit upstream Kubernetes. Over several months we rewrote the project in Rust (from C), due to a strong contingency of Rust knowledge (and interest) between us developing the project and an active interest in the burgeoning Aya project (a Rust framework for developing eBPF programs). We did eventually move the control plane (specifically) to Golang however, so that we could take advantage of the Kubebuilder and controller-runtime ecosystems. Additionally, we augmented our custom program loader (in eBPF, you generally write loaders that load your BPF byte code into the kernel) with bpfman : a project adjacent to us in the Rust + eBPF ecosystem, which helps solve several security and ergonomic problems with managing BPF programs on Linux systems. After the recently completed license review process , which provided a blanket exception for the use of dual licensed eBPF in CNCF code, the project became officially part of Kubernetes and interest has been growing. We have several goals for the project which revolve around the continued development of Gateway API, with a specific focus on helping mature Layer 4 support (e.</description></item><item><title>Blog: Kubernetes supports running kube-proxy in an unprivileged container</title><link>https://kubermates.org/docs/2024-01-05-blog-kubernetes-supports-running-kube-proxy-in-an-unprivileged-container/</link><pubDate>Fri, 05 Jan 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-01-05-blog-kubernetes-supports-running-kube-proxy-in-an-unprivileged-container/</guid><description>This post describes how the &amp;ndash;init-only flag to kube-proxy can be used to run the main kube-proxy container in a stricter securityContext , by performing the configuration that requires privileged mode in a separate init container. Since Windows doesn‚Äôt have the equivalent of capabilities , this only works on Linux. The kube-proxy Pod still only meets the privileged Pod Security Standard , but there is still an improvement because the running container doesn‚Äôt need to run privileged. Please note that kube-proxy can be installed in different ways. The examples below assume that kube-proxy is run from a pod, but similar changes could be made in clusters where it is run as a system service. It is undesirable to run a server container like kube-proxy in privileged mode. Security aware users wants to use capabilities instead. If kube-proxy is installed as a POD, the initialization requires ‚Äúprivileged‚Äù mode, mostly for setting sysctl‚Äôs. However, kube-proxy only tries to set the sysctl‚Äôs if they don‚Äôt already have the right values. In theory, then, if a privileged init container set the sysctls to the right values, then kube-proxy could run unprivileged. The problem is to know what to setup. Until now the only option has been to read the source to see what changes kube-proxy would have made, but with &amp;ndash;init-only you can have kube-proxy itself do the setup exactly as on a normal start, and then exit.</description></item><item><title>Blog: Contextual logging in Kubernetes 1.29: Better troubleshooting and enhanced logging</title><link>https://kubermates.org/docs/2023-12-20-blog-contextual-logging-in-kubernetes-1-29-better-troubleshooting-and-enhanced-l/</link><pubDate>Wed, 20 Dec 2023 09:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2023-12-20-blog-contextual-logging-in-kubernetes-1-29-better-troubleshooting-and-enhanced-l/</guid><description>On behalf of the Structured Logging Working Group and SIG Instrumentation , we are pleased to announce that the contextual logging feature introduced in Kubernetes v1. 24 has now been successfully migrated to two components (kube-scheduler and kube-controller-manager) as well as some directories. This feature aims to provide more useful logs for better troubleshooting of Kubernetes and to empower developers to enhance Kubernetes. Contextual logging is based on the go-logr API. The key idea is that libraries are passed a logger instance by their caller and use that for logging instead of accessing a global logger. The binary decides the logging implementation, not the libraries. The go-logr API is designed around structured logging and supports attaching additional information to a logger. This enables additional use cases: The caller can attach additional information to a logger: When passing this extended logger into a function, and the function uses it instead of the global logger, the additional information is then included in all log entries, without having to modify the code that generates the log entries. This is useful in highly parallel applications where it can become hard to identify all log entries for a certain operation, because the output from different operations gets interleaved. When running unit tests, log output can be associated with the current test. Then, when a test fails, only the log output of the failed test gets shown by go test. That output can also be more verbose by default because it will not get shown for successful tests.</description></item><item><title>Blog: Spotlight on SIG Testing</title><link>https://kubermates.org/docs/2023-11-24-blog-spotlight-on-sig-testing/</link><pubDate>Fri, 24 Nov 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-11-24-blog-spotlight-on-sig-testing/</guid><description>Welcome to another edition of the SIG spotlight blog series, where we highlight the incredible work being done by various Special Interest Groups (SIGs) within the Kubernetes project. In this edition, we turn our attention to SIG Testing , a group interested in effective testing of Kubernetes and automating away project toil. SIG Testing focus on creating and running tools and infrastructure that make it easier for the community to write and run tests, and to contribute, analyze and act upon test results. To gain some insights into SIG Testing, Sandipan Panda spoke with Michelle Shepardson , a senior software engineer at Google and a chair of SIG Testing, and Patrick Ohly , a software engineer and architect at Intel and a SIG Testing Tech Lead. Sandipan: Could you tell us a bit about yourself, your role, and how you got involved in the Kubernetes project and SIG Testing? Michelle: Hi! I‚Äôm Michelle, a senior software engineer at Google. I first got involved in Kubernetes through working on tooling for SIG Testing, like the external instance of TestGrid. I‚Äôm part of oncall for TestGrid and Prow, and am now a chair for the SIG. Patrick: Hello! I work as a software engineer and architect in a team at Intel which focuses on open source Cloud Native projects. When I ramped up on Kubernetes to develop a storage driver, my very first question was ‚Äúhow do I test it in a cluster and how do I log information?‚Äù That interest led to various enhancement proposals until I had (re)written enough code that also took over official roles as SIG Testing Tech Lead (for the E2E framework ) and structured logging WG lead. Sandipan: Testing is a field in which multiple approaches and tools exist; how did you arrive at the existing practices? Patrick: I can‚Äôt speak about the early days because I wasn‚Äôt around yet üòÜ, but looking back at some of the commit history it‚Äôs pretty obvious that developers just took what was available and started using it. For E2E testing, that was Ginkgo+Gomega. Some hacks were necessary, for example around cleanup after a test run and for categorising tests.</description></item><item><title>Blog: Kubernetes Contributor Summit: Behind-the-scenes</title><link>https://kubermates.org/docs/2023-11-03-blog-kubernetes-contributor-summit-behind-the-scenes/</link><pubDate>Fri, 03 Nov 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-11-03-blog-kubernetes-contributor-summit-behind-the-scenes/</guid><description>Every year, just before the official start of KubeCon+CloudNativeCon, there‚Äôs a special event that has a very special place in the hearts of those organizing and participating in it: the Kubernetes Contributor Summit. To find out why, and to provide a behind-the-scenes perspective, we interview Noah Abrahams, whom amongst other roles was the co-lead for the Kubernetes Contributor Summit in 2023. Frederico Mu√±oz (FSM) : Hello Noah, and welcome. Could you start by introducing yourself and telling us how you got involved in Kubernetes? Noah Abrahams (NA) : I‚Äôve been in this space for quite a while. I got started in IT in the mid 90‚Äôs, and I‚Äôve been working in the ‚ÄúCloud‚Äù space for about 15 years. It was, frankly, through a combination of sheer luck (being in the right place at the right time) and having good mentors to pull me into those places (thanks, Tim!), that I ended up at a startup called Apprenda in 2016. While I was there, they pivoted into Kubernetes, and it was the best thing that could have happened to my career. It was around v1. 2 and someone asked me if I could give a presentation on Kubernetes concepts at ‚Äúmy local meetup‚Äù in Las Vegas. The meetup didn‚Äôt exist yet, so I created it, and got involved in the wider community. One thing led to another, and soon I was involved in ContribEx, joined the release team, was doing booth duty for the CNCF, became an ambassador, and here we are today. FM : Before leading the organisation of the KCSEU 2023, how many other Contributor Summits were you a part of? NA : I was involved in four or five before taking the lead.</description></item><item><title>Blog: Spotlight on SIG Architecture: Production Readiness</title><link>https://kubermates.org/docs/2023-11-02-blog-spotlight-on-sig-architecture-production-readiness/</link><pubDate>Thu, 02 Nov 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-11-02-blog-spotlight-on-sig-architecture-production-readiness/</guid><description>This is the second interview of a SIG Architecture Spotlight series that will cover the different subprojects. In this blog, we will cover the SIG Architecture: Production Readiness subproject. In this SIG Architecture spotlight, we talked with Wojciech Tyczynski (Google), lead of the Production Readiness subproject. Frederico (FSM) : Hello Wojciech, could you tell us a bit about yourself, your role and how you got involved in Kubernetes? Wojciech Tyczynski (WT) : I started contributing to Kubernetes in January 2015. At that time, Google (where I was and still am working) decided to start a Kubernetes team in the Warsaw office (in addition to already existing teams in California and Seattle). I was lucky enough to be one of the seeding engineers for that team. After two months of onboarding and helping with different tasks across the project towards 1. 0 launch, I took ownership of the scalability area and I was leading Kubernetes to support clusters with 5000 nodes. I‚Äôm still involved in SIG Scalability as its Technical Lead. That was the start of a journey since scalability is such a cross-cutting topic, and I started contributing to many other areas including, over time, to SIG Architecture. FSM : In SIG Architecture, why specifically the Production Readiness subproject? Was it something you had in mind from the start, or was it an unexpected consequence of your initial involvement in scalability? WT : After reaching that milestone of Kubernetes supporting 5000-node clusters , one of the goals was to ensure that Kubernetes would not degrade its scalability properties over time. While non-scalable implementation is always fixable, designing non-scalable APIs or contracts is problematic.</description></item><item><title>Blog: A Quick Recap of 2023 China Kubernetes Contributor Summit</title><link>https://kubermates.org/docs/2023-10-20-blog-a-quick-recap-of-2023-china-kubernetes-contributor-summit/</link><pubDate>Fri, 20 Oct 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-10-20-blog-a-quick-recap-of-2023-china-kubernetes-contributor-summit/</guid><description>On September 26, 2023, the first day of KubeCon + CloudNativeCon + Open Source Summit China 2023 , nearly 50 contributors gathered in Shanghai for the Kubernetes Contributor Summit. All participants in the 2023 Kubernetes Contributor Summit This marked the first in-person offline gathering held in China after three years of the pandemic. The event began with welcome speeches from Kevin Wang from Huawei Cloud, one of the co-chairs of KubeCon, and Puja from Giant Swarm. Following the opening remarks, the contributors introduced themselves briefly. Most attendees were from China, while some contributors had made the journey from Europe and the United States specifically for the conference. Technical experts from companies such as Microsoft, Intel, Huawei, as well as emerging forces like DaoCloud, were present. Laughter and cheerful voices filled the room, regardless of whether English was spoken with European or American accents or if conversations were carried out in authentic Chinese language. This created an atmosphere of comfort, joy, respect, and anticipation. Past contributions brought everyone closer, and mutual recognition and accomplishments made this offline gathering possible. Face to face meeting in Shanghai The attending contributors were no longer just GitHub IDs; they transformed into vivid faces. From sitting together and capturing group photos to attempting to identify ‚ÄúWho is who,‚Äù a loosely connected collective emerged. This team structure, although loosely knit and free-spirited, was established to pursue shared dreams.</description></item><item><title>Blog: Spotlight on SIG Architecture: Conformance</title><link>https://kubermates.org/docs/2023-10-05-blog-spotlight-on-sig-architecture-conformance/</link><pubDate>Thu, 05 Oct 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-10-05-blog-spotlight-on-sig-architecture-conformance/</guid><description>This is the first interview of a SIG Architecture Spotlight series that will cover the different subprojects. We start with the SIG Architecture: Conformance subproject In this SIG Architecture spotlight, we talked with Riaan Kleinhans (ii-Team), Lead for the Conformance sub-project. Frederico (FSM) : Hello Riaan, and welcome! For starters, tell us a bit about yourself, your role and how you got involved in Kubernetes. Riaan Kleinhans (RK) : Hi! My name is Riaan Kleinhans and I live in South Africa. I am the Project manager for the ii-Team in New Zealand. When I joined ii the plan was to move to New Zealand in April 2020 and then Covid happened. Fortunately, being a flexible and dynamic team we were able to make it work remotely and in very different time zones. The ii. nz team have been tasked with managing the Kubernetes Conformance testing technical debt and writing tests to clear the technical debt. I stepped into the role of project manager to be the link between monitoring, test writing and the community. Through that work I had the privilege of meeting the late Dan Kohn in those first months, his enthusiasm about the work we were doing was a great inspiration. FSM : Thank you - so, your involvement in SIG Architecture started because of the conformance work? RK : SIG Architecture is the home for the Kubernetes Conformance subproject.</description></item><item><title>The Linux Foundation Launches New Event: AI.dev: Open Source GenAI &amp; ML Summit</title><link>https://kubermates.org/events/2023-10-03-the-linux-foundation-launches-new-event-ai-dev-open-source-genai-ml-summit/</link><pubDate>Tue, 03 Oct 2023 13:41:49 +0000</pubDate><guid>https://kubermates.org/events/2023-10-03-the-linux-foundation-launches-new-event-ai-dev-open-source-genai-ml-summit/</guid><description>&lt;p&gt;Happening December 12 ‚Äì 13 in San Jose, CA, and co-locating with Cassandra Summit, AI.dev is where developers ‚Ä¶&lt;/p&gt;</description></item><item><title>Blog: Announcing the 2023 Steering Committee Election Results</title><link>https://kubermates.org/docs/2023-10-02-blog-announcing-the-2023-steering-committee-election-results/</link><pubDate>Mon, 02 Oct 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-10-02-blog-announcing-the-2023-steering-committee-election-results/</guid><description>The 2023 Steering Committee Election is now complete. The Kubernetes Steering Committee consists of 7 seats, 4 of which were up for election in 2023. Incoming committee members serve a term of 2 years, and all members are elected by the Kubernetes Community. This community body is significant since it oversees the governance of the entire Kubernetes project. With that great power comes great responsibility. You can learn more about the steering committee‚Äôs role in their charter. Thank you to everyone who voted in the election; your participation helps support the community‚Äôs continued health and success. Congratulations to the elected committee members whose two year terms begin immediately (listed in alphabetical order by GitHub handle): They join continuing members: Stephen Augustus is a returning Steering Committee Member. Thank you and congratulations on a successful election to this round‚Äôs election officers: Thanks to the Emeritus Steering Committee Members. Your service is appreciated by the community: And thank you to all the candidates who came forward to run for election. This governing body, like all of Kubernetes, is open to all. You can follow along with Steering Committee backlog items and weigh in by filing an issue or creating a PR against their repo.</description></item><item><title>The Linux Foundation Announces Schedule for Annual Member Summit</title><link>https://kubermates.org/events/2023-09-14-the-linux-foundation-announces-schedule-for-annual-member-summit/</link><pubDate>Thu, 14 Sep 2023 14:39:05 +0000</pubDate><guid>https://kubermates.org/events/2023-09-14-the-linux-foundation-announces-schedule-for-annual-member-summit/</guid><description>&lt;p&gt;The annual Linux Foundation Member Summit unites projects and organizations helping to advance open source technologies. SAN FRANCISCO, ‚Ä¶&lt;/p&gt;</description></item><item><title>Blog: Spotlight on SIG ContribEx</title><link>https://kubermates.org/docs/2023-08-14-blog-spotlight-on-sig-contribex/</link><pubDate>Mon, 14 Aug 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-08-14-blog-spotlight-on-sig-contribex/</guid><description>Author : Fyka Ansari Welcome to the world of Kubernetes and its vibrant contributor community! In this blog post, we‚Äôll be shining a spotlight on the Special Interest Group for Contributor Experience (SIG ContribEx), an essential component of the Kubernetes project. SIG ContribEx in Kubernetes is responsible for developing and maintaining a healthy and productive community of contributors to the project. This involves identifying and addressing bottlenecks that may hinder the project‚Äôs growth and feature velocity, such as pull request latency and the number of open pull requests and issues. SIG ContribEx works to improve the overall contributor experience by creating and maintaining guidelines, tools, and processes that facilitate collaboration and communication among contributors. They also focus on community building and support, including outreach programs and mentorship initiatives to onboard and retain new contributors. Ultimately, the role of SIG ContribEx is to foster a welcoming and inclusive environment that encourages contribution and supports the long-term sustainability of the Kubernetes project. In this blog post, Fyka Ansari interviews Kaslin Fields , a DevRel Engineer at Google, who is a chair of SIG ContribEx, and Madhav Jivrajani , a Software Engineer at VMWare who serves as a SIG ContribEx Tech Lead. This interview covers various aspects of SIG ContribEx, including current initiatives, exciting developments, and how interested individuals can get involved and contribute to the group. It provides valuable insights into the workings of SIG ContribEx and highlights the importance of its role in the Kubernetes ecosystem. Fyka: Let‚Äôs start by diving into your background and how you got involved in the Kubernetes ecosystem. Can you tell us more about that journey? Kaslin: I first got involved in the Kubernetes ecosystem through my mentor, Jonathan Rippy, who introduced me to containers during my early days in tech. Eventually, I transitioned to a team working with containers, which sparked my interest in Kubernetes when it was announced.</description></item><item><title>Blog: Spotlight on SIG CLI</title><link>https://kubermates.org/docs/2023-07-20-blog-spotlight-on-sig-cli/</link><pubDate>Thu, 20 Jul 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-07-20-blog-spotlight-on-sig-cli/</guid><description>In the world of Kubernetes, managing containerized applications at scale requires powerful and efficient tools. The command-line interface (CLI) is an integral part of any developer or operator‚Äôs toolkit, offering a convenient and flexible way to interact with a Kubernetes cluster. SIG CLI plays a crucial role in improving the Kubernetes CLI experience by focusing on the development and enhancement of kubectl , the primary command-line tool for Kubernetes. In this SIG CLI Spotlight, Arpit Agrawal, SIG ContribEx-Comms team member, talked with Katrina Verey , Tech Lead &amp;amp; Chair of SIG CLI,and Maciej Szulik , SIG CLI Batch Lead, about SIG CLI, current projects, challenges and how anyone can get involved. So, whether you are a seasoned Kubernetes enthusiast or just getting started, understanding the significance of SIG CLI will undoubtedly enhance your Kubernetes journey. Arpit : Could you tell us a bit about yourself, your role, and how you got involved in SIG CLI? Maciej : I‚Äôm one of the technical leads for SIG-CLI. I was working on Kubernetes in multiple areas since 2014, and in 2018 I got appointed a lead. Katrina : I‚Äôve been working with Kubernetes as an end-user since 2016, but it was only in late 2019 that I discovered how well SIG CLI aligned with my experience from internal projects. I started regularly attending meetings and made a few small PRs, and by 2021 I was working more deeply with the Kustomize team specifically. Later that year, I was appointed to my current roles as subproject owner for Kustomize and KRM Functions, and as SIG CLI Tech Lead and Chair. Arpit : Thank you! Could you share with us the purpose and goals of SIG CLI? Maciej : Our charter has the most detailed description, but in few words, we handle all CLI tooling that helps you manage your Kubernetes manifests and interact with your Kubernetes clusters. Arpit : I see.</description></item><item><title>Blog: Improve your changelogs</title><link>https://kubermates.org/docs/2023-05-24-blog-improve-your-changelogs/</link><pubDate>Wed, 24 May 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-05-24-blog-improve-your-changelogs/</guid><description>A standard part of the Jenkins X pipelines since a long time is the execution of jx changelog create that takes the commit messages between the release currently being created and the previous one and creates a change log from these. The change log is then stored as a release note in GitHub or other git provider. During the last year some improvements have landed in various Jenkins X components to improve the changelogs and their usefulness. So I‚Äôll take this opportunity to describe these improvements and also in general give hints to how to get useful changelogs. Changelogs haven‚Äôt been very informative with regard to upgrades, ie those applied with jx promote or jx updatebot. One example of this is the release notes of jx after the split out of most functionality to plugins. Lately these have improved due to new functionality to propagate changelogs via pull requests. One place where changelogs have been completely lacking is in cluster repositories. But using the functionality for propagation of changelogs and some changes in jx boot job you can now a get a changelog for every successful application of changes in a cluster. An example of what this functionality achieves can be seen in a release of jx: https://github. com/jenkins-x/jx/releases/tag/v3. 10.</description></item><item><title>Blog: Spotlight on SIG Network</title><link>https://kubermates.org/docs/2023-05-09-blog-spotlight-on-sig-network/</link><pubDate>Tue, 09 May 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-05-09-blog-spotlight-on-sig-network/</guid><description>Networking is one of the core pillars of Kubernetes, and the Special Interest Group for Networking (SIG Network) is responsible for developing and maintaining the networking features of Kubernetes. It covers all aspects to ensure Kubernetes provides a reliable and scalable network infrastructure for containerized applications. In this SIG Network spotlight, Sujay Dey talked with Shane Utt , Software Engineer at Kong, chair of SIG Network and maintainer of Gateway API, on different aspects of the SIG, what are the exciting things going on and how anyone can get involved and contribute here. Sujay : Hello, and first of all, thanks for the opportunity of learning more about SIG Network. I would love to hear your story, so could you please tell us a bit about yourself, your role, and how you got involved in Kubernetes, especially in SIG Network? Shane : Hello! Thank you for reaching out. My Kubernetes journey started while I was working for a small data centre: we were early adopters of Kubernetes and focused on using Kubernetes to provide SaaS products. That experience led to my next position developing a distribution of Kubernetes with a focus on networking. During this period in my career, I was active in SIG Network (predominantly as a consumer). When I joined Kong my role in the community changed significantly, as Kong actively encourages upstream participation. I greatly increased my engagement and contributions to the Gateway API project during those years, and eventually became a maintainer. I care deeply about this community and the future of our technology, so when a chair position for the SIG became available, I volunteered my time immediately. I‚Äôve enjoyed working on Kubernetes over the better part of a decade and I want to continue to do my part to ensure our community and technology continues to flourish.</description></item><item><title>Blog: E2E Testing Best Practices, Reloaded</title><link>https://kubermates.org/docs/2023-04-12-blog-e2e-testing-best-practices-reloaded/</link><pubDate>Wed, 12 Apr 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-04-12-blog-e2e-testing-best-practices-reloaded/</guid><description>End-to-end (E2E) testing in Kubernetes is how the project validates functionality with real clusters. Contributors sooner or later encounter it when asked to write E2E tests for new features or to help with debugging test failures. Cluster admins or vendors might run the conformance tests, a subset of all tests in the E2E test suite. The underlying E2E framework for writing these E2E tests has been around for a long time. Functionality was added to it as needed, leading to code that became hard to maintain and use. The testing commons WG started cleaning it up, but dissolved before completely achieving their goals. After the migration to Gingko v2 in Kubernetes 1. 25, I picked up several of the loose ends and started untangling them. This blog post is a summary of those changes. Some of this content is also found in the Kubernetes contributor document about writing good E2E tests and gets reproduced here to raise awareness that the document has been updated. At the moment, the framework is used in-tree for testing against a cluster ( test/e2e ), testing kubeadm ( test/e2e_kubeadm ) and kubelet ( test/e2e_node ). The goal is to make the core test/e2e/framework a package that has no dependencies on internal code and that can be used in different E2E suites without polluting them with features or options that make no sense for them.</description></item><item><title>Blog: From Zero to Kubernets Subproject Lead</title><link>https://kubermates.org/docs/2023-03-29-blog-from-zero-to-kubernets-subproject-lead/</link><pubDate>Wed, 29 Mar 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-03-29-blog-from-zero-to-kubernets-subproject-lead/</guid><description>Getting started in any open-source community can be daunting, especially if it‚Äôs a big one like Kubernetes. I wrote this post to share my experience and encourage others to join up. All it takes is some curiosity and a willingness to show up! Here‚Äôs how my journey unfolded at a high level: First things first. What are you interested in learning more about? There are so many wonderful SIGs and working groups in the Kubernetes community: there‚Äôs something for everyone. And continuing to show up and participate will be so much easier if you think what you are doing is interesting. Likewise, continued participation is what keeps the community thriving, so that interest will drive you to have more of an impact. Also: it‚Äôs ok to show up knowing nothing! I remember showing up knowing very little about Kubernetes or how the community itself worked. And while I know more about how the community functions today, I am still learning all the time about it and the project. Fortunately, the community is full of friendly people who want to help you learn. Learning as you go is expected and celebrated. When you raise your hand to do something, even if you know nothing, people will cheer and help you along the way. This method was my exact story.</description></item><item><title>Blog: Reconcile with kpt live apply</title><link>https://kubermates.org/docs/2023-03-09-blog-reconcile-with-kpt-live-apply/</link><pubDate>Thu, 09 Mar 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-03-09-blog-reconcile-with-kpt-live-apply/</guid><description>Since the dawn of Jenkins X 3 the default last step of reconciling the state of the files in your cluster repository to your cluster has been to execute kubectl apply. You can find more details about this here. There are some drawbacks with kubectl apply though. The one that made me start looking for alternatives was that if you remove a resource from your cluster repository it may not be removed from your cluster. The way deletion works with kubectl apply is that it is handed the option &amp;ndash;prune which will remove resources that are not in the manifests. Except that it doesn‚Äôt always work as expected. It will only remove certain kinds of resources defined in kubectl. In my case I removed an HorizontalPodAutoscaler from my cluster repository, but it wasn‚Äôt removed from my cluster. When trying to find a solution to this I first tried to override this default list in kubectl of things to prune, but this turned out to be difficult in the general case. I also tried the already existing alternative of using kapp to apply the manifests, but I couldn‚Äôt get that to work. Looking for other options I settled for kpt live apply. You enable the use of kpt live apply by adding to the Makefile of your cluster repository anywhere before include versionStream/src/Makefile.</description></item><item><title>Blog: Introducing KWOK: Kubernetes WithOut Kubelet</title><link>https://kubermates.org/docs/2023-03-01-blog-introducing-kwok-kubernetes-without-kubelet/</link><pubDate>Wed, 01 Mar 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-03-01-blog-introducing-kwok-kubernetes-without-kubelet/</guid><description>Author: Shiming Zhang (DaoCloud), Wei Huang (Apple), Yibo Zhuang (Apple) Have you ever wondered how to set up a cluster of thousands of nodes just in seconds, how to simulate real nodes with a low resource footprint, and how to test your Kubernetes controller at scale without spending much on infrastructure? If you answered ‚Äúyes‚Äù to any of these questions, then you might be interested in KWOK, a toolkit that enables you to create a cluster of thousands of nodes in seconds. KWOK stands for Kubernetes WithOut Kubelet. So far, it provides two tools: KWOK has several advantages: KWOK can be used for various purposes: KWOK is not intended to replace others completely. It has some limitations that you should be aware of: If you are interested in trying out KWOK, please check its documents for more details. Using kwokctl to manage simulated clusters If you‚Äôre interested in participating in future discussions or development related to KWOK, there are several ways to get involved: We welcome feedback and contributions from anyone who wants to join us in this exciting project.</description></item><item><title>Blog: Foreign aliases</title><link>https://kubermates.org/docs/2023-02-09-blog-foreign-aliases/</link><pubDate>Thu, 09 Feb 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-02-09-blog-foreign-aliases/</guid><description>In an organisation with many repositories and developers that are frequently shifting the maintenance of OWNERS and OWNERS_ALIASES files can be tedious. In the passing year a couple of functionalities has been added to help with this. To avoid maintaining the OWNERS_ALIASES file in many repositories you can now refer to the OWNERS_ALIASES file in another repository. In the Jenkins X project we have the main OWNERS_ALIASES file in the jx-community repository. So in the jx repository the OWNERS_ALIASES file only looks like this: The organisation defaults to be the same as for the repository, but can specify as well. So in the jx-project repository the OWNERS_ALIASES file looks like this: Using the filed ref you can also specify a branch or tag to use instead of the default one of the repository. When creating or importing a repository using jx project the default content of OWNERS and OWNERS_ALIASES isn‚Äôt that useful since only the current user are put in the files. If you create your own quickstarts you place the OWNERS and / or OWNERS_ALIASES files with the content of your liking in those. A recent new functionality is that you can put OWNERS and / or OWNERS_ALIASES files in the extensions directory of your cluster repository. These files will then be used as the default content of the files in new repositories.</description></item><item><title>Blog: Project ideas for Google Summer of Code 2023 ‚òÄÔ∏è</title><link>https://kubermates.org/docs/2023-02-06-blog-project-ideas-for-google-summer-of-code-2023/</link><pubDate>Mon, 06 Feb 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-02-06-blog-project-ideas-for-google-summer-of-code-2023/</guid><description>We have put together some project ideas as part of our application to participate in the Google Summer of Code 2023 program. The cdEvents project standardises the way systems talk to each other, which enables Interoperability between systems so they speak a common language through the cdEvents spec in the event. Creating a capability in Jenkins X that can receive and sent a cdEvent would benefit the project and the DevOps ecosystem in general, by stopping glue code used to integrate systems and power innovation by letting end users swap out tools with no effort. Jenkins X, Kubernetes, golang, cdEvents 350 hours Hard Jenkins X only applies changes to cluster when contents of the gitops repository changes as part of a git pull request or git commit. This does not satisfy one of the requirements of the gitops model where we need continuous reconciliation. It would be nice to detect drift between the current state (kubernetes) and the desired state (git) and apply only those changes. This has the side effect of making the boot job faster. Also, our bootjob is made up of a makefile which calls cli commands written in golang, instead it would be desirable to move to a controller based approach similar to other tools in the kubernetes ecosystem. Kubernetes, golang, Jenkins X, kubebuiler, basic understanding of gitops. 350 hours Hard In the last GSoC project, we created a modern UI which has more functionalities than the older viewer UI. However, as we added the ability to start/stop pipelines from the new UI, this opened up an issue around access. Users of Jenkins X would like to restrict who can start/stop pipelines from the UI.</description></item><item><title>Blog: Spotlight on SIG Instrumentation</title><link>https://kubermates.org/docs/2023-02-03-blog-spotlight-on-sig-instrumentation/</link><pubDate>Fri, 03 Feb 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-02-03-blog-spotlight-on-sig-instrumentation/</guid><description>Observability requires the right data at the right time for the right consumer (human or piece of software) to make the right decision. In the context of Kubernetes, having best practices for cluster observability across all Kubernetes components is crucial. SIG Instrumentation helps to address this issue by providing best practices and tools that all other SIGs use to instrument Kubernetes components-like the API server , scheduler , kubelet and kube-controller-manager. In this SIG Instrumentation spotlight, Imran Noor Mohamed , SIG ContribEx-Comms tech lead talked with Elana Hashman , and Han Kang , chairs of SIG Instrumentation, on how the SIG is organized, what are the current challenges and how anyone can get involved and contribute. Imran (INM) : Hello, thank you for the opportunity of learning more about SIG Instrumentation. Could you tell us a bit about yourself, your role, and how you got involved in SIG Instrumentation? Han (HK) : I started in SIG Instrumentation in 2018, and became a chair in 2020. I primarily got involved with SIG instrumentation due to a number of upstream issues with metrics which ended up affecting GKE in bad ways. As a result, we ended up launching an initiative to stabilize our metrics and make metrics a proper API. Elana (EH) : I also joined SIG Instrumentation in 2018 and became a chair at the same time as Han. I was working as a site reliability engineer (SRE) on bare metal Kubernetes clusters and was working to build out our observability stack. I encountered some issues with label joins where Kubernetes metrics didn‚Äôt match kube-state-metrics ( KSM ) and started participating in SIG meetings to improve things. I helped test performance improvements to kube-state-metrics and ultimately coauthored a KEP for overhauling metrics in the 1.</description></item><item><title>Blog: Prow and Tide for Kubernetes Contributors</title><link>https://kubermates.org/docs/2022-12-12-blog-prow-and-tide-for-kubernetes-contributors/</link><pubDate>Mon, 12 Dec 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-12-12-blog-prow-and-tide-for-kubernetes-contributors/</guid><description>Authors: Chris Short , Frederico Mu√±oz In my work in the Kubernetes world, I look up a label or Prow command often. The systems behind the scenes ( Prow and Tide ) are here to help Kubernetes Contributors get stuff done. Labeling which SIG, WG, or subproject is as important as the issue or PR having someone assigned. To quote the docs , ‚ÄúTide is a Prow component for managing a pool of GitHub PRs that match a given set of criteria. It will automatically retest PRs that meet the criteria (‚Äôtide comes in‚Äô) and automatically merge them when they have up-to-date passing test results (‚Äôtide goes out‚Äô). ‚Äù What actually prompted this article is the awesomely amazing folks on the Contributor Comms team saying, ‚ÄúI need to squash my commits and push that. ‚Äù Which immediately made me remember the wonder of the Tide label: tide/merge-method-squash. Contributing to Kubernetes will, most of the time, involve some kind of git-based action, specifically on the Kubernetes GitHub. This can be an obstacle to those less exposed to git and/or GitHub, and is especially noticeable when we‚Äôre dealing with non-code contributions (documentation, blog posts, etc. ). When a contributor submits something, it will generally be through a pull request. When it comes to how the change will go from request to approval, there are a number of considerations that must be made, such as: These are some of the main tasks in which Tide will help, allowing us to use the GitHub interface for these tasks (and more), making the actions more visible to the community (since they are visible as plain comments in the GitHub discussion), and allowing us to manage contributions without necessarily having to clone git repositories or having to manually issue git commands.</description></item><item><title>Blog: GSoC 2022 Final Report: Building Jenkins X UI</title><link>https://kubermates.org/docs/2022-11-13-blog-gsoc-2022-final-report-building-jenkins-x-ui/</link><pubDate>Sun, 13 Nov 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-11-13-blog-gsoc-2022-final-report-building-jenkins-x-ui/</guid><description>It is a web application built with Golang for the backend and Sveltekit for the frontend, both of which are built together and used in the same container. To function properly, it must be installed as a helm chart with Jenkins X CRDs. üåü It has light and dark themes. A good UI is essential for a CI/CD tool, as not everyone is familiar with the CLI. The current UI (jx-pipeline-visualizer) is a read-only UI, the user can view the logs of PipelineActivity but neither can start nor stop the pipeline. Features that the UI will provide: New Jenkins X UI focus on Simplicity, Security and a Superb User Experience. This is NOT GA (General Availability) yet. Visit the project repo here to try it. We have added a button in pipelines page and pipelineDetails page, it asks for confirmation and on selecting Yes it will stop the PipelineActivity. We can stop the PipelineActivity from the pipelines tables. We can also stop the PipelineActivity from the pipelines details page. Issue: https://github.</description></item><item><title>Blog: GSoC 2022 Final Report: Improving Supply Chain Security</title><link>https://kubermates.org/docs/2022-11-08-blog-gsoc-2022-final-report-improving-supply-chain-security/</link><pubDate>Tue, 08 Nov 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-11-08-blog-gsoc-2022-final-report-improving-supply-chain-security/</guid><description>Supply chain security is a rising concern in the current software era. Securing the software supply chain encompasses vulnerability remediation and the implementation of controls throughout the software development process. Due to massive increase in attacks on software supply chain and the diversity of its types , Jenkins X has to make efforts to ensure that the build process is secure. As part of securing Jenkins X installation by default I worked on both securing our own components and enabling our users to use these features in their build and release steps. The work done so far covers these four sections. Description: A first step towards securing Jenkins X supply chain is to increase the amount of information gained from running jx version command. Implementation The issue created for this task is here. The PR to fix it is here. Description: As Jenkins X uses tekton as its pipeline execution engine, TaskRun and PipelineRun are considered the key components of Jenkins X pipeline activities and steps Tekton Chains monitors the execution of all TaskRun and PipelineRun inside the cluster and takes a snapshot upon completion of each of them to sign with user-provided cryptographic keys and store them on the backend storage. The payload and signature cn be verified later using cosign verify-blob. Implementation I used the helm chart developed by Chainguard for integrating Chains with Jenkins X. To integrate the chart and added support for it on jx3-versions to make installation of helm chart easy for our users.</description></item><item><title>Blog: Hacktoberfest 2022</title><link>https://kubermates.org/docs/2022-10-03-blog-hacktoberfest-2022/</link><pubDate>Mon, 03 Oct 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-10-03-blog-hacktoberfest-2022/</guid><description>We are excited to announce that Jenkins X will be participating in Hacktoberfest again this year! Hacktoberfest is a month-long global celebration of open source software. All backgrounds and skill levels are encouraged to participate in Hacktoberfest and join a global community of open source contributors. Learn more about Hacktoberfest and sign up here. We welcome your contributions to the Jenkins X project ! Issues labelled ‚Äúhacktoberfest‚Äù generally indicate good first issues. However, all pull requests will count towards your Hacktoberfest challenge. Refer to the contribution guides for making code and documentation changes. Refer to this document to get an idea about the location of the different Jenkins X source code repositories. Normally the labels assigned to the ticket will help you in deciding which part of the Jenkins X codebase to look at. The maintainers will also try to link the relevant repository for hacktoberfest issues in the issue comment. Once you are done with the contribution, request a review from the maintainers by adding the comment in your pull request. We‚Äôre happy to help if you have any questions. Talk to us on our slack channels, which are part of the Kubernetes slack.</description></item><item><title>Blog: Implementing the Auto-refreshing Official Kubernetes CVE Feed</title><link>https://kubermates.org/docs/2022-09-12-blog-implementing-the-auto-refreshing-official-kubernetes-cve-feed/</link><pubDate>Mon, 12 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-09-12-blog-implementing-the-auto-refreshing-official-kubernetes-cve-feed/</guid><description>Author : Pushkar Joglekar (VMware) Accompanying the release of Kubernetes v1. 25, we announced availability of an official CVE feed as an alpha feature. This blog will cover how we implemented this feature. An auto-refreshing CVE feed allows users and implementers to programmatically fetch the list of CVEs announced by the Kubernetes SRC (Security Response Committee). To ensure freshness and minimal maintainer overhead, the feed updates automatically by fetching the CVE related information from the CVE announcement GitHub Issues. Creating these issues is already part of the existing Security Response Committee (SRC) workflow. Until December 2021, it was not possible to filter for issues or PRs that are tied to CVEs announced by Kubernetes SRC. We added a new label, official-cve-feed to address that, and SIG-Security labelled relevant issues with it. The in-scope issues are closed issues for which there is a CVE ID(s) and is officially announced as a Kubernetes security vulnerability by SRC. You can now filter on all of these issues and find them here. For future security vulnerabilities, we added the label to the SRC playbook so that all the future in-scope issues will automatically have this label. For the next step, we created a prow job in order to periodically query the GitHub REST API and pull the relevant issues.</description></item><item><title>Blog: Enhancements Opt-in Process Change for v1.26</title><link>https://kubermates.org/docs/2022-09-09-blog-enhancements-opt-in-process-change-for-v1-26/</link><pubDate>Fri, 09 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-09-09-blog-enhancements-opt-in-process-change-for-v1-26/</guid><description>Author: Grace Nguyen Since the inception of the Kubernetes release team, we have used a spreadsheet to keep track of enhancements for the release. The project has scaled massively in the past few years, with almost a hundred enhancements collected for the 1. 24 release. This process has become error-prone and time consuming. A lot of manual work is required from the release team and the SIG leads to populate KEPs data in the sheet. We have received continuous feedback from our contributors to streamline the process. Starting with the 1. 26 release, we are replacing the enhancements tracking spreadsheet with an automated GitHub project board. The board is populated with a script gathering all KEP issues in the kubernetes/enhancements repo that have the label lead-opted-in. The enhancements‚Äô stage and SIG information will also be automatically pulled from the KEP issue. After the KEP is populated on the Github Project Board, the Enhancements team will manually update the KEP with the label tracked/yes , tracked/no and on occasions, tracked/out-of-tree. The tracked label signifies qualification for the closest approaching milestone.</description></item><item><title>Blog: Spotlight on SIG Storage</title><link>https://kubermates.org/docs/2022-08-22-blog-spotlight-on-sig-storage/</link><pubDate>Mon, 22 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-08-22-blog-spotlight-on-sig-storage/</guid><description>Since the very beginning of Kubernetes, the topic of persistent data and how to address the requirement of stateful applications has been an important topic. Support for stateless deployments was natural, present from the start, and garnered attention, becoming very well-known. Work on better support for stateful applications was also present from early on, with each release increasing the scope of what could be run on Kubernetes. Message queues, databases, clustered filesystems: these are some examples of the solutions that have different storage requirements and that are, today, increasingly deployed in Kubernetes. Dealing with ephemeral and persistent storage, local or remote, file or block, from many different vendors, while considering how to provide the needed resiliency and data consistency that users expect, all of this is under SIG Storage‚Äôs umbrella. In this SIG Storage spotlight, Frederico Mu√±oz (Cloud &amp;amp; Architecture Lead at SAS) talked with Xing Yang , Tech Lead at VMware and co-chair of SIG Storage, on how the SIG is organized, what are the current challenges and how anyone can get involved and contribute. Frederico (FSM) : Hello, thank you for the opportunity of learning more about SIG Storage. Could you tell us a bit about yourself, your role, and how you got involved in SIG Storage. Xing Yang (XY) : I am a Tech Lead at VMware, working on Cloud Native Storage. I am also a Co-Chair of SIG Storage. I started to get involved in K8s SIG Storage at the end of 2017, starting with contributing to the VolumeSnapshot project. At that time, the VolumeSnapshot project was still in an experimental, pre-alpha stage.</description></item><item><title>Blog: Meet Our Contributors - APAC (China region)</title><link>https://kubermates.org/docs/2022-08-15-blog-meet-our-contributors-apac-china-region/</link><pubDate>Mon, 15 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-08-15-blog-meet-our-contributors-apac-china-region/</guid><description>Authors &amp;amp; Interviewers: Avinesh Tripathi , Debabrata Panigrahi , Jayesh Srivastava , Priyanka Saggu , Purneswar Prasad , Vedant Kakde Hello, everyone üëã Welcome back to the third edition of the ‚ÄúMeet Our Contributors‚Äù blog post series for APAC. This post features four outstanding contributors from China, who have played diverse leadership and community roles in the upstream Kubernetes project. So, without further ado, let‚Äôs get straight to the article. Andy Zhang currently works for Microsoft China at the Shanghai site. His main focus is on Kubernetes storage drivers. Andy started contributing to Kubernetes about 5 years ago. He states that as he is working in Azure Kubernetes Service team and spends most of his time contributing to the Kubernetes community project. Now he is the main contributor of quite a lot Kubernetes subprojects such as Kubernetes cloud provider code. His open source contributions are mainly self-motivated. In the last two years he has mentored a few students contributing to Kubernetes through the LFX Mentorship program, some of whom got jobs due to their expertise and contributions on Kubernetes projects. Andy is an active member of the China Kubernetes community. He adds that the Kubernetes community has a good guide about how to become members, code reviewers, approvers and finally when he found out that some open source projects are in the very early stage, he actively contributed to those projects and became the project maintainer.</description></item><item><title>Blog: Enhancing Kubernetes one KEP at a Time</title><link>https://kubermates.org/docs/2022-08-11-blog-enhancing-kubernetes-one-kep-at-a-time/</link><pubDate>Thu, 11 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-08-11-blog-enhancing-kubernetes-one-kep-at-a-time/</guid><description>Author: Ryler Hockenbury (Mastercard) Did you know that Kubernetes v1. 24 has 46 enhancements ? That‚Äôs a lot of new functionality packed into a 4-month release cycle. The Kubernetes release team coordinates the logistics of the release, from remediating test flakes to publishing updated docs. It‚Äôs a ton of work, but they always deliver. The release team comprises around 30 people across six subteams - Bug Triage, CI Signal, Enhancements, Release Notes, Communications, and Docs. Each of these subteams manages a component of the release. This post will focus on the role of the enhancements subteam and how you can get involved. Great question. We‚Äôll get to that in a second but first, let‚Äôs talk about how features are managed in Kubernetes. Each new feature requires a Kubernetes Enhancement Proposal - KEP for short. KEPs are small structured design documents that provide a way to propose and coordinate new features. The KEP author describes the motivation, design (and alternatives), risks, and tests - then community members provide feedback to build consensus.</description></item><item><title>Blog: Spotlight on SIG Docs</title><link>https://kubermates.org/docs/2022-08-02-blog-spotlight-on-sig-docs/</link><pubDate>Tue, 02 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-08-02-blog-spotlight-on-sig-docs/</guid><description>Author: Purneswar Prasad The official documentation is the go-to source for any open source project. For Kubernetes, it‚Äôs an ever-evolving Special Interest Group (SIG) with people constantly putting in their efforts to make details about the project easier to consume for new contributors and users. SIG Docs publishes the official documentation on kubernetes. io which includes, but is not limited to, documentation of the core APIs, core architectural details, and CLI tools shipped with the Kubernetes release. To learn more about the work of SIG Docs and its future ahead in shaping the community, I have summarised my conversation with the co-chairs, Divya Mohan (DM), Rey Lejano (RL) and Natali Vlatko (NV), who ran through the SIG‚Äôs goals and how fellow contributors can help. SIG Docs is the special interest group for documentation for the Kubernetes project on kubernetes. io, generating reference guides for the Kubernetes API, kubeadm and kubectl as well as maintaining the official website‚Äôs infrastructure and analytics. The remit of their work also extends to docs releases, translation of docs, improvement and adding new features to existing documentation, pushing and reviewing content for the official Kubernetes blog and engaging with the Release Team for each cycle to get docs and blogs reviewed. Blogs : This subproject highlights new or graduated Kubernetes enhancements, community reports, SIG updates or any relevant news to the Kubernetes community such as thought leadership, tutorials and project updates, such as the Dockershim removal and removal of PodSecurityPolicy, which is upcoming in the 1. 25 release. Tim Bannister, one of the SIG Docs tech leads, does awesome work and is a major force when pushing contributions through to the docs and blogs. Localization : With this subproject, the Kubernetes community has been able to achieve greater inclusivity and diversity among both users and contributors.</description></item><item><title>Blog: Introduction to Software Bill Of Materials</title><link>https://kubermates.org/docs/2022-07-24-blog-introduction-to-software-bill-of-materials/</link><pubDate>Sun, 24 Jul 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-07-24-blog-introduction-to-software-bill-of-materials/</guid><description>Before going through Software Bill Of Materials (SBOMs), we need to set the ground for a rising concern in the software industry which is Software Supply Chain Security. Like traditional industries, deploying a piece of a software artifact goes through multiple stages composed of collecting source code components, libraries, tools, and processes used in those stages. Fig. 1 https://blog. convisoappsec. com/en/is-your-software-supply-chain-secure/ A supply chain attack can occur along the chain from submitting unauthorized malicious code in your source, unauthorized injection of harmful dependencies, and even replacing packages after being built with other compromised artifacts. A more detailed explanation about those types of attacks is here Due to its importance and being a critical issue, generating SBOM for your software adds another layer of protection to this threat. As far as we know, developers around the world are building web applications using hundreds of third-party open-source libraries and packages. You can confidently tell that 90% of the software products around the world are built over open-source components. With that in mind, we need to keep track of using these dependencies while building our applications. What if there are vulnerabilities in the libraries we use? How to efficiently protect ourselves against it?. Software Bill Of Materials (SBOM) is a complete formally structured list of the materials (components, packages, libraries, SDK) used to build (i.</description></item><item><title>Blog: Software Bill Of Materials Formats</title><link>https://kubermates.org/docs/2022-07-24-blog-software-bill-of-materials-formats/</link><pubDate>Sun, 24 Jul 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-07-24-blog-software-bill-of-materials-formats/</guid><description>If you don‚Äôt understand what is Software Bill of Materials (SBOM), please read this blog post first. The National Telecommunications and Information Administration (NTIA) in the U. S. defined minimum requirements for SBOM formats : In this section, we discuss different kinds and formats for SBOM standards and make a brief comparison between them. Three commonly used standards achieved the NTIA minimum requirements for SBOM generation and each one results in a different final SBOM document. History: SPDX is an open-source machine-readable format adopted by the Linux Foundation as an industry standard. The specifications are implemented as a file format that identifies the software components within a larger piece of computer software and fulfilling the requirements of NTIA. The SPDX project started in 2010 and was initially dedicated to solving the issues around open source licensing compliance. It evolved over the years to adhere supply chain security challenges and has seen extensive uptake by companies and projects in the software industry. Companies like Hitachi, Fujitsu, and Toshiba contributed to furthering the standard in the SPDX v2. 2. 2 specification release.</description></item><item><title>Blog: Software Bill Of Materials generation tools</title><link>https://kubermates.org/docs/2022-07-24-blog-software-bill-of-materials-generation-tools/</link><pubDate>Sun, 24 Jul 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-07-24-blog-software-bill-of-materials-generation-tools/</guid><description>Before you read this, you have to understand what are SBOMs and what are different formats of SBOMs If you got this far, you already realize the importance of SBOM generation, and also it should meet certain requirements to achieve its purpose. Due to various requirements depending on what standard you‚Äôre following, there has to be a way to automatically generate different output formats for different standards. Also, it has to be suited for ci/cd solutions to keep up with the increasing number of releases for each organization. Note: Here we‚Äôre only considering open source tools Introduction: Anchore is a platform that implements sbom-powered supply chain security solutions for developers and enterprises. For generating SBOMs, a CLI tool and library named Syft was developed by Anchore that could be injected into your ci/cd pipeline to generate SBOMs from container images and filesystems at each step. Integration and Support: Syft is supported on Linux, Mac, and Windows and it can run as a docker container which makes it a great suit for CI systems. Other than the 3 SBOM standards, Syft can generate its JSON standard format to be input for other Anchore tools like Grype which is a vulnerability scanner for container images and filesystems. It supports projects based on the following package managers: Features and Specs: For more resources about Syft capabilities refer to the source repo and official documentation Introduction: Opensbom-Generator is an open source project initiated by the Linux Foundation SPDX workgroup to generate SBOMs using CLI tools. Currently, they support the standard spdx 2. 2 formats and JSON with their spdx-sbom-generator tool based on golang. It can only be used to generate SBOMs from a repository containing package files (no container images or archives support yet). They aim to provide SBOM generation support in ci/cd solutions.</description></item><item><title>Blog: GSoC 2022 Community Bonding Period with Jenkins X</title><link>https://kubermates.org/docs/2022-07-12-blog-gsoc-2022-community-bonding-period-with-jenkins-x/</link><pubDate>Tue, 12 Jul 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-07-12-blog-gsoc-2022-community-bonding-period-with-jenkins-x/</guid><description>Hello everyone, I am Rajat Gupta, pursuing my bachelor‚Äôs in Information Technology. In 2022, I have been selected as a student developer in Google Summer of Code under Jenkins X. We will be building a new UI for Jenkins X. I got this news on May 20th, as I received an email from google. The technologies needed were Golang , Kubernetes , and GitOps. I used golang only once before, while linting Jenkins X codebase, I only used Kubernetes once before while setting up a k3s cluster to run Jenkins X pipelines. These tasks were necessary to do for all GSoC participants. Apart from that, I was a total beginner. So, when I got selected, I had a lot to learn, my mentors gave me a 30 Day plan. They also suggested some resources and conference talks which made it simple for me to start. 30 Day plan was: We also had some amazing pair programming sessions, where I used to share my screen and my mentor guided me through, which is not common because mentors have a very busy schedule. But my mentors helped me a lot.</description></item><item><title>Blog: Contextual Logging in Kubernetes 1.24</title><link>https://kubermates.org/docs/2022-05-25-blog-contextual-logging-in-kubernetes-1-24/</link><pubDate>Wed, 25 May 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-05-25-blog-contextual-logging-in-kubernetes-1-24/</guid><description>Authors: Patrick Ohly (Intel) The Structured Logging Working Group has added new capabilities to the logging infrastructure in Kubernetes 1. 24. This blog post explains how developers can take advantage of those to make log output more useful and how they can get involved with improving Kubernetes. The goal of structured logging is to replace C-style formatting and the resulting opaque log strings with log entries that have a well-defined syntax for storing message and parameters separately, for example as a JSON struct. When using the traditional klog text output format for structured log calls, strings were originally printed with \n escape sequences, except when embedded inside a struct. For structs, log entries could still span multiple lines, with no clean way to split the log stream into individual entries: Now, the &amp;lt; and &amp;gt; markers along with indentation are used to ensure that splitting at a klog header at the start of a line is reliable and the resulting output is human-readable: Note that the log message itself is printed with quoting. It is meant to be a fixed string that identifies a log entry, so newlines should be avoided there. Before Kubernetes 1. 24, some log calls in kube-scheduler still used klog. Info for multi-line strings to avoid the unreadable output. Now all log calls have been updated to support structured logging. Contextual logging is based on the go-logr API.</description></item><item><title>Blog: February 2022 Community Meeting Highlights</title><link>https://kubermates.org/docs/2022-05-05-blog-february-2022-community-meeting-highlights/</link><pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-05-05-blog-february-2022-community-meeting-highlights/</guid><description>Author: Nigel Brown (VMware) We just had our first contributor community meeting this year, and it was awesome to be back with you in that format. These meetings will be happening on Zoom once per month, on the third Thursday of the month - that should be available in your calendar if you‚Äôre subscribed to the k-dev mailing list. Community meetings are an opportunity for you to meet synchronously with other members of the Kubernetes community to talk about issues of general appeal. This meeting kicked off with an update on the 1. 24 release with Xander Grzywinski, who is one of the shadows for the release team leads. This release is scheduled for April 19, 2022 with a code freeze scheduled for March 30th. At the time of the meeting there were 66 individual enhancements included, as well as bug fixes. You can join the conversation on Slack in #sig-release. Update: Kubernetes 1. 24 was delayed and released on May 3, 2022. From there, the discussion moved to the dockershim removal and the docs updates we need to make around that, with the discussion led by Kat Cosgrove. The main takeaway was that if you have a platform, talk to folks about this change.</description></item><item><title>Blog: Kubernetes 1.22 - Breaking change!</title><link>https://kubermates.org/docs/2022-04-22-blog-kubernetes-1-22-breaking-change/</link><pubDate>Fri, 22 Apr 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-04-22-blog-kubernetes-1-22-breaking-change/</guid><description>To allow Jenkins X to support Kubernetes 1. 22, we had to update our version of Tekton. This updated version of Tekton contains breaking changes that has consequences if you made your own custom Jenkins X pipelines. To make sure that your custom pipelines continue to work after this upgrade, you must edit the resource settings in your pipelines. Otherwise your pipelines will most likely not be able to start at all, or if they do, consume a lot of resources. Tekton made changes in how to calculate the resources needed to run a pipeline, in order to support the concept of LimitRange in Kubernetes (introduced in Kubernetes version 1. 10). Previously, Tekton simply used the maximum requested cpu and memory of any single step, and set that as limits for the all steps in the pipeline. For more details, please read the Tekton documentation on LimitRange. In the Tekton pipeline files, the StepTemplate needs to be changed to not specify resource requests , but only setting an empty resource limits : The resource requests should be set on only one individual step: To see examples of what changes you need to apply to your custom pipelines you may investigate this PR on The Jenkins X pipeline catalog. The PR will be merged and released simultaneous with the upgrade of Tekton. In the current version of Tekton used by Jenkins X, the resource requests are set on the stepTemplate.</description></item><item><title>Blog: Google Summer of Code 2022 project proposal template ‚òÄÔ∏è</title><link>https://kubermates.org/docs/2022-04-05-blog-google-summer-of-code-2022-project-proposal-template/</link><pubDate>Tue, 05 Apr 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-04-05-blog-google-summer-of-code-2022-project-proposal-template/</guid><description>Repeat this section for as many project ideas as you want (Go in the order most likely to least likely).</description></item><item><title>Blog: Project proposal for Google Season of Docs 2022 üìÑ</title><link>https://kubermates.org/docs/2022-03-23-blog-project-proposal-for-google-season-of-docs-2022/</link><pubDate>Wed, 23 Mar 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-03-23-blog-project-proposal-for-google-season-of-docs-2022/</guid><description>We have put together a project proposal as part of our application to participate in the Google Season of Docs 2022 program. Jenkins X provides automated CI/CD for Kubernetes with Preview Environments on Pull Requests using Cloud Native pipelines from Tekton. The project will take roughly 6 months to complete. Once the technical writer is selected, we will do an orientation to bring him up to speed. This is the timeline we have in mind, but we are flexible.</description></item><item><title>Blog: K8s CI Bot Helper Job: automating "make update"</title><link>https://kubermates.org/docs/2022-03-15-blog-k8s-ci-bot-helper-job-automating-make-update/</link><pubDate>Tue, 15 Mar 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-03-15-blog-k8s-ci-bot-helper-job-automating-make-update/</guid><description>Authors: Subhasmita Swain , Davanum Srinivas If you are contributing to the Kubernetes project and are developing on a Windows PC, it is conceivable that you will encounter certain issues that will cause your pull request to get held up by test failures. This article describes a workaround for a similar issue I encountered when attempting to have my modifications approved and merged into the master branch. While contributing to kubernetes/kubernetes for some minor documentation changes, the pushed changes needed to be updated with other verified contents of the entire documentation. So, in order for the change to take effect, a single command must be performed to ensure that all tests on the CI pipeline pass. The single command make update runs all presubmission verification tests. For some reason on the ‚ÄúWindows Subsystem for Linux‚Äù environment the tests, specifically the update-openapi-spec. sh script, failed (in my case, take a look at the conversation here ), eventually failing the pull-kubernetes-verify tests. You might encounter the following on your PR The tests failing the particular issue: Consecutively, Additionally one can check the failed test via the link provided under details in the above image. Run the failing. sh scripts individually known from the CI job output, to generate the expected files to fix up the failures. The. sh scripts can be found residing under the hack/ directory at the root of the kubernetes/kubernetes code base.</description></item><item><title>Blog: Meet Our Contributors - APAC (Aus-NZ region)</title><link>https://kubermates.org/docs/2022-03-14-blog-meet-our-contributors-apac-aus-nz-region/</link><pubDate>Mon, 14 Mar 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-03-14-blog-meet-our-contributors-apac-aus-nz-region/</guid><description>Authors &amp;amp; Interviewers: Anubhav Vardhan , Atharva Shinde , Avinesh Tripathi , Brad McCoy , Debabrata Panigrahi , Jayesh Srivastava , Kunal Verma , Pranshu Srivastava , Priyanka Saggu , Purneswar Prasad , Vedant Kakde Good day, everyone üëã Welcome back to the second episode of the ‚ÄúMeet Our Contributors‚Äù blog post series for APAC. This post will feature four outstanding contributors from the Australia and New Zealand regions, who have played diverse leadership and community roles in the Upstream Kubernetes project. So, without further ado, let‚Äôs get straight to the article. Caleb Woodbine is currently a member of the ii. nz organisation. He began contributing to the Kubernetes project in 2018 as a member of the Kubernetes Conformance working group. His experience was positive, and he benefited from early guidance from Hippie Hacker , a fellow contributor from New Zealand. He has made major contributions to Kubernetes project since then through SIG k8s-infra and k8s-conformance working group. Caleb is also a co-organizer of the CloudNative NZ community events, which aim to expand the reach of Kubernetes project throughout New Zealand in order to encourage technical education and improved employment opportunities. There need to be more outreach in APAC and the educators and universities must pick up Kubernetes, as they are very slow and about 8+ years out of date. NZ tends to rather pay overseas than educate locals on the latest cloud tech Locally. Dylan Graham is a cloud engineer from Adeliade, Australia.</description></item><item><title>Blog: Google Summer of Code 2022 ‚òÄÔ∏è</title><link>https://kubermates.org/docs/2022-03-12-blog-google-summer-of-code-2022/</link><pubDate>Sat, 12 Mar 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-03-12-blog-google-summer-of-code-2022/</guid><description>We are very happy to announce that Jenkins X has been selected to participate in the Google Summer of Code (GSoC) 2022! If you are new to GSoC and want to learn more about it, check out their new site. You can find the list of project ideas from Jenkins X here. If you are interested in applying to Jenkins X, please check the timeline. Apply as early as possible. This post aims to give GSoC contributors insight into the selection process and how to get started. We have 3 mentors signed up at the moment and are looking to select a maximum of 2 applicants. We want you to have a great experience contributing to Jenkins X and learn lots of new things! You do not have to complete all of these tasks. This is presented here so that you have some clarity around where to start and which resources to follow. We will start interviewing candidates between April 19 and May 12. We want to talk to you individually, and get to know you better. Contributors will be scored based on the following criteria: Important: Nice to have: Technical expertise (basic knowledge goes a long way): Past open source contribution experience Any level of familiarity with Jenkins X (or other CI/CD tools) Complete one of the three tasks listed below (If you have any questions on these tasks, please ask in our slack channel and we‚Äôll be happy to answer your questions): Previous GSoC experience (first time contributors are more than welcome) We will make an effort to talk individually to all the Jenkins X GSoC contributor applicants. We look forward to reviewing your application and working with you.</description></item><item><title>Blog: Project ideas for Google Summer of Code 2022 ‚òÄÔ∏è</title><link>https://kubermates.org/docs/2022-02-20-blog-project-ideas-for-google-summer-of-code-2022/</link><pubDate>Sun, 20 Feb 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-02-20-blog-project-ideas-for-google-summer-of-code-2022/</guid><description>Jenkins X has been accepted into the Google Summer of Code 2022 program, take a look at the follow up blog. Project proposal template can be found here. We have put together some project ideas as part of our application to participate in the Google Summer of Code 2022 program. The only way to trigger jobs/workflows in Jenkins X at the moment is by listening to events from Source Control Management (SCM) providers like github, gitlab, bitbucket, however it would be nice to listen to other event sources and trigger jobs/pipelines in Jenkins X. One interesting application would be to trigger some Jenkins X job in response to some alerting event (pagerduty, opsgenie). As a start we should focus on (emitting and listening to) cloudevents which define a common format for events produced from different sources. This will also help make Jenkins X compatible with other platforms. Golang, kubernetes, cloudevents, familiarity with lighthouse would be great, but not required 350 hours Hard With all the software breach that has happened recently, it has become necessary to add tooling to solve the issue around supply chain security. There are some good open source tools which can help with that (sigstore tools). As a CI/CD platform, Jenkins X needs to be integrated with them so that the end users can get this feature out of the box. Jenkins X leverages tekton as it‚Äôs pipeline execution engine. However, we dont integrate with tekton chain yet.</description></item><item><title>Blog: SIG Node CI Subproject Celebrates Two Years of Test Improvements</title><link>https://kubermates.org/docs/2022-02-16-blog-sig-node-ci-subproject-celebrates-two-years-of-test-improvements/</link><pubDate>Wed, 16 Feb 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-02-16-blog-sig-node-ci-subproject-celebrates-two-years-of-test-improvements/</guid><description>Authors: Sergey Kanzhelev (Google), Elana Hashman (Red Hat) Ensuring the reliability of SIG Node upstream code is a continuous effort that takes a lot of behind-the-scenes effort from many contributors. There are frequent releases of Kubernetes, base operating systems, container runtimes, and test infrastructure that result in a complex matrix that requires attention and steady investment to ‚Äúkeep the lights on. ‚Äù In May 2020, the Kubernetes node special interest group (‚ÄúSIG Node‚Äù) organized a new subproject for continuous integration (CI) for node-related code and tests. Since its inauguration, the SIG Node CI subproject has run a weekly meeting, and even the full hour is often not enough to complete triage of all bugs, test-related PRs and issues, and discuss all related ongoing work within the subgroup. Over the past two years, we‚Äôve fixed merge-blocking and release-blocking tests, reducing time to merge Kubernetes contributors‚Äô pull requests thanks to reduced test flakes. When we started, Node test jobs only passed 42% of the time, and through our efforts, we now ensure a consistent &amp;gt;90% job pass rate. We‚Äôve closed 144 test failure issues and merged 176 pull requests just in kubernetes/kubernetes. And we‚Äôve helped subproject participants ascend the Kubernetes contributor ladder, with 3 new org members, 6 new reviewers, and 2 new approvers. The Node CI subproject is an approachable first stop to help new contributors get started with SIG Node. There is a low barrier to entry for new contributors to address high-impact bugs and test fixes, although there is a long road before contributors can climb the entire contributor ladder: it took over a year to establish two new approvers for the group. The complexity of all the different components that power Kubernetes nodes and its test infrastructure requires a sustained investment over a long period for developers to deeply understand the entire system, both at high and low levels of detail. We have several regular contributors at our meetings, however; our reviewers and approvers pool is still small.</description></item><item><title>Blog: Jenkins X Survey Result Details</title><link>https://kubermates.org/docs/2022-02-15-blog-jenkins-x-survey-result-details/</link><pubDate>Tue, 15 Feb 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-02-15-blog-jenkins-x-survey-result-details/</guid><description>For more information on the Jenkins X survey see Survey results Some highlights from the free text answers. run it against a local running cluster to test changes to Jenkins-X before updated in GitHub. Integration with on-premise (Gitlab) as a lot of organizations are not using public cloud due to security policies, Proper guide on installing into existing cluster without using terraform. Give us back something like jx compliance , jx boot. Would be nice to not be tied to terraform to boot jenkins-x. Just like kubernetes, it would be great to have a jenkins-x the hard way where everything needs to be installed manually. it‚Äôs very difficult to just dive in without previous knowledge of the system extreme lack of quality documentation. Many conversations take place in slack and users cannot find information that somebody had the same problem beforehand and how it was solved More documentation and guides, making sure quickstart guides work without hassle Making it easier to get started Very hard to find any information on how people solve a similar problem before I very much enjoy your documentation, but it is very hard to find anything on google that is a huge disadvantage. Documents need to be clear about what works, and what does not work (kubernetes versions for example, bitbucket etc. ) Could also be nice with an arcitecture illustration or video that could compare jenkins with jenkins X I find that when I‚Äôm looking for information I get a lot of mixed results (v3 vs v2). Integration with vault is confused. Why install vault in docker in k3s environment? I strongly believe in this project! I use Jenkins X as a learning tool.</description></item><item><title>Blog: Jenkins X Survey Results</title><link>https://kubermates.org/docs/2022-02-15-blog-jenkins-x-survey-results/</link><pubDate>Tue, 15 Feb 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-02-15-blog-jenkins-x-survey-results/</guid><description>The Jenkins X survey was active for four weeks and closed on February 11 2022. We received lots of valuable insights into how people are using Jenkins X. We need more contributors in the Jenkins X community, so if you feel strongly about how Jenkins X should evolve, your best bet is to dive in and get your hands dirty:) Some highlights of the free text responses we got are collected here According to the survey, a typical Jenkinx X user works with Devops and Software Engineering This person is using Jenkins X version 3 on Amazon working in a company of 1-50 people. She finds it somewhat difficult to find documentation, average 2. 9 out of 5 The Jenkins X user tries to find information mainly on the main web site, sometimes on slack and less often on github. What he enjoys most about Jenkins X is that it is an easy way to learn, play around and work with Kubernetes. The git(ops(ish)) style of configuration, the preview environments, and the staging /production environments. But one thing is sure, documentation is confusing and shold be improved. The typical Jenkins X user would like to run Jenkins X offline, either to run on a laptop, or behind a (corporate) firewall. Proper support for multi-tenancy would be nice, and who can ignore security in these times. A clear governance of Jenkins X needs to be established, The typical person who answers surveys on Jenkins X is planning to attend Jenkins X Office hours, which is great! See you there next week then!.</description></item><item><title>Blog: Spotlight on SIG Multicluster</title><link>https://kubermates.org/docs/2022-02-04-blog-spotlight-on-sig-multicluster/</link><pubDate>Fri, 04 Feb 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-02-04-blog-spotlight-on-sig-multicluster/</guid><description>Authors: Dewan Ahmed (Aiven) and Chris Short (AWS) SIG Multicluster is the Special Interest Group (SIG) focused on how Kubernetes concepts are expanded and used beyond the cluster boundary. Historically, Kubernetes resources only interacted within that boundary - KRU or Kubernetes Resource Universe (not an actual Kubernetes concept). Kubernetes clusters, even now, don‚Äôt really know anything about themselves or, about other clusters. Absence of cluster identifiers is a case in point. With the growing adoption of multicloud and multicluster deployments, the work SIG Multicluster doing is gaining a lot of attention. In this article, Jeremy Olmsted-Thompson, Google and Chris Short, AWS discuss the interesting problems SIG Multicluster is solving and how you can get involved. Their initials JOT and CS will be used for brevity. CS : How long has the SIG Multicluster existed and how was the SIG in its infancy? How long have you been with this SIG? JOT : I‚Äôve been around for almost two years in the SIG Multicluster. All I know about the infancy years is from the lore but even in the early days, it was always about solving this same problem. Early efforts have been things like KubeFed. I think there are still folks using KubeFed but it‚Äôs a smaller slice. Back then, I think people out there deploying large numbers of Kubernetes clusters were really not at a point where we had a ton of real concrete use cases.</description></item><item><title>Blog: January 2022 updates from the JX community</title><link>https://kubermates.org/docs/2022-02-02-blog-january-2022-updates-from-the-jx-community/</link><pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-02-02-blog-january-2022-updates-from-the-jx-community/</guid><description>Happy new year 2022! This monthly blog post series is an attempt to showcase all the incredible work being done by the Jenkins X community to the wider audience. Lot of exciting features, bug fixes and documentation improvements were made. Huge thanks to all the contributors for their hardwork!.</description></item><item><title>Blog: Jenkins X Survey</title><link>https://kubermates.org/docs/2022-01-21-blog-jenkins-x-survey/</link><pubDate>Fri, 21 Jan 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-01-21-blog-jenkins-x-survey/</guid><description>We have made a short survey where we try to gain insight into how people experience Jenkins X. This is meant to be used as guidelines going forward so we can be more focused on what areas to improve. All contributions are welcome, if you are just browsing or have used it for years, we want to know you all! We have extended the survey period and it will now be open untill Tuesday February 11 2022 midnight UTC. All submissions are anonymous and the results will be published. So don‚Äôt hesitate, fill in the Jenkins X survey today.</description></item><item><title>Blog: Meet Our Contributors - APAC (India region)</title><link>https://kubermates.org/docs/2022-01-10-blog-meet-our-contributors-apac-india-region/</link><pubDate>Mon, 10 Jan 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-01-10-blog-meet-our-contributors-apac-india-region/</guid><description>Authors &amp;amp; Interviewers: Anubhav Vardhan , Atharva Shinde , Avinesh Tripathi , Debabrata Panigrahi , Kunal Verma , Pranshu Srivastava , Pritish Samal , Purneswar Prasad , Vedant Kakde Editor: Priyanka Saggu Good day, everyone üëã Welcome to the first episode of the APAC edition of the ‚ÄúMeet Our Contributors‚Äù blog post series. In this post, we‚Äôll introduce you to five amazing folks from the India region who have been actively contributing to the upstream Kubernetes projects in a variety of ways, as well as being the leaders or maintainers of numerous community initiatives. üí´ Let‚Äôs get started, so without further ado‚Ä¶ Arsh is currently employed with Okteto as a Developer Experience engineer. As a new contributor, he realised that 1:1 mentorship opportunities were quite beneficial in getting him started with the upstream project. He is presently a CI Signal shadow on the Kubernetes 1. 23 release team. He is also contributing to the SIG Testing and SIG Docs projects, as well as to the cert-manager tools development work that is being done under the aegis of SIG Architecture. To the newcomers, Arsh helps plan their early contributions sustainably. I would encourage folks to contribute in a way that‚Äôs sustainable. What I mean by that is that it‚Äôs easy to be very enthusiastic early on and take up more stuff than one can actually handle. This can often lead to burnout in later stages. It‚Äôs much more sustainable to work on things iteratively.</description></item><item><title>Blog: Incident: Kaniko and ACR</title><link>https://kubermates.org/docs/2021-12-28-blog-incident-kaniko-and-acr/</link><pubDate>Tue, 28 Dec 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-12-28-blog-incident-kaniko-and-acr/</guid><description>We‚Äôve recently had an issue with one of our packages come to light. We wanted to talk through the resolution steps we‚Äôre going to put into place. Azure users started reporting seeing the following error within the build step: This seemed to be indicating to an authorization issues with the terraform module. Other users were seemingly unaffected by this issue. Upon further analysis, kaniko seems to have issues with the latest version and grabbing credentials for acr. The latest working version that we are aware of is 1. 3. There wasn‚Äôt a massive influx of people seeing this issue due to it only occuring once versionstream had been updated with jx gitops upgrade. For starters, most users are probably using the latest kaniko features, so we‚Äôre unable to just roll this back for everyone. We‚Äôre getting started by creating a PR to kaniko to resolve the issue. However, due to release schedules etc, this will mean that getting started with azure will be broken for quite a while. If you‚Äôre on azure, the resolution is quite simple, here‚Äôs a step by step guide: 1.</description></item><item><title>Blog: Announcing the Kubernetes Contributor Celebration 2021</title><link>https://kubermates.org/docs/2021-12-07-blog-announcing-the-kubernetes-contributor-celebration-2021/</link><pubDate>Tue, 07 Dec 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-12-07-blog-announcing-the-kubernetes-contributor-celebration-2021/</guid><description>By Debabrata Panigrahi It‚Äôs that time of the year again, Yayy!! Like last year, this year also we are back with Kubernetes Contributor Celebration, the annual end of the year celebration, to recognize our achievements and have some fun! It‚Äôs a time for us to relax, chat and do something fun with your fellow contributors! To register, please fill out the Registration Form. More details on how to join the event. DevOps Party Games ‚ÄúDevOps Party Games takes the idea of ‚Äúonline party games‚Äù and tilts it on its head by adding DevOps-inspired content to existing games, and then streams it live via Twitch for a worldwide audience to watch, comment, and hopefully be entertained. In addition, the hosts (Matt Stratton, Jeremy Meiss, and Dan Maher) will provide color commentary, much like a modern day Cotton McKnight and Pepper Brooks (announcers from Dodgeball). ‚Äù There will be a Kubernetes slant for this edition of DevOps Party Games. The Great Cloud Native Bakeoff A virtual cooking event styled after the Great British Bake-off will be held during the Kubernetes Celebration Event. Contestants will have 90 minutes to bake something related to to your Cloud Native experience. Anything goes, as long as the majority of preparation is done live. The competition is open to all skill levels, from baking from a cake mix to grinding your own flour as long as it‚Äôs somehow Kubernetes-themed. At the end of the contest, each baked good will be judged on ambition, execution, success, and implementation of a Kubernetes theme. The winner will get a unique, handmade, baking-related prize. We‚Äôre looking for cooks of all backgrounds and talents.</description></item><item><title>Blog: Improve your documentation with Mermaid.js diagrams</title><link>https://kubermates.org/docs/2021-12-01-blog-improve-your-documentation-with-mermaid-js-diagrams/</link><pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-12-01-blog-improve-your-documentation-with-mermaid-js-diagrams/</guid><description>By Chris Metz and Tim Bannister Topics covered in this blog. Live editor link: Topics Covered Friendly landing spot. Greeting readers with a page full of text can be intimidating to those new to Kubernetes, software engineering and tech writing. Faster grasp of concepts. A diagram can serve as a visual roadmap for details covered in the accompanying text. Better retention. Humans remember pictures better than words. Most importantly, readers acquire the confidence to explore further. ‚ÄúWow! Okay I get this. What‚Äôs next? Maybe I can contribute. ‚Äù There are many drawing tools to choose from. Traditionally, you create a diagram, save it as png or svg file and then embed the file in the documentation.</description></item><item><title>Blog: Hacktoberfest conclusion 2021</title><link>https://kubermates.org/docs/2021-11-22-blog-hacktoberfest-conclusion-2021/</link><pubDate>Mon, 22 Nov 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-11-22-blog-hacktoberfest-conclusion-2021/</guid><description>Hacktoberfest 2021 is over, and we got quite a few contributions from the open source community. Contributions included various document improvements, adding jira as an issue tracker for generating changelogs and adding initial support for external vault! The top contributors to Jenkins X in hacktoberfest 2021 were: We would also like to thank all the contributors who participated and made it a success. The strength of Jenkins X lies in it‚Äôs vast community, and we hope to see many more major contributions from them in the near and far future. Missed this year‚Äôs hacktoberfest? No worries, you can always contribute to Jenkins X. We‚Äôre happy to help if you have any questions. Talk to us on our slack channels, which are part of the Kubernetes slack. Join Kubernetes slack here and find us on our channels: We look forward to participating in the next hacktoberfest!.</description></item><item><title>Blog: Hacktoberfest 2021</title><link>https://kubermates.org/docs/2021-10-06-blog-hacktoberfest-2021/</link><pubDate>Wed, 06 Oct 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-10-06-blog-hacktoberfest-2021/</guid><description>We are excited to announce that Jenkins X will be participating in Hacktoberfest again this year! Hacktoberfest is a month-long global celebration of open source software. All backgrounds and skill levels are encouraged to participate in Hacktoberfest and join a global community of open source contributors. Learn more about Hacktoberfest and sign up here. We welcome your contributions to the Jenkins X documentation project ! Issues labelled ‚Äúhacktoberfest‚Äù generally indicate good first issues. However, all pull requests will count towards your Hacktoberfest challenge. We‚Äôre happy to help if you have any questions. Talk to us on our slack channels, which are part of the Kubernetes slack. Join Kubernetes slack here and find us on our channels: Find out more about becoming involved in the Jenkins X community here. We look forward to seeing you in open source, fixing all the things!.</description></item><item><title>Blog: Moving Jenkins X v2 artifacts</title><link>https://kubermates.org/docs/2021-08-26-blog-moving-jenkins-x-v2-artifacts/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-08-26-blog-moving-jenkins-x-v2-artifacts/</guid><description>TL;DR - Jenkins X specific helm repositories and container registries hosted on GCP have been moved to GitHub. This will mainly affect jx v2 users but there is expected to be a small impact on v3 users too. Below describes the steps we believe are needed to keep Jenkins X installations working as normal but there will be some action needed. When Jenkins X first started we made heavy use of GCP‚Äôs services for hosting the cloud infrastructure needed by users to install and run Jenkins X. This was great as we could use the same IAM to push and maintain content from our own hosted build infrastructure and ensure we were validating the same experience of using cloud provider hosted services wherever possible. As Jenkins X grew in popularity the cloud costs began to increase with the pricing model from GCP , specifically the networking costs of cross continent egress. Given this, for jx3 we decided to see if switching to GitHub packages for container images and GitHub pages for helm repositories would be better, the result was it is better. In fact we have made it super easy for users to switch to using GitHub pages for releasing helm charts and using GitHub packages. Now that we have validated GitHub is more cost effective for hosting public images and helm charts for the Jenkins X project, we want to switch to using GitHub for all v2 plus v3 users, then shutdown the GCP services which are causing unnecessary cost. It is expected that v3 users will need a small change and v2 slightly more. Details for both will be described below but it is worth noting that there hasn‚Äôt been a v2 release in 9 months and v3 was GA in April earlier this year, so we aren‚Äôt expecting too many folks on v2. We are aiming to limit any disruption and help provide easy steps to handle the move.</description></item><item><title>Blog: How to debug your Tekton pipelines</title><link>https://kubermates.org/docs/2021-08-18-blog-how-to-debug-your-tekton-pipelines/</link><pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-08-18-blog-how-to-debug-your-tekton-pipelines/</guid><description>Tekton recently introduced a debug feature when you create TaskRun resources so that steps can be paused at a breakpoint until told to move forwards so that you can diagnose why pipeline steps fail. The latest Tekton release only supports breakpoints on TaskRun resources but there is a Pull Request #4145 to add support also to debugging PipelineRun resources as well. If you are reading this please add your thumbs up emoji feedback to the PR #4145 We‚Äôve switched Jenkins X to use a preview image of Tekton with PR #4145 included so that Jenkins X developers can easily debug their pipelines (which typically are PipelineRun resources). Here is a demo which shows how to debug pipelines: Make sure your cluster is upgraded to the latest version stream. If you intend to use the jx in the below examples make sure you upgrade the CLI too To enable a breakpoint you can: You can view breakpoints in the Lens UI in the Breakpoints tab or via: Once you have set a breakpoint defined for a particular Pipeline you need to trigger the pipeline. e. g. perform a git commit on the git branch to trigger a new pipeline to execute. The pipeline will execute as normal; you‚Äôll be able to view it execute via: Once your breakpoint is reached the pipeline pod will pause, waiting to continue. At this point you can then open a shell inside the container. The easiest way to do this is via the Lens UI , click on the Pipeline action menu then Shell -&amp;gt; latest step and a shell will open. Otherwise you can use: If you wish to continue the execution of a pipeline there are multiple scripts you can run inside the shell you can run inside the shell in the pipeline to tell the pipeline to continue: There are a few ways to delete breakpoints.</description></item><item><title>Blog: How to use GitOps and Kubernetes External Secrets for better audit and security</title><link>https://kubermates.org/docs/2021-08-17-blog-how-to-use-gitops-and-kubernetes-external-secrets-for-better-audit-and-secu/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-08-17-blog-how-to-use-gitops-and-kubernetes-external-secrets-for-better-audit-and-secu/</guid><description>So GitOps is a cool approach to managing kubernetes resources in a cluster, by checking in the source code for: Then everything is versioned and audited; you know who changed what, when and why. If a change breaks things, just revert via git like any other source code change. You can then add pipelines to verify changes in the Pull Requests result in valid kubernetes YAML etc. Then if you merge changes to git then an operator detect the change and do the kubectl apply (or helm install or whatever). There are a number of tools out there for doing this. e. g. Anthos Config Management , argo cd , fleet , flux cd and kapp controller So why did Jenkins X not use these tools and instead created its own git operator ? Over time it would be great to have more standardisation of the Git layout given the different tool. Our current recommended layout that works with many GitOps tools is described here. A number of solutions in the GitOps space define which helm charts to install in git with configuration files; or specify which kustomize templates to apply etc. However Jenkins X defaults to using helmfile to manage installing, upgrading and configuring multiple helm charts. Then we use helmfile template to render the helm charts as kubernetes resources.</description></item><item><title>Blog: How to choose a SIG as a non-code Kubernetes contributor</title><link>https://kubermates.org/docs/2021-07-09-blog-how-to-choose-a-sig-as-a-non-code-kubernetes-contributor/</link><pubDate>Fri, 09 Jul 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-07-09-blog-how-to-choose-a-sig-as-a-non-code-kubernetes-contributor/</guid><description>By Chris Short Kubernetes contributors aren‚Äôt people in capes or part of some secret society. How to start committing to the GitHub repos that make up the project is well documented , yet it remains intimidating for many. A few years ago, I spoke at an event and jokingly said, ‚ÄúKubernetes is just a bunch of APIs and YAML‚Ä¶ I‚Äôm a contributor; you don‚Äôt believe me?‚Äù After that talk, someone pulled me aside and asked if I was the Kubernetes contributor. They wanted to get involved in the community. Then came the real question, ‚ÄúI don‚Äôt know which special interest group (SIG) I would work in. ‚Äù The SIG you work in depends on your skills and what you want to do with your (or your company‚Äôs) free time. I‚Äôll start with myself as an example. I do not code as part of my day-to-day work; I never have. While I have made bits of code into programs here and there, I always had a helping hand. It‚Äôs also possible my definition of a coder is busted too. But, I consider myself a non-code contributor to Kubernetes. I‚Äôm an Ops person that embraced DevOps early.</description></item><item><title>Blog: Jenkins X 3 and Argo CD</title><link>https://kubermates.org/docs/2021-06-28-blog-jenkins-x-3-and-argo-cd/</link><pubDate>Mon, 28 Jun 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-06-28-blog-jenkins-x-3-and-argo-cd/</guid><description>There have been a number of requests from the Jenkins X community to use Argo CD for the last mile deployment phase of their continuous delivery pipelines. This blog explains some of the advantages of using Jenkins X and Argo CD all together. What‚Äôs included? You might be wondering why you would want to use BOTH Jenkins X and Argo CD together. Jenkins X aims to embrace OSS, where possible providing a nice UX to integrate with other projects and help provide better solutions for building, developing and running software on Kubernetes. Jenkins X indeed does have a git operator that applies Kubernetes YAML from a Git repository but there are some differences: Jenkins X uses GitOps principles for the entire installation, i. e. the starting point is a Git repository which is used to provision a cluster and manage (automatic if users wish) upgrades whereas today Argo CD uses a manual kubectl apply to manage the Argo installation itself. External Secrets integration for Vault, Google Secrets manager etc is something that Jenkins X provides out of the box. When Kubernetes based applications require a secret we prefer the source is stored in a secrets manager and the value synchronised automatically into the cluster enabling easier secret rotation, avoiding local secrets on machines and an easier UX for working with secrets. When adding an Application via Argo CD users can leverage the Jenkins X approach to working with secrets and rely on Argo to manage the deployments. If users prefer to work with SOPS that is totally fine too and will work in the same way as they are used to. If you do not wish to expose direct access for the Kubernetes cluster to developers then using a combination of Jenkins X UI for accessing logs and Argo CD for managing application deployments may be enough.</description></item><item><title>Blog: Continuous microservices with databases in Jenkins X</title><link>https://kubermates.org/docs/2021-06-25-blog-continuous-microservices-with-databases-in-jenkins-x/</link><pubDate>Fri, 25 Jun 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-06-25-blog-continuous-microservices-with-databases-in-jenkins-x/</guid><description>A common question we get asked on the Jenkins X project is how to get started creating microservices that use databases with automated CI/CD with GitOps Promotion and Preview Environments. To make things a little easier to get started we‚Äôve created a new node-postgresql quickstart. If you are using the cloud then we prefer cloud over kubernetes for things like databases, storage, ingress and secret managers so please try use your clouds managed databases if you can. So ideally you‚Äôd set up your database via your infrastructure as code solution, such as terraform , and then associate your kubernetes Service Account to a cloud IAM role to access the database. However to provide something simple that just works in any kubernetes cluster this quickstart uses the postgres-operator to manage setting up each database cluster in each environment. So to be able to use this quickstart you will need to install this operator into your cluster. You can add charts to your cluster via the CLI. From inside a git clone of your cluster git repository run the following command: This will modify the helmfile. yaml to point at a new helmfiles/postgres/helmfile. yaml file to deploy the postgres-operator chart. Then git commit and push that change to your cluster. You can watch it run via jx admin log -w.</description></item><item><title>Blog: Jenkins X at cdCon</title><link>https://kubermates.org/docs/2021-06-22-blog-jenkins-x-at-cdcon/</link><pubDate>Tue, 22 Jun 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-06-22-blog-jenkins-x-at-cdcon/</guid><description>cdCon 2021 is about to start with lots of great sessions. Here‚Äô a list of the Jenkins X related sessions: How Jenkins X is Integrating Observability from the Inside, and the Benefits for its Users by Vincent Behar , Dailymotion What‚Äôs new with Jenkins X 3 by James Rawlings &amp;amp; James Strachan, CloudBees Embrace ChatOps by Following the git-flow as Usual by Rick Zhao , Qingcloud.</description></item><item><title>Blog: Don't use docker, use kubernetes</title><link>https://kubermates.org/docs/2021-05-17-blog-don-t-use-docker-use-kubernetes/</link><pubDate>Mon, 17 May 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-05-17-blog-don-t-use-docker-use-kubernetes/</guid><description>Are you developing software that‚Äôs intended to run on kubernetes? If so we recommend not to use docker on your laptop. Docker on Windows/MacOS helps you run a VM that can then run linux containers easily. But why bother? We highly recommend just use a development kubernetes cluster - build and run your containers there instead then you‚Äôre closer to a production like environment. First you‚Äôll need a kubernetes cluster. I fully agree with James Ward that developers should not need to run kubernetes. Friends don‚Äôt let friends setup and manage kubernetes clusters by hand :). So try ask your infrastructure team for a development cluster or, if you can, use the cloud to set-up a managed kubernetes cluster. All the public clouds have a relatively straightforward way to spin up a fully managed kubernetes cluster for you that will be relatively inexpensive &amp;amp; they are easy to scale down when you don‚Äôt need them. e. g. on Google Cloud it‚Äôs a couple of clicks and about 5 minutes later you‚Äôll have a fully managed kubernetes cluster ready to use. Its easy to enable auto-scaling too.</description></item><item><title>Blog: Jenkins X 3 - May 2021 LTS</title><link>https://kubermates.org/docs/2021-05-12-blog-jenkins-x-3-may-2021-lts/</link><pubDate>Wed, 12 May 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-05-12-blog-jenkins-x-3-may-2021-lts/</guid><description>May 2001 LTS release is now available! LTS is a slower cadence version stream which contains a verified set of releases and configurations that have been used by teams tracking the bleeding edge Jenkins X. Included in this release:.</description></item><item><title>Blog: Jenkins X 3.x GA is here!</title><link>https://kubermates.org/docs/2021-04-15-blog-jenkins-x-3-x-ga-is-here/</link><pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-04-15-blog-jenkins-x-3-x-ga-is-here/</guid><description>I‚Äôm super excited to announce the 3. 0 GA (General Availability) release of Jenkins X! Jenkins X automates your CI/CD on kubernetes to help you accelerate : Here‚Äôs a demo of how to develop code with Jenkins X the main documentation of the changes are: here‚Äôs a brief summary of the differences: The following improvements have been made since the first beta : As a user the high level UX of Jenkins X is similar: We have been using Jenkins X 3. x in production now for many months (for CI/CD of all of the 3. x codebase and continuously upgrading our cluster in the standard way and it‚Äôs been much simpler and easier to use, operate and configure. We have also been continuously delivering changes from Jenkins X into our production cluster for many months now and it‚Äôs been working great - GitOps FTW! In general Jenkins X 3. x is now much simpler and more flexible. It supports lots more platforms than before and should be easy to extend and configure for other platforms too. If you have never tried 3. x before then please follow the Admin Guide to get Jenkins X installed on your cloud provider, on-premises kubernetes cluster or minikube. If you previously tried the 3. x alpha then the migration instructions are here. For folks on older 2.</description></item><item><title>Blog: Jenkins X 3 - April 2021 LTS</title><link>https://kubermates.org/docs/2021-04-12-blog-jenkins-x-3-april-2021-lts/</link><pubDate>Mon, 12 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-04-12-blog-jenkins-x-3-april-2021-lts/</guid><description>This is the second LTS release for Jenkins X 3. x. LTS is a slower cadence version stream which contains a verified set of releases and configurations that have been used by teams tracking the bleeding edge Jenkins X. Initially when we decided to maintain an LTS version stream we thought we‚Äôd aim for monthly releases however this second release comes two months after the first. This has given us more chances to run fixes and chart upgrades on Jenkins X own infrastructure to verify stability. Note This LTS release is intended to be the final one before Jenkins X 3 is made Generally Available so stay tuned for the exciting news coming very soon! We will of course continue to develop and release LTS post GA. Included in this release: Please be aware of these changes.</description></item><item><title>Blog: Traces for your pipelines</title><link>https://kubermates.org/docs/2021-04-08-blog-traces-for-your-pipelines/</link><pubDate>Thu, 08 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-04-08-blog-traces-for-your-pipelines/</guid><description>Now that Jenkins X has solid integration with Grafana for its observability , it‚Äôs time to start building fun things! And the first one is tracing for all your pipelines : With it, you can easily see the timings of all your pipelines, stages, and steps. This is great to inspect a ‚Äúslow‚Äù pipeline and quickly see the slower steps. We are using OpenTelemetry to generate a ‚Äúlogical‚Äù view of the pipeline, with 1 trace per pipeline and 1 span for each stage and step. By default, these traces are ingested by Grafana Tempo. But if you prefer to export them to a different destination, it‚Äôs very easy, and thanks to the OpenTelemetry Collector you can export to a lot of different services. You can see the full list here and here. The trace identifier is also stored in the pipeline itself so that the Jenkins X Pipelines Visualizer UI can link directly to the trace. You just need to enable the observability stack, as explained in the observability admin guide. Then, trigger a pipeline, and once it‚Äôs finished, go to the web UI, and click on the ‚ÄúTrace‚Äù button on the top-right. That‚Äôs it! This is only the first step of native tracing support in Jenkins X. Stay tuned for more!.</description></item><item><title>Blog: Jenkins X v3: now with built-in observability</title><link>https://kubermates.org/docs/2021-04-01-blog-jenkins-x-v3-now-with-built-in-observability/</link><pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-04-01-blog-jenkins-x-v3-now-with-built-in-observability/</guid><description>As a Continuous Delivery platform, Jenkins X has a central part in your infrastructure. If it becomes unstable or unusable, it will impact the whole software delivery of your organization. This is why observability is a critical topic for Jenkins X, and work has started to get observability built-in for Jenkins X v3: We‚Äôre using Grafana as the central visualization component: the main entry point from which you can get a complete overview of both your application‚Äôs lifecycle - development, build, tests, releases, deployments, runtime - and your Continuous Delivery platform. Platform observability is not enabled by default for the moment, so the first step is to enable it, as explained in the platform observability admin guide. Once it‚Äôs done, you‚Äôll get a running Grafana instance, pre-configured with data sources for applications logs - using Loki - and applications metrics - using Prometheus. But most important, it comes with a set of pre-defined Grafana dashboards for the main platform components: Tekton, Lighthouse, cert-manager, ‚Ä¶ Here is an example of such a dashboard, using a mix of data sources to display cert-manager metrics collected by Prometheus - including the certificates expiration dates - and logs collected by Loki/Promtail: Continuous Delivery Indicators‚Äô main goal is to give people insights into their workflows/processes so that they can continuously improve them. This is based on the DORA devops metrics and the SPACE framework. The CD Indicators addon is not enabled by default for the moment, so the first step is to enable it, as explained in the continuous delivery indicators admin guide. Once it‚Äôs done, you‚Äôll get a running collector, along with a PostgreSQL database. The collector will listen for various events, both from the cluster and the git repositories, and store pull requests, pipelines, releases, and deployments data in the PostgreSQL database. The addon will also expose a new Grafana data source along with pre-configured Grafana dashboards, which will be picked up by your running Grafana instance. Here is an example of such a dashboard, displaying various indicators for a single repository/application: contributors, reviews, pull requests, releases, deployments, ‚Ä¶ This is only the beginning! The next steps - in no particular order: Contributions are welcomed:.</description></item><item><title>Blog: Scaling Preview Environments with Osiris</title><link>https://kubermates.org/docs/2021-04-01-blog-scaling-preview-environments-with-osiris/</link><pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-04-01-blog-scaling-preview-environments-with-osiris/</guid><description>One of Jenkins X‚Äôs core features is the preview environments : temporary environments created automatically for each Pull Requests, to deploy your application and its dependencies. You can then use this preview environment to run integration tests, or manually use/test your application. This is all great until you have more and more applications, each with a few dependencies (postgresql, mongodb, ‚Ä¶) and a few opened pull requests at any time. This means that you‚Äôll get more and more pods running in your Kubernetes cluster, in addition to Jenkins X‚Äôs own components, your build pipelines, and of course your staging and production applications - unless you are using multi-cluster. The result is that you‚Äôll need more nodes or bigger nodes. Which means more money. But, these preview environments are in fact idle most of the time: they are only used for the integration tests, and sometimes when someone manually uses them. The rest of the time - including all night for example - they are just staying there, idle, and consuming resources. What if we could easily scale them down when they are idle, and automatically bring them up when we need them? So that a Pull Request staying opened for 2 weeks because someone went on vacation won‚Äôt consume resources in your cluster. Enter Osiris ! Initially created by the Deislabs team , Osiris is a Kubernetes component that will automatically scale down your ‚Äúidle‚Äù pods, and scale them up when a request comes in. Although the original project has been archived, the Dailymotion team has taken over the maintenance of a fork. And they have been using it with success in their Jenkins X dev cluster for more than 2 years: they regularly have around 50 preview environments active at any time, and‚Ä¶ 0 pods from these environments running at night - or on weekends.</description></item><item><title>Blog: cdCon 2021 - Call for Jenkins X Proposals</title><link>https://kubermates.org/docs/2021-02-25-blog-cdcon-2021-call-for-jenkins-x-proposals/</link><pubDate>Thu, 25 Feb 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-02-25-blog-cdcon-2021-call-for-jenkins-x-proposals/</guid><description>Hear ye! Hear ye! Jenkins X Community, cdCon 2021 (the Continuous Delivery Foundation‚Äôs annual flagship event) is happening June 23-24 and its call for papers is open! This is your chance to share what you‚Äôve been doing with Jenkins X. Are you building something cool? Using it to solve real-world problems? Are you making things fast? Secure? Or maybe you‚Äôre a contributor and want to share what‚Äôs new. In all cases, we want to hear from you! Submit your talk for cdCon 2021 to be part of the conversation driving the future of software delivery for technology teams, enterprise leadership, and open-source communities. Submission Deadline : Friday, March 5 at 11:59 PM PST Here are the suggested tracks: Singular project focus and/or interoperability between: View all tracks and read CFP details here. We look forward to reading your proposal! Submit your talk.</description></item><item><title>Blog: GitOps your cloud native pipelines</title><link>https://kubermates.org/docs/2021-02-25-blog-gitops-your-cloud-native-pipelines/</link><pubDate>Thu, 25 Feb 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-02-25-blog-gitops-your-cloud-native-pipelines/</guid><description>Tekton pipelines are cloud native and are designed from the ground up for kubernetes and the cloud: In a previous blog we talked about how you can accelerate your use of tekton with Jenkins X. We are moving towards a microservice kind of world with many teams writing many bits of software in many repositories. So there are lots and lots of pipelines. These pipelines keep getting more sophisticated over time; doing much more (all kinds of building, analysis, reporting, testing, ChatOps etc) and the software/images/approaches they use change. So how can we manage, configure and maintain them all so that there are many pipelines for many repositories; where each repository can customise anything it needs but we can easily maintain everything continuously and its easy to understand and tool around? We‚Äôve tried to tackle this problem in a number of ways over the years; each has pros and cons. One option is to put all your pipelines in a shared library. You can then reference the pipelines by name in each of your repositories. But what if you want to change a bit of a pipeline for a specific repository? If you change it globally for everyone you can break things. You may just want local customisation for your repository only. You can add parameters into your pipelines. They are quite verbose on Pipelines and PipelineRuns ; but it‚Äôs hard to think up front of every parameterisation that may be required by downstream repositories. e.</description></item><item><title>Blog: Jenkins X 3 - February 2021 LTS</title><link>https://kubermates.org/docs/2021-02-01-blog-jenkins-x-3-february-2021-lts/</link><pubDate>Mon, 01 Feb 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-02-01-blog-jenkins-x-3-february-2021-lts/</guid><description>This is the first LTS release for Jenkins X 3. x. We are still in the Beta release and the leadup to GA includes ensuring the process for LTS monthly releases is validated and working well. This first releases focuses on: more documentation and examples can be found here Because Jenkins X uses GitOps we can see the git diff of changes that will be brought in with a cluster upgrade. Here is the Pull Request that has been verified for February LTS release. https://github. com/jenkins-x/jx3-lts-versions/pull/209/files Included in the release is a switch of the NGINX Helm chart from the old Helm stable registry. It was discussed on the community slack that some users on EKS and not using a custom domain had to change the domain in their cluster jx-requirements. yml file. The change of Chart repository meant the old resources were removed and new ones added, resulting in a new Kubernetes LoadBalancer service was created, resulting in a new external IP address. You may need to update the domain in your jx-requirements. yml.</description></item><item><title>Blog: Jenkins X 3.x walkthroughs</title><link>https://kubermates.org/docs/2021-01-26-blog-jenkins-x-3-x-walkthroughs/</link><pubDate>Tue, 26 Jan 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-01-26-blog-jenkins-x-3-x-walkthroughs/</guid><description>Jenkins X 3. x is now looking ahead towards a GA release, with that we are producing walkthroughs for key areas to help users not only get started but get the most out of Jenkins X. To kick this off we are going to start with 9 videos that we‚Äôll follow up with more dedicated blogs over the coming weeks. The complete playlist can be found here however the blog below gives a more context for each one. There are a few key areas we are focusing on here: Starting off with a very quick introduction including what to expect from the walkthrough series. Jenkins X 3. x has focussed on clearer lines of separation, making the architecture significantly more pluggable, extensible and maintainable. With better tooling including UIs and more reliable guard rails for installations and upgrades. Jenkins X 3 also minimises abstractions and wrapping; so it promotes the direct use of open source projects like Helm, Helmfile and Tekton. Decoupling the management of Cloud infrastructure away from Jenkins X to tools that are better suited for the job. Jenkins X has started with Terraform and this manages all the cloud resources needed by Jenkins X Over time Jenkins X plans to support other tools (aided by the Kubernetes Cluster API ) users in the Kubernetes ecosystem leverage such as crossplane. io , Google Config Connector , AWS Controller etc.</description></item><item><title>Blog: New features in the pipelines visualizer UI</title><link>https://kubermates.org/docs/2021-01-18-blog-new-features-in-the-pipelines-visualizer-ui/</link><pubDate>Mon, 18 Jan 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-01-18-blog-new-features-in-the-pipelines-visualizer-ui/</guid><description>The Jenkins X Pipelines Visualizer UI has recently received a number of new features, so let‚Äôs do a little tour of these new features! When viewing a pipeline, the biggest new feature is the collapsed logs. No more hundreds - or thousands - of log lines, we now group the logs per-container (step), which are collapsed by default. Along with the status of the step and its duration, so it‚Äôs easier to go to the interesting part of the logs. Clicking on a log line will expand the logs for this specific container. You can also use the ‚ÄúToggle Steps‚Äù button to expand/collapse the logs for all the steps at once. While we‚Äôre talking about the logs, you can notice the 2 new buttons: On top of the logs, we now display some information about the pipeline: The pipeline timeline has been improved to include all the steps for all stages, but it is currently hidden by default - to avoid using too much space. Clicking on a stage will bring you to the steps, and clicking on a step will bring you to the logs for this step. Note that for a pipeline which includes a deployment to a Preview Environment, the UI will also display a link to the application‚Äôs URL in that specific Preview Environment. The homepage got some love too, with: We started this project at v0, and we believe that now it has enough features to be a v1! On our roadmap - without any specific order - we have: Thanks to all the contributors! All contributions are welcomed, the source code is: github. com/jenkins-x/jx-pipelines-visualizer.</description></item><item><title>Blog: Jenkins X 3.x beta is here!</title><link>https://kubermates.org/docs/2020-12-09-blog-jenkins-x-3-x-beta-is-here/</link><pubDate>Wed, 09 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2020-12-09-blog-jenkins-x-3-x-beta-is-here/</guid><description>I‚Äôm super excited to announce the 3. 0 beta of Jenkins X! Christmas has come early this year! the main documentation of the changes are: but here‚Äôs a brief summary of the differences: As a user the high level UX of Jenkins X is similar: We have been using Jenkins X 3. x in production now for many months (for CI/CD of all of the 3. x codebase and continuously upgrading our cluster in the standard way and it‚Äôs been much simpler and easier to use, operate and configure. In general Jenkins X 3. x is now much simpler and more flexible. It supports lots more platforms than before and should be easy to extend and configure for other platforms too. If you have never tried 3. x before then please follow the Admin Guide to get Jenkins X installed on your cloud provider, on-premises kubernetes cluster or minikube. If you previously tried the 3. x alpha then the migration instructions are here. For folks on older 2.</description></item><item><title>Blog: Jenkins X 3.x - beta is close!</title><link>https://kubermates.org/docs/2020-12-04-blog-jenkins-x-3-x-beta-is-close/</link><pubDate>Fri, 04 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2020-12-04-blog-jenkins-x-3-x-beta-is-close/</guid><description>It has been ‚Äòall hands on deck‚Äô in recent months with the focus on Jenkins X 3 alpha. First off a huge thankyou to everyone involved. The OSS community spirit has really shone through what has been a very difficult year for everyone. Knowing that people from all over the world come and help each other, share banter and work at all hours of the day to help build out a true open source cloud native continuous delivery solution for developers - it‚Äôs quite fantastic to see and amazing to be apart of. As a result of all this hard work the Beta is iminent so this is a good opportunity to thank all involved so far and to outline what to expect in the coming days. While we‚Äôve been in the Alpha phase it has provided us with the opportunity to deprecate and remove APIs, commands and obsolete features that existed in v2. This means we will not have any code dependency on the v2 codebase and so going forward v3 will be easier to maintain without the tech debt. With that, we aim to make a big push and roll out a few last changes in preparation for Beta, here‚Äôs a couple you will notice if you are already on the Alpha. We recommend taking time to understand these, and avoid upgrading for a few days so that changes can be handled in one go, as there will be a constant stream of larger updates happening: Balancing upgrades with continuous delivery brings many challenges. However with Kubernetes, GitOps, Jenkins X version streams (including the monthly LTS coming soon) and other tools like kpt makes the process more transparent and provides a greater level of high availability. Jenkins X automatically upgrades its own infrastructure with every version stream release and we haven‚Äôt experienced any disruption during the alpha period so far. We aim to continue this, but when we know there will be disruption we endeavour to inform and explain why ahead of time.</description></item><item><title>Blog: Accelerate your Tekton with Jenkins X</title><link>https://kubermates.org/docs/2020-11-11-blog-accelerate-your-tekton-with-jenkins-x/</link><pubDate>Wed, 11 Nov 2020 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2020-11-11-blog-accelerate-your-tekton-with-jenkins-x/</guid><description>One of the goals of Jenkins X has always been to help accelerate and automate Continuous Delivery so that developers can focus on delivering value to their customers; either by creating that new microservice or adding features to an existing project and not writing and managing pipelines. Pipeline engines like Jenkins and Tekton are awesome - they can do anything! But they start as a blank sheet of paper where you have to fill in all the details of how to compile your code, test it, verify it, tag it, release, distribute and delivery it to production. Figuring all that stuff out can take a huge amount of time to create and maintain. This gets even more complex as we are all creating more and more microservices each with their own pipelines making more and more things to create and manage. We want to be able to reuse pipelines and tasks to get work done. But at the same time we want flexibility; not all applications are the same and sometimes things need to be changed on a per team or application basis. In Jenkins X 2. x we went with a jenkins-x. xml approach to pipelines which let you inherit pipelines from reuable pipeline library and then use a composition DSL above Tekton which lets you add/remove/replace steps. e. g. to use the javascript pipeline library but override a step you could use: This was a pretty good approach; it lets us reuse common pipelines in a shared git repository and let‚Äôs reuse a composition DSL.</description></item><item><title>Blog: Hacktoberfest</title><link>https://kubermates.org/docs/2020-09-23-blog-hacktoberfest/</link><pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2020-09-23-blog-hacktoberfest/</guid><description>We are excited to announce that Jenkins X will be participating in Hacktoberfest again this year! Hacktoberfest is a month-long global celebration of open source software. From October 1 to October 31, submit four pull requests to qualify for the limited edition Hacktoberfest shirt. All backgrounds and skill levels are encouraged to participate in Hacktoberfest and join a global community of open source contributors. Learn more about Hacktoberfest and sign up here. We welcome your contributions to the Jenkins X project! Issues labelled ‚Äúhacktoberfest‚Äù generally indicate good first issues. However, all pull requests will count towards your Hacktoberfest challenge. Jenkins X welcomes contributors to both: There are plenty of open issues , and we welcome your help in making Jenkins X even more awesome. Jenkins X is written largely in Go, but you don‚Äôt need to be an expert to contribute! If you are new to the project, search for issues labelled ‚Äúgood-first-issue‚Äù. Our Contributing Guide has advice for getting started with contributing to Jenkins X. We welcome your help in improving the Jenkins X documenation. If you see areas of the documentation that need fixing or augmentation please raise a pull request. Our guide for Contributing to the Documentation has advice for getting started with contributing to the Jenkins X docs.</description></item><item><title>Blog: New UI to visualize your pipelines and logs</title><link>https://kubermates.org/docs/2020-09-23-blog-new-ui-to-visualize-your-pipelines-and-logs/</link><pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2020-09-23-blog-new-ui-to-visualize-your-pipelines-and-logs/</guid><description>Welcome to the Jenkins X Pipelines Visualizer : a new open-source read-only UI for Jenkins X, with a very specific goal and scope: visualize the pipelines and logs. This project was started at Dailymotion and quickly shared with the Jenkins X community. There is already the Octant-based UI , so why a new UI? The main reason is that Octant ‚Äúis an application and is intended as a single client tool and at this time there are no plans to support hosted versions of Octant‚Äù - see this thread on the Octant github repository for more information and details. So while Octant answers to a lot of use-cases, there is one for which it is not suited: quickly printing the build logs on a browser, for a specific pipeline. We want to be able to click on a link from a Pull/Merge Request, and get the pipeline logs. This is the specific use-case covered by the Pipelines Visualizer. We want to keep it small, focused, and fast. It‚Äôs a read-only UI, so there won‚Äôt be ‚Äúactions‚Äù to trigger a pipeline - because it can already be done using ‚Äúchatops‚Äù commands in the Pull Request for example. But there are a few interesting features already: This project was shared very early with the community, after just a few hours of work. So our short-term goal is to improve the UI - make it beautiful. We did a demo of jx-pipelines-visualizer at the last office hours : Check out the jx-pipelines-visualizer github repository if you want to install it in your cluster - there is a Helm Chart which can be added to your Jenkins X Dev Environment. And any contributions are welcomed - either create an issue or pull request in the project‚Äôs github repository, or come in the #jenkins-x-dev Slack Channel.</description></item><item><title>Blog: Jenkins X Talks at CDCon</title><link>https://kubermates.org/docs/2020-09-16-blog-jenkins-x-talks-at-cdcon/</link><pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2020-09-16-blog-jenkins-x-talks-at-cdcon/</guid><description>There will be six Jenkins X talks given by various speakers at the first-ever CDCon on October 7-8. The two-day virtual event, hosted by the Continuous Delivery Foundation, will focus on improving the world‚Äôs capacity to deliver software with security and speed. Register to attend the virtual event for only 25 USD and get access to all of the following Jenkins X talks and more. A CI/CD Framework for Production Machine Learning at Massive Scale (using Jenkins X and Seldon Core) Speaker: Alejandro Saucedo, Seldon Time: 1:15 PM PDT Managing production machine learning systems as internal data science infrastructure requirements grow, has uncovered new challenges which have required fundamentally different approaches to that of traditional CI/CD in software engineering. In this talk we will dive into the work we are doing at the SIG-MLOps and the CD Foundation towards developing the methodologies that encompass best practices to continuously integrate and deploy machine learning in production at massive scale. In this talk, we‚Äôll provide key insights on the core MLOps concepts, as well as a hands-on coding example where we take a text classification model through its training, deployment and promotion as canary and shadow deployments, which will also allow us to get deeper and more specific insight on our production environment. Dailymotion‚Äôs Continuous Delivery Story Speaker: Vincent Behar, Dailymotion Time: 3:30 PM PDT In this session, Vincent will share Dailymotion‚Äôs Continuous Delivery story with Jenkins, Jenkins X and Tekton. He will come back on the initial state and issues faced on the CI/CD topic, and how it was solved. He will insist on the practices that were put in place and the benefits that resulted from switching to Jenkins X. He will conclude with the new challenges brought by improving Dailymotion‚Äôs Continuous Delivery platform. If you are wondering if Jenkins X is the right tool for you, and the impact it can have on your team(s), then this is the right session for you! Moving from Jenkins to Jenkins X: Scaling and Accelerating CI/CD Speaker: Dr Michael Garbade, Education Ecosystem Time: 12:00 PM PDT Jenkins has served as a continuous integration (CI) tool long before the emergence of Kubernetes and distributed systems running on cloud-native platforms. Working with Jenkins as a stand-alone open-source tool has proved to be extremely difficult for distributed systems engineers, as it is designed for small projects and not scalable to bigger projects.</description></item><item><title>Blog: Welcome to Jenkins X 3.x alpha!</title><link>https://kubermates.org/docs/2020-09-16-blog-welcome-to-jenkins-x-3-x-alpha/</link><pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2020-09-16-blog-welcome-to-jenkins-x-3-x-alpha/</guid><description>We are very pleased to announce the alpha release of Jenkins X version 3 You can read more about the overview of the architecture and components here. This release has lots of benefits over version 2 is much easier to use, understand and manage. It is more flexible and simpler to configure for different infrastructures and cloud providers. The new release works well with any combination of helm 3, helmfile, kpt and/or kustomize. For those who‚Äôve used Jenkins X version 2 you can check out a comparison of the two versions Here is a demo video showing how to get started with Jenkins X on Google Cloud with Terraform and Vault : We also did a live demo of getting started and using version 3 at the octant office hours last week. We also included the helmfile based preview environments enhancement via the new jx-preview plugin which makes it much easier to make more sophisticated previews such as We now have support, via Terraform for 2 of the big 3 public clouds: Azure support is getting really close; if you‚Äôd like to help get it ready join us on slack Also when using your laptop or local kubernetes cluster without terraform we support: We are working on improving the UX of the installation/upgrade; we‚Äôre hoping to soon have a pure terraform (or Terraform Cloud) way to spin up a Jenkins X installation on a public cloud with a minimum of fuss. We‚Äôll hopefully blog about that soon‚Ä¶ So please take it for a spin and let us know what you think ! If you can think of any ways we can improve let us know!.</description></item><item><title>Blog: Octant: the OSS UI for Jenkins X</title><link>https://kubermates.org/docs/2020-08-06-blog-octant-the-oss-ui-for-jenkins-x/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2020-08-06-blog-octant-the-oss-ui-for-jenkins-x/</guid><description>A common question we have heard in the community over the years is Is there an open source UI for Jenkins X?. Well we now have an answer: its Octant using the octant-jx plugin. We love Octant because: Longer term we‚Äôre planning on making most of the developer and operations features of Jenkins X available through the UI via octant-jx. e. g. we hope as part of Jenkins X 3. x you‚Äôll be able to install or upgrade Jenkins X and watch the installation proceed all via Octant. But already right now today you can: Find out more about installing and using Octant here. We did a demo of octant-jx at the last office hours. We also presented octant-jx at the octant office hours this week. Here is a demo video showing octant in action with Jenkins X :.</description></item></channel></rss>