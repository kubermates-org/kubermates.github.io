<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Docs on Kubermates</title><link>https://kubermates.org/docs/</link><description>Recent content in Docs on Kubermates</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Wed, 24 Dec 2025 17:58:09 +0000</lastBuildDate><atom:link href="https://kubermates.org/docs/index.xml" rel="self" type="application/rss+xml"/><item><title>Do You Need a Service Mesh? Understanding the Role of CNI vs. Service Mesh</title><link>https://kubermates.org/docs/2025-12-24-do-you-need-a-service-mesh-understanding-the-role-of-cni-vs-service-mesh/</link><pubDate>Wed, 24 Dec 2025 17:58:09 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-24-do-you-need-a-service-mesh-understanding-the-role-of-cni-vs-service-mesh/</guid><description>What a CNI Actually Does The CNI’s Core Responsibilities (and Their Limits) What a CNI Does Not Do What is a Service Mesh What a Service Mesh Adds Where CNI and Service Mesh Overlap So When Do You Need a Service Mesh? A Layered Model: Outer Perimeter and Inner Core Calico and Istio: A Combined Approach One Last Thing: Complexity and Tradeoffs Get Started with Calico and Istio Today The world of Kubernetes networking can sometimes be confusing. What’s a CNI? A service mesh? Do I need one? Both? And how do they interact in my cluster? The questions can go on and on. Even for seasoned platform engineers, making sense of where these two components overlap and where the boundaries of responsibility end can be challenging. Seemingly bewildering obstacles can stand in the way of getting the most out of their complementary features. One way to cut through the confusion is to start by defining what each of them is, then look at their respective capabilities, and finally clarify where they intersect and how they can work together. This post will clarify: What a CNI is responsible for What a service mesh adds on top When you need one, the other, or both Container Network Interface (CNI) is a standard way to connect and manage networking for containers in Kubernetes. It is a set of standards defined by Kubernetes for configuring container network interfaces and maintaining connectivity between pods in a dynamic environment where network peers are constantly being created and destroyed. Those standards are implemented by CNI plugins. A CNI plugin is the concrete networking component that runs in your cluster and performs the actual networking tasks defined by the CNI specification. Calico is an example of a CNI plugin, implementing the CNI specification to deliver connectivity and policy enforcement for Kubernetes clusters. At a high level, a CNI plugin: Connects pods to the network Assigns IP addresses Routes traffic between pods and nodes Enforces Kubernetes NetworkPolicy All of this happens at the network and transport layers (L3/L4). The CNI doesn’t understand applications, it just ensures packets get from point A to point B efficiently and securely.</description></item><item><title>Bringing sustainability back into the conversation: CNCF Cloud Native Sustainability Month Tokyo</title><link>https://kubermates.org/docs/2025-12-24-bringing-sustainability-back-into-the-conversation-cncf-cloud-native-sustainabil/</link><pubDate>Wed, 24 Dec 2025 12:31:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-24-bringing-sustainability-back-into-the-conversation-cncf-cloud-native-sustainabil/</guid><description>&lt;p&gt;Open the original post ↗ &lt;a href="https://www.cncf.io/blog/2025/12/24/bringing-sustainability-back-into-the-conversation-cncf-cloud-native-sustainability-month-tokyo/"&gt;https://www.cncf.io/blog/2025/12/24/bringing-sustainability-back-into-the-conversation-cncf-cloud-native-sustainability-month-tokyo/&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Kubernetes v1.35: Fine-grained Supplemental Groups Control Graduates to GA</title><link>https://kubermates.org/docs/2025-12-23-kubernetes-v1-35-fine-grained-supplemental-groups-control-graduates-to-ga/</link><pubDate>Tue, 23 Dec 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-12-23-kubernetes-v1-35-fine-grained-supplemental-groups-control-graduates-to-ga/</guid><description>Kubernetes v1.35: Fine-grained Supplemental Groups Control Graduates to GA Motivation: Implicit group memberships defined in /etc/group in the container image What&amp;rsquo;s wrong with it? Fine-grained supplemental groups control in a Pod: supplementaryGroupsPolicy Attached process identity in Pod status Strict policy requires up-to-date container runtimes Getting involved How can I learn more? On behalf of Kubernetes SIG Node, we are pleased to announce the graduation of fine-grained supplemental groups control to General Availability (GA) in Kubernetes v1.35! The new Pod field, supplementalGroupsPolicy , was introduced as an opt-in alpha feature for Kubernetes v1.31, and then had graduated to beta in v1.33. Now, the feature is generally available. This feature allows you to implement more precise control over supplemental groups in Linux containers that can strengthen the security posture particularly in accessing volumes. Moreover, it also enhances the transparency of UID/GID details in containers, offering improved security oversight. supplementalGroupsPolicy If you are planning to upgrade your cluster from v1.32 or an earlier version, please be aware that some behavioral breaking change introduced since beta (v1.33). For more details, see the behavioral changes introduced in beta and the upgrade considerations sections of the previous blog for graduation to beta. /etc/group Even though the majority of Kubernetes cluster admins/users may not be aware of this, by default Kubernetes merges group information from the Pod with information defined in /etc/group in the container image. /etc/group Here&amp;rsquo;s an example; a Pod manifest that specifies spec. securityContext. runAsUser: 1000 , spec. securityContext. runAsGroup: 3000 and spec.</description></item><item><title>Kyverno at ContribFest: Community, collaboration, and the power of open source in action</title><link>https://kubermates.org/docs/2025-12-23-kyverno-at-contribfest-community-collaboration-and-the-power-of-open-source-in-a/</link><pubDate>Tue, 23 Dec 2025 12:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-23-kyverno-at-contribfest-community-collaboration-and-the-power-of-open-source-in-a/</guid><description>Why ContribFest matters for open source Sharing the Kyverno story Three groups, three journeys Learning together, across experience levels Community, sustainability, and the role of supporting organizations Looking ahead to Europe Posted on December 23, 2025 by Cortney Nickerson, CNCF Ambassador and Head of Community at Nirmata CNCF projects highlighted in this post A few weeks ago at KubeCon + CloudNativeCon North America in Atlanta, the Kyverno community had the opportunity to participate in ContribFest , one of the most energizing and community-driven initiatives in the cloud-native ecosystem. While ContribFest sessions are listed on the official event calendar, many people across the broader community still don’t realize these experiences exist—let alone understand how impactful they can be for networking, learning, and truly “finding your place” within open source. Kyverno has been a longtime host of ContribFest at KubeCon, and our Atlanta event proved to be one for the books with 60+ participants joining in the fun. ContribFest is one of the many initiatives supported by the CNCF that reflects what makes open source truly special: shared learning, collaboration across company lines, and a welcoming on-ramp for people at every stage of their cloud-native journey. As a project, Kyverno strongly believes in these types of programs, not only because they help grow individual projects, but because they also strengthen the entire cloud-native ecosystem. Open source thrives when people with different backgrounds, goals, and experience levels come together. ContribFest events create the perfect environment for this to happen in a way that is both inclusive and deeply practical. At our Kyverno ContribFest event, we welcomed a wide range of participants, including: People new to open source who wanted to understand how projects function behind the scenes and learn to contribute to the community Practitioners exploring Policy as Code and evaluating Kyverno for future adoption Current end users refining their existing implementations and hoping to learn more advanced ways to deepen and optimize their setups Long-time community members and advanced adopters building complex, production-grade workflows excited to contribute to the roadmap and meet other community members for collaboration and learning opportunities The result of this mixture of folks coming together is always the same: an experience that is enriching for everyone involved! Newcomers gain confidence, experienced users deepen their understanding and find ways to give back, and maintainers walk away seeing their work being appreciated, valued, and with a renewed energy for growth and innovation based on the invaluable feedback received from real-world environments and end users. We kicked off the session with a brief project overview to ensure everyone, regardless of background, had a shared foundation. We introduced: The Kyverno maintainer team and how we collaborate How the project is organized and governed Where the repository lives and how the codebase is structured The many ways people can contribute beyond writing code How to connect with the Kyverno community and participate in discussions This framing helped demystify open source, especially since it was the first time seeing what actually happens “on the inside” of a CNCF project for many attendees. After the introduction, we split the room into three focused discussion groups based on experience level and use case. This group explored the fundamentals of Policy as Code and Kyverno adoption.</description></item><item><title>Kubernetes v1.35: Kubelet Configuration Drop-in Directory Graduates to GA</title><link>https://kubermates.org/docs/2025-12-22-kubernetes-v1-35-kubelet-configuration-drop-in-directory-graduates-to-ga/</link><pubDate>Mon, 22 Dec 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-12-22-kubernetes-v1-35-kubelet-configuration-drop-in-directory-graduates-to-ga/</guid><description>Kubernetes v1.35: Kubelet Configuration Drop-in Directory Graduates to GA The problem: managing kubelet configuration at scale Example use cases Managing heterogeneous node pools Gradual configuration rollouts Viewing the merged configuration Good practices Acknowledgments Get involved With the recent v1.35 release of Kubernetes, support for a kubelet configuration drop-in directory is generally available. The newly stable feature simplifies the management of kubelet configuration across large, heterogeneous clusters. With v1.35, the kubelet command line argument &amp;ndash;config-dir is production-ready and fully supported, allowing you to specify a directory containing kubelet configuration drop-in files. All files in that directory will be automatically merged with your main kubelet configuration. This allows cluster administrators to maintain a cohesive base configuration for kubelets while enabling targeted customizations for different node groups or use cases, and without complex tooling or manual configuration management. &amp;ndash;config-dir As Kubernetes clusters grow larger and more complex, they often include heterogeneous node pools with different hardware capabilities, workload requirements, and operational constraints. This diversity necessitates different kubelet configurations across node groups—yet managing these varied configurations at scale becomes increasingly challenging. Several pain points emerge: Configuration drift : Different nodes may have slightly different configurations, leading to inconsistent behavior Node group customization : GPU nodes, edge nodes, and standard compute nodes often require different kubelet settings Operational overhead : Maintaining separate, complete configuration files for each node type is error-prone and difficult to audit Change management : Rolling out configuration changes across heterogeneous node pools requires careful coordination Before this support was added to Kubernetes, cluster administrators had to choose between using a single monolithic configuration file for all nodes, manually maintaining multiple complete configuration files, or relying on separate tooling. Each approach had its own drawbacks. This graduation to stable gives cluster administrators a fully supported fourth way to solve that challenge. Consider a cluster with multiple node types: standard compute nodes, high-capacity nodes (such as those with GPUs or large amounts of memory), and edge nodes with specialized requirements. File: 00-base.</description></item><item><title>Enhance Amazon EKS network security posture with DNS and admin network policies</title><link>https://kubermates.org/docs/2025-12-22-enhance-amazon-eks-network-security-posture-with-dns-and-admin-network-policies/</link><pubDate>Mon, 22 Dec 2025 18:04:38 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-22-enhance-amazon-eks-network-security-posture-with-dns-and-admin-network-policies/</guid><description>Enhance Amazon EKS network security posture with DNS and admin network policies Amazon EKS enhanced network policies Admin network policies DNS-based network policies Implementation across EKS deployment models Use cases 1. Enforcing cluster-level security with Admin network policies 2. Securing access to AWS services in multi-tenant environments 3. Hybrid cloud integration Implementation best practices Building baseline security with deny-by-default rules Using label-based segmentation Combining DNS policies with traditional network policies Monitoring and audit Considerations Understanding policy evaluation order Applying the principle of least privilege Validating your DNS policies Interaction with Amazon Route 53 DNS firewall Conclusion About the authors Amazon Web Services (AWS) announced the availability of DNS-based and Admin network policies for Amazon Elastic Kubernetes Service (EKS) Auto mode and Admin network policies for both EKS Auto mode and EKS on Amazon Elastic Compute Cloud (EC2), providing enhanced capabilities to secure network traffic both within your clusters and to external endpoints. These new policy types enable you to implement stable, domain-based access controls for external services while centrally managing security policies across multiple namespaces, reducing operational complexity and strengthening your overall security posture. In this post, we explore practical use cases that demonstrate how these policies solve real-world challenges and remove the need to rely on third-party software across different deployment scenarios, from securing access to external services to hybrid cloud integration and multi-tenant environments. Application modernization drives demand for sophisticated network security that simplifies operations at scale. Modern containerized applications require granular control over external endpoint access, enabling teams to precisely manage which cluster-external services (such as AWS services, on-premises systems, and third-party APIs) their workloads can reach. Teams want to move beyond IP-based filtering to DNS-based access control, leveraging stable domain names rather than constantly changing IP addresses for more predictable and maintainable security policies. Organizations also need centralized policy management capabilities that enforce consistent security standards across multiple namespaces and workloads, ensuring uniform protection without requiring individual teams to implement policies independently. As these environments grow, there’s an increasing focus on operational simplicity, reducing the administrative overhead of managing network-level security while maintaining strong defensive postures that scale effectively with business growth. Admin network policies provide centralized, cluster-wide network access control that spans multiple namespaces.</description></item><item><title>Avoiding Zombie Cluster Members When Upgrading to etcd v3.6</title><link>https://kubermates.org/docs/2025-12-21-avoiding-zombie-cluster-members-when-upgrading-to-etcd-v3-6/</link><pubDate>Sun, 21 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-21-avoiding-zombie-cluster-members-when-upgrading-to-etcd-v3-6/</guid><description>Avoiding Zombie Cluster Members When Upgrading to etcd v3.6 Issue summary The fix and upgrade path Additional technical detail Key takeaway Acknowledgements This article is a mirror of an original that was recently published to the official etcd blog. The key takeaway ? Always upgrade to etcd v3.5.26 or later before moving to v3.6. This ensures your cluster is automatically repaired, and avoids zombie members. Recently, the etcd community addressed an issue that may appear when users upgrade from v3.5 to v3.6. This bug can cause the cluster to report &amp;ldquo;zombie members&amp;rdquo;, which are etcd nodes that were removed from the database cluster some time ago, and are re-appearing and joining database consensus. The etcd cluster is then inoperable until these zombie members are removed. In etcd v3.5 and earlier, the v2store was the source of truth for membership data, even though the v3store was also present. As a part of our v2store deprecation plan , in v3.6 the v3store is the source of truth for cluster membership. Through a bug report we found out that, in some older clusters, v2store and v3store could become inconsistent. This inconsistency manifests after upgrading as seeing old, removed &amp;ldquo;zombie&amp;rdquo; cluster members re-appearing in the cluster. We’ve added a mechanism in etcd v3.5.26 to automatically sync v3store from v2store, ensuring that affected clusters are repaired before upgrading to 3.6. x.</description></item><item><title>5 Key Principles of Modern Applications</title><link>https://kubermates.org/docs/2025-12-19-5-key-principles-of-modern-applications/</link><pubDate>Fri, 19 Dec 2025 20:24:25 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-19-5-key-principles-of-modern-applications/</guid><description>Framework for Building, Running, and Managing Apps for the Next Decade 1. Cloud Native, Composable, API-First Architecture API-First Architecture 2. Declarative, GitOps-Driven Delivery and Operations 3. Observability and Closed-Loop Automation Tooling and Instrumentation 4. Zero-Trust Security, Compliance, and Resilience by Design VCF and Zero-Trust 5. Platform Engineering as a Foundation Summary Discover more from VMware Cloud Foundation (VCF) Blog Related Articles 5 Key Principles of Modern Applications VCF Breakroom Chats Episode 77: Orchestrate like a Pro – Event Subscriptions and Extensibility for Every Admin with VCF 9.0 VCF Breakroom Chats Episode 76: Cloud Admin Fast Track – Learn Infrastructure as Code the Easy Way with VCF 9.0 In a recent panel discussion titled “Top 5 Criteria to Choose the Right Platform for Your Modern Workloads,” members from the VMware Cloud Foundation (VCF) team and Forrester analyst Brent Ellis explored how enterprises evaluate infrastructure platforms for modern, hybrid workloads. The conversation focused on five priorities: security and resilience, scalability, operational simplicity, developer enablement, and innovation. Forrester’s Ellis emphasized that organizations are seeking standardization in platform engineering, with platforms treating VMs and Kubernetes workloads equally. This is a core design principle of VMware Cloud Foundation 9.0. This discussion laid the groundwork for why platform choices matter, especially for private clouds. Let’s now dive deeper into five key principles for successfully designing and operating modern applications, using best practices from NIST, CNCF, Google SRE, and ITIL. Modern applications are no longer confined to virtual machines, containers, or cloud accounts.</description></item><item><title>Proactive Amazon EKS monitoring with Amazon CloudWatch Operator and AWS Control Plane metrics</title><link>https://kubermates.org/docs/2025-12-19-proactive-amazon-eks-monitoring-with-amazon-cloudwatch-operator-and-aws-control-/</link><pubDate>Fri, 19 Dec 2025 18:30:39 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-19-proactive-amazon-eks-monitoring-with-amazon-cloudwatch-operator-and-aws-control-/</guid><description>Proactive Amazon EKS monitoring with Amazon CloudWatch Operator and AWS Control Plane metrics Understanding basic CloudWatch metrics Enhanced monitoring with CloudWatch Observability Operator Prerequisites Walkthrough Setup Installing and configuring the CloudWatch Observability Operator Basic CloudWatch metrics CloudWatch Container Insights CloudWatch Application Signals Create anomaly detection alerts for cluster Real-world monitoring scenarios Detecting scheduling issues Tracking API server performance Monitoring admission webhook health etcd storage monitoring Cleaning up Conclusion About the authors Organizations running Kubernetes workloads on Amazon Elastic Kubernetes Service (Amazon EKS) need comprehensive monitoring for optimal cluster performance and reliability. Although Amazon EKS manages the control plane, maintaining workload health requires monitoring capabilities. This post explores using the Amazon CloudWatch monitoring, including new Amazon EKS metrics and the CloudWatch Observability Operator, to gain deeper visibility into cluster operations, detect issues, understand bottlenecks, and maintain healthy EKS clusters. Before Amazon EKS 1.28, control plane metrics were available only through the Kubernetes API server’s /metrics endpoint (Prometheus format). Amazon EKS 1.28+ automatically sends core Kubernetes control plane metrics to CloudWatch in the AWS/EKS namespace at no extra cost. These metrics are accessible through Amazon EKS console dashboards and CloudWatch. Key metric categories include the following: /metrics AWS/EKS API server metrics : Monitor API server health through total requests, HTTP 4XX/5XX errors, and throttling to detect bottlenecks or performance degradation. Scheduler metrics : Monitor pod scheduling attempts and pending pods ( active/backoff/gated/unschedulable queues) to detect scheduling bottlenecks, such as under-resourced worker nodes. active/backoff/gated/unschedulable etcd metrics : Monitor database size and performance to maintain a healthy etcd cluster. The following figure shows the architecture for viewing application and cluster metrics through CloudWatch Container Insights and the Amazon EKS console pre-built dashboards. Figure1: Architecture showing enhanced monitoring with CloudWatch Observability Operator for EKS clusters CloudWatch monitors cloud resources and applications. The CloudWatch Observability Operator, when added to Amazon EKS as an Amazon EKS add-on , automatically instruments applications using CloudWatch Application Signals and Container Insights.</description></item><item><title>Kubernetes 1.35: In-Place Pod Resize Graduates to Stable</title><link>https://kubermates.org/docs/2025-12-19-kubernetes-1-35-in-place-pod-resize-graduates-to-stable/</link><pubDate>Fri, 19 Dec 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-12-19-kubernetes-1-35-in-place-pod-resize-graduates-to-stable/</guid><description>Kubernetes 1.35: In-Place Pod Resize Graduates to Stable What is in-place Pod Resize? How can I start using in-place Pod Resize? How does this help me? Changes between beta (1.33) and stable (1.35) What&amp;rsquo;s next? Integration with autoscalers and other projects Feature expansion Improved stability Providing feedback This release marks a major step: more than 6 years after its initial conception, the In-Place Pod Resize feature (also known as In-Place Pod Vertical Scaling), first introduced as alpha in Kubernetes v1.27, and graduated to beta in Kubernetes v1.33, is now stable (GA) in Kubernetes 1.35! This graduation is a major milestone for improving resource efficiency and flexibility for workloads running on Kubernetes. In the past, the CPU and memory resources allocated to a container in a Pod were immutable. This meant changing them required deleting and recreating the entire Pod. For stateful services, batch jobs, or latency-sensitive workloads, this was an incredibly disruptive operation. In-Place Pod Resize makes CPU and memory requests and limits mutable, allowing you to adjust these resources within a running Pod, often without requiring a container restart. Key Concept: Desired Resources: A container&amp;rsquo;s spec. containers[&lt;em&gt;]. resources field now represents the desired resources. For CPU and memory, these fields are now mutable. spec. containers[&lt;/em&gt;]. resources Actual Resources: The status.</description></item><item><title>VCF Breakroom Chats Episode 77: Orchestrate like a Pro – Event Subscriptions and Extensibility for Every Admin with VCF 9.0</title><link>https://kubermates.org/docs/2025-12-19-vcf-breakroom-chats-episode-77-orchestrate-like-a-pro-event-subscriptions-and-ex/</link><pubDate>Fri, 19 Dec 2025 16:08:40 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-19-vcf-breakroom-chats-episode-77-orchestrate-like-a-pro-event-subscriptions-and-ex/</guid><description>About the VCF Breakroom Chat Series Discover more from VMware Cloud Foundation (VCF) Blog Related Articles 5 Key Principles of Modern Applications VCF Breakroom Chats Episode 77: Orchestrate like a Pro – Event Subscriptions and Extensibility for Every Admin with VCF 9.0 VCF Breakroom Chats Episode 76: Cloud Admin Fast Track – Learn Infrastructure as Code the Easy Way with VCF 9.0 In this episode of VCF Breakroom Chats , Alina Thylander and Scott McDermott break down how VMware Cloud Foundation (VCF) 9.0 makes private cloud automation smarter and easier with event‑driven extensibility. You’ll see how real‑time event subscriptions cut out ticket queues, how policy‑as‑code keeps things secure while giving developers freedom, and how orchestrator workflows tie the whole lifecycle together—from deployment to retirement. If you’re ready to ditch manual steps, reclaim costly resources like GPUs, and run your cloud like a pro, this conversation is packed with practical takeaways you can use right away. Want to learn more about VCF Automation? Check out the VCF Automation web page for more resources. Read this blog post: Private Cloud Redefined: Deliver a Unified Cloud Consumption Experience with VMware Cloud Foundation 9.0 Explore VMware Cloud Foundation Automation by the Numbers with the Forrester Total Economic Impact (TEI) Study Download the Self-Service Private Cloud for Dummies Guide ​In this series, we share vlogs with industry-recognized experts from Broadcom and Broadcom partners and customers. These vlogs are concise, like meeting in a breakroom and getting great information from a quick conversation. This series is for you if you are an IT practitioner, IT admin, cloud or platform architect, platform engineer, developer, DevOps, senior IT manager, IT executive, or AI/ML professional. If you are new to the series, please check out our previous episodes.</description></item><item><title>Speed Up Your JavaScript Apps: Native Bun Support is Now Available on App Platform</title><link>https://kubermates.org/docs/2025-12-19-speed-up-your-javascript-apps-native-bun-support-is-now-available-on-app-platfor/</link><pubDate>Fri, 19 Dec 2025 03:17:43 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-19-speed-up-your-javascript-apps-native-bun-support-is-now-available-on-app-platfor/</guid><description>Speed Up Your JavaScript Apps: Native Bun Support is Now Available on App Platform Benefits Deployment Paths in App Platform How to Get Started with Bun How Bun Detection Works Migrating from Node. js to Bun Use Bun as Package Manager Next. js Configuration Get Started About the author(s) Native Bun support in App Platform is now available By Bikram Gupta and Dinesh Murthy Published: December 19, 2025 4 min read The JavaScript ecosystem is rapidly evolving to meet the growing performance and integration demands of modern, AI-driven applications. Bun is a popular framework for developers that offers an all-in-one runtime, bundler, and package manager and is considered a drop-in replacement for Node. js. With its faster startup times and lower memory usage than traditional runtimes, Bun is a top request from our customers. Today, we’re excited to announce native Bun support on DigitalOcean App Platform. Now, you can deploy Bun applications directly from your code repository without writing a single line of configuration. App Platform’s Cloud Native Buildpacks will automatically detect, build, deploy and run your Bun apps. Performance: Bun is a modern, high-performance JavaScript runtime with integrated tooling, optimized for fast startup and efficient execution. Zero Configuration: You don’t need to maintain a Dockerfile. Just push your code into your git repo, and we’ll handle the runtime setup.</description></item><item><title>Attestation vs. integrity in a zero-trust world</title><link>https://kubermates.org/docs/2025-12-19-attestation-vs-integrity-in-a-zero-trust-world/</link><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-19-attestation-vs-integrity-in-a-zero-trust-world/</guid><description>Attestation vs. integrity in a zero-trust world A new frontier in data protection Demystifying the core concepts How attestation and integrity reinforce each other From trusting to knowing Red Hat Product Security About the authors Lukas Vrabec Yash Mankad More like this Red Hat to acquire Chatterbox Labs: Frequently Asked Questions From incident responder to security steward: My journey to understanding Red Hat&amp;rsquo;s open approach to vulnerability management What Is Product Security? | Compiler Technically Speaking | Security for the AI supply chain Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The complex risks facing modern IT environments make IT security a strategic imperative, not a back-end detail. Furthering this is cloud computing, which serves as the foundation of the AI economy, meaning that enterprises and nations require greater control, transparency, and assurance over data location and protection. Trust has become not just a technical question, but a matter of national policy, corporate strategy, and even societal resilience. At the same time, the explosion of AI and machine learning (ML) workloads is reshaping infrastructure requirements. But these shifts pose a complex question—if your most valuable models and datasets are in the cloud, how do you assess their security posture? Here lies the central dilemma. The very abstraction that makes the cloud powerful also makes it opaque. You don’t control the hardware, the hypervisor, or the low-level firmware that your workloads depend on. How do you trust something you cannot see or control? The old paradigm of “trust but verify” no longer works in this modern environment. Instead, the principle of zero trust, “assume nothing, verify everything,” has become a core tenet of modern security strategy. This is where the next generation of security concepts comes into play. Confidential computing, through hardware-based trusted execution environments (TEEs), enables organizations to protect their data not only at rest and in transit, but also in use.</description></item><item><title>Friday Five — December 19, 2025</title><link>https://kubermates.org/docs/2025-12-19-friday-five-december-19-2025/</link><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-19-friday-five-december-19-2025/</guid><description>Friday Five — December 19, 2025 Red Hat Accelerates AI Trust and Security with Chatterbox Labs Acquisition Fierce Network - Red Hat&amp;rsquo;s CTO keeps an &amp;lsquo;open&amp;rsquo; mind on AI Looking ahead to 2026: Red Hat’s view across the hybrid cloud AI Magazine - How Red Hat and AWS Bring Scalable Gen AI to the Enterprise SiliconANGLE - Powering the AI machine: Red Hat leverages the cloud-native open source toolbox to build solutions for next-gen IT About the author Red Hat Corporate Communications More like this Master modern hybrid cloud security and scale with Red Hat updates F5 BIG-IP Virtual Edition is now validated for Red Hat OpenShift Virtualization Technically Speaking | Platform engineering for AI agents Technically Speaking | Driving healthcare discoveries with AI Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Red Hat has acquired Chatterbox Labs, a specialist in AI safety and generative AI guardrails. This acquisition integrates model-agnostic security and transparency tools into the Red Hat AI portfolio, helping enterprises deploy trustworthy, production-grade AI across hybrid cloud environments with automated risk metrics and safety testing. Learn more Red Hat CTO Chris Wright reaffirms the company&amp;rsquo;s commitment to &amp;ldquo;default to open&amp;rdquo; for AI, underscoring how open ecosystems drive responsible innovation, address hybrid cloud complexities, and leverage AI as a force multiplier for developers while shifting focus toward the critical data layer. Learn more Red Hat executives outline their 2026 outlook, focusing on shifting AI from experimentation to production. Key priorities include modernizing infrastructure to tackle technical debt, scaling agentic AI through unified inference platforms, and leveraging open hybrid cloud solutions to ensure security, flexibility, and sovereignty across diverse environments. Learn more Red Hat is expanding its collaboration with AWS to optimize Red Hat AI for AWS Trainium and Inferentia chips, enabling enterprises to scale production-ready generative AI across hybrid cloud environments. Learn more Red Hat is utilizing its open source &amp;ldquo;toolbox,&amp;rdquo; including Kubernetes, KubeVirt, and vLLM, to bridge platform engineering and AI development, providing a secure, interoperable fabric that helps enterprises scale AI workloads, manage VMs, and implement zero-trust security across hybrid environments. Learn more Red Hat is the world’s leading provider of enterprise open source software solutions, using a community-powered approach to deliver reliable and high-performing Linux, hybrid cloud, container, and Kubernetes technologies. Red Hat helps customers integrate new and existing IT applications, develop cloud-native applications, standardize on our industry-leading operating system, and automate, secure, and manage complex environments. Award-winning support, training, and consulting services make Red Hat a trusted adviser to the Fortune 500. As a strategic partner to cloud providers, system integrators, application vendors, customers, and open source communities, Red Hat can help organizations prepare for the digital future.</description></item><item><title>Red Hat OpenShift expands support for VMware vSphere Foundation 9 and VMware Cloud Foundation 9</title><link>https://kubermates.org/docs/2025-12-19-red-hat-openshift-expands-support-for-vmware-vsphere-foundation-9-and-vmware-clo/</link><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-19-red-hat-openshift-expands-support-for-vmware-vsphere-foundation-9-and-vmware-clo/</guid><description>Red Hat OpenShift expands support for VMware vSphere Foundation 9 and VMware Cloud Foundation 9 Red Hat OpenShift Container Platform | Product Trial About the authors Richard Vanderpool Joseph Callen Michal Zasepa More like this 14 software architecture design patterns to know Getting started with socat, a multipurpose relay tool for Linux Technically Speaking | Platform engineering for AI agents Technically Speaking | Driving healthcare discoveries with AI Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share General availability (GA) support is now available for Red Hat OpenShift versions 4.18, 4.19, and 4.20 running on VMware vSphere Foundation 9 (VVF9) and VMware Cloud Foundation 9 (VCF9). As organizations continue to modernize their infrastructure, Red Hat remains committed to providing a stable, certified, and high-performance foundation for Kubernetes workloads across diverse environments. This announcement means that all current Red Hat customers deploying OpenShift on vSphere 8/vCenter 8 or VCF 5 can run their workloads on OpenShift clusters deployed on VVF9 or VCF9 as an infrastructure provider. With the introduction of VCF9, Broadcom has evolved its strategy to offer a comprehensive, all-in-one product suite. Red Hat has aligned its support to integrate with this new architecture. Customers deploying OpenShift clusters on top of VCF9 will use a standardized networking model: Infrastructure network provider : Powered by VMware NSX , helping to ensure robust connectivity at the hypervisor level. Overlay network : Powered by OVN-Kubernetes , providing the standard, agile networking layer required by OpenShift. For NSX Container Network Interface (CNI) integration, Red Hat uses the Partner certification process (see Red Hat Ecosystem Catalog ) to verify partner components are verified for compatibility with Red Hat products, maintaining the stability and security that our customers expect. To support the full lifecycle of your applications, we are also updating our management and storage compatibility for the VVF9 and VCF9 platforms. Red Hat Advanced Cluster Management for Kubernetes (ACM) 2.15.1+ supports both VVF9 and VCF9. This allows operations teams to manage their OpenShift fleet across these new VMware environments with the same governance, visibility, and control they use today. For customers utilizing OpenShift Data Foundation versions 4.19.7 and 4.20 are available as a Technology Preview (TP) on VVF9 and VCF9.</description></item><item><title>The end of static secrets: Ford’s OpenShift strategy</title><link>https://kubermates.org/docs/2025-12-19-the-end-of-static-secrets-ford-s-openshift-strategy/</link><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-19-the-end-of-static-secrets-ford-s-openshift-strategy/</guid><description>The end of static secrets: Ford’s OpenShift strategy Managing 200+ clusters Automation and identity: The keyless mandate Governance: PoC and admission control Building the foundational infrastructure for AI Ready to build your own keyless, automated Red Hat OpenShift platform? Red Hat Learning Subscription | Product Trial About the author Debbie Margulies More like this F5 BIG-IP Virtual Edition is now validated for Red Hat OpenShift Virtualization More than meets the eye: Behind the scenes of Red Hat Enterprise Linux 10 (Part 4) The Containers_Derby | Command Line Heroes Can Kubernetes Help People Find Love? | Compiler Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share At Red Hat OpenShift Commons Gathering in Atlanta on November 10, 2025, Satish Puranam, Director of Cloud and Developer Experience at Ford, and Sitaram Iyer, VP of Emerging Technologies at CyberArk, outlined how Ford manages over 200 Red Hat OpenShift clusters amid a complex digital transition. Their strategy relies on a single, non-negotiable mandate: everything must be keyless. Image 1: From left, Satish Puranam, Director of Cloud and Developer Experience at Ford, and Sitaram Iyer, VP of Emerging Technologies at CyberArk. To achieve this scale, Ford has abandoned static secrets entirely, implementing 100% identity-driven automation and Policy as Code (PoC) enforcement directly on the Kubernetes platform. Ford’s Red Hat OpenShift footprint spans diverse workloads, from consumer-facing applications to highly regulated manufacturing and warranty systems. This highly dynamic environment requires clusters to be regularly spun up and torn down, often pushing core Kubernetes limits (such as etcd capacity) and necessitating constant defragmentation or provisioning of new clusters. To manage this scale, Ford imposes strict guardrails and standardizes on Red Hat OpenShift across its multicloud and multi-datacenter infrastructure. As Satish noted, &amp;ldquo;Our decades-long journey is all about hiding complexity and barriers for the developer. You learn it once, use it over and over again, and it scales very well. It allows us to actually update and upgrade our systems at a rapid clip. &amp;quot; Image 2: Ford’s journey with Red Hat OpenShift By standardizing the platform, Ford can upgrade systems rapidly without burdening teams with infrastructure friction. Ford’s scaling strategy eliminates long-lived, static credentials, or &amp;ldquo;secrets,&amp;rdquo; in favor of 100% automated machine identity.</description></item><item><title>Turning automation spend into a measurable advantage</title><link>https://kubermates.org/docs/2025-12-19-turning-automation-spend-into-a-measurable-advantage/</link><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-19-turning-automation-spend-into-a-measurable-advantage/</guid><description>Turning automation spend into a measurable advantage Financial clarity: Measuring automation’s ROI Strategic governance and risk mitigation Bridging the gap to Business Intelligence (BI) The need for data-driven automation Want to learn more? Red Hat Ansible Automation Platform | Product Trial About the author Harper Buete More like this Accelerating NetOps transformation with Ansible Automation Platform Enterprise automation resilience with EDB and Red Hat Ansible Automation Platform Technically Speaking | Taming AI agents with observability How Do We Make Updates Less Annoying? | Compiler Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share IT automation is no longer optional for executives; it is a critical strategy. Organizations have moved beyond simple scripting to focus on speed and impact, using automation to power growth and reduce risk. But a critical gap remains: visibility. How do you, as a business leader, prove the return on investment (ROI) and confidently decide where to invest your next dollar? Red Hat Ansible Automation Platform&amp;rsquo;s automation dashboard and analytics feature is more than an IT report; it&amp;rsquo;s the financial and operational intelligence layer executives need to realize the full value of enterprise automation. Automation is a cost center until its value is quantifiable. The automation dashboard bridges that gap, converting technical wins—like time saved and job completion rates—into clear financial data. Demonstrate financial impact: No longer guess at ROI. The platform translates key metrics into reportable savings and ROI. This data enables clear reporting to leadership and key stakeholders, elevating automation from isolated IT tools into a mission-critical business asset. Prioritize with the savings planner: Looking ahead is equally as important as current reporting. The savings planner forecasts cost and time savings for potential tasks over the next 1 to 3 years. By prioritizing efforts that deliver maximum financial return, your automation roadmap can align with your business objectives.</description></item><item><title>Why should your organization standardize on Red Hat Enterprise Linux today?</title><link>https://kubermates.org/docs/2025-12-19-why-should-your-organization-standardize-on-red-hat-enterprise-linux-today/</link><pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-19-why-should-your-organization-standardize-on-red-hat-enterprise-linux-today/</guid><description>Why should your organization standardize on Red Hat Enterprise Linux today? What does it mean to standardize on RHEL? Simplifying infrastructure complexity with RHEL standardization How RHEL standardization solves your challenges Outcomes and business value of standardizing on RHEL Red Hat Enterprise Linux | Product trial About the author Gil Cattelain More like this Getting started with socat, a multipurpose relay tool for Linux More than meets the eye: Behind the scenes of Red Hat Enterprise Linux 10 (Part 4) OS Wars_part 1 | Command Line Heroes OS Wars_part 2: Rise of Linux | Command Line Heroes Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share I am currently collaborating with my product marketing team on content explaining why organizations—whether they currently use Red Hat Enterprise Linux (RHEL) or not—should standardize on RHEL today. The content is fresh in my mind, and I wanted to share some of it with you in advance, as I believe it contains compelling insights. As you likely know, we launched numerous new features and capabilities this year with RHEL 10 , including image mode for RHEL , which enables users to create and manage operating system images for consistent deployment, and the RHEL command-line assistant powered by Lightspeed, which uses generative AI to help users execute complex command-line tasks. We also announced the inclusion of post-quantum encryption algorithms to help your organization resist future security attacks. More recently, with the launch of RHEL 10.1 and 9.7 , we included support for AI accelerators via our repositories, and also introduced a RHEL offline command-line assistant in dev preview , along with RHEL HPC for Azure. Furthermore, just a few weeks ago, we announced the Red Hat Project Hummingbird. These innovative features, capabilities, and key announcements—spanning consistency, AI integration, advanced security, and high-performance computing—reinforce RHEL&amp;rsquo;s position as the leading enterprise Linux platform. They are a compelling reason to standardize your infrastructure on RHEL, ensuring you are equipped with a consistent, secure, and modern operating system foundation ready for the future of IT. We have many more exciting developments planned for 2026, so stay tuned and make sure you check out the links above in case you missed any of these significant announcements. In this blog, I want to discuss how standardizing on RHEL across your entire infrastructure will help solve many critical organizational challenges and the specific business outcomes it will deliver. Standardizing on RHEL refers to adopting a uniform approach to using RHEL across your organization. It involves selecting RHEL and implementing it consistently across all systems.</description></item><item><title>How Istio Ambient Mode Delivers Real World Solutions</title><link>https://kubermates.org/docs/2025-12-18-how-istio-ambient-mode-delivers-real-world-solutions/</link><pubDate>Thu, 18 Dec 2025 23:29:43 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-18-how-istio-ambient-mode-delivers-real-world-solutions/</guid><description>The Reality Platform Teams Have Been Living With Why These Challenges Matter Introducing Istio Ambient Mode How Istio Ambient Mode Delivers Real World Solutions What Teams Should Do Next: A More Practical Future for Service Mesh Step 1: Start with mTLS Step 2: Add L7 traffic control where it matters most Step 3: Use observability to validate and troubleshoot Step 4: Scale the same model everywhere Go Deeper and See Istio Ambient Mode in Action For years, platform teams have known what a service mesh can provide: strong workload identity, authorization, mutual TLS authentication and encryption, fine-grained traffic control, and deep observability across distributed systems. In theory, Istio checked all the boxes. In practice though, many teams hit a wall. Across industries like financial services, media, retail, and SaaS, organizations told a similar story. They wanted mTLS between services to meet regulatory or security requirements. They needed safer deployment capabilities like canary rollouts and traffic splitting. They wanted visibility that went beyond IP addresses. However, traditional sidecar based meshes came with real costs: High operational complexity Thousands of sidecars to manage Fragile upgrade paths Hard to debug failure modes In several cases, teams started down the Istio service mesh path, only to pause or roll back entirely because the ongoing operational complexity was too high. The value of a service mesh was clear, but the service mesh architecture based on sidecars was not sustainable for many production environments. In many cases, organizations evaluated service meshes with clear goals in mind. They wanted mTLS between services, better control over traffic during deployments, and observability that could keep up. Some even deployed a service mesh briefly before stepping back.</description></item><item><title>The mentorship flywheel: How CNCF is growing the next generation of cloud native leaders</title><link>https://kubermates.org/docs/2025-12-18-the-mentorship-flywheel-how-cncf-is-growing-the-next-generation-of-cloud-native-/</link><pubDate>Thu, 18 Dec 2025 19:49:40 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-18-the-mentorship-flywheel-how-cncf-is-growing-the-next-generation-of-cloud-native-/</guid><description>2025: A record year for cloud native mentorship Mariam Fahmy: A case study in the flywheel Finding Kyverno The mentorship experience Ten months to Maintainer The flywheel in action Why this matters Advice for aspiring contributors Get involved Posted on December 18, 2025 by Nate Waddington, Head of Mentorship &amp;amp; Documentation at CNCF A record year for mentorship, 187 graduates contributing to the ecosystem, and one engineer’s journey from “I didn’t know about Docker” to Kyverno maintainer. In 2025, the CNCF mentorship programs hit a milestone: 187 successful mentorship projects—a record cohort. But the real story isn’t in the numbers alone. It’s in what happens after the mentorship ends. This year, we’ve been tracking something we call the “mentorship flywheel”—the virtuous cycle where mentees become contributors, contributors become maintainers, and maintainers become mentors who bring in the next generation. The data tells a compelling story, but so do the individuals behind it. One of them is Mariam Fahmy. Her path from complete newcomer to Kyverno maintainer in under a year illustrates exactly what structured mentorship can achieve—and why it matters for the long-term health of open source. The mentorship programs CNCF participates in—spanning LFX Mentorship, Google Summer of Code, and Outreachy—have grown steadily since 2021. This year’s 187 projects represent not just quantity, but breadth: mentees contributed to everything from Linkerd and Kubernetes to Istio, KubeEdge, and dozens of other projects across the CNCF landscape. The numbers tell a story of sustained engagement and accelerating growth. CNCF LFX Mentee contributions have increased dramatically year over year—from nearly 10,000 in 2021 to 40,000 in 2025.</description></item><item><title>Kubernetes v1.35: Job Managed By Goes GA</title><link>https://kubermates.org/docs/2025-12-18-kubernetes-v1-35-job-managed-by-goes-ga/</link><pubDate>Thu, 18 Dec 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-12-18-kubernetes-v1-35-job-managed-by-goes-ga/</guid><description>Kubernetes v1.35: Job Managed By Goes GA Why delegate Job reconciliation? How. spec. managedBy works Ecosystem Adoption How can you learn more? Acknowledgments Get involved In Kubernetes v1.35, the ability to specify an external Job controller (through. spec. managedBy ) graduates to General Availability. spec. managedBy This feature allows external controllers to take full responsibility for Job reconciliation, unlocking powerful scheduling patterns like multi-cluster dispatching with MultiKueue. The primary motivation for this feature is to support multi-cluster batch scheduling architectures, such as MultiKueue. The MultiKueue architecture distinguishes between a Management Cluster and a pool of Worker Clusters: The Management Cluster is responsible for dispatching Jobs but not executing them. It needs to accept Job objects to track status, but it skips the creation and execution of Pods. The Worker Clusters receive the dispatched Jobs and execute the actual Pods. Users usually interact with the Management Cluster.</description></item><item><title>Deep dive: Streamlining GitOps with Amazon EKS capability for Argo CD</title><link>https://kubermates.org/docs/2025-12-18-deep-dive-streamlining-gitops-with-amazon-eks-capability-for-argo-cd/</link><pubDate>Thu, 18 Dec 2025 17:27:31 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-18-deep-dive-streamlining-gitops-with-amazon-eks-capability-for-argo-cd/</guid><description>Deep dive: Streamlining GitOps with Amazon EKS capability for Argo CD Architecture overview: Hub-and-Spoke topology Prerequisites Solution Walkthrough Configure AWS IAM Identity Center Create hub cluster with Argo CD Capability Create spoke clusters Register clusters with Argo CD Configure Git sources Native ECR integration Implement multi-tenancy with projects Scale deployments with ApplicationSets CI/CD pipeline integration Operational visibility Clean up Conclusion About the authors Organizations use GitOps as the standard for managing Kubernetes deployments at scale. Running Argo CD in production means managing high availability, upgrades, Single Sign-On (SSO) configuration, and cross-cluster connectivity. This operational scope grows with each additional cluster across regions or AWS accounts. Amazon Elastic Kubernetes Service (EKS) Capability for Argo CD – referred to as “Argo CD Capability” in the remainder of this blog post – is part of the newly launched Amazon EKS Capabilities feature. It provides a fully managed GitOps continuous deployment solution that eliminates the operational overhead of running Argo CD on your clusters. The capability runs in AWS managed service accounts outside of your clusters, with AWS handling scaling, upgrades, inter-cluster communications, and offering native integrations with other AWS services such as AWS Secret Manager , Amazon Elastic Container Registry (ECR) , AWS CodeCommit and AWS CodeConnections. In this deep dive, we explore advanced scenarios with Argo CD including hub-and-spoke multi-cluster deployments, native AWS service integrations, multi-tenancy implementation, scaling with advanced Argo CD configurations and integration with CI/CD pipeline. For a detailed comparison with self-managed solutions, see Comparing EKS Capability for Argo CD to self-managed Argo CD. In a hub-and-spoke architecture, the Argo CD Capability is created on a dedicated central EKS cluster (the hub) that serves as the control plane for GitOps operations, and it is not created on the spoke clusters. Although Argo CD in the central hub cluster can technically manage and deploy applications to both the hub and the spoke clusters, in this blog the hub cluster is designed exclusively for management tasks and does not host any business workloads. “Figure 1: Sample topology of hub-and-spoke model for Argo CD Capability” This topology provides platform teams with a single pane of glass to orchestrate deployments across an entire fleet of clusters—whether they’re in different regions, accounts, or have private Kubernetes API endpoints. Before you begin, verify that you have the following tools and configurations in place.</description></item><item><title>Cilium releases 2025 annual report: A decade of cloud native networking</title><link>https://kubermates.org/docs/2025-12-18-cilium-releases-2025-annual-report-a-decade-of-cloud-native-networking/</link><pubDate>Thu, 18 Dec 2025 16:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-18-cilium-releases-2025-annual-report-a-decade-of-cloud-native-networking/</guid><description>Posted on December 18, 2025 by epower CNCF projects highlighted in this post A decade on from its first commit in 2015, 2025 marks a significant milestone for the Cilium project. The community has published the 2025 Cilium Annual Report: A Decade of Cloud Native Networking , which reflects on the project’s evolution, key milestones, and notable developments over the past year. What began as an experimental container networking effort has grown into a mature, widely adopted platform, bringing together cloud native networking, observability, and security through an eBPF-based architecture. As Cilium enters its second decade, the community continues to grow in both size and momentum, with sustained high-volume development, widespread production adoption, and expanding use cases including virtual machines and large-scale AI infrastructure. We invite you to explore the 2025 Annual Report and celebrate a decade of cloud native networking with the community. For any questions or feedback, please reach out to contribute@cilium. io. Share.</description></item><item><title>Upgrading VMware Cloud Foundation 5.2 to 9.0: The Top 10 Questions Answered</title><link>https://kubermates.org/docs/2025-12-18-upgrading-vmware-cloud-foundation-5-2-to-9-0-the-top-10-questions-answered/</link><pubDate>Thu, 18 Dec 2025 15:34:01 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-18-upgrading-vmware-cloud-foundation-5-2-to-9-0-the-top-10-questions-answered/</guid><description>Question 1: How does VMware SDDC Manager handle upgrades? Are there major changes with upgrades in version 9.0? Question 2: Are there considerations for VMware vSAN Original Storage Architecture (OSA) clusters during the upgrade? Question 3: How is the VMware NSX upgrade performed? Question 4: If I have VMware Aria Suite deployed in VCF aware mode in version 5.2, do I need to decouple Aria Suite from the deployment before upgrading? Question 5: Is it possible to upgrade from VCF 5.2 without LCM and Aria Suite configured? Question 6: How many hosts are allowed in a consolidated design in VCF 9.0? Question 7: How can I transition from VMware Identity Manager (vIDM) to VCF Identity Broker (VIDB) in VCF 9? Question 8: For VCF Operations do we need to download the binaries and where should they be placed? Question 9: Is there a regression / rollback pathway if I have an error in my upgrade? Question 10: Is there any path for VMware Cloud Director (VCD) to VCF Automation Migrations? On-Demand Replay Need Help? Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Upgrading VMware Cloud Foundation 5.2 to 9.0: The Top 10 Questions Answered Upgrading VMware Cloud Foundation 5.2 to 9.0: Webinar Takeaways Set Your Implementation Up for Success with VCF Jumpstart Workshop VMware Cloud Foundation (VCF) 9.0 provides a quick and easy way to deploy a private cloud. While the upgrade from VCF 5. x is designed to be streamlined, it introduces mandatory changes in management methodologies and requires careful, phased execution. I recently presented a packed webinar with Brent Douglas, diving deep into the VCF 5.2 to VCF 9.0 upgrade process. With hundreds of attendees and a flood of questions, it’s clear this transition is top-of-mind for many of you. I’ve sifted through the noise, merging similar inquiries into single, comprehensive challenges. Below are the Top 10 “must-know” questions submitted by the audience, complete with the detailed answers you need to navigate your VCF 9.0 journey with confidence. There were several questions around SDDC Manager and upgrades. There are no major changes to the way upgrades are performed. If you are familiar with VCF 5.2 the asynchronous patching functionality is built into the console in the same way in the 9.0 release. This lets you schedule upgrades/patches as needed. The big difference is that the SDDC Manager interface has been incorporated into the VCF Operations console located under the fleet management section.</description></item><item><title>10 must-read articles to master modern hybrid cloud security and scale</title><link>https://kubermates.org/docs/2025-12-18-10-must-read-articles-to-master-modern-hybrid-cloud-security-and-scale/</link><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-18-10-must-read-articles-to-master-modern-hybrid-cloud-security-and-scale/</guid><description>10 must-read articles to master modern hybrid cloud security and scale What&amp;rsquo;s new in RHEL 10.1: Offline assistance, convenient AI accelerators, and more Red Hat Insights is now Red Hat Lightspeed: Accelerating AI-powered management Introducing Red Hat Enterprise Linux 9.7 Red Hat OpenShift 4.20 accelerates virtualization and enterprise AI innovation Introducing Red Hat Satellite 6.18: New AI, management, and system health capabilities Red Hat Performance and Scale Engineering Prepare for a post-quantum future with RHEL 9.7 Introducing OpenShift Service Mesh 3.2 with Istio’s ambient mode The new and simplified AI accelerator driver experience on Red Hat Enterprise Linux Red Hat collaborating with Omnissa to bring Horizon virtual desktops to OpenShift Virtualization What’s next? About the author Isabel Lee More like this F5 BIG-IP Virtual Edition is now validated for Red Hat OpenShift Virtualization More than meets the eye: Behind the scenes of Red Hat Enterprise Linux 10 (Part 4) Technically Speaking | Platform engineering for AI agents Technically Speaking | Driving healthcare discoveries with AI Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The final quarter of the year is marked by accelerated innovation. Red Hat is leading the way by integrating AI into management, fortifying our security foundations, and readying your infrastructure for both present and future demands like quantum computing and highly scaled virtualization. This month brought significant platform updates across Red Hat Enterprise Linux (RHEL) and Red Hat OpenShift, along with key insights from our performance engineering teams. The following articles provide the essential technical and strategic information you need to make the most of your hybrid cloud journey. What&amp;rsquo;s new in RHEL 10.1: Offline assistance, convenient AI accelerators, and more The RHEL team continues to evolve your security-hardened foundation. The latest RHEL 10.1 update is a significant step, especially for those in highly secure or air-gapped environments. We’re excited to announce the developer preview of an offline, locally available RHEL command-line assistant for customers with Red Hat Satellite. This means AI-powered guidance on troubleshooting and installation is now available without external network access, prioritizing your security. Plus, RHEL 10.1 makes embracing AI easier than ever by providing vendor-validated AI accelerator drivers directly in our trusted repositories. You’ll also find soft-reboots for minimal downtime and a host of updated developer toolsets, supporting an environment that is modern, efficient, and ready for what’s next. Red Hat Insights is now Red Hat Lightspeed: Accelerating AI-powered management We’re taking the management and analytics capabilities you know and trust to the next level. Red Hat Insights is now Red Hat Lightspeed, a transition that reflects our expanded commitment to intelligent, AI-powered management across RHEL, Red Hat OpenShift, and Red Hat Ansible Automation Platform.</description></item><item><title>F5 BIG-IP Virtual Edition is now validated for Red Hat OpenShift Virtualization</title><link>https://kubermates.org/docs/2025-12-18-f5-big-ip-virtual-edition-is-now-validated-for-red-hat-openshift-virtualization/</link><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-18-f5-big-ip-virtual-edition-is-now-validated-for-red-hat-openshift-virtualization/</guid><description>F5 BIG-IP Virtual Edition is now validated for Red Hat OpenShift Virtualization Run BIG-IP where you’re already running the rest of your platform No change to your security model or traffic logic Supported by F5 What this means in practice Red Hat OpenShift Virtualization Engine | Product Trial About the author Shane Heroux More like this More than meets the eye: Behind the scenes of Red Hat Enterprise Linux 10 (Part 4) Enhance workload security with confidential containers on Azure Red Hat OpenShift The Containers_Derby | Command Line Heroes You Can’t Automate Collaboration | Code Comments Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share A lot of teams we work with are juggling 2 worlds at once: the Red Hat OpenShift projects that keep accelerating, and the previous hypervisor platforms or physical infrastructure that need to be kept to run the traffic management and security policies of the BIG-IP appliances. This gets the job done, but nobody enjoys maintaining 2 sets of infrastructure with 2 different lifecycles. It’s a tax we all pay because there hasn’t been a clean alternative. Now there is. F5 BIG-IP Virtual Edition (VE) is officially validated by F5 to support Red Hat OpenShift Virtualization and listed in the Red Hat Ecosystem Catalog. With this milestone achievement, F5 provides further validation of customer demand for a hybrid container and virtualization platform. OpenShift Virtualization lets you run virtual machines (VMs) directly on your worker nodes. With the validation in place, BIG-IP VE can now be deployed among them, rather than requiring a separate environment you keep around solely for network appliances. Administrators can enjoy 1 control plane, 1 lifecycle, 1 place to look when something needs patching. This isn’t a cut-down version or a feature-limited build. VE inside OpenShift is the very same BIG-IP. Your existing configs—WAF policies, iRules, routing decisions, all the customizations required for your applications—can be used as is.</description></item><item><title>Resilient model training on Red Hat OpenShift AI with Kubeflow Trainer</title><link>https://kubermates.org/docs/2025-12-18-resilient-model-training-on-red-hat-openshift-ai-with-kubeflow-trainer/</link><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-18-resilient-model-training-on-red-hat-openshift-ai-with-kubeflow-trainer/</guid><description>Resilient model training on Red Hat OpenShift AI with Kubeflow Trainer Problem: Training failures are expensive Real-world cost impact Challenges in shared cluster environments Periodic checkpointing and its limitations How periodic checkpointing works Critical limitations of periodic checkpointing The need for a better solution The solution: Just-in-time (JIT) checkpointing How JIT checkpointing works How JIT checkpointing overcomes periodic checkpointing limitations Windows of vulnerability Training interruption Unpredictable preemption Storage overhead Cost savings Use cases and common scenarios Kueue preemption protection Planned maintenance windows Resource rebalancing GPU-as-a-service Combining JIT with periodic checkpointing Red Hat OpenShift AI integration with Kubeflow Trainer v2 Coming soon: Full JIT checkpointing support Red Hat OpenShift AI (Self-Managed) | Product Trial About the author Esa Fazal More like this Looking ahead to 2026: Red Hat’s view across the hybrid cloud Red Hat to acquire Chatterbox Labs: Frequently Asked Questions Technically Speaking | Platform engineering for AI agents Technically Speaking | Driving healthcare discoveries with AI Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Imagine that after 60 hours of training, a large language model (LLM) on an 8x NVIDIA H100 GPU cluster costing $55 an hour, your job fails at 90% completion. You must restart from your last checkpoint, which was saved 3 hours ago, wasting $165 in compute costs, and delaying model deployment. This kind of scenario isn&amp;rsquo;t hypothetical, it&amp;rsquo;s a daily reality for organizations running distributed AI training workloads in production environments. LLM training represents one of the most compute-intensive workloads in modern AI infrastructure. With GPU clusters costing thousands of dollars and training jobs running for days or weeks, any interruption can result in catastrophic financial losses and project delays. This article explores the challenges of distributed model training, examines the limitations of existing periodic checkpointing approaches, and introduces just-in-time (JIT) checkpointing, a new capability coming to Red Hat OpenShift AI 3.2 that protects your training investments while enabling new operational patterns like GPU-as-a-service and sustainable AI training practices. The financial and operational impact of training failures extends far beyond individual job restarts. Industry-facing studies show that a substantial fraction of GPU compute is wasted. For example, one study found many training jobs operating at less than 50% GPU utilization. Others show that interruptions and slowdowns due to failures or stragglers can delay job completion by roughly 2 times or more than planned. Taken together, findings across large-scale machine learning (ML) clusters suggest that 30% or more of GPU spending may be lost to idle time, interruptions, and inefficiencies. Consider a financial services organization training a fraud detection model on an 8 GPU cluster: Training duration : 72 hours planned GPU cost : $55/hour for 8 GPU cluster (AWS p5.</description></item><item><title>RPM and DNF features and enhancements in Red Hat Enterprise Linux 10.1</title><link>https://kubermates.org/docs/2025-12-18-rpm-and-dnf-features-and-enhancements-in-red-hat-enterprise-linux-10-1/</link><pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-18-rpm-and-dnf-features-and-enhancements-in-red-hat-enterprise-linux-10-1/</guid><description>RPM and DNF features and enhancements in Red Hat Enterprise Linux 10.1 RPM signature improvements Modularity and DNF Better software management Red Hat Enterprise Linux | Product trial About the author Samantha Bueno More like this More than meets the eye: Behind the scenes of Red Hat Enterprise Linux 10 (Part 4) Looking ahead to 2026: Red Hat’s view across the hybrid cloud At Your Serverless | Command Line Heroes The Overlooked Operating System | Compiler: Stack/Unstuck Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Red Hat Enterprise Linux 10.1 (RHEL) features some significant updates to RPM and DNF, two technologies designed to help you manage software installs and updates. The RPM package manager (RPM) creates installation files used to install and uninstall an application, and that can be queried for information about what libraries and binaries the application contains. The dnf command is the tool used on RHEL to search for available applications, and then to install, update, or uninstall them. These are important components of a computer system, so we&amp;rsquo;ve worked hard to improve them. dnf As we prepare for the next generation of security threats and adapt to the new and evolving post-quantum computing world, we&amp;rsquo;ve made a number of enhancements to RPM&amp;rsquo;s signature capabilities. RPM signatures are a security feature used with RPM packages to verify the package&amp;rsquo;s authenticity and integrity, ensuring it came from a trusted source and hasn&amp;rsquo;t been tampered with since it was signed. These changes include improvements to support differing formats and algorithms, and adding options that give customers greater control over managing signatures. It offers select signature algorithms of your choice like ML-DSA, which can be used for post-quantum signing. The introduction of RPMv6 signatures enables multiple signatures per package and adds support for the new, stronger OpenPGP v6 standard. OpenPGP v6 is the latest version of the OpenPGP cryptographic standard, finalized as RFC 9580, which updates the standard with modern cryptographic practices. Customers will also have the freedom to select signature algorithms of their choice. These new features ultimately enable us to ship packages with a set of signatures utilizing different algorithms currently thought to be post-quantum safe.</description></item><item><title>VCF Breakroom Chats Episode 76: Cloud Admin Fast Track – Learn Infrastructure as Code the Easy Way with VCF 9.0</title><link>https://kubermates.org/docs/2025-12-17-vcf-breakroom-chats-episode-76-cloud-admin-fast-track-learn-infrastructure-as-co/</link><pubDate>Wed, 17 Dec 2025 22:13:13 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-17-vcf-breakroom-chats-episode-76-cloud-admin-fast-track-learn-infrastructure-as-co/</guid><description>About the VCF Breakroom Chat Series Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VCF Breakroom Chats Episode 76: Cloud Admin Fast Track – Learn Infrastructure as Code the Easy Way with VCF 9.0 VCF Breakroom Chats Episode 75 - Breaking the GitOps Barrier: Continuous Delivery for Modern Apps with VCF 9 VMware Cloud Foundation Automation – Consume and Deploy Virtual Machines and Kubernetes Clusters Ready to make Infrastructure as Code feel easy? In this episode of VCF Breakroom Chats , Alina Thylander chats with Scott McDermott about how VMware Cloud Foundation (VCF) 9.0 helps VI Admins step into the role of Cloud Admins with confidence. From guided setup and a modern cloud interface to low‑code blueprints and YAML templates, you’ll see how automation makes private cloud operations simpler, safer, and more approachable. If you’re looking to fast-track your cloud skills, this is the episode you won’t want to miss. Want to learn more about VCF Automation? Check out the VCF Automation web page for more resources. Read this blog post: Private Cloud Redefined: Deliver a Unified Cloud Consumption Experience with VMware Cloud Foundation 9.0 Explore VMware Cloud Foundation Automation by the Numbers with the Forrester Total Economic Impact (TEI) Study Download the Self-Service Private Cloud for Dummies Guide ​In this series, we share vlogs with industry-recognized experts from Broadcom and Broadcom partners and customers. These vlogs are concise, like meeting in a breakroom and getting great information from a quick conversation. This series is for you if you are an IT practitioner, IT admin, cloud or platform architect, platform engineer, developer, DevOps, senior IT manager, IT executive, or AI/ML professional. If you are new to the series, please check out our previous episodes.</description></item><item><title>A More Powerful, Code-First Knowledge Base Experience on the DigitalOcean Gradient™ AI Platform</title><link>https://kubermates.org/docs/2025-12-17-a-more-powerful-code-first-knowledge-base-experience-on-the-digitalocean-gradien/</link><pubDate>Wed, 17 Dec 2025 19:50:21 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-17-a-more-powerful-code-first-knowledge-base-experience-on-the-digitalocean-gradien/</guid><description>A More Powerful, Code-First Knowledge Base Experience on the DigitalOcean Gradient™ AI Platform Flexible, production-ready toolkit What’s new in the public preview Get started with the improved Knowledge Base experience today About the author Try DigitalOcean for free Related Articles Introducing DigitalOcean Gradient™ AI Agent Development Kit: A code-first way to build production-ready AI agents Now Available: Remote MCP for DigitalOcean Services DigitalOcean MCP Server is now available By Grace Morgan Updated: December 18, 2025 2 min read Building production-ready retrieval-augmented generation (RAG) systems can be complex, time-consuming, and often requires months of engineering effort. Developers and enterprises struggle to ingest diverse data sources, structure content for semantic search, and maintain accurate, verifiable answers. Enhancements to DigitalOcean Gradient™ AI Knowledge Bases , now in public preview , are designed to solve this problem. Its code-first feature lets developers create, manage, and query knowledge bases entirely from code, giving full control over ingestion, chunking, embedding, and retrieval without having to worry about the underlying infrastructure. Many existing solutions let developers create a basic knowledge base, but they often struggle to scale, customize, or integrate it into production workflows. The improvements address this by providing a code-first, developer-focused toolkit that handles the full knowledge base lifecycle. Developers can ingest data from files, Dropbox, web crawlers, control chunking and embedding strategies, and run natural language queries that return citation-backed answers with metadata filters. With well-documented APIs and SDKs, these integrations are seamless, letting developers manage everything entirely in code. The public preview highlights the essential tools developers need to build and manage knowledge bases effectively: Direct API Access: Query knowledge bases directly without needing an agent, giving full control for integration into apps or RAG pipelines. Customizable ingestion: Ingest content from supported sources such as files, web crawlers, and Dropbox datasets. Supports structured data, sitemap crawling, and accurate parsing of complex PDFs. Flexible chunking and embedding: Choose the chunking strategy that fits your content and select from high-performance embedding models (including a multi-lingual embedding model).</description></item><item><title>Kubernetes v1.35: Timbernetes (The World Tree Release)</title><link>https://kubermates.org/docs/2025-12-17-kubernetes-v1-35-timbernetes-the-world-tree-release/</link><pubDate>Wed, 17 Dec 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-12-17-kubernetes-v1-35-timbernetes-the-world-tree-release/</guid><description>Kubernetes v1.35: Timbernetes (The World Tree Release) Release theme and logo Spotlight on key updates Stable: In-place update of Pod resources Beta: Pod certificates for workload identity and security Alpha: Node declared features before scheduling Features graduating to Stable PreferSameNode traffic distribution Job API managed-by mechanism Reliable Pod update tracking with. metadata. generation Configurable NUMA node limit for topology manager New features in Beta Expose node topology labels via Downward API Native support for storage version migration Mutable Volume attach limits Opportunistic batching maxUnavailable for StatefulSets Configurable credential plugin policy in kuberc KYAML Configurable tolerance for HorizontalPodAutoscalers Support for user namespaces in Pods VolumeSource: OCI artifact and/or image Enforced kubelet credential verification for cached images Fine-grained Container restart rules CSI driver opt-in for service account tokens via secrets field Deployment status: count of terminating replicas New features in Alpha Gang scheduling support in Kubernetes Constrained impersonation Flagz for Kubernetes components Statusz for Kubernetes components CCM: watch-based route controller reconciliation using informers Extended toleration operators for threshold-based placement Mutable container resources when Job is suspended Other notable changes Continued innovation in Dynamic Resource Allocation (DRA) Comparable resource version semantics Graduations, deprecations, and removals in v1.35 Graduations to stable Deprecations, removals and community updates Release notes Availability Release team Project velocity Events update Upcoming release webinar Get involved Editors : Aakanksha Bhende, Arujjwal Negi, Chad M. Crowell, Graziano Casto, Swathi Rao Similar to previous releases, the release of Kubernetes v1.35 introduces new stable, beta, and alpha features. The consistent delivery of high-quality releases underscores the strength of our development cycle and the vibrant support from our community. This release consists of 60 enhancements, including 17 stable, 19 beta, and 22 alpha features. There are also some deprecations and removals in this release; make sure to read about those. 2025 began in the shimmer of Octarine: The Color of Magic (v1.33) and rode the gusts Of Wind &amp;amp; Will (v1.34). We close the year with our hands on the World Tree, inspired by Yggdrasil, the tree of life that binds many realms. Like any great tree, Kubernetes grows ring by ring and release by release, shaped by the care of a global community. At its center sits the Kubernetes wheel wrapped around the Earth, grounded by the resilient maintainers, contributors and users who keep showing up. Between day jobs, life changes, and steady open-source stewardship, they prune old APIs, graft new features and keep one of the world’s largest open source projects healthy.</description></item><item><title>Introducing DigitalOcean Gradient™ AI Agent Development Kit: A code-first way to build production-ready AI agents</title><link>https://kubermates.org/docs/2025-12-17-introducing-digitalocean-gradient-ai-agent-development-kit-a-code-first-way-to-b/</link><pubDate>Wed, 17 Dec 2025 17:32:25 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-17-introducing-digitalocean-gradient-ai-agent-development-kit-a-code-first-way-to-b/</guid><description>Introducing DigitalOcean Gradient™ AI Agent Development Kit: A code-first way to build production-ready AI agents Why we built the ADK What’s included in the public preview Try the Agent Development Kit today About the author Try DigitalOcean for free Related Articles A More Powerful, Code-First Knowledge Base Experience on the DigitalOcean Gradient™ AI Platform Now Available: Remote MCP for DigitalOcean Services DigitalOcean MCP Server is now available By Grace Morgan Updated: December 17, 2025 3 min read Developers everywhere face a common challenge: it’s getting easier and easier to prototype an AI agent, but harder to turn that prototype into something reliable, testable, and ready for production. Orchestrating LLM interactions, managing state, wiring up function calling, integrating multiple tools, evaluating performance, tracing failures, and deploying to production often require complex custom code and scattered tooling. The DigitalOcean Gradient™ AI Agent Development Kit (ADK) , now available in public preview , is designed to solve exactly that problem. It’s a powerful, code-first SDK that lets you build, test, and deploy sophisticated multi-step agent workflows from within their existing development environments, with built-in support for evaluations, traceability, and knowledge bases. To see the ADK in action and for help getting started, check out our tutorial. Most AI frameworks solve the “day-one” problem: how to create a working agent. But they fall short on the “day-two” problem: how to operate that agent in production. We built the ADK to give you a standardized, production-grade framework that handles the full lifecycle: Orchestration: Build multi-step workflows without boilerplate State management: Keep track of what your agent is doing across steps Tool integration: Register custom functions/APIs as first-class tools Knowledge base support: Connect agents to your existing DO Knowledge Bases Evaluations: Measure correctness, security, tone, and retrieval quality Tracing: Understand how your agent behaves at every span and step Deployment: Ship your entire agent system, from logic to KBs to tools, with one command The result is a code-first experience that lets you move from local development → observability → evaluation → deployment in a single, consistent workflow. Our goal is to provide a strong foundation for developers to build, test, and deploy AI agent workflows directly from their development environment. In the private preview, you could leverage the core SDK framework with CRUD APIs, connect models, organize projects in workspaces, use the entry point decorator and CLI, and take advantage of MCP support on the Agent Platform. Building on this foundation, the public preview introduces a set of powerful new features to help you run production-level workflows more efficiently and with deeper visibility: Traces &amp;amp; Insights for Any Workflow : Add full tracing to your agent logic using custom decorators, even without LangGraph. Spans, steps, and KB interactions appear in the Gradient AI Platform UI, helping you understand your agent’s behavior at every step.</description></item><item><title>CNCF Welcomes 12 New Silver Members Emphasizing a Growing Need for Observability and Automation</title><link>https://kubermates.org/docs/2025-12-17-cncf-welcomes-12-new-silver-members-emphasizing-a-growing-need-for-observability/</link><pubDate>Wed, 17 Dec 2025 17:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-17-cncf-welcomes-12-new-silver-members-emphasizing-a-growing-need-for-observability/</guid><description>New Silver Members About the Newest End User Members About Cloud Native Computing Foundation Media Contact New global members join CNCF to strengthen community contributions in observability, cloud infrastructure, and cloud native AI SAN FRANCISCO, CA – December 17, 2025 – The Cloud Native Computing Foundation® (CNCF®) , which builds sustainable ecosystems for cloud native software, today announced the addition of 12 new Silver Members, reinforcing the continued momentum of cloud native adoption across industries and further strengthening the foundation’s global community. According to CNCF’s latest State of Cloud Native Development report, more than 15 million developers now use cloud native technologies, with backend and DevOps professionals leading adoption at 58%. As enterprises shift toward hybrid, multi-cloud, and distributed infrastructure strategies, cloud native tooling has become foundational for scaling systems, reducing operational costs, and supporting AI workloads. By joining CNCF, these companies gain access to a global ecosystem where they can shape emerging standards, collaborate on open source innovation, and meet the increasing demand for reliable, cost-efficient infrastructure. “The continued expansion of our Silver Member community underscores the growing importance of cloud native technologies in shaping the future of infrastructure,” said Jonathan Bryce, executive director of the Cloud Native Computing Foundation (CNCF). “These new members bring deep expertise in areas like observability, cloud native AI, and cloud infrastructure. They reflect a growing need to automate and secure cloud native platforms, while keeping cost and complexity under control. We’re excited to collaborate with them and advance our mission to make cloud native more accessible and sustainable for all. ” CNCF Silver Members receive various benefits valued at over $300,000 USD. These benefits include training and educational subscriptions, event sponsorship discounts, access to The Linux Foundation ’s legal resources, opportunities to join project working groups , and more. The following organizations have recently joined CNCF as Silver Members: ClickHouse is a high-performance, column-oriented SQL database management system for online analytical processing (OLAP). It is available as both an open source software and a cloud offering.</description></item><item><title>A Year of Innovation: DigitalOcean Managed Databases in 2025</title><link>https://kubermates.org/docs/2025-12-17-a-year-of-innovation-digitalocean-managed-databases-in-2025/</link><pubDate>Wed, 17 Dec 2025 16:37:44 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-17-a-year-of-innovation-digitalocean-managed-databases-in-2025/</guid><description>A Year of Innovation: DigitalOcean Managed Databases in 2025 MCP Server (August) Advanced configuration for Managed Databases UI (August) Storage autoscaling (October) PostgreSQL 18 (November) Support for Remote MCP Server (December) Improved migration tooling in the cloud console (December) What’s to come in 2026 About the author Try DigitalOcean for free By Nicole Ghalwash Published: December 17, 2025 4 min read 2025 was a big year for DigitalOcean Managed Databases, packed with meaningful enhancements designed to deliver more power, flexibility, and simplicity for developers and businesses of all sizes. With performance boosts and new database engines introduced to improve scalability, automation, and observability, this year’s releases focused on making it easier than ever to build and run reliable data-backed applications on DigitalOcean. In this recap, we’ll walk through the most impactful launches in both the first and second half of the year and how they help teams move faster while keeping complexity low. First, let’s take a look at our releases in the first half of the year (H1) : – Managed PostgreSQL support for v17 [February] – Managed MongoDB support for v8 [March] – Support for up to 20TB for Managed MySQL and 30TB for PostgreSQL [March] – Introducing DigitalOcean Managed Caching for Valkey [April] – Introducing Role-Based Access Control to DigitalOcean Managed MongoDB with Predefined Roles [May] – Database Observability, Monitoring, and Hardening Advancements [June] – Support for Kafka Schema Registry [July] The second half (H2) of 2025 saw even more big developments for DigitalOcean Managed Databases: The DigitalOcean MCP (Model Context Protocol) Server lets you manage your cloud resources using simple, natural-language commands through AI-powered tools like Cursor, Claude, or your own custom LLMs. Running locally, it streamlines tasks such as provisioning Managed Databases, making cloud operations faster, easier, and more intuitive for developers. DigitalOcean MCP Server marks a major step forward in bringing AI directly into cloud infrastructure workflows. It allows users to seamlessly integrate cloud management into AI assistants or custom-built agents while eliminating context switching, manual API calls, and constant console navigation. MCP Servers make managing cloud infrastructure with LLMs faster, more secure, and far more accessible. We introduced advanced configuration options in the Managed Databases UI, giving developers more control without needing API calls or manual setup. With this update, you can fine-tune performance settings, customize connection parameters, and tailor your database environment directly from the dashboard. It’s a faster, more intuitive way to optimize your databases for your specific workloads. Launched at our Deploy conference in London, this feature marks another major milestone for automation in database environments.</description></item><item><title>KubeVirt undergoes OSTIF security audit</title><link>https://kubermates.org/docs/2025-12-17-kubevirt-undergoes-ostif-security-audit/</link><pubDate>Wed, 17 Dec 2025 15:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-17-kubevirt-undergoes-ostif-security-audit/</guid><description>Audit Process Audit Results Resources Posted on December 17, 2025 by By Helen Woeste Operations, Communications, and Community at Open Source Technology Improvement Fund CNCF projects highlighted in this post The Open Source Technology Improvement Fund (OSTIF) is proud to share the results of a recent security audit of KubeVirt , a Kubernetes virtualization API and runtime for managing virtual machines. With the continued support of Quarkslab and the Cloud Native Computing Foundation (CNCF), KubeVirt maintains support for end-users running virtual-machine workloads that need to containerize applications. This audit took place over 37 days in early 2025. Two auditors reviewed the function and structure of KubeVirt to create a threat model that would inform the following work. The threat model, which was discussed with the project maintainers, defines threat actors, attack scenarios, and attack surfaces of the project. It also directed the next part of the audit, which consisted of automated testing and manual code review in areas scoped based on the threat model’s recommended weak areas. 15 Findings with Security Impact 1 High 7 Medium 4 Low 3 Informational CVE 2025-64324 1 High 7 Medium 4 Low 3 Informational CVE 2025-64324 Custom Threat Model Fix Recommendations The auditors pointed out that the architecture of the project prioritizes sandboxing and isolation, making it harder to escalate the exploitation of vulnerabilities. The majority of the reported findings from this audit fall under those conditions, which limits their impact and informs their severity ranking. Thank you to the individuals and groups that made this engagement possible: KubeVirt maintainers and community, especially: Andrew Burden, Fabian Deutsch Quarkslab: Sébastien Rolland, Mihail Kirov, and Pauline Sauder The Cloud Native Computing Foundation You can read the Audit Report HERE You can read KubeVirt’s Blog HERE Share.</description></item><item><title>Deploy VCF Private AI Services in Minimal VMware Cloud Foundation Environments</title><link>https://kubermates.org/docs/2025-12-17-deploy-vcf-private-ai-services-in-minimal-vmware-cloud-foundation-environments/</link><pubDate>Wed, 17 Dec 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-17-deploy-vcf-private-ai-services-in-minimal-vmware-cloud-foundation-environments/</guid><description>Deployment Workflow Overview Prerequisites Deploy VCF Private AI Services 1. Install Private AI Services on the Supervisor 2. Create a namespace through the vSphere Client 3. Prepare NVIDIA configmap and secret 4. Prepare trust bundles for Private AI Services 5. Prepare Private AI Services configuration YAML file 6. Create a context for the namespace using VCF Consumption CLI 7. Activate Private AI Services on the namespace Use VCF Private AI Services Use Model Store Deploy Model Endpoints Deliver RAG Applications by Using VCF Private AI Services Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Deploy VCF Private AI Services in Minimal VMware Cloud Foundation Environments Using Harbor as a Proxy Cache for Cloud-Based Registries The New Paradigm: MLPerf Inference 5.1 Confirms VCF is the Future of AI/ML Performance VMware Cloud Foundation (VCF) offers a comprehensive suite of software-defined services, enabling enterprises to build reliable and efficient cloud infrastructure with consistent operations across diverse environments. The latest addition to this platform is VCF Private AI Services , a secure set of services for deploying AI applications using models and data sources. VCF Private AI Services integrates with VCF Automation to provide a simplified, cloud-like experience that allows users to deploy models into production easily, often within minutes. For instance, the workflow for deploying model endpoints is available in the VCF Automation UI, as shown below. Without VCF Automation, deploying model endpoints to a namespace would require using VCF Consumption Command Line Interface (VCF Consumption CLI) and kubectl.</description></item><item><title>Beyond modularity and other upgrades: The game-changer for your IT planning</title><link>https://kubermates.org/docs/2025-12-17-beyond-modularity-and-other-upgrades-the-game-changer-for-your-it-planning/</link><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-17-beyond-modularity-and-other-upgrades-the-game-changer-for-your-it-planning/</guid><description>Beyond modularity and other upgrades: The game-changer for your IT planning What does &amp;ldquo;removal of support for modularity&amp;rdquo; mean? Why should a 3-year-early feature announcement even matter? What is the digital roadmap? Where is Red Hat Lightspeed planning headed? Planning a RHEL future with confidence? Red Hat Enterprise Linux | Product trial About the authors Scott McCarty (fatherlinux) Rebecca Combs More like this More than meets the eye: Behind the scenes of Red Hat Enterprise Linux 10 (Part 4) Looking ahead to 2026: Red Hat’s view across the hybrid cloud OS Wars_part 1 | Command Line Heroes OS Wars_part 2: Rise of Linux | Command Line Heroes Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Scott and I talk to a lot of customers, and one theme that comes up over and over is that it’s difficult to plan for future releases of Linux. Sometimes, support drops for a feature or capability on which they rely. Other times, it’s that they can’t wait for a new feature, bug fix, and so on. Planning for Red Hat Enterprise Linux (RHEL) with its built-in digital roadmap, powered by Red Hat Lightspeed, is changing that. Product managers within RHEL now communicate directly with you about upcoming changes in the code base. This gives you more time to plan for changes, as well as clearer information about what’s changing. This simplifies upgrades and helps inform the automation your teams are building for the platform. Executing on this vision, we are eager to announce our first planned change to the next major version of RHEL: Removal of support for modularity in RHEL 11! This update excites RHEL product managers, who work tirelessly to keep feature roadmaps perfectly updated for customers, partners, and Red Hat engineers alike, but many readers may find this a curious announcement. Read on to better understand why it&amp;rsquo;s so exciting. Since the launch of RHEL in 2002, there has been an ongoing challenge to enable the multitude of developer applications, languages, and tools, as well as their differing versions function seamlessly together on a stable, enterprise-grade operating system. Initial workarounds include providing one-version-only support at each major OS release, or in 2013 introducing Red Hat Software Collections to allow for newer versioned applications to be installed in isolated directories. Given a developer&amp;rsquo;s need for modern software versions before a major OS upgrade, on top of the tediousness of hard-isolating every newly updated application, a core versioning strategy was needed for RHEL to succeed.</description></item><item><title>Enterprise automation resilience with EDB and Red Hat Ansible Automation Platform</title><link>https://kubermates.org/docs/2025-12-17-enterprise-automation-resilience-with-edb-and-red-hat-ansible-automation-platfor/</link><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-17-enterprise-automation-resilience-with-edb-and-red-hat-ansible-automation-platfor/</guid><description>Enterprise automation resilience with EDB and Red Hat Ansible Automation Platform Unlocking true resilience How does it work? The benefits This is the beginning of the evolution Want to know more? Red Hat Ansible Automation Platform | Product Trial About the author Phil Griffiths More like this Accelerating NetOps transformation with Ansible Automation Platform Slash VM provisioning time on Red Hat Openshift Virtualization using Red Hat Ansible Automation Platform Technically Speaking | Taming AI agents with observability Transforming Your Database | Code Comments Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share In today&amp;rsquo;s modern technological world, IT infrastructure must deliver uncompromising speed and reliability to meet the critical demands of the business. Centralized automated platforms, such as Red Hat Ansible Automation Platform , are essential for managing critical IT infrastructure. These platforms streamline operations and increase efficiency by standardizing and scaling automation across the entire organization, transforming complex, repetitive tasks into a governed, security-focused, and repeatable enterprise strategy. The platform’s fundamental value lies in its resilience. For mission-critical automation, a resilient foundation is not merely a best practice—it’s essential. High Availability (HA) at the database layer serves as a critical safety net, protecting automation processes from interruption. This resilient environment moves beyond basic uptime to deliver the continuous availability required for the database, supporting your entire automation strategy. With this in mind, Red Hat is collaborating with EDB to deliver a more resilient enterprise automation solution. Customers can now officially deploy production-ready EDB PostgreSQL clusters with enterprise-grade HA and failover capabilities for Ansible Automation Platform. This collaboration provides continuous Postgres availability to your automation platform, focusing on the database layer. The solution integrates core EDB Postgres AI capabilities with Ansible Automation Platform, delivering a fully tested, validated, and reliable foundation for your mission-critical automation. This approach gives you a production-ready PostgreSQL backend, making your database ready to support continuous operations.</description></item><item><title>More than meets the eye: Behind the scenes of Red Hat Enterprise Linux 10 (Part 4)</title><link>https://kubermates.org/docs/2025-12-17-more-than-meets-the-eye-behind-the-scenes-of-red-hat-enterprise-linux-10-part-4/</link><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-17-more-than-meets-the-eye-behind-the-scenes-of-red-hat-enterprise-linux-10-part-4/</guid><description>More than meets the eye: Behind the scenes of Red Hat Enterprise Linux 10 (Part 4) 2025 (6 months until Summit 2025) More like this F5 BIG-IP Virtual Edition is now validated for Red Hat OpenShift Virtualization Looking ahead to 2026: Red Hat’s view across the hybrid cloud OS Wars_part 1 | Command Line Heroes OS Wars_part 2: Rise of Linux | Command Line Heroes Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share This series takes a look at the people and planning that went into building and releasing Red Hat Enterprise Linux 10. From the earliest conceptual stages to the launch at Red Hat Summit 2025, we’ll hear firsthand accounts of how RHEL 10 came into being. Part 1 | Part 2 | Part 3 In our previous installment of the story of how Red Hat Enterprise Linux (RHEL) 10 came to be, we got insights into the testing process and how the headline features (and the stories around those features) started coming together. In part 4, those stories come into clearer focus as the team works to put the finishing touches on various features before code freeze. Brian Stinson, principal software engineer &amp;ldquo;The last stretch of time: That part is actually where things get a little bit more intense for the individual teams because it’s not only ‘Are we buttoning up features?’ but ‘Were we able to cover the rest of the baseline enablement stuff we needed to do as part of the release?’ Are all my packages in? Did we get them QE’d on the right schedule? Have they gone out for feedback? Those types of activities actually ramp up quite a bit as we head toward code freeze. &amp;quot; Chris Wells, senior director, Product Marketing - RHEL Business Unit &amp;ldquo;I knew we needed to take this story and try and make it exciting. But we weren&amp;rsquo;t going to change what features were in this release, we could only change how we could talk about those features. So I had a meeting here in Columbus where I pulled in Marty Loveless, who was the lead product marketing manager on RHEL 10, and Scott McCarty, who lives just an hour or two up the road in Akron. He drove down for the day. We locked ourselves in a conference room, and we just brainstormed ‘what were the different angles we could talk about things in?’ Trying to look at something that was new and figure out: Was there a different angle?&amp;rdquo; Major Hayden, senior principal software engineer &amp;ldquo;It was another Red Hatter and I on the engineering side building the code, so he and I divided the work. RAG [retrieval augmented generation] was our number one challenge. We kind of assumed that it was: You throw a bunch of PDFs in a bucket, boom, let’s search.</description></item><item><title>Run containerized AI models locally with RamaLama</title><link>https://kubermates.org/docs/2025-12-17-run-containerized-ai-models-locally-with-ramalama/</link><pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-17-run-containerized-ai-models-locally-with-ramalama/</guid><description>Run containerized AI models locally with RamaLama Why run AI models locally? What is RamaLama? Installing RamaLama and inspecting your environment How RamaLama selects the right image Running your first model with RamaLama Serving an OpenAI-compatible API with RamaLama Adding external data with RAG using RamaLama From local workflows to edge and Kubernetes Wrapping up The adaptable enterprise: Why AI readiness is disruption readiness About the author Cedric Clyburn More like this Looking ahead to 2026: Red Hat’s view across the hybrid cloud Resilient model training on Red Hat OpenShift AI with Kubeflow Trainer Technically Speaking | Platform engineering for AI agents Technically Speaking | Driving healthcare discoveries with AI Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The open source AI ecosystem has matured quickly, and many developers start by using tools such as Ollama or LM Studio to run large language models (LLMs) on their laptops. This works well for quickly testing out a model and prototyping, but things become complicated when you need to manage dependencies, support different accelerators, or move workloads to Kubernetes. Thankfully, just as containers solved development problems like portability and environment isolation for applications, the same applies to AI models too! RamaLama is an open source project that makes running AI models in containers simple, or in the project’s own words, “boring and predictable. ” Let’s take a look at how it works, and get started with local AI inference , model serving, and retrieval augmented generation (RAG). There are several reasons developers and organizations want local or self-hosted AI: Control for developers: You can run models directly on your own hardware instead of relying on a remote LLM API. This avoids vendor lock-in and gives you full control over how models are executed and integrated. Data privacy for organizations: Enterprises often cannot send sensitive data to external services. Running AI workloads on-premises or in a controlled environment keeps data inside your own infrastructure. Cost management at scale: When you are generating thousands or millions of tokens per day, usage-based cloud APIs can become expensive very quickly. Hosting your own models offers more predictable cost profiles. With RamaLama, you can download, run, and manage your own AI models, just as you would with any other type of workload like a database, backend, etc. RamaLama is a command-line interface (CLI) for running AI models in containers on your machine.</description></item><item><title>Introducing the Custom Date Range Billing View</title><link>https://kubermates.org/docs/2025-12-16-introducing-the-custom-date-range-billing-view/</link><pubDate>Tue, 16 Dec 2025 21:27:44 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-16-introducing-the-custom-date-range-billing-view/</guid><description>Introducing the Custom Date Range Billing View What’s included in this new feature Key use cases for growing teams Ready to get started? About the author(s) Try DigitalOcean for free Related Articles Speed Up Your JavaScript Apps: Native Bun Support is Now Available on App Platform A More Powerful, Code-First Knowledge Base Experience on the DigitalOcean Gradient™ AI Platform Introducing DigitalOcean Gradient™ AI Agent Development Kit: A code-first way to build production-ready AI agents By Nicole Ghalwash and Rebecca Davis Published: December 16, 2025 3 min read If you’ve ever stared at an unexpected cloud bill spike and sifted through invoices trying to find the cause, you know how time-consuming cost investigations can be. Until now, identifying cost anomalies often meant waiting for your monthly invoice or manually calculating month-to-date usage. That changes today. All DigitalOcean customers now have access to the new custom date range billing view. This new feature is accessible in the Billing Console by going to Billing → Insights. Additionally, you can download your report as a CSV so you can view your granular billing insights offline. This update gives you clearer, more granular visibility into your cloud costs (segmented into daily, weekly, and monthly spend by product) making budgeting and forecasting far easier. DevOps, Finance, and growing teams can now manage spend proactively instead of reacting after the fact. For all customers, this feature is available globally and fully self-service via the Cloud Console and API. Here is a close look into the features of this new billing view: Custom date range filtering: You are no longer limited to invoices for viewing your spending. Choose any start and end date to view total spend for that exact range. This makes it easy to align cloud costs with development sprints, project timelines, or product launches–and improves internal chargebacks and budgeting accuracy.</description></item><item><title>Ingress NGINX Controller Is Dead — Should You Move to Gateway API?</title><link>https://kubermates.org/docs/2025-12-16-ingress-nginx-controller-is-dead-should-you-move-to-gateway-api/</link><pubDate>Tue, 16 Dec 2025 20:20:13 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-16-ingress-nginx-controller-is-dead-should-you-move-to-gateway-api/</guid><description>Now What? Understanding the Impact of the Ingress NGINX Deprecation A Fork in the Road: Choosing Your Path Beyond Ingress NGINX Key Benefits Summary: Choosing Your Path Forward: Ingress or Gateway API? The Easy Path to Gateway API Ingress NGINX Controller, the trusty staple of countless platform engineering toolkits, is about to be put out to pasture. This news was announced by the Kubernetes community recently, and very quickly circulated throughout the cloud-native space. It’s big news for any platform team that currently uses the NGINX Controller because, as of March 26, 2026, there will be no more bug fixes, no more critical vulnerability patches and no more enhancements when Kubernetes continues to release new versions. If you’re feeling ambushed, you’re not alone. For many teams, this isn’t just an inconvenient roadmap update, its unexpected news that now puts long-term traffic management decisions front and center. You know you need to migrate yesterday but the best path forward can be a confusing labyrinth of platforms and unfamiliar tools. Questions you might ask yourself: ❓Do you find a quick drop-in Ingress replacement? ❓Does moving to Gateway API make sense and can you commit enough resources to do a full migration? ❓If you decide on Gateway API then what is the best option for a smooth transition? With Ingress NGINX on the way out, platform teams are standing at a familiar but uncomfortable moment: deciding whether to stick with what works today or invest in what will carry them forward tomorrow. Ingress API continues to serve as a solid and reliable solution for straightforward routing needs. Gateway API builds on that foundation with a more expressive, extensible, and standardized model suited for modern production environments. To understand what’s really at stake, it helps to look beyond the headlines and compare what Ingress and Gateway API actually offer in practice. The chart below shows how the two compare across key features. Ingress API (Basic): Ideal for simpler, smaller applications or teams that don’t need advanced traffic management or cross-namespace operations.</description></item><item><title>How to build a cost-effective observability platform with OpenTelemetry</title><link>https://kubermates.org/docs/2025-12-16-how-to-build-a-cost-effective-observability-platform-with-opentelemetry/</link><pubDate>Tue, 16 Dec 2025 15:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-16-how-to-build-a-cost-effective-observability-platform-with-opentelemetry/</guid><description>The Challenge Observability Architecture Overview Key Architectural Decisions Centralized Backend, Distributed Collectors OpenTelemetry as the Universal Ingestion Layer Key Configurations Patterns Key Challenges The Metric Explosion Version Alignment Small Node OOM Conclusion Posted on December 16, 2025 by By Grace Park, DevOps Engineer, STCLab SRE Team CNCF projects highlighted in this post Managing millions of concurrent connections during global events like flash sales and online voting requires resilient, scalable observability. At STCLab, we operate platforms such as NetFUNNEL and BotManager that support up to 3.5 million simultaneous users across 200 countries. In 2023, we made a pivotal decision to sunset our 20-year legacy, on-premise architecture. We completely restructured our platform, building NetFUNNEL 4. x from the ground up as a global, Kubernetes-native SaaS. Cost wasn’t the only issue. It was the consequences. We were forced to disable APM entirely in dev/staging environments and sample just 5% of production traffic. Performance regressions were only caught after hitting production. This reactive firefighting was unsustainable. We needed a solution that could monitor all environments cost-effectively without compromises. We migrated to open observability standards with a full CNCF backing: OpenTelemetry for instrumentation and the LGTM stack—Loki, Grafana, Tempo, and Mimir.</description></item><item><title>NVMe Memory Tiering Design and Sizing on VMware Cloud Foundation 9 Part 4: vSAN Compatibility and Storage Considerations</title><link>https://kubermates.org/docs/2025-12-16-nvme-memory-tiering-design-and-sizing-on-vmware-cloud-foundation-9-part-4-vsan-c/</link><pubDate>Tue, 16 Dec 2025 12:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-16-nvme-memory-tiering-design-and-sizing-on-vmware-cloud-foundation-9-part-4-vsan-c/</guid><description>Storage Considerations Discover more from VMware Cloud Foundation (VCF) Blog Related Articles NVMe Memory Tiering Design and Sizing on VMware Cloud Foundation 9 Part 4: vSAN Compatibility and Storage Considerations Using Harbor as a Proxy Cache for Cloud-Based Registries What to Look for in Network Switches for VMware vSAN We’ve covered a lot of ground in the first 3 parts of this series: PART 1: Prerequisites and Hardware Compatibility PART 2: Design for Security, Redundancy, and Scalability PART 3: Sizing for Success But there is a lot more to learn about Memory Tiering. In fact, vSAN often comes up in conversations about Memory Tiering given its similarities, but also due to compatibility inquiries, so let’s dive in. When we first started working with Memory Tiering, the similarities between Memory Tiering and vSAN OSA were quite evident. Both have a multi-tier approach where active data is on fast devices and dormant data is on less expensive devices, thereby helping reduce TCO and the need for expensive devices for dormant data. They are also both deeply integrated into vSphere and are easy to implement. But aside from similarities, there was initially some confusion about compatibility, integration, and having both features enabled at the same time. So, let me answer those questions. Yes, you can have vSAN and Memory Tiering enabled on the same clusters at the same time. The confusion that exists is more around vSAN providing storage to Memory Tiering which is definitely not supported. I’ve covered this before, but I want to reiterate that although both solutions may be using NVMe devices, it does not mean they can share resources. Memory Tiering requires its own physical or logical device strictly for memory allocation. We do not want to share this physical/logical device with anything else including vSAN or other datastores.</description></item><item><title>Using Harbor as a Proxy Cache for Cloud-Based Registries</title><link>https://kubermates.org/docs/2025-12-16-using-harbor-as-a-proxy-cache-for-cloud-based-registries/</link><pubDate>Tue, 16 Dec 2025 11:26:21 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-16-using-harbor-as-a-proxy-cache-for-cloud-based-registries/</guid><description>The Problem: Public Registry Challenges Why Use a Proxy Cache? How it works: The Benefits: Setting Up Harbor as a Proxy Cache Step 1: Deploy and Access Harbor Step 2: Configure the Proxy Target (External Registry) Step 3: Create a New Project for the Proxy Using the Proxy Cache How it Works in Practice What happens behind the scenes: Visual Walkthrough Cache Invalidation and Retention Other Supported Registries Conclusion Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Deploy VCF Private AI Services in Minimal VMware Cloud Foundation Environments NVMe Memory Tiering Design and Sizing on VMware Cloud Foundation 9 Part 4: vSAN Compatibility and Storage Considerations Using Harbor as a Proxy Cache for Cloud-Based Registries In the world of containerization, pulling images from public registries is a daily task for development teams, CI/CD pipelines, and production deployments. But what happens when your team scales? What starts as a simple docker pull nginx command can quickly become a bottleneck. docker pull nginx Picture this scenario: Your organization has 50 developers, each running builds multiple times a day. Your CI/CD system spins up fresh containers for every pipeline run. Each environment such as development, staging, and production pulls the same base images repeatedly. All of these pulls hit the public registry directly. Soon, you start experiencing: Rate Limiting Issues: Some public registries limit anonymous users to a specific number pulls per hour. With multiple teams and automated systems, you’ll hit these limits quickly, causing builds to fail with cryptic “too many requests” errors. Slow Build Times: Every pull traverses the internet, adding latency. A 500MB image that could be retrieved from your local network in seconds takes minutes from a public registry. Network Costs: Cloud providers charge for egress traffic. Repeatedly pulling multi-gigabyte images from external registries can result in significant monthly bills.</description></item><item><title>Red Hat to acquire Chatterbox Labs: Frequently Asked Questions</title><link>https://kubermates.org/docs/2025-12-16-red-hat-to-acquire-chatterbox-labs-frequently-asked-questions/</link><pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-16-red-hat-to-acquire-chatterbox-labs-frequently-asked-questions/</guid><description>Red Hat to acquire Chatterbox Labs: Frequently Asked Questions The Announcement Strategic Rationale Customer and Partner Impact About the author Red Hat More like this Looking ahead to 2026: Red Hat’s view across the hybrid cloud Resilient model training on Red Hat OpenShift AI with Kubeflow Trainer Technically Speaking | Platform engineering for AI agents Technically Speaking | Driving healthcare discoveries with AI Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share On December 16, 2025, Red Hat announced the acquisition of Chatterbox Labs , a pioneer in model-agnostic AI safety and generative AI (gen AI) guardrails. The following questions and answers provide additional context on the acquisition and what it means for Red Hat customers and partners. What is the news? Red Hat announced on December 16, 2025, that it has acquired Chatterbox Labs, a private company founded in 2011 that provides model testing and gen AI guardrails. Chatterbox Labs technology, known as the AIMI platform, delivers quantitative AI risk metrics to help organizations validate that their data and models are safe, transparent, and compliant with global regulations. Who is Chatterbox Labs? Founded in 2011 and headquartered in London with an office in New York City, Chatterbox Labs is a team of researchers and engineers focused on addressing the unintended consequences of AI. Their team brings a deep academic pedigree and more than a decade of experience in machine learning systems. Why is Red Hat acquiring Chatterbox Labs? As enterprises move AI from experimentation to production, the ability to monitor models for bias, toxicity, and vulnerabilities is critical. Guardrails and safety testing have become &amp;ldquo;table stakes&amp;rdquo; for modern MLOps and LLMOps platforms. This acquisition directly addresses the growing need for &amp;ldquo;security for AI. &amp;quot; By integrating Chatterbox Labs technology, Red Hat aims to provide a comprehensive enterprise open source AI platform that enables customers to run production workloads with confidence. How does Chatterbox Labs fit into the Red Hat AI portfolio? Chatterbox Labs complements Red Hat’s existing AI portfolio, including Red Hat AI Inference Server, Red Hat Enterprise Linux AI (RHEL AI) and Red Hat OpenShift AI. Following the recent launch of Red Hat AI 3 , which introduced capabilities for agentic AI and Model Context Protocol (MCP), Chatterbox Labs adds a critical layer of safety and transparency for these advanced workloads.</description></item><item><title>Amazon EKS introduces enhanced network policy capabilities</title><link>https://kubermates.org/docs/2025-12-15-amazon-eks-introduces-enhanced-network-policy-capabilities/</link><pubDate>Mon, 15 Dec 2025 22:25:17 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-15-amazon-eks-introduces-enhanced-network-policy-capabilities/</guid><description>Amazon EKS introduces enhanced network policy capabilities What are Admin Network Policies? Admin Policy examples What are Application Network Policies? How are Application Network Policies different from regular Network Policies? Application Network Policy example Conclusion About the authors Today, we are excited to announce the expansion of native network policy support in Amazon EKS to include both Admin Policies and Application Network Policies. With these additional policies, Cluster Administrators (e. g. platform or security teams) can set cluster-wide security rules for their clusters to enhance the overall network security for their Kubernetes workloads. In addition, Namespace Administrators (e. g. application teams) can now control pod traffic to external resources using domain names as filters. This approach replaces the need to maintain lists of specific IP addresses (which frequently change) or broad CIDR ranges (which often conflict with corporate security policies), instead enabling the creation of a trusted list of external website and services that pods are allowed to access. You can think of this as a “permitted destinations” list for your cluster’s outbound traffic. Standard Kubernetes Network Policies in a cluster allow you to implement a virtual firewall, segmenting network traffic inside a cluster. These policies let you create rules that govern both incoming (ingress) and outgoing (egress) traffic. You can restrict communication based on several parameters, including pod labels, namespaces, IP ranges (CIDR), and specific ports.</description></item><item><title>Kubernetes Security: 2025 Stable Features and 2026 preview</title><link>https://kubermates.org/docs/2025-12-15-kubernetes-security-2025-stable-features-and-2026-preview/</link><pubDate>Mon, 15 Dec 2025 21:44:01 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-15-kubernetes-security-2025-stable-features-and-2026-preview/</guid><description>2025 Kubernetes security: Stable graduates The future is now: What to expect in 2026 Conclusion Posted on December 15, 2025 by Matteo Bisi, DevSecOps Team Leader, ReeVo Cloud &amp;amp; Cyber Security CNCF projects highlighted in this post It’s time to recap the key Kubernetes security highlights from 2025 and outline features likely to graduate to stable in early 2026. From a DevSecOps perspective, 2025 brought several meaningful security improvements that directly influenced day-2 operations and production hardening efforts. With Kubernetes v1.35 scheduled for release on December 17, now is an ideal moment to review the past year’s progress and prepare for what’s ahead. 2025 marked important progress in Kubernetes security, with key features graduating to stable status between versions 1.32 and 1.35. These advancements enhance authentication, authorization, workload isolation, and overall hardening for production cloud-native environments. Here are the most impactful stable graduates from 2025: Looking at the alpha and beta features in Kubernetes 1.35 gives us a clear indication of what’s likely to graduate to stable in 2026. These are the advancements that will further solidify Kubernetes as a secure platform: User namespaces continue to progress, reaching beta and on-by-default status in recent releases—an important hardening capability for improving workload isolation. Looking ahead, advancements in secrets management such as Pod certificates for mTLS (beta in v1.35, KEP-4317) and robust image pull authorization (beta, KEP-2535) are poised to strengthen credential handling and workload identity across Kubernetes environments. Tracking the alpha-to-stable lifecycle remains a helpful practice for DevSecOps teams. Evaluating emerging features in pre-production environments can support informed adoption and help organizations stay ahead of evolving security needs. Share.</description></item><item><title>The New Paradigm: MLPerf Inference 5.1 Confirms VCF is the Future of AI/ML Performance</title><link>https://kubermates.org/docs/2025-12-15-the-new-paradigm-mlperf-inference-5-1-confirms-vcf-is-the-future-of-ai-ml-perfor/</link><pubDate>Mon, 15 Dec 2025 20:23:02 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-15-the-new-paradigm-mlperf-inference-5-1-confirms-vcf-is-the-future-of-ai-ml-perfor/</guid><description>MLPerf Inference 5.1 Performance with VCF on SuperMicro server with NVIDIA 8xB200 MLPerf Inference 5.1 Performance with VCF on Dell server with NVIDIA 8xH200 MLPerf Inference 5.1 Performance in VCF with Intel Xeon 6 Processor Conclusion Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Deploy VCF Private AI Services in Minimal VMware Cloud Foundation Environments Using Harbor as a Proxy Cache for Cloud-Based Registries The New Paradigm: MLPerf Inference 5.1 Confirms VCF is the Future of AI/ML Performance Broadcom collaborated with Dell, Intel, NVIDIA, and SuperMicro to highlight the advantages of virtualization, delivering standout MLPerf Inference v5.1 results. VMware Cloud Foundation (VCF) 9.0 achieved performance on par with bare-metal environments across key AI benchmarks—including Speech-to-Text (Whisper), Text-to-Video (Stable Diffusion XL), LLMs (Llama 3.1-405B and Llama 2-70B), Graph Neural Networks (R-GAT), and Computer Vision (RetinaNet). These results were achieved across GPU and CPU solutions with NVIDIA’s virtualized 8x H200 GPUs, passthrough/DirectPath I/O 8x B200 GPUs, and Intel’s virtualized dual-socket Xeon 6787P processors. Refer to the official MLCommons Inference 5.1 results for the raw comparison of relevant metrics. With these results, Broadcom once again demonstrates that VCF virtualized environments perform on par with bare metal, allowing customers to benefit from the increased Agility, Availability, and Flexibility that VCF provides while leveraging excellent performance. VMware Private AI is an architectural approach that balances the business gains from AI with the privacy and compliance needs of the organization. Built on top of the industry-leading private cloud platform, VMware Cloud Foundation (VCF), this approach ensures privacy and control of their data, choice of open-source and commercial AI solutions, optimum cost, performance, and compliance. Broadcom aims to democratize AI and ignite business innovation for all organizations. VMware Private AI enables enterprises to use a range of AI solutions for their environment—NVIDIA, AMD, Intel, open–source community repositories, and independent software vendors. With VMware Private AI enterprises can deploy confidently, knowing that Broadcom has built partnerships with the leading AI providers. Broadcom brings the power of its partners Dell, Intel, NVIDIA, and SuperMicro to VCF to simplify management of AI accelerated data centers and enable efficient application development and execution for demanding AI/ML workloads. We showcase three configurations in VCF: SuperMicro GPU SuperServer AS-4126GS-NBR-LCC with NVLinked 8xB200s in DirectPath I/O, Dell PowerEdge XE9680 with NVlinked 8xH200s in vGPU mode, and 1-node-2S-GNR_86C_ESXi_172VCPU-VM with INTEL(R) XEON(R) 6787P 86-core CPUs.</description></item><item><title>Automate java performance troubleshooting with AI-Powered thread dump analysis on Amazon ECS and EKS</title><link>https://kubermates.org/docs/2025-12-15-automate-java-performance-troubleshooting-with-ai-powered-thread-dump-analysis-o/</link><pubDate>Mon, 15 Dec 2025 18:57:47 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-15-automate-java-performance-troubleshooting-with-ai-powered-thread-dump-analysis-o/</guid><description>Automate java performance troubleshooting with AI-Powered thread dump analysis on Amazon ECS and EKS Overview of the solution Prerequisites Walkthrough Step 1: Deploying the base infrastructure Step 2: Setting up container environment and deploy the monitoring and analysis stack Implementation details Design principles JMX metrics with Spring Boot Automated thread dump collection via Grafana webhook AI-powered analysis with Amazon Bedrock Triggering a thread dump analysis Example Analysis Output Cleaning up Conclusion About the authors Picture this: your containerized Java application that was running smoothly yesterday is now consuming 90% CPU and barely responding to user requests. Now your customers are experiencing timeouts, and your ops team is under pressure to resolve the issue quickly. When debugging unresponsive applications or excessive CPU consumption, one of the most valuable diagnostic tools available is the thread dump. A thread dump provides a snapshot of the threads in a Java Virtual Machine (JVM) at a specific moment, revealing thread states, stack traces, and lock information. While thread dumps are essential for diagnosing complex performance issues, traditional thread dump analysis presents several challenges. Analyzing thread dumps requires deep JVM expertise. In many teams, only a handful of engineers know how to interpret them, which slows down troubleshooting. Manual analysis can also be time‑consuming, taking hours, if not days to sift through, especially with hundreds of threads. Furthermore, this reactive approach only identifies issues after degradation has already occurred, potentially impacting your customers. Generative AI can make proactive, expert-level troubleshooting faster and more straightforward for developers and operations teams. In this blog, we’ll walk through how to build an automated thread dump analysis pipeline that uses Prometheus for monitoring, Grafana for alerting, AWS Lambda for orchestration, and Amazon Bedrock for AI‑powered analysis. The solution works on both Amazon Elastic Container Servics (Amazon ECS ) and Amazon Elastic Kubernetes Service (Amazon EKS ), helping teams go from raw thread dumps to actionable insights within seconds of detecting an issue.</description></item><item><title>Leveling Up Kubernetes: Key DigitalOcean Managed Kubernetes Releases in 2025</title><link>https://kubermates.org/docs/2025-12-15-leveling-up-kubernetes-key-digitalocean-managed-kubernetes-releases-in-2025/</link><pubDate>Mon, 15 Dec 2025 18:30:03 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-15-leveling-up-kubernetes-key-digitalocean-managed-kubernetes-releases-in-2025/</guid><description>Leveling Up Kubernetes: Key DigitalOcean Managed Kubernetes Releases in 2025 Next Evolution of DigitalOcean Kubernetes [March] Q3 DigitalOcean Kubernetes Features/Releases [July] DOKS Support for DigitalOcean MCP Server [August] DOKS Managed Gateway API [September] Prioritize Node Pool Selection with Priority Expander Managed Gateway API [September] VPC NAT Gateway and Network File Storage (NFS) are now available on DOKS and NFS [November] Support for Multi-node GPU [December] What’s to come in 2026 About the author Try DigitalOcean for free Related Articles Speed Up Your JavaScript Apps: Native Bun Support is Now Available on App Platform A More Powerful, Code-First Knowledge Base Experience on the DigitalOcean Gradient™ AI Platform Introducing DigitalOcean Gradient™ AI Agent Development Kit: A code-first way to build production-ready AI agents By Nicole Ghalwash Published: December 15, 2025 5 min read 2025 was a busy and transformative year for DigitalOcean Managed Kubernetes, marked by a series of releases that make DigitalOcean Kubernetes simpler, more secure, and more scalable for developers and growing businesses. Across engine upgrades, networking and security enhancements, autoscaling improvements, and new ecosystem integrations, this year’s updates aimed to give teams more power with less operational overhead. Whether users are running production workloads, experimenting with microservices, or scaling customer-facing applications, the enhancements launched throughout 2025 make it easier than ever to deploy, manage, and optimize Kubernetes on DigitalOcean. In this recap, we’ll walk through the major releases that shaped the platform over the past year and how they help developers move faster with confidence. In March, we rolled out four major upgrades to DigitalOcean Kubernetes (DOKS) that make it easier to run larger and more efficient workloads: increased cluster capacity, VPC-native networking, eBPF-powered routing, and Managed Cilium. Here’s a look at each one: Cluster capacity has doubled from 500 to 1,000 worker nodes , allowing bigger applications to run on a single cluster without the overhead of managing multiple environments. Cluster capacity has doubled from 500 to 1,000 worker nodes , allowing bigger applications to run on a single cluster without the overhead of managing multiple environments. VPC-native Kubernetes now assigns IPs directly from your VPC, improving performance and simplifying communication with other cloud resources. Replacing kube-proxy with eBPF-based networking and routing delivers faster packet processing and lower latency, benefiting high-traffic and real-time workloads. Managed Cilium with Hubble for observability adds stronger security, modern networking, and easier troubleshooting. Together, these features significantly boost scalability, performance, and reliability while reducing operational complexity of DOKS, so developers gain clearer visibility and a simpler networking stack, while businesses benefit from lower overhead and the ability to scale applications more efficiently. In July, we introduced four powerful new features to help you build and deploy more efficient applications on DigitalOcean Kubernetes—especially AI and machine-learning workloads.</description></item><item><title>Powering the Next Leap in AI: GPU Droplets accelerated by NVIDIA HGX™ B300 are coming soon to DigitalOcean</title><link>https://kubermates.org/docs/2025-12-15-powering-the-next-leap-in-ai-gpu-droplets-accelerated-by-nvidia-hgx-b300-are-com/</link><pubDate>Mon, 15 Dec 2025 17:51:51 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-15-powering-the-next-leap-in-ai-gpu-droplets-accelerated-by-nvidia-hgx-b300-are-com/</guid><description>Powering the Next Leap in AI: GPU Droplets accelerated by NVIDIA HGX™ B300 are coming soon to DigitalOcean Why NVIDIA HGX™ B300? Benefits of GPU Droplets accelerated by NVIDIA HGX™ B300 Reach out to us to learn more About the author Connect with our sales team Related Articles Evaluate your AI agents faster and more effectively Streamline Your Workflow: Announcing Environment Support for DigitalOcean App Platform GPU Observability: Get Deeper Insights into Your Droplets and DOKS Clusters By Waverly Swinton Published: December 15, 2025 3 min read AI continues to evolve at an unprecedented pace, with new models and demanding workloads pushing the boundaries of what’s possible. From complex large language models (LLMs) to intricate scientific simulations, developers and businesses need access to the most powerful and efficient computing infrastructure. At DigitalOcean, we’re committed to providing the cutting-edge tools you need to build, deploy, and scale your AI initiatives with simplicity and affordability. That’s why we’re excited to announce that GPU Droplets accelerated by NVIDIA HGX™ B300 are coming soon to DigitalOcean, marking a significant upgrade to our GPU offerings. The NVIDIA Blackwell Ultra accelerated computing platform represents a leap forward in AI reasoning. Designed for both training and inference, the NVIDIA HGX B300 offers substantial improvements in computational power, memory bandwidth, and energy efficiency compared to previous generations. The NVIDIA Blackwell architecture at the heart of the HGX B300 is not just about raw power; it’s also about efficiency and innovation. With 1. 5X more dense Tensor Core FLOPS, enhanced attention performance, and significantly expanded memory, the HGX B300 is optimized for the most demanding AI workloads including generative AI, data analytics, and high-performance computing (HPC). Featuring 7X more AI compute than NVIDIA Hopper platforms, 2. 1TB of HBM3e memory, and high-performance networking integration with NVIDIA ConnectX-8 SuperNICs, Blackwell Ultra delivers breakthrough performance on the most complex workloads from agentic systems and reasoning, to real-time video generation. Developers will find that fine-tuning LLMs, running complex simulations, or processing massive multimedia datasets becomes more efficient with Blackwell Ultra.</description></item><item><title>Building platforms using kro for composition</title><link>https://kubermates.org/docs/2025-12-15-building-platforms-using-kro-for-composition/</link><pubDate>Mon, 15 Dec 2025 12:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-15-building-platforms-using-kro-for-composition/</guid><description>The rise of Kubernetes-native composition Where kro fits in platform design Where platforms need more than kro Looking ahead at a growing ecosystem Posted on December 15, 2025 by Abby Bangser, CNCF Ambassador CNCF projects highlighted in this post Recent industry developments, such as Amazon’s announcement of the new EKS capabilities, highlight a trend toward supporting platforms with managed GitOps, cloud resource operators, and composition tooling. In particular, the involvement of Kube Resource Orchestrator (kro) —a young, cross-cloud initiative—reflects growing ecosystem interest in simplifying Kubernetes-native resource grouping. Its inclusion in the capabilities package signals that major cloud providers recognize the value of the SIG Cloud Provider –maintained project and its potential role in future platform-engineering workflows. This is a win for platform engineers. The composition of Kubernetes resources is becoming increasingly important as declarative Infrastructure as Code (IaC) tooling expands the number of objects we manage. For example, CNCF graduated project Crossplane , and the cloud-specific alternatives, such as AWS Controller for Kubernetes (ACK), which is packaged with EKS Capabilities both can add hundreds or even thousands of new CRDs to a cluster. With composition available as a managed service, platform teams can focus on their mission to build what is unique to their business but common to their teams. They achieve this by combining composition with encapsulation of all associated processes and decoupled delivery across any target environment. The core value of kro lies in the idea of a ResourceGraphDefinition. Each definition abstracts many Kubernetes objects behind a single API. This API specifies what users may configure when requesting an instance, which resources are created per request, how those sub-resources depend on each other, and what status should be exposed back to the users and dependent resources. kro then acts as a controller that responds to these definitions by creating a new user-facing CRD and managing requests against it through an optimized resource DAG.</description></item><item><title>What to Look for in Network Switches for VMware vSAN</title><link>https://kubermates.org/docs/2025-12-12-what-to-look-for-in-network-switches-for-vmware-vsan/</link><pubDate>Fri, 12 Dec 2025 13:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-12-what-to-look-for-in-network-switches-for-vmware-vsan/</guid><description>Why Network Switches Are so Important for vSAN Recommendations for ToR Switches Used with vSAN Downlink Port Count and Speed Uplink Port Count and Speed Switch Capacity Packets Per Second Port Buffers Summary Discover more from VMware Cloud Foundation (VCF) Blog Related Articles NVMe Memory Tiering Design and Sizing on VMware Cloud Foundation 9 Part 4: vSAN Compatibility and Storage Considerations Using Harbor as a Proxy Cache for Cloud-Based Registries What to Look for in Network Switches for VMware vSAN Since the recent series of blog posts on VMware vSAN networking came out earlier this year, one of the more common questions received has been “ What should I use as a Top of Rack (ToR) network switch in my vSAN environment?” Our Broadcom Compatibility Guide (BCG) for vSAN details compatibility and requirements for the hosts that make up a vSAN cluster, but it does not address network switches. Almost any network switch will work with vSAN, but that does not mean they all meet your data center requirements. There are characteristics of modern network switches that you should consider when moving forward with your latest hardware refresh, or new cluster build. Let’s look at what warrants attention, and why these specifications are so important. vSAN is a distributed storage solution. It stores data across hosts in a cluster to ensure data resilience and availability. The hosts that make up a vSAN cluster depend on fast, reliable networking to provide consistent, low latency storage. Figure 1. vSAN’s distributed storage model and its reliance on networking. The dramatic increase in hardware capabilities found in servers over the past two decades is stunning. CPU cores have increased anywhere between 32-128 times of what they were 20 years ago. The same goes for RAM.</description></item><item><title>Friday Five — December 12, 2025</title><link>https://kubermates.org/docs/2025-12-12-friday-five-december-12-2025/</link><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-12-friday-five-december-12-2025/</guid><description>Friday Five — December 12, 2025 Demystifying llm-d and vLLM: The race to production Forbes - Open Source And Orchestration Will Define Enterprise AI Leadership In 2026 Accelerating open source development with AI CRN - 2025 Products Of The Year Key considerations for 2026 planning: Insights from IDC About the author Red Hat Corporate Communications More like this Solving the scaling challenge: 3 proven strategies for your AI infrastructure From incident responder to security steward: My journey to understanding Red Hat&amp;rsquo;s open approach to vulnerability management Technically Speaking | Platform engineering for AI agents Technically Speaking | Driving healthcare discoveries with AI Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share As organizations accelerate the journey to production for large language model (LLM) workloads, the ecosystem of open source tools is growing fast. Two powerful projects—vLLM and llm-d—have recently emerged to tackle the complexity of inference at scale. Learn more Red Hat’s CEO Matt Hicks discusses how enterprise AI leadership in 2026 will hinge on open source adoption and orchestration capabilities. This shifts the focus from model performance to control, interoperability, flexibility, and strategic integration across complex systems. Learn more Open source communities are skeptical of generative AI, and these concerns should be addressed directly. The article outlines guidelines that we&amp;rsquo;ve established for Red Hat engineers rooted in open source principles and offers context for today’s emerging AI tools. Learn more CRN identified top partner-friendly products launched or updated over the past year and consulted solution providers to crown the winners. Red Hat was selected in two categories: Ansible Automation Platform for Enterprise IT Management and Red Hat OpenShift for Hybrid Cloud Infrastructure. Learn more Based on insights from an IDC study, Red Hat advises IT leaders to focus on unified hybrid cloud platforms like OS, development, and automation to support AI adoption, enhance agility, and guide strategic planning for the year ahead. Learn more Red Hat is the world’s leading provider of enterprise open source software solutions, using a community-powered approach to deliver reliable and high-performing Linux, hybrid cloud, container, and Kubernetes technologies. Red Hat helps customers integrate new and existing IT applications, develop cloud-native applications, standardize on our industry-leading operating system, and automate, secure, and manage complex environments. Award-winning support, training, and consulting services make Red Hat a trusted adviser to the Fortune 500.</description></item><item><title>VCF Breakroom Chats Episode 78: Build, Deploy, and Scale with vSphere Kubernetes Service 3.5</title><link>https://kubermates.org/docs/2025-12-11-vcf-breakroom-chats-episode-78-build-deploy-and-scale-with-vsphere-kubernetes-se/</link><pubDate>Thu, 11 Dec 2025 23:31:04 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-11-vcf-breakroom-chats-episode-78-build-deploy-and-scale-with-vsphere-kubernetes-se/</guid><description>Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VCF Breakroom Chats Episode 78: Build, Deploy, and Scale with vSphere Kubernetes Service 3.5 Integrating VMware Data Services Manager with Harbor for a Production-Ready Registry Making Harbor Production-Ready: Essential Considerations for Deployment Welcome back to another episode of VCF Breakroom Chats! Today, Audrey Bian is joined by Manjunath Sanjeev , Technical Product Manager at Broadcom, to unpack the new release of vSphere Kubernetes Service 3.5. Together, they discuss what’s new, what’s exciting, and how these enhancements directly address customers’ toughest challenges. Learn more about running and managing modern applications with vSphere Kubernetes Service: Read our recent blog about the vSphere Kubernetes Service 3.5 release Discover IDC’s latest insights on running VMs and containers on a single platform See the infographic from S&amp;amp;P Global Market Intelligence: The promise of platform engineering Check out the vSphere Kubernetes Service product page About the VCF Breakroom Chat Series ​ In this series, we share vlogs with industry-recognized experts from Broadcom and Broadcom partners and customers. These vlogs are concise, like meeting in a breakroom and getting great information from a quick conversation. This series is for you if you are an IT practitioner, IT admin, cloud or platform architect, platform engineer, developer, DevOps, senior IT manager, IT executive, or AI/ML professional. If you are new to the series, please check out our previous episodes.</description></item><item><title>An In-Depth Look at Istio Ambient Mode with Calico</title><link>https://kubermates.org/docs/2025-12-11-an-in-depth-look-at-istio-ambient-mode-with-calico/</link><pubDate>Thu, 11 Dec 2025 19:35:56 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-11-an-in-depth-look-at-istio-ambient-mode-with-calico/</guid><description>The Next Step Toward a Unified Kubernetes Platform: Istio Ambient Mode The Tigera Unified Strategy: Addressing Fragmentation What is a Service Mesh What is Istio Ambient Mode? Why Istio Ambient Mode? Solving the Sidecar Problem Istio Ambient Mode in Detail mTLS Everywhere by Default How Calico + Istio Ambient Mode Leverage mTLS Seamless use of Istio and Calico Policies Together Traffic Control Using Istio Ambient Mode Istio’s Waypoint Proxy: Enabling L7 Control Routing and Traffic Shaping Resilience and Reliability Features Safe Production Deployment Controls Identity-Aware Authorization Application (L7) Observability A Unified, Operator-Managed Architecture How to Get Started with Istio Ambient Mode 1. Enable Istio Ambient Mode with the Tigera Operator 2. Add Workloads to the Ambient Mesh 3. mTLS Authentication and Encryption 4. Combine Calico and Istio Network Policies Seamlessly A Simpler, More Secure, More Scalable Platform Organizations are struggling with rising operational complexity, fragmented tools, and inconsistent security enforcement as Kubernetes becomes the foundation for modern application platforms. As a result of this complexity and fragmentation, platform teams are increasingly burdened by the need to stitch together separate solutions for networking, network security, and observability. This fragmentation also creates higher operating costs, security gaps, inefficient troubleshooting, and an elevated risk of outages in mission-critical environments. The challenge is even greater for companies running multiple Kubernetes distributions, as relying on each platform’s unique and often incompatible networking stack can lead to significant vendor lock-in and operational overhead. Tigera’s unified platform strategy is designed to address these challenges by providing a single solution that brings together all the essential Kubernetes networking and security capabilities enterprises need, that includes Istio Ambient Mode, delivered consistently across every Kubernetes distribution. Istio Ambient Mode brings sidecarless service-mesh functionality that includes authentication, authorization, encryption, L4/L7 traffic controls, and deep application-level (L7) observability directly into the unified Calico platform. By including Istio Ambient Mode with Calico and making it easy to install and manage with the Tigera Operator and including enterprise support, Tigera is giving customers a simpler, more scalable, and more secure way to achieve secure networking across their Kubernetes environments. The result is reduced operational strain, lower costs, and a single consistent platform for networking, network security, and observability across every cluster.</description></item><item><title>Platform Engineering’s New Superpower – Capturing Specialized Knowledge with Anthropic SKILLs</title><link>https://kubermates.org/docs/2025-12-11-platform-engineering-s-new-superpower-capturing-specialized-knowledge-with-anthr/</link><pubDate>Thu, 11 Dec 2025 16:47:42 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-11-platform-engineering-s-new-superpower-capturing-specialized-knowledge-with-anthr/</guid><description>Platform Engineering’s New Superpower – Capturing Specialized Knowledge with Anthropic SKILLs The Power of Anthropic SKILLs – Codifying Expertise Real World Test – The MongoDB Firefight The Process – From Troubleshooting to Reusable SKILL The Unexpected Policy Guardrail The Takeaway – The Missing Link in Platform Engineering Here’s the reality for any platform team – No single person can be an expert in all of it. This constant demand for deep, specialized knowledge across dozens of systems creates inevitable bottlenecks and forces firefighting. This specialization gap is the raison d’être for our field, and there needs to be a way to bridge it. Recently, we found a potential answer in Anthropic SKILLs , and a production MongoDB alert proved its transformative value. Anthropic recently released the concept of a SKILL. Simply put, a SKILL is a way to give an LLM reliable, specialized capabilities that go beyond its general training data. You’re essentially providing the AI with a custom, specialized tool and clear instructions on how and when to use it. It’s the mechanism to transform a general purpose LLM into a highly effective specialist. At Nirmata, we recognized this potential, and immediately integrated support for it in our Nirmata AI platform engineering assistant. Our assistant came pre-loaded with native SKILLs focused on our domain, such as Policy conversion ( OPA to Kyverno , between Kyverno versions), Kyverno policy generation and Chainsaw tests. But the real game-changer is the ability for the agent to discover and learn new SKILLs. The opportunity to test this capability came quickly.</description></item><item><title>Lima v2.0: New features for secure AI workflows</title><link>https://kubermates.org/docs/2025-12-11-lima-v2-0-new-features-for-secure-ai-workflows/</link><pubDate>Thu, 11 Dec 2025 12:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-11-lima-v2-0-new-features-for-secure-ai-workflows/</guid><description>What is Lima ? Updates in v2.0 GPU acceleration Model Context Protocol Other improvements Expanding the focus to hardening AI AI inside Lima AI outside Lima Getting started: AI inside Lima Posted on December 11, 2025 by Akihiro Suda, Lima Project Maintainer CNCF projects highlighted in this post On November 6th, the Lima project team shipped the second major release of Lima. In this release, the team are expanding the project focus to cover AI as well as containers. Lima (Linux Machines) is a command line tool to launch a local Linux virtual machine, with the primary focus on running containers on a laptop. The project began in May 2021, with the aim of promoting containerd including nerdctl (contaiNERD CTL) to Mac users. The project joined the CNCF in September 2022 as a Sandbox project, and was promoted to the Incubating level in October 2025. Through the growth of the project, the scope has expanded to support non-container workloads and non-macOS hosts too. See also: “ Lima becomes a CNCF incubating project ”. Lima now provides the plugin infrastructure that allows third-parties to implement new features without modifying Lima itself: VM driver plugins : for additional hypervisors. CLI plugins : for additional subcommands of the &lt;code&gt;limactl&lt;/code&gt; command. URL scheme plugins : for additional URL schemes to be passed as &lt;code&gt;limactl create SCHEME:SPEC&lt;/code&gt;. The plugin interfaces are still experimental and subject to change. The interfaces will be stabilized in future releases.</description></item><item><title>From incident responder to security steward: My journey to understanding Red Hat's open approach to vulnerability management</title><link>https://kubermates.org/docs/2025-12-11-from-incident-responder-to-security-steward-my-journey-to-understanding-red-hat-/</link><pubDate>Thu, 11 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-11-from-incident-responder-to-security-steward-my-journey-to-understanding-red-hat-/</guid><description>From incident responder to security steward: My journey to understanding Red Hat&amp;rsquo;s open approach to vulnerability management 5 ways Red Hat&amp;rsquo;s vulnerability management is different 1. Risk-based prioritization, not just CVSS scores 2. Intelligent fix deferral 3. Combating false positives with scanner certification 4. Transparency and modern data exchange (CSAF VEX) 5. Container Health Index (CHI) Looking to the future: Red Hat&amp;rsquo;s commitment to security and AI Wrapping up Learn more Red Hat Product Security About the author Darius Williams More like this File encryption and decryption made easy with GPG Deploy Confidential Computing on AWS Nitro Enclaves with Red Hat Enterprise Linux What Is Product Security? | Compiler Technically Speaking | Security for the AI supply chain Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share For years, my career in cybersecurity was defined by a sense of urgency and criticality. As a leader of incident response teams, I lived on the front lines, constantly reacting to the latest software vulnerabilities, cyberattacks, and anomalies. My days were a blur of alerts, patch deployments, and the relentless pressure to mitigate risk and restore operations. It was a challenging, high-stakes environment where every vulnerability felt like a direct threat. Now, I&amp;rsquo;ve traded the immediate firefight for a more proactive battlefield as a manager within Red Hat Product Security. This has given me a unique perspective—shifting from addressing vulnerabilities after they occur to understanding how they&amp;rsquo;re managed from the ground up. What I’ve discovered here isn&amp;rsquo;t just a process, it’s a philosophy that resonates deeply with my past experiences and offers a refreshing approach to security in the open source world.</description></item><item><title>Solving the scaling challenge: 3 proven strategies for your AI infrastructure</title><link>https://kubermates.org/docs/2025-12-11-solving-the-scaling-challenge-3-proven-strategies-for-your-ai-infrastructure/</link><pubDate>Thu, 11 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-11-solving-the-scaling-challenge-3-proven-strategies-for-your-ai-infrastructure/</guid><description>Solving the scaling challenge: 3 proven strategies for your AI infrastructure The common challenges of scaling AI 1. Optimizing GPU resources with GPU-as-a-Service Case study: Turkish Airlines Demo: Implementing GPU-as-a-Service 2. Improving control with Models-as-a-Service From Infrastructure-as-a-Service to Models-as-a-Service API-driven model access 3. Scaling inference with vLLM and llm-d The case for vLLM Introducing llm-d for distributed inference Demo: Scaling with llm-d Bringing it all together Learn more The adaptable enterprise: Why AI readiness is disruption readiness About the authors James Harmison Philip Hayes Will McGrath More like this How Red Hat OpenShift AI simplifies trust and compliance A 5-step playbook for unified automation and AI Technically Speaking | Platform engineering for AI agents Technically Speaking | Driving healthcare discoveries with AI Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Every team that starts experimenting with generative AI (gen AI) eventually runs into the same wall: scaling it. Running 1 or 2 models is simple enough. Running dozens, supporting hundreds of users, and keeping GPU costs under control, is something else entirely. Teams often find themselves juggling hardware requests, managing multiple versions of the same model, and trying to deliver performance that actually holds up in production. These are the same kinds of infrastructure and operations challenges we have seen in other workloads, but now applied to AI systems that demand far more resources and coordination. In this post, we look at 3 practical strategies that help organizations solve these scaling problems: GPU-as-a-Service to make better use of expensive GPU hardware Models-as-a-Service to give teams controlled and reliable access to shared models Scalable inference with vLLM and llm-d to achieve production-grade, run-time performance efficiently When moving from proof of concept to production, cost quickly becomes the first barrier. Running large models in production is expensive, especially when training, tuning, and inference workloads all compete for GPU capacity. Control is another challenge. IT teams must balance freedom to experiment with guardrails that strengthen security and compliance.</description></item><item><title>Unlocking Innovation on the Factory Floor: How VMware VCF Edge Empowers OT Citizen Developers</title><link>https://kubermates.org/docs/2025-12-10-unlocking-innovation-on-the-factory-floor-how-vmware-vcf-edge-empowers-ot-citize/</link><pubDate>Wed, 10 Dec 2025 20:03:31 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-10-unlocking-innovation-on-the-factory-floor-how-vmware-vcf-edge-empowers-ot-citize/</guid><description>OT Modernization Challenges VMware VCF Edge: A Unified Platform for Both IT and OT Real-World Impact: The Citizen Developer Workflow The Outcome: Actual Productivity and Faster Modernization Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Unlocking Innovation on the Factory Floor: How VMware VCF Edge Empowers OT Citizen Developers Integrating VMware Data Services Manager with Harbor for a Production-Ready Registry VCF Breakroom Chats Episode 75 - Breaking the GitOps Barrier: Continuous Delivery for Modern Apps with VCF 9 The push for application modernization isn’t just an IT department concern anymore—it’s a fundamental requirement for the modern Operational Technology (OT) environment. Factory floors, energy grids, and logistics hubs are brimming with opportunities for improvement, and the people who know these processes best are your OT experts: the factory workers, engineers, and floor supervisors. They have the knowledge and motivation to optimize their daily operations. The problem? Traditional, complex IT infrastructure has been a towering obstacle to getting their ideas off the ground. For years, operational staff have sought ways to digitize and improve their workflows, often turning to low-code development tools like Node-RED to quickly build applications. These tools are fantastic for rapid prototyping and connecting industrial systems, but they still run into challenges when it comes to deployment, security, and enterprise scale. The primary obstacles for these citizen developers include the following. Complexity of Deployment: Setting up robust, secure, and scalable environments (like Kubernetes clusters or virtual machines) is a full-time IT job, not an OT one. Lack of Unified Support: OT and IT platforms often live in silos, leading to fragmented security, inconsistent management, and painful handoffs. Security and Robustness: A quickly deployed application must run on a secure, reliable foundation that meets industrial uptime and regulatory standards. This is where VMware Cloud Foundation (VCF) Edge completely changes the game. VCF Edge is designed to bring the robustness and management simplicity of the core data center right to your remote or edge sites—whether that’s a factory floor, a retail branch, or a substation.</description></item><item><title>CNCF Unveils Schedule for KubeCon + CloudNativeCon Europe 2026</title><link>https://kubermates.org/docs/2025-12-10-cncf-unveils-schedule-for-kubecon-cloudnativecon-europe-2026/</link><pubDate>Wed, 10 Dec 2025 17:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-10-cncf-unveils-schedule-for-kubecon-cloudnativecon-europe-2026/</guid><description>Highlights at KubeCon + CloudNativeCon Europe 2026 include: AI Observability Platform Engineering Security Emerging + Advanced CNCF Hosted Co-Located Events KubeCon + CloudNativeCon Europe returns 23-26 March in Amsterdam, bringing the global community together to drive the future of cloud native computing SAN FRANCISCO, December 10, 2025 – The Cloud Native Computing Foundation ® (CNCF®), which builds sustainable ecosystems for cloud native software, today announced the schedule for KubeCon + CloudNativeCon Europe 2026 , taking place in Amsterdam from 23 to 26 March. The event will bring together adopters and technologists from leading open source and cloud native communities. CNCF’s recent State of Cloud Native Development report , in partnership with SlashData , found that cloud native adoption has reached 15.6 million developers globally, with 77% of backend developers reportedly using at least one cloud native technology. The report further highlights that the cloud native ecosystem is entering a new maturity phase where automation, observability, and resilience drive competitive advantage. “We’re pleased to bring KubeCon + CloudNativeCon back to Amsterdam,” said Jonathan Bryce, executive director of CNCF. “This event continues to be a cornerstone for the industry to share ideas and tackle infrastructure challenges together. The 2026 schedule reflects the priorities we’re hearing from across the ecosystem and community—from building more secure, resilient systems to helping organizations deploy, secure, observe, and scale AI across use cases. ” The schedule highlights key cloud native trends and technologies. Attendees can choose from 224 keynotes, lightning talks, maintainer track sessions, and CFP breakout sessions. View the full schedule. According to the Q4 2025 CNCF Technology Landscape Radar report , 41% of AI developers are now cloud native, a figure projected to increase. This track highlights the growing convergence of AI and cloud native practices and is dedicated to exploring innovative applications, essential tools, and effective techniques for implementing AI and ML successfully within cloud-native ecosystems.</description></item><item><title>Erasure Codes in VMware vSAN versus Storage Arrays</title><link>https://kubermates.org/docs/2025-12-10-erasure-codes-in-vmware-vsan-versus-storage-arrays/</link><pubDate>Wed, 10 Dec 2025 13:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-10-erasure-codes-in-vmware-vsan-versus-storage-arrays/</guid><description>The Purpose of Erasure Coding Data Storage in vSAN versus a Storage Array Comparing Erasure Codes in vSAN versus Traditional Storage Storage Array vSAN Decoupling Cluster Size and Availability Summary Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Erasure Codes in VMware vSAN versus Storage Arrays Integrating VMware Data Services Manager with Harbor for a Production-Ready Registry NVMe Memory Tiering Design and Sizing on VMware Cloud Foundation 9 Part 3: Sizing for Success Data availability is a core competency of enterprise storage systems. For decades, these systems have attempted to deliver high levels of data availability while ensuring that performance and space efficiency expectations are also met. Achieving all three at the same time is not easy. Erasure coding has played an important role in storing data in a resilient yet space-efficient way. This post will help you better understand how erasure coding is implemented in VMware vSAN, how it is different from what may be found in traditional storage arrays, and how best to interpret an erasure code’s capabilities with data availability. The primary responsibility of any storage system is to give back the bit of data that has been requested. To ensure it can do this reliably, storage systems must store that data in a resilient way. A simple form of data resilience would be through the use of multiple copies, or “mirrors” that would help maintain availability in the event of some type of discrete failure in the storage system, such as a disk in a storage array, or a host in a distributed storage system like vSAN. One of the challenges with this approach is storing full copies of data becomes very costly in terms of capacity consumption. An additional copy would double the amount of data stored, while two additional copies would triple the amount of data stored. Erasure codes are used to store data in a resilient way, but with much more space efficiency relative to traditional mirroring of data. It does not use the approach of copies.</description></item><item><title>Optimizing Kyverno CLI performance: My LFX mentorship journey</title><link>https://kubermates.org/docs/2025-12-10-optimizing-kyverno-cli-performance-my-lfx-mentorship-journey/</link><pubDate>Wed, 10 Dec 2025 12:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-10-optimizing-kyverno-cli-performance-my-lfx-mentorship-journey/</guid><description>Why Open Source? What is Kyverno? Introduction to LFX Mentorship Selection process and onboarding My mentorship work Problem statement Analysis and solution Results and impact Reflections and key learnings References Posted on December 10, 2025 by Abhishek Dhiman, Kyverno Contributor CNCF projects highlighted in this post While exploring CNCF Projects in early 2025, I stumbled upon Kyverno and its community impressed me. The mentors and community members encouraged me to become a part of LFX Mentorship and motivated me to help new people start their open source journey. What started as a curiosity quickly became a meaningful journey into open source contribution. Kyverno is a cloud-native policy engine that runs in a Kubernetes cluster. It enforces best practices for security and compliance for platforms and applications through policies as code. Kyverno operates as a dynamic admission controller to mutate, validate, or generate resources based on policy rules. The LFX mentorship is a 12-week long program that provides mentees with guidance and support to get involved in open source projects. The project maintainers provide students with the opportunity to learn while gaining real-world experience by contributing to community-driven projects. The program runs three times a year during Spring, Summer and Fall. Once I learnt about the LFX mentorship, I applied through the official process. After a few weeks of contributing, I got familiar with Kyverno’s codebase, community, and maintainers. I realised Kyverno’s project was the best fit for me.</description></item><item><title>Slash VM provisioning time on Red Hat Openshift Virtualization using Red Hat Ansible Automation Platform</title><link>https://kubermates.org/docs/2025-12-10-slash-vm-provisioning-time-on-red-hat-openshift-virtualization-using-red-hat-ans/</link><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-10-slash-vm-provisioning-time-on-red-hat-openshift-virtualization-using-red-hat-ans/</guid><description>Slash VM provisioning time on Red Hat Openshift Virtualization using Red Hat Ansible Automation Platform 3 pillars of a modern IT service platform From provisioning to patching Stage 1: One-click provisioning Stage 2: Proactive self-healing with Event-Driven Ansible Stage 3: Simplified Day 2 operations with automated patching A better experience for everyone Red Hat Ansible Automation Platform | Product Trial About the author Christopher Hammer More like this Red Hat Ansible Automation Platform: Measuring Business Impact with Dashboard and Analytics A 5-step playbook for unified automation and AI Technically Speaking | Taming AI agents with observability You Can&amp;rsquo;t Automate Buy-In | Code Comments Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Developers are accustomed to the cloud, where a virtual machine (VM) can be launched in seconds. But in many enterprises, especially in regulated industries, requesting and receiving a VM can take a staggering 60 to 90 days. This kind of delay can stifle innovation and slow down critical projects. But what if you could provide your developers with a more seamless, self-service experience that delivers a fully configured VM in under an hour, with automated lifecycle management? This isn&amp;rsquo;t a far-off dream, it&amp;rsquo;s a reality you can build with Red Hat&amp;rsquo;s integrated toolset. To transform a slow and manual process into a rapid, automated one, you need 3 key building blocks: A user-friendly portal, a powerful hypervisor, and an intelligent automation engine. The portal: Red Hat Developer Hub provides a single, streamlined user experience (based on Backstage) for requesting any resource, starting with VMs. It&amp;rsquo;s an enterprise-grade, hardened platform with the support complex organizations require. The hypervisor: Red Hat OpenShift Virtualization brings traditional VMs and modern containers together on a single Kubernetes-based platform. Built on KVM, it allows you to manage your entire estate, including legacy Windows and Linux VMs, to cloud-native applications from a unified control plane. The automation engine: Red Hat Ansible Automation Platform is the orchestration engine that ties everything together. It automates every complex, behind-the-scenes task, eliminating manual handoffs that cause delays. Provisioning a VM is just the beginning.</description></item><item><title>Now Available: Remote MCP for DigitalOcean Services</title><link>https://kubermates.org/docs/2025-12-09-now-available-remote-mcp-for-digitalocean-services/</link><pubDate>Tue, 09 Dec 2025 18:53:48 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-09-now-available-remote-mcp-for-digitalocean-services/</guid><description>Now Available: Remote MCP for DigitalOcean Services Why Remote MCP? Endpoint mapping: one MCP server per service Example Remote MCP configuration Local configuration (existing) Authentication and request model What’s next About the author(s) Try DigitalOcean for free Related Articles DigitalOcean MCP Server is now available Choosing the Right GPU Droplet for your AI/ML Workload By Bikram Gupta and Dinesh Murthy Published: December 9, 2025 5 min read Earlier this year, DigitalOcean introduced the DigitalOcean Model Context Protocol (MCP) Server , allowing developers to connect applications and AI-assistants like Cursor and Claude Desktop directly to their DigitalOcean cloud infrastructure. You could ask your AI assistant to deploy apps, check database status, or troubleshoot droplet issues – all conversationally, making your infrastructure “AI-readable”. Until now, this required running the MCP server locally using the npx binary on your computer. Remote MCP support is now available on DigitalOcean. You can now connect your AI tools to DigitalOcean services without installing any binaries locally. Remote MCP endpoints are live for 9 DigitalOcean services: Accounts, App Platform, Databases, DigitalOcean Kubernetes, Droplets, Insights, Marketplace, Networking, and Spaces. Each service runs as its own MCP server at a dedicated HTTPS endpoint (example: https://apps. mcp. digitalocean. com/mcp for App Platform). Just update your MCP client configuration to point at our hosted endpoints, include your DigitalOcean API token and you’re ready to go with immediate, authenticated access to your infrastructure. All existing DigitalOcean MCP tutorials and videos continue to work.</description></item><item><title>Building microservices the easy way with Dapr</title><link>https://kubermates.org/docs/2025-12-09-building-microservices-the-easy-way-with-dapr/</link><pubDate>Tue, 09 Dec 2025 15:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-09-building-microservices-the-easy-way-with-dapr/</guid><description>The need that sparked Dapr The journey to open source Observability built-in Latest capabilities and roadmap A perfect companion to KEDA Posted on December 9, 2025 by Dotan Horovits CNCF projects highlighted in this post Ambassador post originally published on Medium by Dotan Horovits Microservices have been with us for nearly two decades, but it’s still far from easy to develop at scale, in a polyglot environment. The Dapr project, the Distributed Application Runtime, is an open source graduated project under the Cloud Native Computing Foundation (CNCF) , which comes to address this pain. In my latest OpenObservability Talks episode I hosted Yaron Schneider , co-creator of Dapr (as well as of KEDA that will be covered in the next blog post ), and co-founder and CTO of Diagrid, a lead backer company to Dapr, to hear all about the project. Building microservices is hard. Anyone who has worked with distributed systems knows the challenges: connecting services, tracing asynchronous events, handling failures, scaling dynamically, accessing caching and messaging and more. Before Dapr, developers had to implement these patterns manually. Take the example of tracing, pulling SDKs for OpenTelemetry, wiring trace propagation through Kafka headers, and coordinating requests across multiple services. Even at Microsoft, where Yaron worked on Azure Container Apps, these issues were apparent at scale. Teams spent months and significant resources simply maintaining consistency across distributed workloads. Dapr was designed to address these inefficiencies — reducing what once took months to five minutes with a single developer. Dapr provides an application runtime for building microservices on Kubernetes. An intelligent sidecar alongside your application takes care of the messaging, pub-sub, service-to-service communications, storage, caching, secrets mgmt and more, so your application doesn’t need to be bothered with anything other than the local sidecar.</description></item><item><title>Exploring AI, observability and community at OSS Summit Korea 2025</title><link>https://kubermates.org/docs/2025-12-09-exploring-ai-observability-and-community-at-oss-summit-korea-2025/</link><pubDate>Tue, 09 Dec 2025 12:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-09-exploring-ai-observability-and-community-at-oss-summit-korea-2025/</guid><description>Key Sessions My key takeaways AI – The central theme Meeting Linus Torvalds and others leaders Linux mentorship showcase Networking and swag Seoul: City, culture, and food Tips for first timers 🌱 Stay active in local communities Final Thoughts About me Posted on December 9, 2025 by Akash Jaiswal CNCF projects highlighted in this post Attending OpenSearchCon, OpenSSF Day, and the Open Source Summit in Seoul was a truly memorable experience, especially because this was the first-ever Open Source Summit hosted in South Korea. I have previously attended two KubeCons in India including the Open Source Summit in Kubecon Hyderabad, but experiencing a Linux Foundation conference in another country offered a completely new perspective. Thanks to the Linux Foundation for sponsoring me to make this journey possible. The events were packed with insightful sessions. Here are a few of the most interesting talks I attended, focusing heavily on AI, Observability, and LLM performance. You can find the slides and recordings via the official event links and on YouTube. Keynote: Linus Torvalds (Creator of Linux &amp;amp; Git) in conversation with Dirk Hohndel (Head of the Open Source Program Office, Verizon) Link Tutorial: Building Custom MCP Servers With FastMCP and Integrating AI Agents Using Google ADK – Mehmet Hilmi Emel (Acedemand) Link Who Ate My Resources? Trace LLM Performance in Real Time With OTel – Aditya Soni (Forrester Research) &amp;amp; Seema Saharan (Autodesk) Beyond Keywords: Building an AI-Powered Search That Reads Your Mind – Prashant Agrawal (Amazon Web Services) From Telemetry to Insight: Building AI-Powered Observability Pipelines with OpenSearch – Neel Shah (Middleware) Link Standardizing the Unstandardized: Securing AI Supply Chain With Model-Spec and Kitops – Prasanth Baskar (8gears) Link Event Schedules and Resources You can find the full schedules and details here: Open Source Summit Korea Schedule OpenSSF Day Schedule OpenSearchCon Korea Schedule 2025 truly felt like the year of AI. As with much of the current tech landscape, there were countless talks and sessions centered around AI and its related technologies. Jung-Woo Ha delivered a keynote on AI that genuinely shifted how I think about the field. As the Senior Secretary to the President for AI and Future Planning in Korea, he offered a rare policymaker’s perspective on how countries are preparing for large-scale AI adoption, covering responsible AI, infrastructure readiness, and national research strategies. David Hirsch from Dynatrace talked about OSPOs, Open Source Program Offices and how enterprises govern open source at scale. Jay Lee from Microsoft shared insights into how they build and run on open source.</description></item><item><title>KubeCon + CloudNativeCon NA 2025 End User Summit Report: Engaging with AI</title><link>https://kubermates.org/docs/2025-12-09-kubecon-cloudnativecon-na-2025-end-user-summit-report-engaging-with-ai/</link><pubDate>Tue, 09 Dec 2025 11:28:31 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-09-kubecon-cloudnativecon-na-2025-end-user-summit-report-engaging-with-ai/</guid><description>Pain points Patterns Solutions Conclusion Posted on December 9, 2025 by Arun Gupta &amp;amp; Boris Kurktchiev, CNCF AI Technical Community Group Organizers KubeCon + CloudNativeCon North America 2025 hosted an exclusive end-user summit for cloud-native leaders and innovators. This gathering provided a unique opportunity to network with industry experts, exchange ideas, and learn about the latest trends in cloud native technology. This year’s theme, “Engaging with AI,” brought together technology leaders from top enterprises to explore how organizations are adopting and scaling artificial intelligence responsibly and effectively. The afternoon featured fireside chats with executives from End User member companies, followed by focused breakout discussions on vendor selection, scaling AI initiatives, and demonstrating ROI. The conversations revealed both the promise and the complexity of enterprise AI, highlighting the tension between rapid innovation and structured governance. The event underscored a shared theme: AI transformation is much more about organizational agility and culture than it is about technology. Members of the Artificial Intelligence Technical Community Group attended the event and participated in multiple conversations. This blog highlights the pain points, patterns, and solutions that emerged from the discussions. Despite enthusiasm for AI’s potential, most organizations are struggling to reconcile innovation with operational constraints. The summit identified several recurring pain points, including vendor management, governance, ROI measurement, standardization, and driving adoption. These challenges are often interdependent, slowing execution even when leadership support is strong and the budget is available. AI ecosystems evolve quickly.</description></item><item><title>Integrating VMware Data Services Manager with Harbor for a Production-Ready Registry</title><link>https://kubermates.org/docs/2025-12-09-integrating-vmware-data-services-manager-with-harbor-for-a-production-ready-regi/</link><pubDate>Tue, 09 Dec 2025 07:37:13 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-09-integrating-vmware-data-services-manager-with-harbor-for-a-production-ready-regi/</guid><description>The Solution: Leveraging VMware Data Services Manager with Harbor Prerequisites Step 1: Provisioning PostgreSQL with VMware DSM Step 2: Preparing the Harbor Helm Chart Configuration Step 3: Deploying Harbor on Kubernetes Step 4: Verification and Maintenance Conclusion Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VCF Breakroom Chats Episode 78: Build, Deploy, and Scale with vSphere Kubernetes Service 3.5 Unlocking Innovation on the Factory Floor: How VMware VCF Edge Empowers OT Citizen Developers Erasure Codes in VMware vSAN versus Storage Arrays Harbor is a widely adopted, open-source registry that secures artifacts with role-based access control, scans images for vulnerabilities, and ensures images are replicated and trusted. As a cloud-native registry, it is a critical component for any organization leveraging Kubernetes and containerization. In our previous blog , we established that while Harbor includes a built-in PostgreSQL database, this setup is generally not recommended for production use. Deploying a resilient, highly available container registry requires separating the application and database lifecycle to ensure operational excellence. Here are the key reasons why a dedicated, external database is critical for production Harbor deployments: Lack of High Availability (HA): The default internal PostgreSQL setup is typically a single instance, creating a single point of failure. A database pod failure means your entire Harbor instance becomes unavailable. Limited Scalability: An embedded database is not designed for independent scaling. Database performance bottlenecks that arise from growth can be difficult to address without disrupting Harbor itself. Complex Lifecycle Management: Managing critical database operations such as backups, point-in-time recovery, patching, and upgrades directly within an application’s Helm chart is significantly more complex and error-prone than with dedicated database solutions. To address these challenges, we need a highly available PostgreSQL cluster. This is where VMware Data Services Manager (DSM) comes in. VMware Cloud Foundation (VCF) is the private cloud platform that delivers on-premises security, resilience, and performance, providing the underlying infrastructure for modern private cloud environments, including the Kubernetes clusters where applications like Harbor are deployed.</description></item><item><title>Don’t just automate, validate: How to measure and grow your return on investment</title><link>https://kubermates.org/docs/2025-12-09-don-t-just-automate-validate-how-to-measure-and-grow-your-return-on-investment/</link><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-09-don-t-just-automate-validate-how-to-measure-and-grow-your-return-on-investment/</guid><description>Don’t just automate, validate: How to measure and grow your return on investment Automation dashboard: On-premise visibility Automation analytics: Cloud-based insights and advanced reporting What&amp;rsquo;s next: Scaling automation across the enterprise Gain data-driven insights Translating operational success into personal impact Resources Red Hat Ansible Automation Platform | Product Trial About the author Jaime Biskup More like this Slash VM provisioning time on Red Hat Openshift Virtualization using Red Hat Ansible Automation Platform A 5-step playbook for unified automation and AI Technically Speaking | Taming AI agents with observability Transforming Your Database | Code Comments Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Successfully delivering automation demands technical excellence, quality code, and reliable execution. However, scaling this success requires translating those technical wins into measurable business impact that can be communicated to leadership or integrated into business strategy. Red Hat Ansible Automation Platform offers two different capabilities that help practitioners and leaders turn operational metrics into a clear, auditable business case for expansion: Automation dashboard and automation analytics. The automation dashboard provides a comprehensive view of automation performance, enabling you to measure and demonstrate the value of your initiatives. Through the on-premise dashboard, you gain detailed insights into automation usage, job outcomes, and associated financial impact. Key metrics such as job success rates, time savings, and return on investment (ROI) support data-driven decision-making and help identify opportunities to scale automation effectively. With robust customization and reporting capabilities, you can filter, save, export, and share detailed analyses on project performance, user activity, and operational efficiency. Reports can be exported as PDF files for a quick snapshot of the dashboard showing metrics such as cost and time savings, job success and failure rates, and total hours of automation. For deeper analysis, data can also be exported as CSV files and ingested into existing business intelligence (BI) tools to enable detailed reporting. Because the dashboard is a self-contained, on-premise utility, all data remains securely within your environment. It can aggregate data from multiple Ansible Automation Platform instances and is accessible directly in the Ansible interface for system_admin and system_auditor accounts. This makes it ideal when data security is a priority, and when you want local, real-time visibility in your automation performance.</description></item><item><title>How Red Hat OpenShift AI simplifies trust and compliance</title><link>https://kubermates.org/docs/2025-12-09-how-red-hat-openshift-ai-simplifies-trust-and-compliance/</link><pubDate>Tue, 09 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-09-how-red-hat-openshift-ai-simplifies-trust-and-compliance/</guid><description>How Red Hat OpenShift AI simplifies trust and compliance Moving the platform to the data Compliance as the foundation for scalable AI Zero trust by design End-to-end security capabilities across the stack Continuous compliance and governance Protecting the AI software supply chain Hybrid cloud consistency and compliance, built with choice in mind How compliance enables AI innovation Trust is the currency of AI in the hybrid cloud Red Hat OpenShift AI (Self-Managed) | Product Trial About the author Christopher Nuland More like this Solving the scaling challenge: 3 proven strategies for your AI infrastructure A 5-step playbook for unified automation and AI Technically Speaking | Platform engineering for AI agents Technically Speaking | Driving healthcare discoveries with AI Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Artificial intelligence (AI) is reshaping every industry, but in highly regulated sectors, success isn’t measured only by accuracy but also by trust. Public agencies, healthcare providers, and financial institutions face a common challenge of delivering the benefits of AI while staying compliant with frameworks like FedRAMP, HIPAA, PCI DSS, and NIST 800-53. These standards set the rules for encryption, access control, auditing, and data handling. They also introduce operational constraints that limit where and how AI runs. Red Hat OpenShift AI helps to bridge that divide, allowing organizations to build and deploy protected AI where the data lives, across datacenters, public clouds, and edge environments. Regulatory data often can’t move freely. Privacy laws, jurisdictional boundaries, and internal risk policies typically govern how and where clinical records, payment data, and sensitive telemetry can be used. That immobility of the data gravity challenge is one of the most significant barriers to enterprise AI adoption. OpenShift AI reverses that equation. Instead of relocating data to cloud AI services, OpenShift AI brings the AI platform to the data. Since OpenShift AI runs consistently across on-premises, cloud, and edge environments, organizations can train and serve models near sensitive datasets, maintaining compliance, while using flexible compute resources as they see fit. Every platform layer reinforces this trust boundary: encryption, role-based access control (RBAC), network isolation, and continuous compliance scanning.</description></item><item><title>From User to Trusted Advisor: How Jeff Fan Powers Customer Success at DigitalOcean</title><link>https://kubermates.org/docs/2025-12-08-from-user-to-trusted-advisor-how-jeff-fan-powers-customer-success-at-digitalocea/</link><pubDate>Mon, 08 Dec 2025 16:32:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-08-from-user-to-trusted-advisor-how-jeff-fan-powers-customer-success-at-digitalocea/</guid><description>From User to Trusted Advisor: How Jeff Fan Powers Customer Success at DigitalOcean What makes DigitalOcean the right place for you? How would you describe your role at DigitalOcean? How is the pace of innovation at DO? What does customer-centricity mean to you at DO? What makes DigitalOcean feel like the right place for you? Dive into the future with DigitalOcean About the author Try DigitalOcean for free Related Articles Leading the Cloud With Curiosity : Spotlight on Pranav Nambiar, SVP, AI/ML &amp;amp; PaaS Is DigitalOcean Your Next Career Spot? A 5-Year Insider on Why It Should Be Sharks of DigitalOcean: Archana Kamath, Senior Director, IaaS By Sujatha R Technical Writer Published: December 8, 2025 3 min read Jeff Fan, Senior Solutions Architect at DigitalOcean, brings a blend of deep technical expertise and customer empathy to his role. He serves customers across Europe and Asia and helps businesses turn complex problems into simple cloud solutions with DigitalOcean products. “I first discovered DigitalOcean as a customer, and I’m still one today. What really drew me in was the simplicity of the platform. Every time I opened the dashboard, it felt like a masterclass in thoughtful design: clean, intuitive, and never overwhelming. Compared to hyperscalers, deploying what you need takes just a few clicks. That clarity and ease of use made me feel instantly aligned with the company’s philosophy. I knew this was a place where I could grow while contributing to a product I genuinely believed in. ” 🎥 Have a look at Jeff Fan’s full conversation ⬇️ “In our industry, the titles vary like pre-sales engineer, solutions engineer, solution architect. But the role is the same at its core: being a trusted technical advisor. Our customers are deeply technical, and they want to speak with someone who understands their challenges, vision, and future goals. My job is to bridge their current state with what they aspire to build.</description></item><item><title>Harbor: Enterprise-grade container registry for modern private cloud</title><link>https://kubermates.org/docs/2025-12-08-harbor-enterprise-grade-container-registry-for-modern-private-cloud/</link><pubDate>Mon, 08 Dec 2025 16:31:28 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-08-harbor-enterprise-grade-container-registry-for-modern-private-cloud/</guid><description>What is Harbor? Setting Up Harbor Prerequisites Deploying Harbor on a VM Using Harbor as an Image Registry Conclusion Posted on December 8, 2025 by By Dhruv Tyagi, Broadcom CNCF projects highlighted in this post In the evolving landscape of application deployment, containerization with Kubernetes (K8s) has become the new standard. As organizations adopt Kubernetes at scale, public image registries often bring new challenges, from rate limits and escalating costs to limited control over sensitive data. Harbor bridges this gap as an open-source, enterprise-grade container registry that brings security, performance, and sovereignty to container image management. It seamlessly integrates with existing operational paradigms, providing a robust solution for modern container image management needs. Open-sourced in 2016, Harbor joined the Cloud Native Computing Foundation (CNCF) on July 31, 2018, and graduated on June 15, 2020, as its eleventh project – a milestone that reflects both its maturity and its vibrant community. Today, Harbor remains one of the most widely adopted CNCF projects for secure image management across hybrid and private cloud environments. The active community drives continuous innovation. Each release adds new integrations, security features, and performance improvements, ensuring Harbor evolves alongside enterprise cloud-native needs. Recent highlights include: ​​v2.13: Integration with CloudNativeAI (CNAI) for AI model management, extended audit logging, and enhanced OIDC with PKCE support v2.12: Integration with ACR and ACR EE Registry v2.11: Native SBOM generation and management At its core, Harbor provides a centralized repository for managing container images. Think of it as a private, feature-rich registry for your organization, with robust security and management features built in. Key features include: Security: Image vulnerability scanning, content trust, and role-based access control. Replication: Replicate images across multiple Harbor instances for disaster recovery and content distribution.</description></item><item><title>Private Cloud Operations Made Easy</title><link>https://kubermates.org/docs/2025-12-08-private-cloud-operations-made-easy/</link><pubDate>Mon, 08 Dec 2025 15:23:44 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-08-private-cloud-operations-made-easy/</guid><description>Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Private Cloud Operations Made Easy VCF Breakroom Chats Episode 75 - Breaking the GitOps Barrier: Continuous Delivery for Modern Apps with VCF 9 VMware Cloud Foundation Automation – Consume and Deploy Virtual Machines and Kubernetes Clusters Introducing Our Latest eBook: Private Cloud Operations for Dummies Are you looking to simplify IT infrastructure operations, reduce management complexity, and gain the flexibility to address your digital business objectives? Whether your company expects to run the latest AI-enabled applications or maintain performance and uptime across existing workloads, you need deep capabilities and insights to run your infrastructure effectively. In his new For Dummies guide, Lawrence C. Miller explains how to operate VMware Cloud Foundation (VCF) , the platform that provides public cloud scale and private cloud control. This easy-to-understand resource covers private cloud operations and explains how to: Slash total cost of ownership (TCO) – Use capacity planning with chargeback/showback to optimize costs. Streamline operations – Unify fleet management with centralized licensing, certificate, and password management to improve efficiency. Boost resilience – Leverage AI and machine learning analytics for intelligent troubleshooting, leading to faster resolution and improved productivity. Accelerate modernization – Enable scalability and growth by seamlessly moving vSphere workloads where needed without the cost of redesigning applications or networks. Don’t let unexpected costs of public cloud sprawl dictate your IT initiatives. Learn how to modernize your vSphere infrastructure with full-stack visibility using VMware Cloud Foundation. Read the Private Cloud Operations For Dummies Guide.</description></item><item><title>Istio at KubeCon + CloudNativeCon North America 2025: Community highlights and project progress</title><link>https://kubermates.org/docs/2025-12-08-istio-at-kubecon-cloudnativecon-north-america-2025-community-highlights-and-proj/</link><pubDate>Mon, 08 Dec 2025 10:36:54 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-08-istio-at-kubecon-cloudnativecon-north-america-2025-community-highlights-and-proj/</guid><description>Istio Day at KubeCon + CloudNativeCon NA Istio at KubeCon + CloudNativeCon Community spaces: ContribFest, Maintainer Track and the Project Pavilion Istio maintainers eecognized at the CNCF Community Awards What we heard in Atlanta Looking ahead Posted on December 8, 2025 by Faseela K, KubeCon + CloudNativeCon NA 2025 Co-Chair and Istio Steering Committee Member CNCF projects highlighted in this post KubeCon + CloudNativeCon North America 2025 lit up Atlanta from November 10–13, bringing together one of the largest gatherings of open-source practitioners, platform engineers, and maintainers across the cloud native ecosystem. For the Istio community, the week was defined by packed rooms, long hallway conversations, and a genuine sense of shared progress across service mesh, Gateway API, security, and AI-driven platforms. Before the main conference began, the community kicked things off with Istio Day on November 10, a colocated event filled with deep technical sessions, migration stories, and future-looking discussions that set the tone for the rest of the week. Istio Day brought together practitioners, contributors, and adopters for an afternoon of learning, sharing, and open conversations about where service mesh, and Istio, are headed next. Istio Day opened with welcome remarks from the program co-chairs, setting the tone for an afternoon focused on real-world mesh evolution and the rapid growth of the Istio community. The agenda highlighted three major themes driving Istio’s future: AI-driven traffic patterns , the advancement of Ambient Mesh—including multicluster adoption , and modernizing traffic entry with Gateway API. Speakers across the ecosystem shared practical lessons on scaling, migration, reliability, and operating increasingly complex workloads with Istio. The co-chairs closed the day by recognizing the speakers, contributors, and a community continuing to push service-mesh innovation forward. Recordings of all sessions are available at the CNCF YouTube channel. Outside of Istio Day, the project was highly visible across KubeCon + CloudNativeCon Atlanta, with maintainers, end users, and contributors sharing technical deep dives, production stories, and cutting-edge research. Istio appeared not only across expo booths and breakout sessions, but also throughout several of the keynotes, where companies showcased how Istio plays a critical role in powering their platforms at scale. The week’s momentum fully met its stride when the Istio community reconvened with the Istio Project Update , where project leads shared latest releases and roadmap advances.</description></item><item><title>Accelerating open source development with AI</title><link>https://kubermates.org/docs/2025-12-08-accelerating-open-source-development-with-ai/</link><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-08-accelerating-open-source-development-with-ai/</guid><description>Accelerating open source development with AI A little historical context Principles of AI adoption in open source Innovating responsibly Being transparent Respecting the community Innovation in action at Red Hat Open source is about principled innovation The adaptable enterprise: Why AI readiness is disruption readiness About the authors Chris Wright Josh Berkus More like this Solving the scaling challenge: 3 proven strategies for your AI infrastructure How Red Hat OpenShift AI simplifies trust and compliance Technically Speaking | Platform engineering for AI agents Technically Speaking | Driving healthcare discoveries with AI Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share In many open source communities, there’s a fair amount of skepticism around the use of generative AI (gen AI) tools for contribution and development. There are valid reasons for concern. Our goal in this article, and in Red Hat&amp;rsquo;s own practice, is to address those concerns directly and not dismiss them. Our answers aren&amp;rsquo;t just advice for others—they enable our own engineers, most of whom are open source contributors as well. We&amp;rsquo;ll share with you the guidelines we&amp;rsquo;ve established for Red Hat engineers, based on our use of open source principles in practice. But first, we&amp;rsquo;d like to put the current wave of new tools into context. For the last 4 decades, we&amp;rsquo;ve been regularly implementing new and improved tools and processes for software development. You name it: Compilers, version control systems, IDEs, virtual machines (both kinds), cloud instances, agile development, containers, configuration management, and automated testing. Every set of tools was once new, and many of them triggered heated arguments about authorship, quality, and legitimacy. There was a time when both compiler flags and auto-complete in IDEs were hot-button issues. AI-based development tooling is no different. Nor should it be.</description></item><item><title>Key considerations for 2026 planning: Insights from IDC</title><link>https://kubermates.org/docs/2025-12-08-key-considerations-for-2026-planning-insights-from-idc/</link><pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-08-key-considerations-for-2026-planning-insights-from-idc/</guid><description>Key considerations for 2026 planning: Insights from IDC Your key considerations for 2026 planning The adaptable enterprise: Why AI readiness is disruption readiness About the author Ashesh Badani More like this Sovereignty emerges as the defining cloud challenge for EMEA enterprises Enhance workload security with confidential containers on Azure Red Hat OpenShift Crack the Cloud_Open | Command Line Heroes Edge computing covered and diced | Technically Speaking Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share As IT leaders and business executives, we&amp;rsquo;re looking beyond the current year to make strategic plans and prepare our organizations for what&amp;rsquo;s next. While AI is, without a doubt, the focus for everyone going into 2026, it&amp;rsquo;s equally important to ensure the underlying technology platforms support your AI adoption strategy and your investment decisions for the future. The choices we make now regarding our technology platforms will directly impact our agility, efficiency, and ability to innovate in the years to come. We&amp;rsquo;re thrilled to see a recent IDC analyst study not only validate but also offer a comprehensive look into the value of an integrated platform approach, providing a great starting point for any organization&amp;rsquo;s future-facing strategy. The IDC paper, The Business Value of Red Hat Hybrid Cloud Solutions , validates that a unified hybrid cloud platform—consisting of a scalable operating system (OS), cloud application development platform, and enterprise-grade automation platform—delivers significant, measurable results and prepares your organization to support your AI adoption strategy. These findings come directly from in-depth interviews with Red Hat customers that have standardized much of their IT operations on our hybrid cloud solutions. Customers achieved: 74% faster business change execution. 38% higher development team productivity, translating to a $54.45 million average annual gain per organization. 60% faster new application development lifecycle. These metrics underscore the importance of choosing a foundational platform that eliminates organizational and technological barriers and boosts efficiency. As one customer noted: “ Instead of managing four vendors for different parts of the stack, we work with one. ” Another emphasized that Red Hat solutions should be a critical consideration for any innovative, AI-ready strategy.</description></item><item><title>DoTs SDK Development: Automating TypeScript Client Generation</title><link>https://kubermates.org/docs/2025-12-05-dots-sdk-development-automating-typescript-client-generation/</link><pubDate>Fri, 05 Dec 2025 20:10:20 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-05-dots-sdk-development-automating-typescript-client-generation/</guid><description>DoTs SDK Development: Automating TypeScript Client Generation Why TypeScript? Automating SDK Generation with OpenAPI, GitHub Actions and Kiota Requirements: Kiota: A Tool for Client Generation Authenticating Client in Kiota Setting Up the Client Creating a DigitalOcean Droplet and Attaching a Volume: Automated Testing Mocked Tests Integration Tests Automated Documentation CI/CD Integration: Keeping the Client Up-to-Date Kiota Behavior Conclusion About the author Try DigitalOcean for free Related Articles How startups scale on DigitalOcean Kubernetes: Best Practices Part VI - Security Introducing new GitHub Actions for App Platform How SMBs and startups scale on DigitalOcean Kubernetes: Best Practices Part V - Disaster Recovery By mchittupolu Published: December 5, 2025 7 min read At DigitalOcean, our mission is to simplify cloud and AI to empower developers to focus on building great software. As part of that mission, we strive to make our tools and services accessible to developers in their preferred languages. Starting with GoDo , our Go SDK, we made it easy for developers to interact with DigitalOcean resources. Then came PyDo , our Python SDK, expanding support for even more ecosystems. Now, we’re excited to introduce DoTs. Now, our TypeScript SDK empowers developers to leverage TypeScript’s type safety and modern development features, enabling them to manage their DigitalOcean resources. In this blog, we’ll share insights into the making of DoTs and the approach we took to develop it. TypeScript has been a game-changer for modern development, especially when working on large, complex applications. By introducing static type checking, it helps catch errors during compilation, long before they can cause issues in production. This not only improves the reliability and maintainability of the codebase but also saves significant time and effort that would otherwise go into debugging post-deployment. On top of that, TypeScript enhances the developer experience with features like intelligent code completion, inline documentation, and clear error highlighting in IDEs, making the development process smoother and more efficient. Traditionally, building SDKs for new ecosystems required extensive manual effort, including writing boilerplate code in multiple programming languages.</description></item><item><title>Friday Five — December 5, 2025</title><link>https://kubermates.org/docs/2025-12-05-friday-five-december-5-2025/</link><pubDate>Fri, 05 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-05-friday-five-december-5-2025/</guid><description>Friday Five — December 5, 2025 Red Hat to deliver enhanced AI inference across AWS Technically Speaking with Chris Wright: Platform engineering for AI agents ft. Tushar Katarki Optimizing cloud spend with Red Hat OpenShift Service on AWS (ROSA) The New Stack : Red Hat&amp;rsquo;s Hummingbird project to offer Linux container images Techzine EU : Red Hat sees AI and sovereignty reshaping hybrid cloud About the author Red Hat Corporate Communications Team More like this File encryption and decryption made easy with GPG Red Hat OpenShift Service on AWS supports Capacity Reservations and Capacity Blocks for Machine Learning Technically Speaking | Platform engineering for AI agents Technically Speaking | Driving healthcare discoveries with AI Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Red Hat collaborates with AWS to power enterprise-grade gen AI on AWS with Red Hat AI and AWS AI silicon. With this collaboration, Red Hat focuses on empowering IT decision-makers with the flexibility to run high-performance, efficient AI inference at scale, regardless of the underlying hardware. Learn more As we move from chatbots to autonomous AI agents, complexity is exploding. Red Hat’s Tushar Katarki joins Chris Wright to discuss building a &amp;ldquo;Kubernetes for Agents,&amp;rdquo; the importance of the Model Context Protocol (MCP), and how to engineer platforms that get AI out of PoC purgatory and into production. Learn more Rising virtualization costs, licensing constraints, and operational complexity are driving teams to evaluate more flexible and cost-effective paths to the cloud. Red Hat OpenShift Virtualization, combined with Red Hat OpenShift Service on AWS, supports hardware overcommit in the cloud, allowing customers to run more VMs on fewer cloud resources. Learn more Red Hat will provide hardened container images to free and paying customers with new Project Hummingbird to help support “zero-CVE” software builds. The service releases to select customers in December ahead of a full rollout. Learn more Red Hat is positioning its open source platform as the foundation for companies navigating AI adoption and digital sovereignty. In a recent interview Ashesh Badani explained how the over-25-year-old open source philosophy now applies to the AI era, where choice and control matter more than ever. Learn more Red Hat is the world’s leading provider of enterprise open source solutions, using a community-powered approach to deliver high-performing Linux, cloud, container, and Kubernetes technologies.</description></item><item><title>Meet the latest Red Hat OpenShift Superheroes</title><link>https://kubermates.org/docs/2025-12-05-meet-the-latest-red-hat-openshift-superheroes/</link><pubDate>Fri, 05 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-05-meet-the-latest-red-hat-openshift-superheroes/</guid><description>Meet the latest Red Hat OpenShift Superheroes Meet the superheroes Larry Green (Neuberger Berman) Luis Duran (ABB) Marcus Dobeck (PNC) Michael Lewis (Defense Intelligence Agency) Reynaldo Linares (Verizon) Nominate today Red Hat OpenShift Container Platform | Product Trial About the author Debbie Margulies More like this Behind the scenes of RHEL 10, part 3 How Discover cut $1.4 million from its annual AWS budget in two game days The Ground Floor | Compiler: Tales From The Database Bad Bosses | Compiler: Tales From The Database Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Earlier this month at Red Hat OpenShift Commons, co-located with KubeCon + CloudNativeCon NA in Atlanta, we had the chance to recognize a few OpenShift Superheroes. While each member of the OpenShift community is a hero for helping contribute to a project’s success and growth, some members really stand out. They are the advocates and champions who make the community strong and successful. OpenShift Superheroes are made up of: Builders: Contributing to the evolution of OpenShift Advocates: Amplifying their OpenShift experience and learnings through events, blogs, meetups, and more Ambassadors: Shares knowledge across diverse teams, industries, organizations Here&amp;rsquo;s a look at who we recognized for helping to advance the community through their contributions, participation, and innovation. These nominations have been edited slightly for readability and context. The Ambassador “I’m nominating Larry Green from Neuberger Berman for the OpenShift Commons Super Hero Award. Larry has been instrumental in championing platform adoption across NB, implementing and managing a Kafka cluster on OpenShift that supports diverse enterprise workflows. His architectural leadership unified multiple vertical teams—spanning data, DevOps, and application domains—under a common OpenShift platform. Through collaboration, mentorship, and a strong technical vision, Larry has helped establish OpenShift as the standard across NB’s modernization efforts. ” The Advocate “Luis has been our OpenShift evangelist within ABB and in the process automation industry since the beginning of our partnership. We would like to thank him for his work and trust in Red Hat! Luis has been advocating Red Hat&amp;rsquo;s technology stack (in particular OpenShift) within ABB, allowing Red Hat to navigate the complexity of ABB and enabling cross collaboration among different teams in ABB and Red Hat. His business acumen and understanding of process automation has been a real asset for Red Hat to make important business cases to further develop the product.</description></item><item><title>My first KubeCon + CloudNativeCon: A journey through community, inclusivity, and neurodiversity</title><link>https://kubermates.org/docs/2025-12-04-my-first-kubecon-cloudnativecon-a-journey-through-community-inclusivity-and-neur/</link><pubDate>Thu, 04 Dec 2025 23:14:44 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-04-my-first-kubecon-cloudnativecon-a-journey-through-community-inclusivity-and-neur/</guid><description>Finding my place in the community Celebrating the OpenTelemetry community Embracing Merge Forward Introducing Merge Forward to the industry Tips for neurodiverse folks attending their first (or next) KubeCon + CloudNativeCon Connecting, collaborating, and welcoming new voices A neurodivergent perspective Looking forward: An invitation to join us Closing thoughts Resources Posted on December 4, 2025 by Diana Todea, Developer Experience Engineer at VictoriaMetrics, co-lead Neurodiversity WG, CNCF Merge-Forward CNCF projects highlighted in this post Stepping into my first KubeCon + CloudNativeCon, held this year in November in Atlanta, Georgia, felt like entering a world I had been following from afar for years; a world full of brilliant minds, bold ideas, and a shared dedication to building a more open and inclusive cloud-native ecosystem. What made this experience even more meaningful was navigating it as a neurodivergent person and discovering spaces where people like me not only belong but are celebrated. KubeCon + CloudNativeCon is often talked about as a conference but in reality, it’s a community gathering. Since Day 1 the OpenTelemetry Observatory was filled with conversations of contributors and maintainers to which I participated, and it felt like a safe space for me. One of the most unforgettable moments of the week was receiving the 2025 OpenTelemetry Community Award. This recognition wasn’t just about my personal contributions, it was a reflection of the support, encouragement and generosity of a community that thrives on collaboration. The award reminded me of why open source matters: because it’s built by people who care. A particularly powerful part of my KubeCon journey was engaging with the Merge Forward initiative. Merge Forward is a CNCF Technical Community Group made up of several subgroups, including Deaf &amp;amp; Hard of Hearing, Blind &amp;amp; Visually Impaired, Stuttering &amp;amp; Speech Diversity, Women in Cloud Native, Deep Roots, Friends of Dorothy and Neurodiversity. Its mission is to build supportive networks and safe spaces where underrepresented communities and their allies can come together to learn, mentor, grow, and simply be themselves. Merge Forward provides a safe place for people to share experiences, find community, and ask for help without judgment. But we cannot do this alone; we need allies.</description></item><item><title>Evaluate your AI agents faster and more effectively</title><link>https://kubermates.org/docs/2025-12-04-evaluate-your-ai-agents-faster-and-more-effectively/</link><pubDate>Thu, 04 Dec 2025 16:20:20 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-04-evaluate-your-ai-agents-faster-and-more-effectively/</guid><description>Evaluate your AI agents faster and more effectively What’s changed for agent evaluations? Why you should use evaluations How to get started with agent evaluations About the author Try DigitalOcean for free Related Articles Streamline Your Workflow: Announcing Environment Support for DigitalOcean App Platform GPU Observability: Get Deeper Insights into Your Droplets and DOKS Clusters Image and audio models from fal now available on DigitalOcean By Grace Morgan Updated: December 4, 2025 3 min read Evaluating AI agents can be tricky, especially when your tools aren’t built around how you think and work. That’s why we’re excited to announce that we’ve updated our agent evaluations experience in the DigitalOcean Gradient™ AI Platform. These improvements make it faster and easier to evaluate your AI agents, understand results, and debug issues. The original evaluations feature was powerful but presented friction points that made it hard for developers to adopt. This redesign tackles those challenges head-on: Goal-oriented metric grouping: Metrics are now organized into intuitive, goal-oriented groups such as Safety &amp;amp; Security, Correctness, and RAG Performance. The Safety &amp;amp; Security group is preselected to help developers get started quickly and confidently. Example datasets : A list of example data sets are now available for common evaluations. This allows developers to create their own datasets quickly and efficiently. Clear, persistent error messaging : Upload errors are now clear, persistent, and specific, with messages like “Validation Error: ‘query’ column is missing”. Developers can easily understand and fix issues, reducing friction in the testing process. Interpretable results with trace integration : Results are organized by the same metric groups used in setup, with tooltips to explain each metric and its scoring. Deep integration with observability tools allows developers to jump directly from a low score to the full trace for fast debugging and improvement.</description></item><item><title>VCF Breakroom Chats Episode 75 –  Breaking the GitOps Barrier: Continuous Delivery for Modern Apps with VCF 9</title><link>https://kubermates.org/docs/2025-12-04-vcf-breakroom-chats-episode-75-breaking-the-gitops-barrier-continuous-delivery-f/</link><pubDate>Thu, 04 Dec 2025 13:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-04-vcf-breakroom-chats-episode-75-breaking-the-gitops-barrier-continuous-delivery-f/</guid><description>VCF Breakroom Chats Episode 75 About the VCF Breakroom Chat Series Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VCF Breakroom Chats Episode 75 - Breaking the GitOps Barrier: Continuous Delivery for Modern Apps with VCF 9 What’s Next for Cloud Native: Highlights from KubeCon North America 2025 Making Harbor Production-Ready: Essential Considerations for Deployment Welcome to the next episode of the VCF Breakroom Chats. Today, we are happy to present this vLog with Jad El-Zein , Principal Technologist at Broadcom. In this episode, Jad El-Zein and Taka Uenishi explore how VCF 9 integrated with ArgoCD delivers modern, GitOps-enabled continuous delivery for private cloud environments. Learn how it empowers platform engineers and IT teams to accelerate and automate application delivery for AI, K8s, and modern workloads. Want to learn more about VCF Automation? Check out the VCF Automation web page for more resources. Read this blog post: Private Cloud Redefined: Deliver a Unified Cloud Consumption Experience with VMware Cloud Foundation 9.0 Listen to our previous topics. This webinar series focuses on VMware Cloud Foundation (VCF). In this series, we share conversations with industry-recognized experts from Broadcom and with solution partners and customers. You’ll gain relevant insights whether you’re an IT practitioner, IT admin, cloud or platform architect, developer, DevOps, senior IT manager, IT executive, or AI/ML professional. If you are new to the series, please check out our previous episodes.</description></item><item><title>AI ambitions meet automation reality: The case for a unified automation platform</title><link>https://kubermates.org/docs/2025-12-04-ai-ambitions-meet-automation-reality-the-case-for-a-unified-automation-platform/</link><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-04-ai-ambitions-meet-automation-reality-the-case-for-a-unified-automation-platform/</guid><description>AI ambitions meet automation reality: The case for a unified automation platform The DIY dilemma: More tools, more problems Automation translates to tangible benefits for AI Additional resources Red Hat Ansible Automation Platform | Product Trial About the author Harper Buete More like this A 5-step playbook for unified automation and AI Solving tool overload, one automation step at a time Technically Speaking | Platform engineering for AI agents Technically Speaking | Driving healthcare discoveries with AI Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share IT teams are stuck between wanting to implement AI solutions across their organizations and dealing with the messy reality of increasingly complex infrastructure. Many are attempting to build their own automation solutions, cobbling together a patchwork of tools that, while well-intentioned, can actually make things worse. Red Hat dug into this with S&amp;amp;P Global Market Intelligence 451 Research, and their findings point to a simpler alternative : use a unified platform instead of patchworking tools together. Most teams are drowning in tools, often ending up with too many dashboards with too little coordination. A remarkable 72% of organizations are using up to 50 different IT tools, while nearly a third (28%) are managing over 50. This fragmented approach creates significant hurdles for effective IT automation, including: Complex implementations : This is a top obstacle for 58% of organizations. We often see this arise when customers deploy too many resources and move components piecemeal, slowing progress and increasing effort. Integration challenges : Integrating existing systems is a challenge for 51% of IT organizations. This isn&amp;rsquo;t just about connecting tools, it&amp;rsquo;s about achieving true interoperability for automation workflows. Skills gaps : 40% of respondents point to specific skills shortages. Maintaining expertise across a vast array of different tools is a constant drain on time and resources. This &amp;ldquo;do-it-yourself&amp;rdquo; mentality is particularly detrimental to AI initiatives.</description></item><item><title>CIS publishes hardening guidance for Red Hat OpenShift Virtualization</title><link>https://kubermates.org/docs/2025-12-04-cis-publishes-hardening-guidance-for-red-hat-openshift-virtualization/</link><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-04-cis-publishes-hardening-guidance-for-red-hat-openshift-virtualization/</guid><description>CIS publishes hardening guidance for Red Hat OpenShift Virtualization Who is CIS and what is a CIS Benchmark? Key security optimizations How to implement Get the CIS Benchmark Red Hat Product Security About the author Dan Bettinger More like this File encryption and decryption made easy with GPG Deploy Confidential Computing on AWS Nitro Enclaves with Red Hat Enterprise Linux What Is Product Security? | Compiler Technically Speaking | Security for the AI supply chain Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The Center for Internet Security ® (CIS ® ) has officially published guidance for hardening Red Hat OpenShift Virtualization. The official publication of the new CIS Benchmark ® for Red Hat OpenShift Virtualization is an important development for organizations running traditional virtual machines (VMs) alongside modern containers. OpenShift Virtualization is a feature of Red Hat OpenShift that allows existing VM-based workloads to run directly on the platform. This globally recognized, consensus-driven benchmark provides recommendations for creating a security-focused configuration for those environments. CIS is a community-driven nonprofit organization, which aims to &amp;ldquo;make the connected world a safer place&amp;rdquo; for businesses, governments, and people by developing and promoting best practice solutions. The CIS Benchmarks are one of those core solutions. They are a set of globally recognized best practices to help secure configuring operating systems (OSs), servers, and other technology. Developed and maintained by a global community of IT professionals, the CIS Benchmarks provide prescriptive instructions for creating a security-focused configuration baseline. The new CIS Benchmark for OpenShift Virtualization was developed based on the OpenShift Virtualization Hardening Guide. The CIS Benchmark provides detailed recommendations to strengthen your security posture by focusing on 4 key areas of optimization, including: Harden the platform from the ground up : This includes guidance on restricting GPU and USB pass-through to approved devices and disabling non-essential feature gates. Control workloads at every layer : The CIS Benchmark provides fine-grained controls, such as restricting exec and virtual network computing (VNC) access to approved administrators and disabling features like guest-memory overcommit. Segment and protect network traffic : This area focuses on using networking controls like Virtual Local Area Networks (VLANs) to isolate tenant or application traffic and applying Media Access Control (MAC) spoof filtering.</description></item><item><title>From vision to reality: A 5-step playbook for unified automation and AI</title><link>https://kubermates.org/docs/2025-12-04-from-vision-to-reality-a-5-step-playbook-for-unified-automation-and-ai/</link><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-04-from-vision-to-reality-a-5-step-playbook-for-unified-automation-and-ai/</guid><description>From vision to reality: A 5-step playbook for unified automation and AI 1. Assess your current state 2. Build your business case 3. Choose the right platform 4. Adopt a phased approach 5. Measure success Additional resources Red Hat Ansible Automation Platform | Product Trial About the author Harper Buete More like this AI ambitions meet automation reality: The case for a unified automation platform Solving tool overload, one automation step at a time Technically Speaking | Platform engineering for AI agents Technically Speaking | Driving healthcare discoveries with AI Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Twenty-eight percent of businesses surveyed in the recent S&amp;amp;P Global Market Intelligence 451 Research report, “ The value of a unified automation platform ,” responded that their company uses 50-100+ tools that don’t seamlessly integrate. This widespread adoption of disparate solutions, often driven by a &amp;ldquo;do it yourself&amp;rdquo; mentality, can lead to overwhelming tool sprawl. The resulting lack of interoperability directly hinders innovation, fragments data insights, and ultimately undermines the effective delivery of AI solutions. As automation and AI become increasingly interdependent, systems must be capable of cohesive information exchange. This critical need points directly to platform engineering (PE), which enables self-service capabilities through a, &amp;ldquo;bidirectional exchange of both data and workflows,&amp;rdquo; between tools. A unified automation platform isn&amp;rsquo;t merely a convenience, it&amp;rsquo;s the strategic foundation for AI development and deployment. Take a moment to evaluate your organization&amp;rsquo;s automation maturity: Are your IT operations bogged down by manual, repetitive tasks across various domains: Network, security, cloud, and infrastructure? Do different teams (IT Ops, DevOps, SecOps, Network Ops, Line of Business) grapple with inconsistent automation tools and processes, fostering silos and inefficiency? Is your current automation fragmented, making it challenging to scale or maintain consistency across hybrid and multicloud environments? Can you accurately measure the ROI of your automation efforts, and pinpoint where automation could deliver the most significant time and cost savings? Do you have a clear strategy for integrating emerging technologies like AI into your IT operations? Is your automation foundation truly prepared to support these initiatives? The next hurdle is effectively articulating the value of a unified platform to your stakeholders and decision-makers.</description></item><item><title>Getting Started with OpenShift Virtualization</title><link>https://kubermates.org/docs/2025-12-04-getting-started-with-openshift-virtualization/</link><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-04-getting-started-with-openshift-virtualization/</guid><description>Getting Started with OpenShift Virtualization Install OpenShift and virtualization operators Create a new VM 1. Select a boot image 2. Select an instance type 3. Set virtual machine details Import your own base image Migrate a VM to OpenShift Virtualization Time to integrate Red Hat OpenShift Virtualization Engine | Product Trial About the author Seth Kenlon More like this Sovereignty emerges as the defining cloud challenge for EMEA enterprises Red Hat OpenShift Virtualization: The strategic platform for virtualization customers Crack the Cloud_Open | Command Line Heroes Edge computing covered and diced | Technically Speaking Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share There are many reasons you might be running a virtual machine (VM) at your organization, and you&amp;rsquo;re probably also using or planning to use containers at the same time. Whether you&amp;rsquo;re looking to migrate away from your current virtualization platform or you&amp;rsquo;re just looking for a unified platform for both VMs and containers, Red Hat OpenShift is a centralized control center for all the services you provide to your users. Here&amp;rsquo;s how to get started with OpenShift Virtualization, a feature of Red Hat OpenShift. To use OpenShift Virtualization, you must have access to a bare metal OpenShift cluster. There are several ways to install OpenShift: Interactive : Deploy a cluster with the web-based Assisted Installer. This provides smart and safe defaults, and performs several pre-flight validations before installing a cluster. Local agent : In an air-gapped or network-restricted environment, you can deploy a cluster locally with the agent-based installer. Automated : Deploy a cluster on infrastructure provisioned by the installer using your host&amp;rsquo;s baseboard management controller (BMC). Full control : Deploy a cluster on your own infrastructure.</description></item><item><title>Red Hat OpenShift sandboxed containers 1.11 and Red Hat build of Trustee 1.0 accelerate confidential computing across the hybrid cloud</title><link>https://kubermates.org/docs/2025-12-04-red-hat-openshift-sandboxed-containers-1-11-and-red-hat-build-of-trustee-1-0-acc/</link><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-04-red-hat-openshift-sandboxed-containers-1-11-and-red-hat-build-of-trustee-1-0-acc/</guid><description>Red Hat OpenShift sandboxed containers 1.11 and Red Hat build of Trustee 1.0 accelerate confidential computing across the hybrid cloud OpenShift sandboxed containers 1.11: A focus on security and enterprise readiness Red Hat build of Trustee 1.0: Simplifying the configuration and deployment Production-ready on Azure: Confidential containers GA on Azure Red Hat OpenShift Expanding to bare metal: Technology preview for Intel TDX and AMD SEV-SNP Real-world use cases and problems solved What&amp;rsquo;s next Try confidential containers on OpenShift today Red Hat OpenShift Container Platform | Product Trial About the authors Ariel Adam Marcos Entenza Jens Freimann Danilo de Paula More like this File encryption and decryption made easy with GPG Deploy Confidential Computing on AWS Nitro Enclaves with Red Hat Enterprise Linux What Is Product Security? | Compiler Technically Speaking | Security for the AI supply chain Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Red Hat is excited to announce the release of Red Hat OpenShift sandboxed containers 1.11 and Red Hat build of Trustee 1.0, marking a significant milestone in our confidential computing journey. These releases bring production-grade support for confidential containers in Microsoft Azure Red Hat OpenShift and introduce technology preview support for bare metal environments with Intel TDX and AMD SEV-SNP processors. Organizations can now protect their most sensitive workloads with hardware-based memory encryption and attestation capabilities across cloud and on-premises infrastructure. Across both cloud and bare metal, OpenShift sandboxed containers 1.11 introduces features that harden your security posture and improve usability. Secure by default : We are implementing a new restrictive agent policy by default. This policy blocks host-level commands like oc exec for confidential containers, providing true isolation from the administrator. A debug mode can be enabled via a pod annotation for development. oc exec Trusted supply chain : We&amp;rsquo;ve enhanced support for signed container images, a critical part of a trusted software supply chain. Secure secret release : A key value of attestation is the secure delivery of secrets. Red Hat build of Trustee is used to verify the integrity of a pod before releasing sensitive data, like database credentials or private keys. Such secrets are retrieved from the attestation service. Improved supportability : We have improved our must-gather tooling to automatically collect Trustee logs, making it easier for our support teams to help you troubleshoot attestation workflows.</description></item><item><title>Solving tool overload, one automation step at a time</title><link>https://kubermates.org/docs/2025-12-04-solving-tool-overload-one-automation-step-at-a-time/</link><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-04-solving-tool-overload-one-automation-step-at-a-time/</guid><description>Solving tool overload, one automation step at a time Key insights from the report How unified automation and AI can propel your operations Additional resources Red Hat Ansible Automation Platform | Product Trial About the author Caroline Smith More like this A 5-step playbook for unified automation and AI AI ambitions meet automation reality: The case for a unified automation platform Technically Speaking | Taming AI agents with observability You Can’t Automate Cultural Change | Code Comments Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Modern IT departments are wrestling with a sprawling array of automation and operations tools, often numbering in the dozens or even hundreds. This complexity makes efficient management and integration a significant obstacle, especially as organizations accelerate their investment in hybrid IT ecosystems, cloud services, and cloud-native application modernization. To help overcome this &amp;ldquo;tool sprawl&amp;rdquo; and its impact on productivity, enterprises are working to establish a common environment for orchestrating and managing critical IT processes—a &amp;ldquo;unified IT automation platform. &amp;quot; To understand the business imperatives driving this shift, we asked S&amp;amp;P Global Market Intelligence 451 Research to conduct a comprehensive survey with 900 business and IT decision-makers and influencers. These leaders provided their perspectives on the current state of IT automation, anticipated changes, and the implications of relying on so many different tools. These insights also helped shape our understanding of the definition and expectations for a unified IT automation platform. Automation challenges: The journey to stable and reliable IT automation isn&amp;rsquo;t without its challenges. The top hurdles identified by respondents were complex and lengthy implementation processes , cited by a substantial 58%, closely followed by integration with existing systems and software, at 51%. Budget constraints (44%) and skills shortages (40%) also pose obstacles. Excess IT tooling: The pervasive challenge of tool sprawl is evident, with 42% of respondents reporting the use of 26-50 different tools, with 28% reporting more than 50 tools in their toolbox. This fragmentation underscores the need for enhanced tool interoperability. Encouragingly, a significant two-thirds (69%) of enterprises have already established platform engineering programs or are actively committing resources to integrate these tools through a unified orchestration layer.</description></item><item><title>Celebrating 100 Jobs on GitJobs!</title><link>https://kubermates.org/docs/2025-12-03-celebrating-100-jobs-on-gitjobs/</link><pubDate>Wed, 03 Dec 2025 12:42:37 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-03-celebrating-100-jobs-on-gitjobs/</guid><description>The 100th Job: Soch Street Join the Community Posted on December 3, 2025 by Jeffrey Sica, Head of Projects at CNCF We are thrilled to announce a major milestone for our community: GitJobs has reached its 100th job posting last month! As an open source forward job board, our mission has always been to connect talented developers with companies that value open source, transparency, and community collaboration. With the site now averaging 4,000 monthly views (and growing), reaching this number is a testament to the growing ecosystem of organizations that are building the future of software. We want to give a special shout-out to Soch Street for being our 100th job posting! Published on October 28th, this listing marks a significant moment in our journey. You can check out the milestone posting here. A big shout-out to everyone who has posted a job, applied for a role, or contributed to the GitJobs platform. Here’s to our next milestone of 1000 posted opportunities! Any company can create a job posting, and anyone can apply. Additionally, the platform itself is open source! Head over to https://github. com/cncf/gitjobs to see behind the curtain, and don’t hesitate to make suggestions to improve it! Share.</description></item><item><title>Red Hat OpenShift Service on AWS supports Capacity Reservations and Capacity Blocks for Machine Learning</title><link>https://kubermates.org/docs/2025-12-03-red-hat-openshift-service-on-aws-supports-capacity-reservations-and-capacity-blo/</link><pubDate>Wed, 03 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-03-red-hat-openshift-service-on-aws-supports-capacity-reservations-and-capacity-blo/</guid><description>Red Hat OpenShift Service on AWS supports Capacity Reservations and Capacity Blocks for Machine Learning Red Hat OpenShift Container Platform | Product Trial About the authors Bala Chandrasekaran Brae Troutman More like this Optimize Cloud Costs with Red Hat OpenShift Virtualization and ROSA on AWS DxOperator from DH2i is now certified for Red Hat OpenShift 4.19 SREs on a plane | Technically Speaking Get started with ROSA Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Red Hat OpenShift Service on AWS (ROSA) is a fully managed application platform that offers a more seamless experience for building, deploying, and scaling applications. For machine learning (ML) workloads, ROSA now supports On-Demand Capacity Reservations (ODCR) and Capacity Blocks for ML, allowing cloud architects and platform administrators to strategically utilize their existing AWS purchases to help deliver uninterrupted access to essential compute infrastructure. Today, ROSA is available in over 30 regions and supports over 600 instance types, allowing customers to run diverse workloads according to their business needs. However, maintaining guaranteed or uninterrupted access to a specific infrastructure type in a particular availability zone (AZ) is important for several critical scenarios: GPU-based accelerated computing workloads: Gaining uninterrupted access to accelerated computing (GPU) instances is vital for AI/ML teams conducting training, fine-tuning, or inference workloads. Capacity reservation helps eliminate the risk of compute unavailability for these time-sensitive, resource-intensive tasks. Planned scaling events: Enabling infrastructure scaling events to confidently support planned business events—such as peak traffic seasons, major product launches, or scheduled batch processing—without provisioning delays. High availability and disaster recovery: Enhancing resiliency by guaranteeing capacity when deploying workloads across multiple AZs or executing disaster recovery protocols across regions. Amazon EC2 Capacity Reservations allow you to reserve compute capacity for your Amazon EC2 instances in a specific AZ for any duration. Capacity Blocks for ML allow you to reserve GPU-based accelerated computing instances on a future date to support your short duration ML workloads. With the support for Capacity Reservations for clusters with hosted control planes (HCP), platform administrators can now create ROSA machine pools in their cluster that directly consume the capacity already reserved with AWS. Key best practices for effectively leveraging Capacity Reservations with ROSA: Pre-planning of AZs, instance types, and capacity: Before creation, ensure a precise match between the reserved capacity and the ROSA machine pool attributes. This includes VPC subnets, the number of node replicas, and the instance type.</description></item><item><title>AI Meets Kubernetes Security: Tigera CEO Reveals What Comes Next for Platform Teams</title><link>https://kubermates.org/docs/2025-12-02-ai-meets-kubernetes-security-tigera-ceo-reveals-what-comes-next-for-platform-tea/</link><pubDate>Tue, 02 Dec 2025 22:36:58 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-02-ai-meets-kubernetes-security-tigera-ceo-reveals-what-comes-next-for-platform-tea/</guid><description>How Can Teams Better Manage the Kubernetes Blast Radius and Skills Gap? How Does Calico AI Simplify Kubernetes Operations Through Conversation? Why Does Support for Istio Ambient Mode Matter for Unified Kubernetes Security? What Challenges Do AI Agents Introduce for Kubernetes Security? Why Are Enterprises Accelerating VM to Kubernetes Migrations? What Comes Next for Kubernetes Security and AI-driven Operations? Ready to Go Deeper on Calico AI and Kubernetes Security? Platform teams are tasked with keeping clusters secure and observable while navigating a skills gap. At KubeCon + CloudNativeCon North America, The New Stack spoke with Ratan Tipirneni, President and CEO of Tigera, about the future of Kubernetes security, AI-driven operations, and emerging trends in enterprise networking. The highlights from that discussion are summarized below. Portions of this article are adapted from a recorded interview between The New Stack’s Heather Joslin and Tigera CEO Ratan Tipirneni. You can watch the full conversation on The New Stack’s YouTube channel. Watch the full interview here Tipirneni emphasizes the importance of controlling risk in Kubernetes clusters. “You want to be able to microsegment your workloads so that if you do come under an attack, you can actually limit the blast radius,” he says. Egress traffic is another area of concern. According to Tipirneni, identifying what leaves the cluster is critical for security and compliance. Platform engineers are often navigating complex configurations without decades of networking experience. “Calico AI is a great mechanism to help them understand some of the nuances and intricacies of how to troubleshoot these things without having to get a PhD in networking,” Tipirneni explains. Tigera’s new Calico AI aims to make cluster operations more approachable.</description></item><item><title>VMware Cloud on AWS: What’s New (December 2025)</title><link>https://kubermates.org/docs/2025-12-02-vmware-cloud-on-aws-what-s-new-december-2025/</link><pubDate>Tue, 02 Dec 2025 18:58:50 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-02-vmware-cloud-on-aws-what-s-new-december-2025/</guid><description>Optimize Your SDDC Deployments: Stretched Cluster Enhancements Non-Stretched Secondary Clusters in Stretched SDDCs Some key benefits include: Improved Scale-Down Options for Stretched Clusters Key use cases: Take Control of Your Data: New Host Usage Report API Product Enhancement: HCX version 4.11.3 Now Available for VMC What’s New in HCX 4.11.3? Improved Experience: Redesigned VMware Cloud on AWS UI Improved Sizing Recommendations: VMC Sizer &amp;amp; Cluster Conversion Updates What’s New for VMC Cluster Conversion estimates? Stay Tuned for Future Updates Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VMware Cloud on AWS: What&amp;rsquo;s New (December 2025) The Great Cloud Charade: Why &amp;ldquo;Data Residency&amp;rdquo; Isn&amp;rsquo;t &amp;ldquo;Data Sovereignty&amp;rdquo; VMware Cloud on AWS: VMC Console UI Migration to Broadcom Updated: 12/02/2025 At Broadcom, we are focused on helping customers modernize their infrastructure, improve resiliency, and simplify operations – without adding complexity for their teams to manage. Over the past few months, we’ve released several updates to make VMware Cloud on AWS (VMC) more flexible and easier to use. It’s easy to miss the latest updates on our release notes , so we want to use this opportunity to highlight some of the most recent feature releases. Our latest features provide enterprise customers more cost-efficient resilience with non-stretched secondary clusters and improved scale-down, clearer operational insights via a redesigned user interface, an improved VMC Sizer, and new Host Usage APIs, and continued product enhancements through HCX 4.11.3. Here’s a look at what’s new. When you deploy a Software Defined Datacenter (SDDC) in VMC, you are given the choice of either a standard or a stretched SDDC deployment. While a standard cluster is deployed in a single AWS availability zone (AZ), a stretched cluster offers improved availability by deploying the SDDC across three AWS availability zones. Two of the availability zones are selected for the instance deployments and the third hosts the vSAN witness component. Because SDDCs in stretched clusters deploy hosts in two AWS availability zones, they require customers to size at two to one for their resource needs. This can be cost prohibitive for workloads that don’t require high availability. Additionally, once stretched clusters have been scaled beyond six hosts, they were previously unable to scale back down. To improve both of these experiences, the VMC team has introduced Non-Stretched Secondary Clusters in Stretched Cluster SDDCs and Improved Scale-Down Options for Stretched Clusters.</description></item><item><title>What’s Next for Cloud Native: Highlights from KubeCon North America 2025</title><link>https://kubermates.org/docs/2025-12-02-what-s-next-for-cloud-native-highlights-from-kubecon-north-america-2025/</link><pubDate>Tue, 02 Dec 2025 14:37:54 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-02-what-s-next-for-cloud-native-highlights-from-kubecon-north-america-2025/</guid><description>Cloud Native and AI Continue to Advance Together VKS: Certified Kubernetes AI Conformant Supply Chain Security and Identity Are Top Priorities Platform Engineering Is Becoming a Repeatable Discipline AI Networking, Compute, and Storage Are Converging Sessions, Demos, and Industry Engagement Sessions Our Upstream Contributions: Strengthening Kubernetes for Everyone Sharing Practical Expertise: Demos and Technical Deep Dives Looking Ahead Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VCF Breakroom Chats Episode 75 - Breaking the GitOps Barrier: Continuous Delivery for Modern Apps with VCF 9 What’s Next for Cloud Native: Highlights from KubeCon North America 2025 Making Harbor Production-Ready: Essential Considerations for Deployment KubeCon + CloudNativeCon North America once again brought together thousands of developers, maintainers, operators, and end users from across the cloud native ecosystem. More than 9,000 attendees gathered in Atlanta, with nearly half joining for the first time, reflecting the accelerating global adoption of Kubernetes and open source innovation. For Broadcom, KubeCon is where we connect directly with the community that has shaped Kubernetes from its earliest days. It’s where we learn, share, and collaborate on the technologies that define modern cloud infrastructure. Here are the major themes that stood out this year and how we are helping customers take advantage of what’s next. A clear message emerged among a number of the keynotes and technical sessions: The future of AI is cloud native, and the future of cloud native is AI. Whether it was Adobe’s “Maximum Acceleration” keynote, Niantic’s real-time ML workflows, or Cohere’s enterprise AI architecture, it became evident that Kubernetes has become the foundation for training, serving, and governing AI models. These were some of the main takeaways : Inference is still the dominant enterprise AI workload, projected to drive hundreds of billions in investment over the next decade AI workloads need portability and interoperability across data centers and public clouds DRA (Dynamic Resource Allocation) is now the standard Kubernetes API for orchestrating accelerators and other specialized resources including GPUs, networking interfaces, and fine-grained CPI sharing. This is now GA in Kubernetes v1.34 Multi-cluster and multi-cloud operations are becoming standard patterns For more than two decades, we have engineered the operational foundations now required for large-scale AI: predictable scheduling, isolation boundaries, memory and CPU efficiency, and robust lifecycle automation. These strengths directly support the direction Kubernetes is heading. Our upstream work, including etcd improvements, leadership on long-term lifecycle stability, conformance engagement, and contributions across SIGs ensures our platforms adopt these emerging AI-native standards in a way enterprises can trust. This year, the CNCF formally launched Kubernetes AI Conformance, validating the essential set of capabilities required for portable and interoperable AI workloads.</description></item><item><title>NVMe Memory Tiering Design and Sizing on VMware Cloud Foundation 9 Part 3: Sizing for Success</title><link>https://kubermates.org/docs/2025-12-02-nvme-memory-tiering-design-and-sizing-on-vmware-cloud-foundation-9-part-3-sizing/</link><pubDate>Tue, 02 Dec 2025 12:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-02-nvme-memory-tiering-design-and-sizing-on-vmware-cloud-foundation-9-part-3-sizing/</guid><description>Discover more from VMware Cloud Foundation (VCF) Blog Related Articles NVMe Memory Tiering Design and Sizing on VMware Cloud Foundation 9 Part 3: Sizing for Success Making Harbor Production-Ready: Essential Considerations for Deployment Reducing Harbor Deployment Complexity on Kubernetes So far in this blog series, we have highlighted the value that NVMe Memory Tiering delivers to our customers and how this is driving adoption. Who doesn’t want to reduce their cost by ~40% just by adopting VMware Cloud Foundation 9?! We’ve also touched on pre-requisites, and hardware in Part 1 , and design in Part 2 ; so, let’s now talk about properly sizing your environment so you can maximize your investment while reducing your cost. Proper sizing for NVMe Memory Tiering is mainly on the hardware side, but there are two possible ways to look at this; greenfield and brownfield deployments. Let’s start with brownfield deployments, which is adopting Memory Tiering on existing infrastructure. You’ve came to the realization that VCF 9 is truly an integrated product delivering a cohesive cloud like solution and decided to deploy it but just learned about Memory Tiering. Don’t worry, you can still introduce NVMe memory Tiering after deployment of VCF 9. After reading Part 1 and Part 2, we’ve learned the importance of the NVMe performance and endurance classes as well as the understanding the 50% active memory requirement. This means that we need to think about purchasing an NVMe device that is at least the same size of our DRAM, since we will double our memory capacity. So, if each of your hosts have 1TB of DRAM we should at least have 1TB NVMe devices, easy enough. However, we can go bigger and still be cheaper than buying more DIMMS, let me explain. I’ve made it a point to say “buy an NVMe device to be at least the same size of DRAM”, and this is because we use a DRAM:NVMe ratio of 1:1 by default, so half of the memory comes from DRAM and half comes from NVMe. Now, there are workloads that may not be doing a whole lot as far as memory activity, think some VDI workloads.</description></item><item><title>Making Harbor Production-Ready: Essential Considerations for Deployment</title><link>https://kubermates.org/docs/2025-12-02-making-harbor-production-ready-essential-considerations-for-deployment/</link><pubDate>Tue, 02 Dec 2025 11:58:04 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-02-making-harbor-production-ready-essential-considerations-for-deployment/</guid><description>&lt;ol&gt;
&lt;li&gt;High Availability (HA) and Scalability 2. Security Best Practices 3. Storage Considerations 4. Monitoring and Alerting 5. Network Configuration Conclusion Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VCF Breakroom Chats Episode 75 - Breaking the GitOps Barrier: Continuous Delivery for Modern Apps with VCF 9 What’s Next for Cloud Native: Highlights from KubeCon North America 2025 NVMe Memory Tiering Design and Sizing on VMware Cloud Foundation 9 Part 3: Sizing for Success Harbor is an open-source container registry that secures artifacts with policies and role-based access control, ensuring images are scanned for vulnerabilities and signed as trusted. To learn more about Harbor and how to deploy it on a Virtual Machine (VM) and in Kubernetes (K8s), refer to parts 1 and 2 of the series. While deploying Harbor is straightforward, making it production-ready requires careful consideration of several key aspects. This blog outlines critical factors to ensure your Harbor instance is robust, secure, and scalable for production environments. For this blog, we will focus on upstream Harbor (v 2.14) deployed on Kubernetes via Helm as our base and provide suggestions for this specific deployment. For a production environment, single points of failure are unacceptable. This is especially true for image registries that acts as a central repository for storing and pulling images and artifacts.&lt;/li&gt;
&lt;/ol&gt;</description></item><item><title>Optimizing cloud spend with Red Hat OpenShift Service on AWS (ROSA)</title><link>https://kubermates.org/docs/2025-12-02-optimizing-cloud-spend-with-red-hat-openshift-service-on-aws-rosa/</link><pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-02-optimizing-cloud-spend-with-red-hat-openshift-service-on-aws-rosa/</guid><description>Optimizing cloud spend with Red Hat OpenShift Service on AWS (ROSA) Increased efficiency Red Hat OpenShift Virtualization Engine | Product Trial About the author Courtney Grosch More like this Red Hat OpenShift Service on AWS supports Capacity Reservations and Capacity Blocks for Machine Learning DxOperator from DH2i is now certified for Red Hat OpenShift 4.19 SREs on a plane | Technically Speaking FAQ Analyst report E-book Webinar Documentation Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Rising virtualization costs, licensing constraints, and operational complexity are driving teams to evaluate more flexible and cost-effective paths to the cloud. Red Hat OpenShift Virtualization, combined with Red Hat OpenShift Service on AWS (ROSA), supports hardware overcommit in the cloud, allowing customers to run more virtual machines (VMs) on fewer cloud resources. By aligning resource consumption more closely to actual usage patterns, organizations can significantly reduce compute costs. This provides a practical way to consolidate VM footprints and maximize the value of cloud investments. Customers can also take advantage of AWS buying programs to further improve cost efficiency. ROSA supports pay-as-you-go pricing, the use of existing AWS cloud committed spend agreements, reserved instances, savings plans and, of course, unified billing. These options offer multiple paths to align OpenShift consumption with financial planning strategies and help organizations convert Red Hat software and managed site reliability engineering (SRE) services into predictable, planned expenditures. Operational efficiency is another key benefit. ROSA is jointly supported by Red Hat and AWS, and Red Hat SRE teams manage the day-to-day operations of the service. This includes upgrades, monitoring, scaling, and incident response. Offloading these activities can help reduce infrastructure management efforts and free teams to focus on delivering applications, accelerating time to value. For organizations adopting AI and GPU-accelerated workloads, ROSA offers additional scalability and cost advantages when compared to on-premise environments.</description></item><item><title>VMware Cloud Foundation Automation – Consume and Deploy Virtual Machines and Kubernetes Clusters</title><link>https://kubermates.org/docs/2025-12-01-vmware-cloud-foundation-automation-consume-and-deploy-virtual-machines-and-kuber/</link><pubDate>Mon, 01 Dec 2025 16:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-01-vmware-cloud-foundation-automation-consume-and-deploy-virtual-machines-and-kuber/</guid><description>Virtual Machine Service vSphere Kubernetes Service Summary Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VCF Breakroom Chats Episode 75 - Breaking the GitOps Barrier: Continuous Delivery for Modern Apps with VCF 9 VMware Cloud Foundation Automation – Consume and Deploy Virtual Machines and Kubernetes Clusters VCF Breakroom Chats Episode 74 - From VI Admin to Private Cloud Architect: New VCAP &amp;amp; VCDX Certification Explained In our previous blog , we talked about how an organization admin leveraging VMware Cloud Foundation Automation enables their organization to be effectively ready for application teams to self-serve and provision infrastructure and applications. In this blog, we are going to shed light on two foundational infrastructure services that are enabled and available out of the box when configuring the tenant organization that leverages the K8S Style API: the Virtual Machine Service and the vSphere Kubernetes Service. The Kubernetes declarative API model has already transformed the way organizations build and operate modern applications. But in recent years, the same Kubernetes model has been extended beyond containerized workloads into the world of infrastructure itself. What started with kubectl apply for Pods and Deployments has evolved into using Kubernetes APIs to provision and manage virtual machines, K8S clusters, networking, storage, load balancers, and even databases. Kubernetes is no longer “just the platform for cloud-native apps. ” It’s steadily becoming the universal control plane for both applications and infrastructure. kubectl apply The Virtual Machine (VM) Service in VMware Cloud Foundation 9.0 provides a unified, Kubernetes-native interface for provisioning and managing VMs directly through namespaces. By exposing VM classes, images, storage policies, and networking configurations as declarative Kubernetes resources, the VM Service enables platform and application teams to consume vSphere-backed compute using familiar Kubernetes tooling. This service ensures consistent governance and lifecycle management by enforcing the policies defined at the organization, region, and project levels in VCF Automation, while leveraging vSphere’s mature virtualization capabilities underneath. Below is a detailed walkthrough of the VM provisioning workflow experience in VMware Cloud Foundation Automation. The vSphere Kubernetes Service (VKS) in VMware Cloud Foundation 9.0 delivers a fully integrated, upstream-compatible Kubernetes control plane that runs natively on vSphere.</description></item><item><title>A guide to restarting pods in Kubernetes using kubectl</title><link>https://kubermates.org/docs/2025-12-01-a-guide-to-restarting-pods-in-kubernetes-using-kubectl/</link><pubDate>Mon, 01 Dec 2025 12:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-01-a-guide-to-restarting-pods-in-kubernetes-using-kubectl/</guid><description>When should you restart a Kubernetes pod? What are the different pod states in Kubernetes? How to restart pods in Kubernetes using kubectl Conclusion Posted on December 1, 2025 by Kevel Bhogayata, Principal Engineer, Middleware CNCF projects highlighted in this post This Member Blog was originally published on the Middleware blog and is republished here with permission. kubectl is the command-line interface for managing Kubernetes clusters. It allows you to manage pods, deployments, and other resources from the terminal, helping you troubleshoot Kubernetes issues. , check pod health, and scale applications easily. Most kubectl commands follow a simple structure. For example, kubectl get pods lists running pods, and kubectl delete pod &lt;pod-name&gt; removes a pod. kubectl get pods kubectl delete pod &lt;pod-name&gt; Many users wonder how to restart a Kubernetes pod using kubectl. Contrary to popular belief, there is no direct kubectl restart pod command. Instead, Kubernetes expects you to work with higher-level objects, such as Deployments. kubectl restart pod This guide covers the safest and most effective methods for restarting pods, including rollout restarts, deleting pods, scaling replicas, and updating environment variables, helping you manage pod restarts in a predictable and controlled way. Knowing when to restart a Kubernetes pod is key to maintaining application stability and performance. Here are the most common scenarios that require a pod restart: When you update your application’s settings (such as environment variables or resource limits), the pod continues to use the old configurations.</description></item><item><title>Frequently asked questions about Red Hat Ansible Automation Platform 2.6</title><link>https://kubermates.org/docs/2025-12-01-frequently-asked-questions-about-red-hat-ansible-automation-platform-2-6/</link><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-12-01-frequently-asked-questions-about-red-hat-ansible-automation-platform-2-6/</guid><description>Frequently asked questions about Red Hat Ansible Automation Platform 2.6 Installations, upgrades, and migrations Automation dashboard Ansible Lightspeed intelligent assistant Self-service automation portal Additional resources Red Hat Ansible Automation Platform | Product Trial About the author Tricia McConnell More like this A 5-step playbook for unified automation and AI AI ambitions meet automation reality: The case for a unified automation platform Technically Speaking | Taming AI agents with observability You Can&amp;rsquo;t Automate Buy-In | Code Comments Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Last month, we launched Red Hat Ansible Automation Platform 2.6, and introduced several new features including an automation dashboard, a self-service automation portal, and the Ansible Lightspeed intelligent assistant. We hosted a follow-up webinar, What’s new with Ansible Automation Platform 2.6 , during which we received some great questions from the audience about how to install, migrate, and upgrade to the latest version. To help you prepare for and navigate the Ansible Automation Platform 2.6 release, we&amp;rsquo;ve compiled the top questions and their answers. Starting at the beginning, here are some common questions and answers about the initial setup of Ansible Automation Platform, whether you&amp;rsquo;re trying it for the first time or just updating to the latest version. Is there documentation for installing or migrating to Ansible Automation Platform 2.6? Red Hat&amp;rsquo;s official product documentation for Ansible Automation Platform 2.6 provides an installation guide to help you get up and running. If you&amp;rsquo;re migrating a previous install to the latest version, read our 2.6 migration documentation. For a quick overview of the process, read this installation and upgrade blog. When moving from using RPM to containerized installations, do I need three separate VMs for service containers (automation controller, database, and automation hub)? Yes, we still recommend having one VM instance for each service container. See the installation and migration guides above. There are additional changes, so be sure to review the tested deployment models. Can I upgrade or migrate 2.4 (RPM) on host A to 2.6 (containerized) on host B? First upgrade to 2.6, then migrate to the containerized installation. Is a database with high availability (HA) available in Ansible Automation Platform 2.6? Red Hat does not provide HA solutions.</description></item><item><title>KubeCon NA 2025: Three Core Kubernetes Trends and a Calico Feature You Should Use Now</title><link>https://kubermates.org/docs/2025-11-28-kubecon-na-2025-three-core-kubernetes-trends-and-a-calico-feature-you-should-use/</link><pubDate>Fri, 28 Nov 2025 18:35:16 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-28-kubecon-na-2025-three-core-kubernetes-trends-and-a-calico-feature-you-should-use/</guid><description>🤖 Trend 1: Kubernetes is Central to AI Workload Orchestration 🌐 Trend 2: Growth in Edge Deployments Increases Complexity 🛠️ Trend 3: Platform Teams Seek Consolidation to Combat Tool Fatigue You Might Be Missing Key Calico Features 💡 🎉 Community &amp;amp; Engagement Highlights Your KubeCon Recap Reel 🤝 Stay Connected: Join the Calico Community! The Tigera team recently returned from KubeCon + CloudNativeCon North America and CalicoCon 2025 in Atlanta, Georgia. It was great, as always, to attend these events, feel the energy of our community, and hold in-depth discussions at the booth and in our dedicated sessions that revealed specific, critical shifts shaping the future of cloud-native platforms. We pulled together observations from our Tigera engineers and product experts in attendance to identify three key trends that are directly influencing how organizations manage Kubernetes today. A frequent and significant topic of conversation was the role of Kubernetes in supporting Artificial Intelligence and Machine Learning (AI/ML) infrastructure. The consensus is clear: Kubernetes is becoming the standard orchestration layer for these specialized workloads. This requires careful consideration of networking and security policies tailored to high-demand environments. Observations from the Tigera team indicated a consistent focus on positioning Kubernetes as the essential orchestration layer for AI workloads. This trend underscores the need for robust, high-performance CNI solutions designed for the future of specialized computing. Conversations pointed to a growing and tangible expansion of Kubernetes beyond central data centers and core clouds. Organizations are actively deploying lightweight clusters in highly distributed locations, or “the edge. ” This growth covers a wide variety of environments—from retail sites and branch offices to factory floors. Various discussions highlighted that Kubernetes at the edge is becoming popular.</description></item><item><title>runc container breakout vulnerabilities: A technical overview</title><link>https://kubermates.org/docs/2025-11-28-runc-container-breakout-vulnerabilities-a-technical-overview/</link><pubDate>Fri, 28 Nov 2025 12:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-28-runc-container-breakout-vulnerabilities-a-technical-overview/</guid><description>The vulnerabilities Exploitation scenarios and threat model Kubernetes and cloud native implications Affected versions and patches Mitigations The bigger picture: Secure-by-default configurations Credits Posted on November 28, 2025 by Matteo Bisi, DevSecOps Team Leader at ReeVo and CNCF KCD Organizer CNCF projects highlighted in this post A set of high-severity vulnerabilities in runc were publicly disclosed in November 2025, allowing for full container breakouts. Runc is the cornerstone of containerization on Linux, serving as the default low-level container runtime for industry-standard tools like Docker, Podman, and Kubernetes. Its ubiquity means that a vulnerability in runc has far-reaching implications for the entire cloud-native ecosystem. This post summarizes the vulnerabilities, the affected versions, and the recommended actions to mitigate them. Three vulnerabilities were discovered, all related to bypassing runc’s restrictions on writing to arbitrary /proc files. These vulnerabilities can be exploited by starting containers with custom mount configurations, including those defined in Dockerfiles (RUN –mount=…). CVE-2025-31133: “container escape via ‘masked path’ abuse due to mount race conditions” CVSS: 7.3 Description: This vulnerability exploits an issue with how masked paths are implemented. An attacker can replace /dev/null with a symlink to a procfs file (e. g. , /proc/sys/kernel/core_pattern or /proc/sysrq-trigger). Runc will then bind-mount the symlink target read-write, leading to a container breakout or a host crash. CVE-2025-52565: “container escape with malicious config due to /dev/console mount and related races” CVSS: 7.3 Description: Similar to the previous CVE, this vulnerability exploits a flaw in /dev/console bind-mounts.</description></item><item><title>From chaos to clarity: How OpenTelemetry unified observability across clouds</title><link>https://kubermates.org/docs/2025-11-27-from-chaos-to-clarity-how-opentelemetry-unified-observability-across-clouds/</link><pubDate>Thu, 27 Nov 2025 12:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-27-from-chaos-to-clarity-how-opentelemetry-unified-observability-across-clouds/</guid><description>The problem: Observability tool sprawl The turning point: Why OpenTelemetry The solution: Implementing OpenTelemetry Benefits realized Lessons learned Conclusion Posted on November 27, 2025 by Arunvel Arunachalam, Infosys CNCF projects highlighted in this post Modern applications rarely live in a single place anymore. One organization’s application footprint was spread across AWS, Azure, and GCP , with some workloads still running on-prem. This multi-cloud approach gave them resilience and flexibility, but it came with a hidden cost: observability sprawl. Each cloud provider brought its own native observability stack. On AWS, they used CloudWatch ; on Azure, Azure Monitor ; on GCP, Stackdriver ; and in their on-prem setup, a mix of Prometheus and ELK. Add to that some third-party APM tools, and suddenly engineers were juggling five dashboards just to debug one request. This was slowing them down. Mean Time to Resolution (MTTR) kept climbing, and developers spent more time stitching together logs and traces than writing code. The turning point came when the organization adopted OpenTelemetry (OTel) , a CNCF graduated project and community-driven standard for observability. What started as a small experiment soon became the backbone of their observability strategy, aligning with the broader trend across the CNCF community toward platform engineering maturity and standardized telemetry practices. Let’s break down what was happening before OpenTelemetry: Multiple tools for the same purpose Metrics: Prometheus (on-prem) + CloudWatch (AWS) + Azure Monitor (Azure). Logs: ELK (on-prem) + Stackdriver (GCP).</description></item><item><title>Amazon EKS introduces Provisioned Control Plane</title><link>https://kubermates.org/docs/2025-11-27-amazon-eks-introduces-provisioned-control-plane/</link><pubDate>Thu, 27 Nov 2025 00:32:32 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-27-amazon-eks-introduces-provisioned-control-plane/</guid><description>Amazon EKS introduces Provisioned Control Plane Delivering predictable and high performance at scale How did we unlock this? Getting started with Provisioned Control Plane Creating a cluster with Provisioned Control Plane Updating control plane scaling tier Monitoring control plane scaling tier utilization Benchmarking with Provisioned Control Plane Conclusion About the authors Amazon Elastic Kubernetes Service (Amazon EKS) powers tens of millions of clusters annually, with an architecture refined by years of real-world insights from thousands of customers running diverse workloads. EKS automatically scales your cluster’s control plane to meet your workload demands. This dynamic, intelligent scaling powers most use cases, handling everything from startup applications to enterprise platforms to mission critical workloads. Building on this proven architecture, we’re introducing capabilities designed for next-generation workloads with specialized requirements. When you’re running AI training or inference workloads at ultra scale, running multi-tenant SaaS platforms, or running mission-critical web applications where every second matters, you need absolute predictability – the ability to guarantee control plane responsiveness before peak demand arrives. To meet these advanced requirements, we’re introducing EKS Provisioned Control Plane, an enhanced option that complements the Standard Control Plane capabilities. Amazon EKS Provisioned Control Plane gives you the ability to pre-allocate control plane capacity from a set of new scaling tiers, ensuring predictable and high performance for your most demanding workloads. By provisioning capacity ahead of time, your cluster handles instantaneous traffic bursts without the need for control plane scaling, which is crucial for serving traffic during high-demand events or sudden workload spikes. These new scaling tiers unlock significantly higher control plane performance and scalability required for emerging workload patterns like ultra scale AI training/inference, high performance computing, or large-scale data processing. Provisioned Control Plane allows you to choose from multiple control plane scaling tiers (XL, 2XL, 4XL) with each tier offering well defined performance on these Kubernetes attributes: API request concurrency – the volume of requests processed by the Kubernetes API servers concurrently Pod scheduling rate – the throughput at which the Kubernetes default scheduler assigns pods to nodes Cluster database size – the storage space allocated to etcd, the database that holds the cluster state/metadata Once you designate a scaling tier to your cluster, EKS ensures sufficient capacity is always available to the control plane to meet the attribute values for that tier. You also get comprehensive visibility into your tier utilization through granular metrics for each attribute, available through both the EKS Prometheus metrics endpoint and Amazon CloudWatch metrics vended to your account. As your workload demands evolve, you can switch between scaling tiers or go back to the standard control plane at any time.</description></item><item><title>Amazon EKS Blueprints for CDK: Now supporting Amazon EKS Auto Mode</title><link>https://kubermates.org/docs/2025-11-26-amazon-eks-blueprints-for-cdk-now-supporting-amazon-eks-auto-mode/</link><pubDate>Wed, 26 Nov 2025 22:28:12 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-26-amazon-eks-blueprints-for-cdk-now-supporting-amazon-eks-auto-mode/</guid><description>Amazon EKS Blueprints for CDK: Now supporting Amazon EKS Auto Mode What is EKS Blueprints for CDK? What is EKS Auto Mode? Prerequisites Implementing EKS Auto Mode with EKS Blueprints for CDK Pattern 1: Basic EKS Auto Mode cluster Pattern 2: EKS Auto Mode cluster with custom ARM NodePool for workloads Pattern 3: EKS Auto Mode cluster with custom AI Accelerator NodePool for AI/ML workloads Cleaning up Benefits of using EKS Auto Mode with EKS Blueprints Conclusion About the authors Amazon EKS Blueprints for CDK has recently added support for EKS Auto Mode , a significant enhancement that streamlines Kubernetes management by automatically provisioning infrastructure, choosing optimal compute instances, dynamically scaling resources, continuously optimizing costs, managing core add-ons, patching operating systems, and integrating with Amazon Web Services (AWS) security services. EKS Blueprints for CDK is an open source framework that helps AWS customers bootstrap and configure production-ready Amazon Elastic Kubernetes Service (Amazon EKS) clusters with the AWS Cloud Development Kit (AWS CDK). Customers can describe the desired state of their Amazon EKS environment with worker nodes, auto scaling, networking, and Kubernetes add-ons as an infrastructure as code (IaC) blueprint. These blueprints can be used in pipelines to set up consistent environments across AWS accounts and AWS Regions. EKS Blueprints is part of the broader initiative by AWS launched in 2022: Bootstrapping clusters with EKS Blueprints | Amazon Web Services. EKS Blueprints can bootstrap your clusters with Amazon EKS add-ons, and many popular open source add-ons, such as ArgoCD, Nginx, Keda, Fluent Bit, FluxCD, and more. The framework automatically chooses compatible versions for core Amazon EKS add-ons based on your Kubernetes version, eliminating the guesswork of which add-on versions work together. When you upgrade your cluster, the add-on versions automatically update to maintain compatibility, preventing version mismatch errors. EKS Blueprints comes with built-in compatibility handling for your add-ons, so that each add-on you deploy is compatible with your cluster’s configuration. Feedback and support for this framework is available through GitHub issues. EKS Blueprints provides specialized cluster builders that come pre-configured with the right add-ons and best practices for specific workloads. Whether you’re building observability stacks with Prometheus and Grafana, GPU clusters for machine learning (ML), Windows environments for.</description></item><item><title>Enhancing and monitoring network performance when running ML Inference on Amazon EKS</title><link>https://kubermates.org/docs/2025-11-26-enhancing-and-monitoring-network-performance-when-running-ml-inference-on-amazon/</link><pubDate>Wed, 26 Nov 2025 20:55:13 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-26-enhancing-and-monitoring-network-performance-when-running-ml-inference-on-amazon/</guid><description>Enhancing and monitoring network performance when running ML Inference on Amazon EKS Current challenges with network observability for ML inference workloads Deep-dive into Container Network Observability in Amazon EKS ML inference workload scenario Setting up Container Network Observability use cases for ML inference workload Visualize and confirm intercommunication between services for troubleshooting Analyze Availability Zone (AZ) traffic pattern between deployments Network Health Indicator Investigating ML inference Latency using performance metrics in Amazon Manged Grafana Cleaning up Conclusion About the authors Amazon Elastic Kubernetes Service (Amazon EKS) has become a popular choice for customers looking to run their workloads in the Amazon Web Services (AWS) Cloud with customers increasingly choosing to run their AI and Machine Learning (AI/ML) workloads on Amazon EKS. Customers can use Amazon EKS to customize configuration to match their workload requirements. Furthermore, Platform teams can use it to transfer their existing container orchestration model and expertise when deploying new workloads and standardize on Amazon EKS. Kubernetes also provides access to a rich environment of popular open source AI/ML frameworks, tools, and inference engines such as Ray, vLLM, Triton, PyTorch. Lastly, they can use Kubernetes’ tested capability to auto-scale, deploy and manage containerized workloads at scale, and implement the full cluster automation capabilities of EKS. Some use cases of AI/ML workloads deployed on Amazon EKS include generative AI Model training for Large Language Models (LLMs), real-time and batch ML inference, and Retrieval Augmented Generation (RAG) Pipelines. ML inference is the process where a trained model generates predictions on a user’s input prompt or query. Inference has become an important part of modern applications and powers applications such as content generation, intelligent assistants, recommendation engines. Over the years AWS has released a suite of resources and artifacts to accelerate and streamline customers’ usage of Amazon EKS as their service of choice for running AI/ML workloads. These include AI on EKS , Best Practices for Running AI/ML workloads , Amazon EKS-optimized accelerated AMIs for GPU Instances , and AWS Deep Learning Containers. Recently AWS announced Container Network Observability in Amazon EKS , a set of Amazon EKS network observability features that customers can use to observe, visualize, and enhance their Amazon EKS network environment. In this post we explore the feature sets, deep dive into how it works, and explore an ML inference workload scenario where we use it to monitor and enhance its network performance.</description></item><item><title>Announcing Kyverno release 1.16</title><link>https://kubermates.org/docs/2025-11-26-announcing-kyverno-release-1-16/</link><pubDate>Wed, 26 Nov 2025 20:44:24 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-26-announcing-kyverno-release-1-16/</guid><description>CEL policy types Kyverno Authz Server Introducing the Kyverno SDK Other features and enhancements Getting started and backward compatibility Roadmap Conclusion Posted on November 26, 2025 by Shuting Zhao, Kyverno Maintainer and a Staff Engineer at Nirmata CNCF projects highlighted in this post Kyverno 1.16 delivers major advancements in policy as code for Kubernetes, centered on a new generation of CEL-based policies now available in beta with a clear path to GA. This release introduces partial support for namespaced CEL policies to confine enforcement and minimize RBAC, aligning with least-privilege best practices. Observability is significantly enhanced with full metrics for CEL policies and native event generation, enabling precise visibility and faster troubleshooting. Security and governance get sharper controls through fine-grained policy exceptions tailored for CEL policies, and validation use cases broaden with the integration of an HTTP authorizer into ValidatingPolicy. Finally, we’re debuting the Kyverno SDK, laying the foundation for ecosystem integrations and custom tooling. CEL policy types are introduced as v1beta. The promotion plan provides a clear, non‑breaking path: v1 will be made available in 1.17 with GA targeted for 1.18. This release includes the cluster‑scoped family (Validating, Mutating, Generating, Deleting, ImageValidating) at v1beta1 and adds namespaced variants for validation, deleting, and image validation; namespaced Generating and Mutating will follow in 1.17. PolicyException and GlobalContextEntry will advance in step to keep versions aligned; see the promotion roadmap in this tracking issue. Kyverno 1.16 introduces namespaced CEL policy types— NamespacedValidatingPolicy , NamespacedDeletingPolicy , and NamespacedImageValidatingPolicy —which mirror their cluster-scoped counterparts but apply only within the policy’s namespace. This lets teams enforce guardrails with least-privilege RBAC and without central changes, improving multi-tenancy and safety during rollout. Choose namespaced types for team-owned namespaces and cluster-scoped types for global controls.</description></item><item><title>How to Turbocharge Your Kubernetes Networking With eBPF</title><link>https://kubermates.org/docs/2025-11-26-how-to-turbocharge-your-kubernetes-networking-with-ebpf/</link><pubDate>Wed, 26 Nov 2025 20:12:38 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-26-how-to-turbocharge-your-kubernetes-networking-with-ebpf/</guid><description>Why eBPF Matters for Kubernetes Networking What is eBPF? Performance Improvements Through eBPF Key Performance Advantages Observability: Real-Time Insights Without Agents Key Observability Advantages Security at the Kernel Layer Key Security Advantages of eBPF: eBPF Use Cases When Not to Use eBPF: How Calico Uses eBPF Examples of Calico’s eBPF Capabilities Modern Kubernetes Needs a Modern Data Plane Explore eBPF Further with Calico When your Kubernetes cluster handles thousands of workloads, every millisecond counts. And that pressure is no longer the exception; it is the norm. According to a recent CNCF survey, 93% of organizations are using, piloting, or evaluating Kubernetes , revealing just how pervasive it has become. Kubernetes has grown from a promising orchestration tool into the backbone of modern infrastructure. As adoption climbs, so does pressure to keep performance high, networking efficient, and security airtight. However, widespread adoption brings a difficult reality. As organizations scale thousands of interconnected workloads, traditional networking and security layers begin to strain. Keeping clusters fast, observable, and protected becomes increasingly challenging. Innovation at the lowest level of the operating system—the kernel—can provide faster networking, deeper system visibility, and stronger security. But developing programs at this level is complex and risky. Teams running large Kubernetes environments need a way to extend the Linux kernel safely and efficiently, without compromising system stability. Enter eBPF (extended Berkeley Packet Filter), a powerful technology that allows small, verified programs to run safely inside the kernel.</description></item><item><title>Data-driven Amazon EKS cost optimization: A practical guide to workload analysis</title><link>https://kubermates.org/docs/2025-11-26-data-driven-amazon-eks-cost-optimization-a-practical-guide-to-workload-analysis/</link><pubDate>Wed, 26 Nov 2025 17:32:47 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-26-data-driven-amazon-eks-cost-optimization-a-practical-guide-to-workload-analysis/</guid><description>Data-driven Amazon EKS cost optimization: A practical guide to workload analysis Common pattern of resource waste The greedy workload caused oversized pod resources Problem: Impact: Resolution: Recommendations: Tools to help with this: The pet workload causes excessive replica counts Problem: Impact: Overly strict topology spread constraints: Recommendation: Overly strict Pod Distribution Budget (PDB): Recommendations: The isolated workloads configured with fragmented node pools Why the savings occur: Recommendations: Conclusion About the authors This post introduces some of the key considerations for optimizing Amazon Elastic Kubernetes Service (Amazon EKS) costs in production environments. Through detailed workload analysis and comprehensive monitoring, we demonstrate a proven best practice to maximize cost savings while maintaining performance and resilience supported by real-world examples and practical implementation guidelines. In pursuing optimal performance and resilience, organizations often struggle to balance cost efficiency, as shown in the following figure. Figure 1: Strategic balance triangle showing trade-offs Through collaboration with application owners and developers, a clear pattern emerges: the primary driver of cloud cost waste is overprovisioned resources, justified by performance and resilience considerations that may no longer reflect actual needs. In this post we discuss three critical areas of waste: Greedy workload : oversized pod resources for performance Pet workload : excessive replica counts for resilience Isolated workload: fragmented node pools with stranded capacity for performance Each decision, made with good intentions, accumulates into unnecessary spending over time. The challenge is finding the optimal balance through data-driven rightsizing and architectural optimization. We can call them greedy workloads if a Pod’s requests are higher than what the application actually tries to use. resources: requests: memory: 3Gi cpu: 800m limits: memory: 3Gi resources: requests: memory: 3Gi cpu: 800m limits: memory: 3Gi The application tries to actually use ~0. 2vCPU (200 m) and ~1 Gi of memory. Figure 2: Request and actual usage We can see that in total we’re actually only using ~400 m/1930 m (~21%) vCPU, and ~2 Gi/6.8 Gi (~29%) memory, as shown in the preceding figure. Despite having plenty of actual resources for more copies of this Pod, Kubernetes doesn’t place any more replicas on this node because the allocatable resources have been almost completely allocated. We adjust the requests to be 200 m CPU and 1 Gi of memory to match the application’s workload.</description></item><item><title>How educators and Red Hat Academy help shape the next generation of IT leaders</title><link>https://kubermates.org/docs/2025-11-26-how-educators-and-red-hat-academy-help-shape-the-next-generation-of-it-leaders/</link><pubDate>Wed, 26 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-26-how-educators-and-red-hat-academy-help-shape-the-next-generation-of-it-leaders/</guid><description>How educators and Red Hat Academy help shape the next generation of IT leaders Red Hat Academy: The educator&amp;rsquo;s platform The educator&amp;rsquo;s impact: From classroom to innovation hub What&amp;rsquo;s in it for the student? Get started with Red Hat Academy Red Hat Learning Subscription | Product Trial About the author Syed S Ahmed More like this Listening, learning, and leading: How customer feedback shapes the future of Red Hat Learning Subscription Red Hat Learning Subscription: Expert chat for premium and standard users A vested interest in 5G | Technically Speaking Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Technology is the fundamental foundation of modern business and the catalyst for career advancement. While students seek skills that inspire innovation and leadership, it&amp;rsquo;s the educator who specializes in making the transformation possible. A great educator acts as a dedicated mentor, and doesn&amp;rsquo;t just teach theoretical concepts but actively unlocks potential in the next generation. The path to IT leadership is rarely linear, especially when traditional academic curriculum struggles to maintain pace with rapidly evolving industry practices. A significant gap often exists between classroom knowledge and practical skills employers demand, meaning many graduates lack exposure to the enterprise-grade tools used by real companies. Employers have made their priority clear: They need practical, certified skills over purely theoretical knowledge. Red Hat Academy is a no-cost academic partnership program that Red Hat offers educational institutions (like universities and colleges) and turns them into authorized centers for delivering Red Hat&amp;rsquo;s enterprise-ready curriculum for students, focusing on the latest enterprise open-source technologies, including Red Hat Enterprise Linux administration, artificial intelligence, cloud computing, automation, and application development. Educators can easily integrate these industry-ready courses as part of their official or supplemental curriculum plan so students can gain access to the same open-source technologies utilized by major corporations worldwide. The content is delivered through a dedicated, web-based platform where instructors create and manage virtual classes and enroll students to access materials, free cloud labs for hands-on activities. Students also receive digital Credly badges and industry-recognized certificates when they complete a course or certification, respectively. Bridging the skill gap between industry and academia requires strong partnerships that enable instructors to bring industry standards directly into the classroom. This is where educators can leverage Red Hat Academy to transform their teaching environment.</description></item><item><title>Kubernetes v1.35 Sneak Peek</title><link>https://kubermates.org/docs/2025-11-26-kubernetes-v1-35-sneak-peek/</link><pubDate>Wed, 26 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-26-kubernetes-v1-35-sneak-peek/</guid><description>Kubernetes v1.35 Sneak Peek Deprecations and removals for Kubernetes v1.35 cgroup v1 support Deprecation of ipvs mode in kube-proxy Kubernetes is deprecating containerd v1. y support Featured enhancements of Kubernetes v1.35 Node declared features In-place update of Pod resources Pod certificates Numeric values for taints User namespaces Support for mounting OCI images as volumes Want to know more? Get involved As the release of Kubernetes v1.35 approaches, the Kubernetes project continues to evolve. Features may be deprecated, removed, or replaced to improve the project&amp;rsquo;s overall health. This blog post outlines planned changes for the v1.35 release that the release team believes you should be aware of to ensure the continued smooth operation of your Kubernetes cluster(s), and to keep you up to date with the latest developments. The information below is based on the current status of the v1.35 release and is subject to change before the final release date. On Linux nodes, container runtimes typically rely on cgroups (short for &amp;ldquo;control groups&amp;rdquo;). Support for using cgroup v2 has been stable in Kubernetes since v1.25, providing an alternative to the original v1 cgroup support. While cgroup v1 provided the initial resource control mechanism, it suffered from well-known inconsistencies and limitations. Adding support for cgroup v2 allowed use of a unified control group hierarchy, improved resource isolation, and served as the foundation for modern features, making legacy cgroup v1 support ready for removal. The removal of cgroup v1 support will only impact cluster administrators running nodes on older Linux distributions that do not support cgroup v2; on those nodes, the kubelet will fail to start. Administrators must migrate their nodes to systems with cgroup v2 enabled. More details on compatibility requirements will be available in a blog post soon after the v1.35 release.</description></item><item><title>Streamline Your Workflow: Announcing Environment Support for DigitalOcean App Platform</title><link>https://kubermates.org/docs/2025-11-25-streamline-your-workflow-announcing-environment-support-for-digitalocean-app-pla/</link><pubDate>Tue, 25 Nov 2025 16:06:22 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-25-streamline-your-workflow-announcing-environment-support-for-digitalocean-app-pla/</guid><description>Streamline Your Workflow: Announcing Environment Support for DigitalOcean App Platform The Core Concept Configuring Environments in the UI Managing Environments with doctl Step 1: Create an Environment-Tagged Project Step 2: Create an App and Assign it to the Project Step 3: Find an App’s Environment Accelerate Your Workflow with App Cloning Start Organizing About the author(s) Try DigitalOcean for free Related Articles Evaluate your AI agents faster and more effectively GPU Observability: Get Deeper Insights into Your Droplets and DOKS Clusters Image and audio models from fal now available on DigitalOcean By Waverly Swinton and Bikram Gupta Published: November 25, 2025 5 min read As developers, we love building, but we also know that as an application portfolio grows, managing it gets complicated. The line between your production services, your staging environment, and your new feature-branch deployments can blur. Keeping track of which app serves which purpose and quickly identifying all your “production” apps at a glance becomes a significant organizational challenge. We’re excited to announce a powerful new way to manage this complexity: Environment Support for DigitalOcean App Platform, powered by DigitalOcean Projects. We’re also introducing App Cloning , a new feature to replicate your apps in just a few clicks. This feature allows you to explicitly tag your resource groups (Projects) with a specific environment, such as Development, Staging, or Production and then assign your App Platform apps to those projects. This gives you a high-level, filterable view of your entire application landscape, right from the control panel and via the CLI. The logic is simple but effective: A Project is a “bucket” for your resources. You can group Droplets, Load Balancers, Databases, and App Platform Apps into a single Project. A Project is a “bucket” for your resources. You can group Droplets, Load Balancers, Databases, and App Platform Apps into a single Project. A Project can now be assigned an environment.</description></item><item><title>Reducing Harbor Deployment Complexity on Kubernetes</title><link>https://kubermates.org/docs/2025-11-25-reducing-harbor-deployment-complexity-on-kubernetes/</link><pubDate>Tue, 25 Nov 2025 10:01:39 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-25-reducing-harbor-deployment-complexity-on-kubernetes/</guid><description>Deploying Harbor on Kubernetes using Helm Prerequisites Step 1: Download Harbor Deployment Manifests Step 2: Configure values. yaml Step 3: Deploy Harbor Step 4: Verify Harbor Installation Leveraging VKS Standard Packages for Harbor setup Prerequisites: Step 1: Associate a VKS Standard Package Repository Step 2: Deploy Prerequisites (For Production-Ready Harbor) Step 3: Deploy Harbor Step 4: Verify Harbor Installation Deploying Harbor as a Supervisor Service in VCF 9 Prerequisites Step 1: Download and Update the Harbor Supervisor Service YAML Step 2: Deploy the Harbor Supervisor Service Step 3: Monitor the Harbor Supervisor Service Deployment Conclusion Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Harbor: Your Enterprise-Ready Container Registry for a Modern Private Cloud VCF Breakroom Chats Episode 75 - Breaking the GitOps Barrier: Continuous Delivery for Modern Apps with VCF 9 What’s Next for Cloud Native: Highlights from KubeCon North America 2025 Harbor is an indispensable open-source container image registry, offering robust features like policy-driven security, role-based access control (RBAC), vulnerability scanning, image signing, image replication and distribution. Deploying Harbor is a common and critical step for organizations looking to streamline their containerization workflows. As we discussed in our previous blog post , Harbor offers significant value through its comprehensive features and can be deployed on a virtual machine (VM). This blog post will pick up where we left off, guiding you through the process of deploying Harbor on an upstream conformant Kubernetes platform using VMware vSphere Kubernetes Service (VKS) as an example first via Helm, then via VKS standard packages, and finally showcasing the simplified deployment as a supervisor service in VMware Cloud Foundation (VCF) 9. This progression highlights how complexity is progressively reduced at each level. If you are interested to learn more about Harbor and how to deploy it on a VM, check out our previous blog. Before diving into the specifics of VKS, let’s briefly outline the general steps for deploying the bare-minimum components of Harbor on any standard Kubernetes cluster (non production-ready). This foundational understanding will help you grasp the components involved. A running Kubernetes cluster (We will use VMware vSphere Kubernetes Service (VKS) for this, however the process remains the same for any upstream conformant Kubernetes platform). Additionally, configure kubectl to interact with your cluster. kubectl Helm (recommended) for simplified deployment.</description></item><item><title>CKAD Exam Verification Guide</title><link>https://kubermates.org/docs/2025-11-25-ckad-exam-verification-guide/</link><pubDate>Tue, 25 Nov 2025 03:58:21 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-25-ckad-exam-verification-guide/</guid><description>Join 1M+ Learners Exam Setup - The Speed Booster (Optional, But Highly Recommended!) Should you set this up? Good news about autocomplete: The time-saving aliases: Configure vim for YAML editing: Recommendation: 1. Application Design and Build (20% of the exam) Container Images Workload Resources Multi-Container Pod Patterns Persistent and Ephemeral Volumes 2. Application Deployment (20% of the exam) Deployment Strategies Helm Package Manager Kustomize 3. Application Observability and Maintenance (15% of the exam) API Deprecations Probes and Health Checks Built-in CLI Monitoring Tools Container Logs Debugging in Kubernetes 4. Application Environment, Configuration and Security (25% of the exam) Custom Resources (CRDs) and Operators Authentication, Authorization, and Admission Control Resource Requests, Limits, and Quotas ConfigMaps Secrets ServiceAccounts Application Security (SecurityContexts, Capabilities) 5. Services and Networking (20% of the exam) Services Ingress NetworkPolicies Imperative Commands &amp;amp; Time-Savers Generate YAML (Most Important!) kubectl explain (2x faster than docs!) Quick Creation (No YAML Needed) Exam Workflow Step-by-step process: Time Management Real-World Troubleshooting Scenarios Scenario 1: Pod Stuck in Pending Scenario 2: CrashLoopBackOff Scenario 3: Service Not Accessible Scenario 4: Ingress Returns 404 Scenario 5: NetworkPolicy Not Working Quick Reference Card Success Checklist Final Exam Tips One week before: Day before: Exam day: During exam: Common Mistakes to Avoid You&amp;rsquo;ve Got This! FAQ Join 1M+ Learners Istio Certified Associate (ICA) Study Guide From AI Literacy to AI Readiness: The 2026 Playbook for Universities Want a Competitive Edge in 2026? Follow This AI-Powered Roadmap for DevOps &amp;amp; Cloud Engineers Top AWS Certifications in 2026: Which Are Worth Your Investment? Focus : Master verification &amp;amp; speed - 15-20 tasks in 120 mins = ~6-8 mins each Setup : Use aliases ( k , $do , $now ) only if you&amp;rsquo;ve practiced them k $do $now YAML Tip : Generate with &amp;ndash;dry-run=client -o yaml ; never write from scratch &amp;ndash;dry-run=client -o yaml Docs Shortcut : Use kubectl explain , not web docs kubectl explain Verify Everything : Always get , describe , logs , and check Events get describe logs Probes : Liveness restarts, Readiness controls traffic - know the difference Multi-Container : Init containers run first, sidecars run alongside - verify both ConfigMaps vs Secrets : ConfigMaps for config, Secrets for sensitive data - verify consumption Services : Endpoints tell truth; empty endpoints = selector mismatch Time Rule : Max 8 mins per question; verify before moving on Golden Trick : Imperative → YAML → Apply → Verify = full marks Practice Goal : Score 90%+ in mock exams, finish in &amp;lt;100 mins Mindset : Speed + verification = success in CKAD Welcome! You&amp;rsquo;re about to dive into a comprehensive guide that covers commands which can be leveraged as a verification method for the CKAD exam. Here&amp;rsquo;s the reality: you have 120 minutes for around 15-20 questions, which means about 6-8 minutes per task. Sounds tight? It is! But master these verification techniques, and you&amp;rsquo;ll be confident in finishing with time to spare. Think of this guide as your exam companion. Each section is designed to be practical and focused on what actually matters during those 2 hours. Honestly, it&amp;rsquo;s up to you! Some folks love aliases and can&amp;rsquo;t imagine working without them. Others prefer typing full commands every time.</description></item><item><title>Kubernetes Configuration Good Practices</title><link>https://kubermates.org/docs/2025-11-25-kubernetes-configuration-good-practices/</link><pubDate>Tue, 25 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-25-kubernetes-configuration-good-practices/</guid><description>Kubernetes Configuration Good Practices General configuration practices Use the latest stable API version Store configuration in version control Write configs in YAML not JSON Keep configuration simple and minimal Group related objects together Add helpful annotations Managing Workloads: Pods, Deployments, and Jobs Use Deployments for apps that should always be running Use Jobs for tasks that should finish Service Configuration and Networking Create Services before workloads that use them Use DNS for Service discovery Avoid hostPort and hostNetwork unless absolutely necessary Use headless Services for internal discovery Working with labels effectively Use semantics labels Use common Kubernetes labels Manipulate labels for debugging Handy kubectl tips Apply entire directories Use label selectors to get or delete resources Quickly create Deployments and Services Conclusion Configuration is one of those things in Kubernetes that seems small until it&amp;rsquo;s not. Configuration is at the heart of every Kubernetes workload. A missing quote, a wrong API version or a misplaced YAML indent can ruin your entire deploy. This blog brings together tried-and-tested configuration best practices. The small habits that make your Kubernetes setup clean, consistent and easier to manage. Whether you are just starting out or already deploying apps daily, these are the little things that keep your cluster stable and your future self sane. This blog is inspired by the original Configuration Best Practices page, which has evolved through contributions from many members of the Kubernetes community. Kubernetes evolves fast. Older APIs eventually get deprecated and stop working. So, whenever you are defining resources, make sure you are using the latest stable API version. You can always check with kubectl api-resources kubectl api-resources This simple step saves you from future compatibility issues. Never apply manifest files directly from your desktop.</description></item><item><title>5 Reasons to Switch to the Calico Ingress Gateway (and How to Migrate Smoothly)</title><link>https://kubermates.org/docs/2025-11-24-5-reasons-to-switch-to-the-calico-ingress-gateway-and-how-to-migrate-smoothly/</link><pubDate>Mon, 24 Nov 2025 21:35:29 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-24-5-reasons-to-switch-to-the-calico-ingress-gateway-and-how-to-migrate-smoothly/</guid><description>The End of Ingress NGINX Controller is Coming: What Comes Next? Reason 1: The Future Is Gateway API and Ingress Is Being Left Behind Reason 2: Production-Grade Envoy for Performance &amp;amp; Reliability Reason 3: Built-In Web Application Firewall (WAF) &amp;amp; L3–L7 Security Reason 4: Advanced Traffic Management Out of the Box Reason 5: A Smooth Migration Path Using Standard Gateway API Resources Why Migrate to the Calico Ingress Gateway? Migration Prerequisites Best Practices for a Smooth Migration Migration Workflow Overview 1️⃣ Step 1: Enable the Calico Ingress Gateway 2️⃣ Step 2: Create Your Gateway 3️⃣ Step 3: Convert Ingress resources to HTTPRoutes 4️⃣ Step 4: Validate and Redirect Traffic Integrating Calico Network Policy The Ingress NGINX Controller is approaching retirement , which has pushed many teams to evaluate their long-term ingress strategy. The familiar Ingress resource has served well, but it comes with clear limits: annotations that differ by vendor, limited extensibility, and few options for separating operator and developer responsibilities. The Gateway API addresses these challenges with a more expressive, standardized, and portable model for service networking. For organizations migrating off Ingress NGINX, the Calico Ingress Gateway, a production-hardened, 100% upstream distribution of Envoy Gateway, provides the most seamless and secure path forward. If you’re evaluating your options, here are the five biggest reasons teams are switching now followed by a step-by-step migration guide to help you make the move with confidence. Ingress NGINX is entering retirement. Maintaining it will become increasingly difficult as ecosystem support slows. The Gateway API is the replacement for Ingress and provides: A portable and standardized configuration model Consistent behaviour across vendors Cleaner separation of roles More expressive routing Support for multiple protocols Calico implements the Gateway API directly and gives you an upgrade path without shortcuts or proprietary extensions. The Gateway API is designed to replace these gaps with: A vendor-neutral, standard spec Role separation through GatewayClass, Gateway, HTTPRoute Support for richer routing, policy, and protocols A more extensible, future-proof foundation The Calico Ingress Gateway implements the Gateway API using Envoy Gateway which gives you all of these benefits with upstream compatibility. Calico Ingress Gateway is built on a 100 percent upstream and production-hardened Envoy distribution. You get: High-performance L7 traffic handling Seamless Envoy ecosystem compatibility Robust resiliency features (timeouts, retries, circuit breaking, etc. ) Envoy-native telemetry and observability If you’re currently struggling with NGINX’s limited L7 behaviour or annotation-driven configuration, Envoy provides a dramatic step up in capability.</description></item><item><title>The new era of customer and AI-driven network investment</title><link>https://kubermates.org/docs/2025-11-24-the-new-era-of-customer-and-ai-driven-network-investment/</link><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-24-the-new-era-of-customer-and-ai-driven-network-investment/</guid><description>The new era of customer and AI-driven network investment A new model for network intelligence Industry collaboration drives innovation The path to business value through customer and AI-driven network investments Continuous improvement The adaptable enterprise: Why AI readiness is disruption readiness About the authors Rob McManus Nobuhiko Nagataki Sam Sun Wilson Toh Masaya Muraki Atul Deshpande More like this A 5-step playbook for unified automation and AI AI ambitions meet automation reality: The case for a unified automation platform Technically Speaking | Platform engineering for AI agents Technically Speaking | Driving healthcare discoveries with AI Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Telecommunication service providers face the challenge of maximizing return on investment (ROI) on massive 5G investments. Efficient infrastructure deployment is particularly difficult in areas with real-time demand fluctuations, such as stadiums and tourist spots. One key challenge lies in comprehending usage patterns and connectivity performance from the end-user&amp;rsquo;s perspective. A new paradigm is emerging: Customer and AI-driven network investments. This paradigm is being realized as a TM Forum catalyst project to build an AI-driven platform for network investments. Imagine every customer&amp;rsquo;s device acting as a real-time sensor for a service provider’s network. This is the promise of decentralized physical infrastructure networks (DePIN), an approach that puts customer experience at the center of every network decision. Red Hat, along with other industry leaders, are collaborating on this TM Forum catalyst project to develop a solution that combines the power of crowdsourced data with advanced analytics and artificial intelligence (AI) to deliver actionable insights. TM Forum is a global industry association that helps its members digitally transform through collaboration, innovation, and the development of standards and best practices. They provide resources like the open digital architecture , open APIs, and catalyst innovation programs to help members reduce costs, accelerate time-to-market, and deliver new services. The platform is built on three core pillars: User insights: With user consent, a lightweight application on customer devices collects quality-of-experience (QoE) and radio metrics to create a high-resolution, low-cost view of network performance. Following the DePIN approach, users are rewarded with on-chain tokens to create a transparent and verifiable incentive system.</description></item><item><title>Unifying multivendor DPUs in Red Hat OpenShift</title><link>https://kubermates.org/docs/2025-11-24-unifying-multivendor-dpus-in-red-hat-openshift/</link><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-24-unifying-multivendor-dpus-in-red-hat-openshift/</guid><description>Unifying multivendor DPUs in Red Hat OpenShift A unified management platform for diverse hardware Standardized discovery through abstraction Deploying workloads with Kubernetes-native primitives Enabling true workload portability and interoperability What this means for Red Hat customers Red Hat OpenShift Container Platform | Product Trial About the author Balazs Nemeth More like this File encryption and decryption made easy with GPG Red Hat OpenShift Service on AWS supports Capacity Reservations and Capacity Blocks for Machine Learning Technically Speaking | Platform engineering for AI agents Technically Speaking | Driving healthcare discoveries with AI Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Data Processing Units (DPUs) represent a significant evolution in datacenter architecture. By offloading infrastructure tasks like networking, security, and storage from the main CPU, they promise to unlock new levels of cloud capabilities. Red Hat OpenShift 4.20 delivers a major breakthrough in solving the primary challenge of DPU adoption: vendor lock-in. This new capability provides unified, vendor-agnostic support for different DPUs, all within a single cluster. This is the result of our focused, standards-based strategy. We began this journey by introducing tech preview support for the first DPU (the Intel IPU) in Red Hat OpenShift 4.19. By building on the open standards of the Open Programmable Infrastructure (OPI) project, our work for the 4.20 release has expanded this to include 2 additional DPUs: the Senao SX904 (Intel NetSec Accelerator) and the Marvell Octeon 10. This directly addresses the industry&amp;rsquo;s largest DPU challenge: the fragmented landscape. Until now, organizations have been forced into isolated, vendor-specific environments, each with proprietary tools, drivers, and APIs. This approach eliminates that fragmentation. As we&amp;rsquo;ve discussed in cloud-native enablement of DPUs , our goal is to make DPUs a smoothly integrated and abstracted component of Red Hat OpenShift. Our integration work is centered on a standard Red Hat OpenShift cluster.</description></item><item><title>Introducing the fully managed Amazon EKS MCP Server (preview)</title><link>https://kubermates.org/docs/2025-11-21-introducing-the-fully-managed-amazon-eks-mcp-server-preview/</link><pubDate>Fri, 21 Nov 2025 21:43:33 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-21-introducing-the-fully-managed-amazon-eks-mcp-server-preview/</guid><description>Introducing the fully managed Amazon EKS MCP Server (preview) Amazon EKS MCP Server tools Getting started with Amazon EKS MCP Server Prerequisites Configuration Tool access levels Scenario 1: Upgrading an EKS cluster with conversational AI Checking upgrade readiness Upgrade readiness report Key benefits of using EKS MCP for upgrades Scenario 2: Deploying applications through natural language Key EKS MCP tools in action Deployment summary Scenario 3: Troubleshooting infrastructure issues Key EKS MCP tools in action Troubleshooting summary Enhanced EKS console experience with Amazon Q Integrated AI assistance Contextual intelligence Conclusion About the authors Learn how to manage your Amazon Elastic Kubernetes Service (Amazon EKS) clusters through simple conversations instead of complex kubectl commands or deep Kubernetes expertise. This post shows you how to use the new fully managed EKS Model Context Protocol (MCP) Server in Preview to deploy applications, troubleshoot issues, and upgrade clusters using natural language with no deep Kubernetes expertise required. We’ll walk through real scenarios showing how conversational AI turns multi-step manual tasks into simple natural language requests. Teams managing Kubernetes workloads require expertise across container orchestration, infrastructure, networking, and security. While Large Language Models (LLMs) help developers write code and manage workloads, they’re limited without real-time cluster access. Generic recommendations based on outdated training data don’t meet real-world needs. Model Context Protocol (MCP) solves this by giving AI models secure access to live cluster data. MCP is an open-source standard that lets AI models securely access external tools and data sources for better context. It provides a standardized interface that enriches AI applications with real-time, contextual knowledge of EKS clusters, enabling more accurate and tailored guidance throughout the application lifecycle, from development through operations. Earlier this year, AWS was one of the first managed Kubernetes service providers to announce an MCP server, within a few months of the release of MCP protocol. Customers could install this EKS MCP Server on their machines for EKS and Kubernetes resource management. This initial, locally installable version of EKS MCP Server enabled us to rapidly validate our approach and gather valuable customer feedback, which has directly shaped today’s announcement.</description></item><item><title>Powered by DigitalOcean Hatch: How Ex-human uses GPU Droplets to Build Empathetic AI that Serves Customers</title><link>https://kubermates.org/docs/2025-11-21-powered-by-digitalocean-hatch-how-ex-human-uses-gpu-droplets-to-build-empathetic/</link><pubDate>Fri, 21 Nov 2025 16:36:21 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-21-powered-by-digitalocean-hatch-how-ex-human-uses-gpu-droplets-to-build-empathetic/</guid><description>Powered by DigitalOcean Hatch: How Ex-human uses GPU Droplets to Build Empathetic AI that Serves Customers From film to the future Innovation shouldn’t come at a high cost Musings from the founder About the author Try DigitalOcean for free Related Articles Hacktoberfest 2025 Comes to a Close Hacktoberfest 2025: How to Participate Hacktoberfest 2025: Celebrate All Things Open Source! By Martin Nguyen Updated: November 21, 2025 3 min read GPU Droplets are now DigitalOcean GradientAI GPU Droplets. Learn more about DigitalOcean GradientAI , our suite of AI products. Hatch is DigitalOcean’s global program for startups, which provides startups with credits and discounts on computing resources so they can build and scale without worrying about costs. In 2024, Hatch significantly revamped the program to better support those building businesses in the artificial intelligence sector, providing AI/ML startups with discounts for GPU Droplets and free access to DigitalOcean’s premium support in addition to credits. Ex-human , a startup building empathetic generative AI solutions backed by A16Z GAMES’ SPEEDRUN accelerator, chose the Hatch program to help serve their customers with AI models—read on to hear about their experience with Hatch so far. Inspired by the characters Sam from Her (2013) and Joi from Blade Runner 2049 (2017) , Artem Rodichev, founder and CEO of Ex-human, sought to build a future where humans interact with digital humans and characters on a daily basis, taking the form of friends, mentors, lovers, and more. After a stint at Replika, Rodichev saw that humans were building deeper connections with their AI companions and wanted to create a solution that was both viable for consumers and businesses. Ex-human brings their AI characters to life through text, images, video, and audio with unlimited scalability and customizability that opens up possibilities for numerous use cases. Partnering with companies such as Grindr, who are using Ex-human’s technology to bring an AI wingman to life, this technology has the opportunity to help humans form stronger bonds in both physical and digital worlds. Building AI technology requires large amounts of computing power, which can come at a high price, especially for a new business. Ex-human chose the Hatch program because it offers exclusive pricing for access to on-demand NVIDIA H100 GPU Droplets at $1.90/GPU/hour *. Despite the favorable cost, Ex-human finds that there are no tradeoffs with power and reliability, giving them a solution that works for their budget and delivers for their needs.</description></item><item><title>Hacktoberfest 2025 Comes to a Close</title><link>https://kubermates.org/docs/2025-11-21-hacktoberfest-2025-comes-to-a-close/</link><pubDate>Fri, 21 Nov 2025 15:45:11 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-21-hacktoberfest-2025-comes-to-a-close/</guid><description>Hacktoberfest 2025 Comes to a Close A change for Hacktoberfest Highlights of Hacktoberfest 2025 Hacktoberfest Hackathons Digital badges from Holopin A vibrant online community Help make Hacktoberfest better—tell us what you think. Gratitude for our sponsors and community partners About the author(s) Try DigitalOcean for free Related Articles Powered by DigitalOcean Hatch: How Ex-human uses GPU Droplets to Build Empathetic AI that Serves Customers Hacktoberfest 2025: How to Participate Hacktoberfest 2025: Celebrate All Things Open Source! By Haimantika Mitra and Bedabrata Bagchi Updated: November 21, 2025 4 min read October rolled around again, and with it came Hacktoberfest for its 12th year where contributors and open-source communities across the globe came together to build, contribute, and make tech more accessible for everyone. This year saw—as it does every year—developers, maintainers, and first-time contributors working side by side, creating solutions that matter to open source. A huge thank you to everyone who participated and made this Hacktoberfest special. We saw a lot of new faces join the celebration this year, and it’s exciting to see the community grow! What really stood out this year? The genuine enthusiasm. People were here to contribute, collaborate, and make a tangible impact on open source projects. That’s what Hacktoberfest is all about: getting more people involved in open source and working together to improve the software that powers our world. It’s fulfilling to see how Hacktoberfest continues to inspire developers and successfully achieves what we set out to do. Here’s what Mike Swift, Co-founder and CEO of MLH has to say about Hacktoberfest this year: “What I enjoyed most about this year’s Hacktoberfest was seeing next-gen developers step into open source with genuine curiosity and realize they could make meaningful contributions. They approached issues, learned from maintainers, and connected those experiences to the AI tools shaping their work. Being part of moments like that is incredibly rewarding and exactly why the MLH community exists. ” This year we increased the number of PRs from 4 to 6.</description></item><item><title>VCF Breakroom Chats Episode 74 – From VI Admin to Private Cloud Architect: New VCAP &amp; VCDX Certification Explained</title><link>https://kubermates.org/docs/2025-11-21-vcf-breakroom-chats-episode-74-from-vi-admin-to-private-cloud-architect-new-vcap/</link><pubDate>Fri, 21 Nov 2025 13:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-21-vcf-breakroom-chats-episode-74-from-vi-admin-to-private-cloud-architect-new-vcap/</guid><description>VCF Breakroom Chats Episode 74 About the VCF Breakroom Chat Series Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VCF Breakroom Chats Episode 75 - Breaking the GitOps Barrier: Continuous Delivery for Modern Apps with VCF 9 What’s Next for Cloud Native: Highlights from KubeCon North America 2025 Making Harbor Production-Ready: Essential Considerations for Deployment Welcome to the next episode of the VCF Breakroom Chats. Today, we are happy to present this vLog with Drew Nielsen, Head of Value Tooling and Technical Certification at Broadcom. In this episode, Drew Nielsen and Taka Uenishi talk about the new VCAPs and VCDX certification s, what they mean for your career, why they matter right now, and how getting certified positions you at the forefront of private cloud innovation. Want to learn more about VCF Automation? Check out the VCF Automation web page for more resources. Read this blog post: Private Cloud Redefined: Deliver a Unified Cloud Consumption Experience with VMware Cloud Foundation 9.0 Listen to our previous topics. This webinar series focuses on VMware Cloud Foundation (VCF). In this series, we share conversations with industry-recognized experts from Broadcom and with solution partners and customers. You’ll gain relevant insights whether you’re an IT practitioner, IT admin, cloud or platform architect, developer, DevOps, senior IT manager, IT executive, or AI/ML professional. If you are new to the series, please check out our previous episodes.</description></item><item><title>Guide to Amazon EKS and Kubernetes sessions at AWS re:Invent 2025</title><link>https://kubermates.org/docs/2025-11-21-guide-to-amazon-eks-and-kubernetes-sessions-at-aws-re-invent-2025/</link><pubDate>Fri, 21 Nov 2025 00:26:18 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-21-guide-to-amazon-eks-and-kubernetes-sessions-at-aws-re-invent-2025/</guid><description>Guide to Amazon EKS and Kubernetes sessions at AWS re:Invent 2025 Key breakout sessions Simplified Kubernetes management Generative AI &amp;amp; Agentic AI Deploying AI/ML workloads on Amazon EKS AI-powered Kubernetes development Platform engineering Performance and cost optimization Migration and modernization Security and observability Beyond the sessions Virtual participation Planning your week AWS re:Invent 2025 is back in Las Vegas from December 1-5 with the most comprehensive Kubernetes programming we’ve ever assembled. This year’s event features 48 dedicated sessions covering the full spectrum of cloud-native technologies and variety of use cases ranging from foundational cluster management to advanced AI/ML workload orchestration and deployment on Amazon Elastic Kubernetes Service (Amazon EKS). Amazon EKS and Kubernetes sessions are part of the Containers and Serverless (CNS) track this year, featuring the latest innovations that help customers build, run, and scale production-ready Kubernetes applications easily across any environment. The Amazon EKS and Amazon Elastic Container Registry (Amazon ECR) teams have curated an extensive catalog of track sessions that address all aspects of the real challenges teams face when running Kubernetes in production. From automation capabilities that reduce operational burden to architectural patterns for AI workloads and strategies for optimizing performance and costs, these sessions deliver practical guidance you can apply immediately to your Kubernetes environments. Be sure not to miss Amazon EKS Auto Mode which fully automates cluster management for compute, storage, and networking on AWS with a single click and Amazon EKS Hybrid Nodes which extends Kubernetes management to on-premises and edge environments for unified operations across distributed infrastructure. This year we also have a special session focused on the engineering that powers the scale of Amazon EKS to support ultra-scale clusters of up to 100K nodes and enable cutting-edge AI-powered developer experiences. These include Model Context Protocol (MCP) integration with Amazon EKS for context-aware Kubernetes workflows and support for deploying sophisticated multi-agent AI systems with secure agent-to-agent communication. With these innovations, customers from small startups to sophisticated enterprises have everything they need to get started with and run production-grade Kubernetes applications at scale on AWS. Whether you’re evaluating Kubernetes for the first time or architecting multi-region platforms, you’ll find sessions tailored to your needs. For a detailed explanation of the different session formats, check out the event page. Don’t miss our key breakout sessions featuring new launches and innovations: CNS205: The future of Kubernetes on AWS , featuring the strategic direction for Kubernetes on AWS and key innovations for running Kubernetes across cloud, on-premises, and edge.</description></item><item><title>Friday Five — November 21, 2025</title><link>https://kubermates.org/docs/2025-11-21-friday-five-november-21-2025/</link><pubDate>Fri, 21 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-21-friday-five-november-21-2025/</guid><description>Friday Five — November 21, 2025 Techaisle - Red Hat’s AI Platform Play: From &amp;ldquo;Any App&amp;rdquo; to &amp;ldquo;Any Model, Any Hardware, Any Cloud&amp;rdquo; Red Hat Introduces Project Hummingbird to Accelerate Cloud-Native Development and “Zero-CVE” Strategies OpenShift Virtualization now generally available on Azure Red Hat OpenShift Technically Speaking - Driving healthcare discoveries with AI ft. Jianying Hu LinuxInsider - Red Hat’s Evolution: How a Subsidiary Became an AI Powerhouse About the author Red Hat Corporate Communications More like this Forging the open path: How Red Hat engineering is adopting AI and what it means for open source Red Hat OpenShift Virtualization: The strategic platform for virtualization customers Technically Speaking | Driving healthcare discoveries with AI What Is Product Security? | Compiler Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Red Hat&amp;rsquo;s AI 3 strategy aims to be the &amp;ldquo;Linux of enterprise AI,&amp;rdquo; offering an open, standardized platform to solve the complexity and cost of AI adoption by connecting any model, any hardware, any cloud. Learn more Project Hummingbird is an early access program providing minimal, hardened container images for subscription customers. The goal is to accelerate cloud-native development by offering a &amp;ldquo;zero-CVE&amp;rdquo; foundation, significantly reducing the attack surface and strengthening the software supply chain without sacrificing speed or agility. Learn more OpenShift Virtualization on Azure Red Hat OpenShift (ARO) allows organizations to run traditional VMs and modern containers on a single, unified Kubernetes platform on Azure. It simplifies operations, enables lower-risk migration and provides a streamlined path for modernization and AI adoption. Learn more How is AI tackling our most fundamental challenges in medicine? Explore the new frontier of AI in healthcare and domain-specific foundation models as Red Hat CTO Chris Wright is joined by Dr. Jianying Hu, IBM Fellow and Global Science Leader of AI for Health. Learn more Leveraging its open source DNA, Red Hat is building a foundational platform to fuel the next wave of AI model and agent development and utilization in enterprise and cloud data centers. Learn more Red Hat is the world’s leading provider of enterprise open source software solutions, using a community-powered approach to deliver reliable and high-performing Linux, hybrid cloud, container, and Kubernetes technologies. Red Hat helps customers integrate new and existing IT applications, develop cloud-native applications, standardize on our industry-leading operating system, and automate, secure, and manage complex environments. Award-winning support, training, and consulting services make Red Hat a trusted adviser to the Fortune 500.</description></item><item><title>Monitoring network performance on Amazon EKS using AWS Managed Open-Source Services</title><link>https://kubermates.org/docs/2025-11-20-monitoring-network-performance-on-amazon-eks-using-aws-managed-open-source-servi/</link><pubDate>Thu, 20 Nov 2025 18:48:46 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-20-monitoring-network-performance-on-amazon-eks-using-aws-managed-open-source-servi/</guid><description>Monitoring network performance on Amazon EKS using AWS Managed Open-Source Services Architecture diagram Walkthrough Prerequisites Setup Network performance testing and visualization: Key considerations Managing dashboards and data sources with Grafana Operator Using ADOT for telemetry collection Cleanup Conclusion About the authors As organizations scale their microservices architectures on Amazon Elastic Kubernetes Service (Amazon EKS) , platform and development teams face mounting challenges in monitoring network performance across distributed workloads. While VPC Flow Logs provide visibility into IP traffic, they lack the Kubernetes context needed to correlate network flows to specific pods, services, and namespaces. This makes it difficult to diagnose connectivity issues, identify packet drops, or investigate security incidents. Platform teams struggle to answer critical questions such as: “ Which services are communicating with each other? Where are the latency bottlenecks? Are there any security policy violations?” Without Kubernetes-enriched network telemetry integrated into their observability and Security information and event management (SIEM) systems, teams spend excessive time troubleshooting network-related issues and lack the real-time visibility needed to ensure optimal performance and security across their EKS environments. In this post, we will discuss how to monitor the network performance of your workloads running in Amazon EKS clusters using new advanced network observability features which are a part of Container Network Observability in EKS. This includes capturing network performance metrics and exporting them to AWS Managed Open-Source services such as Amazon Managed Service for Prometheus , Amazon Managed Grafana , etc. You can leverage the same approach to integrate with third-party observability solutions such as Datadog, New Relic, etc. , or self-managed open-source tools like Prometheus. Amazon EKS introduced new advanced network observability features that give you the ability to dynamically visualize and quickly understand the landscape, performance and security of the network environment of your Kubernetes clusters. At a cluster level, it provides you with a Service Map that depicts end-to-end visibility of network traffic flows for workloads in the cluster (east ↔ west traffic). Alongside this, you can access Network Flow Analysis which provide more granular information around application network activity and the network security posture for your workloads. Additionally, you can export Kubernetes-enriched network performance metrics from all your EKS clusters to be analyzed in your preferred monitoring solution or SIEM.</description></item><item><title>Upgrading VMware Cloud Foundation 5.2 to 9.0: Webinar Takeaways</title><link>https://kubermates.org/docs/2025-11-20-upgrading-vmware-cloud-foundation-5-2-to-9-0-webinar-takeaways/</link><pubDate>Thu, 20 Nov 2025 17:18:25 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-20-upgrading-vmware-cloud-foundation-5-2-to-9-0-webinar-takeaways/</guid><description>Webinar Highlights Key Takeaways On-Demand Replay Need Help? Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Upgrading VMware Cloud Foundation 5.2 to 9.0: Webinar Takeaways VMware Cloud Foundation is the Gold Standard for Virtual Desktop Infrastructure VCF Breakroom Chats Episode 73 - Next-Gen Data Services: The DBaaS Revolution with VCF 9 VMware Cloud Foundation (VCF) 9.0 represents a significant leap forward in architectural unification and operational management. While the upgrade from 5. x is designed to be streamlined, it introduces mandatory changes in management methodologies and requires careful, phased execution. I’ve previously shared the typical process that VCF Professional Services uses to perform upgrades to VCF 9.0. In my recent webinar with Brent Douglas , I walked through the process (with demos!) of upgrading VCF 5.2 to VCF 9.0. You can find the full webinar replay at the end of this blog, but here are some of the highlights. The webinar starts with an overview of VCF 9.0. This release introduces a unified platform designed to manage both traditional VMs and modern container-based Kubernetes workloads from a single foundation. The new “fleet” construct fundamentally changes how multiple VCF instances are managed, enabling centralized automation and operations across geographically dispersed or logically grouped private clouds. Expanded multi-site, multi-rack, and stretched cluster topologies are now officially supported and documented. VMware SDDC Manager, although currently still available, is being deprecated. Its functions, such as licensing, certificates, and password management, have been migrated to VMware Cloud Foundation Operations which is now a mandatory component in VCF 9.0.</description></item><item><title>A Detailed Look at the Calico Ingress Gateway</title><link>https://kubermates.org/docs/2025-11-20-a-detailed-look-at-the-calico-ingress-gateway/</link><pubDate>Thu, 20 Nov 2025 17:01:42 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-20-a-detailed-look-at-the-calico-ingress-gateway/</guid><description>The Role of the Calico Ingress Gateway in Modern Kubernetes So Why an Ingress Gateway? What is the Calico Ingress Gateway? Why Kubernetes Gateway API Matters Now More Than Ever Why Calico Ingress Gateway? How Calico Ingress Gateway Fits into the Calico Platform Calico Ingress Gateway Capabilities Based on the Envoy Gateway Advanced Traffic Management Resilience Features Role-Based Access Control (RBAC) Comprehensive Observability Easy Deployment and Configuration Future Developments for Calico Ingress Gateway Conclusion: Preparing for a Post–Ingress NGINX World The Kubernetes community recently announced that Ingress NGINX, one of the most widely used Ingress controllers, will be retired. This change means teams need to plan for a secure, modern, and future-proof alternative for managing Kubernetes traffic. The Kubernetes SIG Network and the Security Response Committee confirmed that the project will only receive basic maintenance until March 2026. After that, there will be no new releases, bug fixes, or security updates. For organizations that have depended on Ingress NGINX for many years, this is more than a routine update. It represents a major change in how Kubernetes ingress traffic will need to be managed going forward. This raises an important question: What should replace Ingress NGINX in a way that is secure, modern, standardized, and reliable for the long term? As the industry moves toward the Kubernetes Gateway API and expectations for security and reliability grow, this is the right time to adopt an ingress solution designed for the future. Calico Ingress Gateway is a 100% upstream distribution of the Envoy Gateway, supported by Tigera’s enterprise expertise, and offers a smooth and dependable path forward. Managing traffic in Kubernetes environments presents serious security and operational challenges. Traditional ingress solutions lack flexibility, rely on proprietary configurations, and offer limited traffic control, creating security gaps and inefficiencies. What’s needed is a more flexible, scalable, and policy-driven approach to ingress traffic management. Enter Calico Ingress Gateway—built to eliminate these limitations while enhancing security, visibility, and control over ingress traffic at scale.</description></item><item><title>Operationalizing the Edge with VMware Cloud Foundation</title><link>https://kubermates.org/docs/2025-11-20-operationalizing-the-edge-with-vmware-cloud-foundation/</link><pubDate>Thu, 20 Nov 2025 16:22:21 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-20-operationalizing-the-edge-with-vmware-cloud-foundation/</guid><description>Now Available: A Fully Automated Deployment Script for Edge Key Benefits of the Automation Script What to Expect from the Script This automation script will cover the following key functions: A Unified Cloud Operating Model from Core to Edge Learn More: Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Operationalizing the Edge with VMware Cloud Foundation VMware Cloud Foundation is the Gold Standard for Virtual Desktop Infrastructure VCF Breakroom Chats Episode 73 - Next-Gen Data Services: The DBaaS Revolution with VCF 9 As organizations extend operations beyond the data center, the edge has become a vital part of their digital strategy. Whether in retail outlets, manufacturing plants, healthcare facilities, energy plants or remote branch offices, IT teams want infrastructure that delivers cloud like agility, consistency and unified operations in the most resource-constrained environments. With VMware Cloud Foundation 9.0 for Edge, a single-host edge site deployment powered by VMware vSphere Supervisor enables organizations to run both VMs and container workloads on the same platform, eliminating the need for separate infrastructure and reducing cost and operational complexity in resource-constrained edge environments. VCF 9.0 also introduces a GitOps model powered by Argo CD Operator, using a centrally managed Git repository as the source of truth, organizations can drive consistent, repeatable and automatically reconciled deployments of VMs and container workloads across multiple distributed edge sites from a single repository simplifying and scaling management across thousands of edge sites. The journey to modernize the edge, as we’ve explored in our previous posts, has paved the way for robust, simplified infrastructure. We’ve introduced the power of the single-node vSphere Supervisor architecture in our series, starting with the Modernizing Your Edge with Single Node vSphere Supervisor blogs. Following that, we demonstrated how application deployment at scale is achieved using a GitOps approach with Argo CD in VCF 9.0 for Edge – Automating App Deployment at Scale with GitOps Using Argo CD. Now, it’s time to bring these concepts together and address the crucial next step: accelerating the infrastructure deployment itself. A core challenge for distributed edge sites is the consistent, rapid, and error-free provisioning of the underlying hardware and software stack. A fully automated deployment script is now available, enabling customers to deploy a single-host vSphere Supervisor environment with Argo CD automation. Automating deployment of vSphere Supervisor and Argo CD at scale minimizes setup time, standardizes configurations, and improves consistency across distributed edge locations. This new automation script streamlines everything from initial cluster configuration to continuous application delivery, helping organizations operationalize the edge faster and more reliably.</description></item><item><title>Mapping VLAN Tags to Virtual Private Cloud Subnets</title><link>https://kubermates.org/docs/2025-11-20-mapping-vlan-tags-to-virtual-private-cloud-subnets/</link><pubDate>Thu, 20 Nov 2025 15:49:06 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-20-mapping-vlan-tags-to-virtual-private-cloud-subnets/</guid><description>Traditional Guest VLAN Tagging (Trunking) Guest VLAN Tagging on NSX Overlay Segments Implementing VLAN-to-Subnet Mapping in a VPC Key Benefits of Subnet-Level Mapping Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Mapping VLAN Tags to Virtual Private Cloud Subnets Unified Authentication in VMware Cloud Foundation SDK 9.0: Seamless authentication across vSphere and vSAN APIs NVMe Memory Tiering Design and Sizing on VMware Cloud Foundation 9 Part 2: Design for Security, Redundancy, and Scalability In a typical virtualized environment, a VM sends untagged traffic from its virtual NIC (vNIC). By default, the virtual switch drops any traffic that carries an 802. 1Q VLAN tag. To allow this traffic, a feature known as Guest VLAN Tagging (GVT) must be explicitly enabled on the vNIC’s port connection. Let’s review how this is traditionally handled before diving into the simple and powerful method available in Virtual Private Cloud (VPC) subnets. On a vSphere Distributed Port Group (dvPortGroup) or a VLAN-backed NSX segment, GVT is enabled by configuring the port as a trunk. This involves specifying a range of allowed VLAN IDs instead of just one. In this trunking model, traffic sent by a VM with VLAN tag X is forwarded onto the physical network, still tagged with VLAN X. The physical network infrastructure is then responsible for handling that tagged traffic. Enabling GVT on an NSX overlay segment (configuring a VLAN range) works similarly at first. Traffic from a VM with a VLAN tag is injected into the overlay segment while retaining its tag. Because NSX sees tagged frames, it treats this as simple Layer 2 traffic.</description></item><item><title>An architectural decision: Containers on bare metal or on virtual machines</title><link>https://kubermates.org/docs/2025-11-20-an-architectural-decision-containers-on-bare-metal-or-on-virtual-machines/</link><pubDate>Thu, 20 Nov 2025 13:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-20-an-architectural-decision-containers-on-bare-metal-or-on-virtual-machines/</guid><description>Posted on November 20, 2025 by Pankaj Gupta, VCF Division, Broadcom CNCF projects highlighted in this post Building and running modern applications begins with selecting Kubernetes distribution as a baseline. Once a platform team has selected its orchestration layer, one of the next architectural choices involves the deployment architecture where that cluster will run. Containers can be deployed directly on either bare metal servers or virtual machines. This article examines the characteristics, tradeoffs, and community learnings around deploying containers on bare metal compared to virtual machines. Historically, running workloads in containers on bare metal appealed to organizations that prioritized maximum performance and minimal infrastructure overhead. By bypassing the hypervisor layer, containers could directly access compute and storage resources. However, advancements in hypervisor technology have significantly improved the performance and efficiency of virtualized environments, making containers on virtual machines (VMs) viable for production workloads with added operational benefits and flexibility. As IT requirements have grown in recent years, platform teams now face expanded responsibilities, including enforcing stricter security policies, reducing fault domains for higher application availability, supporting multiple versions of conformant Kubernetes, and meeting tighter service-level agreements (SLAs). These are themes frequently discussed in CNCF TAG Runtime and TAG Security , particularly around multi-tenancy models, workload isolation, and lifecycle management. These modern demands and technological enhancements have renewed community discussions on where clusters should run and how platform teams can meet SLA, security, and multi-version requirements IT practitioners should consider the following factors when making this decision: Historically, bare metal offered a performance edge. Direct hardware access reduced latency and overhead, giving it an advantage in CPU- or GPU-intensive workloads. Recent benchmark studies indicate that the historical performance gap between bare metal and virtualized environments is now negligible.</description></item><item><title>Integrating Red Hat Lightspeed in 2025: From observability to actionable automation</title><link>https://kubermates.org/docs/2025-11-20-integrating-red-hat-lightspeed-in-2025-from-observability-to-actionable-automati/</link><pubDate>Thu, 20 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-20-integrating-red-hat-lightspeed-in-2025-from-observability-to-actionable-automati/</guid><description>Integrating Red Hat Lightspeed in 2025: From observability to actionable automation Security-focused, scalable API access with service account authentication Introducing the MCP Server: A new way to connect Red Hat Lightspeed and AI workflows Expanded integrations across ITSM, observability, and automation ServiceNow flow templates for Red Hat Lightspeed PagerDuty integration Event-Driven Ansible collection Red Hat Satellite and Ansible Automation Platform integrations Putting it all together Looking ahead Red Hat Ansible Automation Platform | Product Trial About the author Jerome Marc More like this Red Hat Ansible Certified Collection for amazon. ai: Automate AI Infrastructure Setting up logrotate in Linux Technically Speaking | Taming AI agents with observability You Can’t Automate Collaboration | Code Comments Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Red Hat Lightspeed (formerly Red Hat Insights) has long helped operations teams detect risks, open tickets, and share findings with the right tools, connecting proactive intelligence to everyday workflows. Much has changed, not only in Red Hat Lightspeed itself, but also in how organizations are using it. Across industries, teams have built custom dashboards, reporting portals, and IT service management (ITSM) integrations powered by the Red Hat Lightspeed API. Others have connected Red Hat Lightspeed data into continuous integration and delivery (CI/CD) pipelines, monitoring environments, and automated remediation workflows, turning operational intelligence into action. Today, that vision has expanded. Integrations have matured, authentication has evolved, and new automation paths now let you go beyond “alerting” toward closed-loop, intelligent remediation—where Red Hat Lightspeed findings can directly trigger actions across your automation and observability stack. This post explores what’s new in 2025, including token-based authentication, updated integrations, Event-Driven Ansible, and the new Model Context Protocol (MCP) server, and how these security-focused capabilities help you connect insight to action. The Red Hat Lightspeed team has modernized its API authentication model to support service account–based, token-driven access , marking a major step forward in both security and automation flexibility. Previously, API integrations relied on user credentials and basic authentication. While functional at the time, this approach made it difficult to automate at scale or enforce consistent access controls across multiple systems. Basic authentication has since been deprecated and is no longer supported, reinforcing a shift toward more security-focused, scalable, token-based access.</description></item><item><title>More than meets the eye: Behind the scenes of Red Hat Enterprise Linux 10 (Part 3)</title><link>https://kubermates.org/docs/2025-11-20-more-than-meets-the-eye-behind-the-scenes-of-red-hat-enterprise-linux-10-part-3/</link><pubDate>Thu, 20 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-20-more-than-meets-the-eye-behind-the-scenes-of-red-hat-enterprise-linux-10-part-3/</guid><description>More than meets the eye: Behind the scenes of Red Hat Enterprise Linux 10 (Part 3) 2023 (18 months until Summit 2025) 2024 (12 months until Summit 2025) More like this Alliander modernises its electricity grid with Red Hat for long-term reliability in balance with rapid innovation Ping command basics for testing and troubleshooting OS Wars_part 1 | Command Line Heroes OS Wars_part 2: Rise of Linux | Command Line Heroes Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share This series takes a look at the people and planning that went into building and releasing Red Hat Enterprise Linux 10. From the earliest conceptual stages to the launch at Red Hat Summit 2025, we’ll hear firsthand accounts of how RHEL 10 came into being. Part 1 | Part 2 In our previous installment of the story of how Red Hat Enterprise Linux (RHEL) 10 came to be, we learned about a new approach to building the platform. In part 3, the team focuses on the delicate balancing act of keeping thousands of moving parts in sync while features like image mode and RHEL Lightspeed (and the stories we’ll tell about them) start to take shape. Mike McGrath, vice president, Core Platforms Engineering This is when things really start to kick into overdrive. The big thing that we watch for internally is what we call the compose. The compose is basically a nightly—or roughly, it could be a few times a day—build of the latest and greatest in RHEL. That’s what I always look to to see how things are going. Major Hayden, senior principal software engineer It’s kind of like a Jenga tower where all the pieces are slightly different sizes. And so you may look at it and say, ‘Well, my piece is really small. You can poke this thing out and nothing will fall. ’ And everyone around you is saying, ‘I believe you, but we’re going to have to check.</description></item><item><title>Why is OpenShift Virtualization becoming the strategic platform for virtualization customers?</title><link>https://kubermates.org/docs/2025-11-20-why-is-openshift-virtualization-becoming-the-strategic-platform-for-virtualizati/</link><pubDate>Thu, 20 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-20-why-is-openshift-virtualization-becoming-the-strategic-platform-for-virtualizati/</guid><description>Why is OpenShift Virtualization becoming the strategic platform for virtualization customers? Additional resources 15 reasons to adopt Red Hat OpenShift Virtualization About the authors Sachin Mullick Doron Fediuck More like this Red Hat OpenShift Virtualization 4.20: Hybrid cloud-flexibility and enhanced VM management Alliander modernises its electricity grid with Red Hat for long-term reliability in balance with rapid innovation Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share We are 2 years in on a strategic shift in virtualization. Although this shift created some initial panic, most customers have taken advantage of this shift as an opportunity to make a more strategic decision on where they want to go on their virtualization journey. Red Hat OpenShift Virtualization is built on innovation, where customer needs inspire our engineers to deliver high-impact solutions to help them move from concept to general availability in a matter of months, not years. It’s no surprise that this commitment to rapid, customer-focused innovation earned OpenShift Virtualization the 2025 CRN Innovator Award for the launch of Red Hat OpenShift Virtualization Engine , a virtualization-only edition for Red Hat OpenShift. The engineering team velocity is one of the core reasons OpenShift Virtualization has become a trusted platform for many of the top enterprises across the world. Customer feedback backs the confidence in our platform, with one customer calling out that the product enhancements they had asked for at the beginning of a proof of concept (POC) were actually delivered by the time the POC was finishing. To us, that’s trusted, enterprise-ready innovation! Beyond comprehensive engineering skills, this velocity requires 2 more things: A profound understanding of the space and customer goals , which we provide through our engineers meeting with customers, getting real time feedback on platform capabilities, and together, shaping the outcomes for a given project. Mature technologies and knowing how to harness them into a cloud-native solution , driven by our nearly 2 decades of KVM virtualization experience and many years of Kubernetes expertise. Combined, our engineering teams are capable of reimagining a virtualization suite which augments and complements Kubernetes at every technology and stack layer. Additionally, combining Red Hat OpenShift and Red Hat Ansible Automation Platform can increase customers’ overall level of automation. OpenShift features like GitOps help automate the infrastructure management of adding nodes and maintenance of the infrastructure. Ansible Automation Platform collections automate Day 2 VM operations such as patch management.</description></item><item><title>VMware Cloud Foundation is the Gold Standard for Virtual Desktop Infrastructure</title><link>https://kubermates.org/docs/2025-11-19-vmware-cloud-foundation-is-the-gold-standard-for-virtual-desktop-infrastructure/</link><pubDate>Wed, 19 Nov 2025 19:10:37 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-19-vmware-cloud-foundation-is-the-gold-standard-for-virtual-desktop-infrastructure/</guid><description>Security and Compliance Efficiency and Resource Optimization Unfettered Performance Memory Storage Network Graphics Day-2 Operations Use Case: Developer Workbench as a Service Additional Use Cases Introducing the “Omnissa Horizon 8 on VMware Cloud Foundation” Reference Architecture Wrapping Up Additional Resources Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Upgrading VMware Cloud Foundation 5.2 to 9.0: Webinar Takeaways Operationalizing the Edge with VMware Cloud Foundation VMware Cloud Foundation is the Gold Standard for Virtual Desktop Infrastructure VMware Cloud Foundation (VCF) delivers unmatched benefits for virtual desktop infrastructure (VDI). Even after the spin-out of VMware’s End User Computing (EUC) business group into Omnissa , the fundamentals of this combined solution remain intact. However, the release of VCF 9.0 deserves revisiting those fundamentals to expose how the platform holds up. VCF 9.0 consolidates the private-cloud substrate (vSphere, vSAN, NSX) with cloud automation (VCF Automation), integrated Kubernetes (VMware vSphere Kubernetes Service / VKS) and other advanced services for VCF such as VMware Private AI Services and VMware Data Services Manager (DSM). These innovations, among many others, benefit the Omnissa Horizon VDI solution by providing an intrinsically secure, optimized and scalable foundation for the most demanding virtual desktops. Running Horizon on VCF 9.0 allows customers to benefit from the full set of services of a unified private cloud platform. VCF delivers workload domains, orchestrated upgrades, VPC-based network isolation, and a modern consumption API. It’s a platform that treats desktops as first-class workloads. Security is where VCF immediately shines. Use NSX firewall to wrap a least-privilege policy around Horizon Connection Servers, UAGs, and desktop pools without hair-pinning traffic into external firewalls. VCF 9.0’s VPC constructs enable stamping out a repeatable network perimeter for each Horizon function: Edge (UAG) Brokering (Connection Servers) Desktops Shared services These protections scale with the fleet instead of complicating it. VCF 9.0 also introduces a suite of integrated security and compliance features critical for VDI environments: Centralized network policy management with NSX bolster lateral traffic protection for sensitive VDI desktops, aligning with stringent compliance frameworks.</description></item><item><title>VCF Breakroom Chats Episode 73 –  Next-Gen Data Services: The DBaaS Revolution with VCF 9</title><link>https://kubermates.org/docs/2025-11-19-vcf-breakroom-chats-episode-73-next-gen-data-services-the-dbaas-revolution-with-/</link><pubDate>Wed, 19 Nov 2025 13:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-19-vcf-breakroom-chats-episode-73-next-gen-data-services-the-dbaas-revolution-with-/</guid><description>VCF Breakroom Chats Episode 73 About the VCF Breakroom Chat Series Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Upgrading VMware Cloud Foundation 5.2 to 9.0: Webinar Takeaways Operationalizing the Edge with VMware Cloud Foundation VMware Cloud Foundation is the Gold Standard for Virtual Desktop Infrastructure Welcome to the next episode of the VCF Breakroom Chats. Today, we are happy to present this vLog with Michael Gandy, Technology Product Management at Broadcom. In this episode, Michael Gandy and Taka Uenishi explore how VCF Automation and DSM simplify Database as a Service (DBaaS) for the modern private cloud and how it ties into broader IT trends. Want to learn more about VCF Automation? Check out the VCF Automation web page for more resources. Read this blog post: Private Cloud Redefined: Deliver a Unified Cloud Consumption Experience with VMware Cloud Foundation 9.0 Listen to our previous topics. This webinar series focuses on VMware Cloud Foundation (VCF). In this series, we share conversations with industry-recognized experts from Broadcom and with solution partners and customers. You’ll gain relevant insights whether you’re an IT practitioner, IT admin, cloud or platform architect, developer, DevOps, senior IT manager, IT executive, or AI/ML professional. If you are new to the series, please check out our previous episodes.</description></item><item><title>What is platform engineering?</title><link>https://kubermates.org/docs/2025-11-19-what-is-platform-engineering/</link><pubDate>Wed, 19 Nov 2025 12:05:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-19-what-is-platform-engineering/</guid><description>History and evolution of platform engineering Platform as a product Comparison with other traditional models (ticket ops vs. self-service) The benefits of platform engineering Problems it seeks to solve Benefits: Posted on November 19, 2025 by Natália Granato, CNCF Ambassador Platform engineering is a discipline focused on building and maintaining software development platforms that provide self-service for developer teams, offering the necessary infrastructure for provisioning an application, for example. The entire flow of development, testing, documentation, deployment, rollback, etc. , can happen through developer self-service. It emerged from the need to overcome the limitations of traditional software development, which was often characterized by slow cycles, isolated teams, silos, and complex infrastructures. The main goal of Platform Engineering is to improve the developer experience (DevEx) and optimize software delivery. In essence, platform engineering seeks to: Provide standardized tools and services: Create a set of tools, services, and workflows that developers can easily use to build, deploy, and manage their applications. These tools can include IDEs, CLIs, project templates, code versioning, CI/CD pipelines, container orchestration tools, configuration management tools, and observability systems. Reduce complexity: Abstract the complexity of the underlying infrastructure and operations, allowing developers to focus on code and business logic. The growing complexity of infrastructure, the cloud computing era with different providers and vendors, multi-cloud adoption, and the proliferation of tools and artifacts in the “everything as code” era led to an increase in developers’ cognitive load. The lack of design patterns also became a problem. Increase productivity: Accelerate the development and deployment cycle, eliminating bottlenecks and repetitive tasks.</description></item><item><title>Unified Authentication in VMware Cloud Foundation SDK 9.0: Seamless authentication across vSphere and vSAN APIs</title><link>https://kubermates.org/docs/2025-11-19-unified-authentication-in-vmware-cloud-foundation-sdk-9-0-seamless-authenticatio/</link><pubDate>Wed, 19 Nov 2025 09:54:31 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-19-unified-authentication-in-vmware-cloud-foundation-sdk-9-0-seamless-authenticatio/</guid><description>Authentication dilemma with vSphere APIs The unified authentication Sample Code What is happening behind the scenes? Resources Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Upgrading VMware Cloud Foundation 5.2 to 9.0: Webinar Takeaways Mapping VLAN Tags to Virtual Private Cloud Subnets VMware Cloud Foundation is the Gold Standard for Virtual Desktop Infrastructure VMware Cloud Foundation (VCF) 9 introduces a Unified VCF Software Development Kit (SDK) for Python and Java. The key highlight of this release was the unification of all the major components into a single deliverable package to deliver simple, extensible, and consistent automation experience across the stack. Currently, the following VCF components are included as a part of the unified VCF SDK and rest of the components will be made available very soon- VMware vSphere VMware vSAN VMware vSAN Data Protection VMware SDDC Manager If you are reading the term Unified VCF SDK for the first time, do not worry about it. You can visit some of the existing content such as the announcement blog post or visit the VMware Explore Las Vegas 2025 session to get yourself up to speed with all the enhancements introduced with VCF APIs and the VCF SDK. In this blog post I will focus on a small yet very effective enhancement to our vSphere API authentication and explain its usage across the vSphere and vSAN APIs. For context, VMware vSphere exposes two major categories of APIs: vSphere Web Services APIs and vSphere Automation APIs. The primary difference between the two lies in their underlying communication protocol. vSphere Web Services APIs use the Simple Object Access Protocol (SOAP) protocol. vSphere Automation APIs use Representational State Transfer (REST). If you have ever explored the Managed Object Browser (MOB) from the vSphere Client, you were interacting with the Web Services (SOAP) APIs. In contrast, the API Explorer in the Developer Center exposes the vSphere Automation (REST) APIs. For building complete solutions, developers often need to work with both sets of APIs.</description></item><item><title>Forging the open path: How Red Hat engineering is adopting AI and what it means for open source</title><link>https://kubermates.org/docs/2025-11-19-forging-the-open-path-how-red-hat-engineering-is-adopting-ai-and-what-it-means-f/</link><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-19-forging-the-open-path-how-red-hat-engineering-is-adopting-ai-and-what-it-means-f/</guid><description>Forging the open path: How Red Hat engineering is adopting AI and what it means for open source The &amp;ldquo;why:&amp;rdquo; AI, Red Hat, and the open source imperative The rollout: Choice and productivity The sandbox: Internal experimentation The platform: Running it the Red Hat way The lessons: AI demands better engineering The journey ahead Get started with AI for enterprise: A beginner’s guide About the authors Chris Wright Josh Boyer More like this Red Hat Satellite 6.18: New AI, Management, and Security Capabilities GPU-as-a-Service for AI at scale: Practical strategies with Red Hat OpenShift AI Technically Speaking | Driving healthcare discoveries with AI Technically Speaking | Security for the AI supply chain Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Over the past several months, Red Hat has been diving into one of the most significant shifts in our industry: the practical, large-scale adoption of generative AI (gen AI) within a major engineering organization. We are not unique in this journey, but at Red Hat, &amp;ldquo;in the open&amp;rdquo; isn&amp;rsquo;t just a development model—it&amp;rsquo;s our culture. We believe it&amp;rsquo;s important to share what we&amp;rsquo;re doing, what we&amp;rsquo;re learning, and how we see this shaping the future of open source collaboration. To be clear, this isn&amp;rsquo;t a publicity piece about a single, perfect tool or an instant success. It&amp;rsquo;s a story about culture, choice, and how AI—when grounded in solid engineering principles—becomes a powerful accelerator for open source innovation. It&amp;rsquo;s impossible to ignore AI’s hype, but what’s obscured by all that buzz is a fundamental truth—AI is a new layer of the technology stack. It&amp;rsquo;s a capability multiplier, much like compilers, the IDE, and the cloud were before it. For Red Hat, our &amp;ldquo;why&amp;rdquo; is twofold: Internal acceleration : We have thousands of engineers working on millions of lines of code across thousands of upstream projects. The potential for our engineers to augment their work with AI to reduce toil, accelerate problem-solving, and automate mundane tasks is massive. We owe it to our engineers to provide them with the best tools to do their best work. The open source future : More importantly, Red Hat’s mission is to be the defining technology company of the 21st century, and we believe open source is the best way to build technology. If AI remains a proprietary &amp;ldquo;black box&amp;rdquo; world, it runs counter to everything we stand for.</description></item><item><title>What's new in Red Hat OpenShift Virtualization 4.20</title><link>https://kubermates.org/docs/2025-11-19-what-s-new-in-red-hat-openshift-virtualization-4-20/</link><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-19-what-s-new-in-red-hat-openshift-virtualization-4-20/</guid><description>What&amp;rsquo;s new in Red Hat OpenShift Virtualization 4.20 Hybrid cloud flexibility OpenShift Virtualization on Microsoft Azure Red Hat OpenShift OpenShift Virtualization on Oracle Cloud Infrastructure Two-node OpenShift with arbiter Support for ARM Intuitive installation Upgrades and workload mobility Advancing VM operations Simplified management Supercharge VM migration with storage offloading Learn More Upstream project and community Become a certified specialist 15 reasons to adopt Red Hat OpenShift Virtualization About the authors Peter Lauterbach Courtney Grosch More like this Red Hat OpenShift Virtualization: The strategic platform for virtualization customers Alliander modernises its electricity grid with Red Hat for long-term reliability in balance with rapid innovation Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Red Hat OpenShift Virtualization 4.20 is now generally available , helping organizations modernize their virtualization strategy with greater speed and confidence through expanded platform availability, enhanced virtual machine (VM) management, and new capabilities for hybrid cloud infrastructure. With networking enhancements, optimized live migration, and improved user experience, OpenShift Virtualization 4.20 enables teams to deliver more agile, cloud-ready, and consistent operations wherever they choose to run their workloads. Our collaborations with customers and partners on their migration journeys have helped deliver significant value. Get an inside look from leaders at Ford and Emirates NBD on how their unified modern platform helped power critical workloads for their business. Additionally, you&amp;rsquo;ll learn how both companies benefit from an efficient, security-focused, and reliable infrastructure that integrates with their existing infrastructure, IT tooling, and development platforms, positioning them for success with newer projects like AI adoption and enablement. The strategic shift: How Ford and Emirates NBD stopped paying the complexity tax for virtualization With the release of OpenShift Virtualization 4.20, customers can more easily extend their VM workloads from on-premise environments to multiple clouds, giving them greater flexibility as they modernize their infrastructure. As organizations work to simplify their platforms and reduce operational complexity, the general availability of OpenShift Virtualization on Azure Red Hat OpenShift offers a security-focused path forward. Built and backed jointly by Red Hat and Microsoft, this solution enables customers to migrate and run VMs alongside containers on a single, enterprise-grade platform. Tools such as the migration toolkit for virtualization and Red Hat Ansible Automation Platform help automate migrations from legacy systems with minimal disruption. This collaboration streamlines operations, strengthens security, and helps optimize cloud spending—empowering teams to modernize faster across hybrid and multicloud environments. Azure NetApp Files is also available in public preview as a storage option for this solution. Watch the demo below to see how you can now deploy your VMs on Azure Red Hat OpenShift.</description></item><item><title>What's new in the migration toolkit for virtualization 2.10</title><link>https://kubermates.org/docs/2025-11-19-what-s-new-in-the-migration-toolkit-for-virtualization-2-10/</link><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-19-what-s-new-in-the-migration-toolkit-for-virtualization-2-10/</guid><description>What&amp;rsquo;s new in the migration toolkit for virtualization 2.10 Faster migrations, greater confidence with storage offloading Updates to storage offloading in MTV 2.10 Additional updates in MTV 2.10 Learn more 15 reasons to adopt Red Hat OpenShift Virtualization About the authors Jochen Schroder Carolyn May More like this Red Hat OpenShift Virtualization: The strategic platform for virtualization customers Red Hat OpenShift Virtualization 4.20: Hybrid cloud-flexibility and enhanced VM management Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The migration toolkit for virtualization 2.10 (MTV) is now generally available and expands on capabilities introduced in the recent release to better support your virtual machine (VM) migration journey with minimal downtime and reduced risk. With updates to storage offloading, your organization can efficiently plan and execute your VM migrations with MTV 2.10. Speed is a critical component of a VM migration. Storage offloading is now available in Technology Preview in MTV 2.10. For large-scale migrations, network bandwidth is often a primary bottleneck. Storage offloading eliminates this constraint by leveraging the power of existing storage arrays to manage the data transfer. Instead of pushing data over the network, MTV&amp;rsquo;s plug-in module coordinates with the storage provider to move VM disk data directly over the storage system. This results in significantly faster migrations, reduced downtime, and greater reliability. Learn more about storage offloading. Warm migrations are now supported in Developer Preview: Previously limited to cold migrations (in Technology Preview), MTV 2.10 now supports storage offloading with warm migrations. You can keep critical VMs running while data is migrated quickly and safely, with minimal downtime during the cutover. Please verify with your account team if this option makes sense for your migration plan.</description></item><item><title>The Great Cloud Charade: Why “Data Residency” Isn’t “Data Sovereignty”</title><link>https://kubermates.org/docs/2025-11-18-the-great-cloud-charade-why-data-residency-isn-t-data-sovereignty/</link><pubDate>Tue, 18 Nov 2025 17:17:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-18-the-great-cloud-charade-why-data-residency-isn-t-data-sovereignty/</guid><description>Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Upgrading VMware Cloud Foundation 5.2 to 9.0: Webinar Takeaways VMware Cloud Foundation is the Gold Standard for Virtual Desktop Infrastructure VCF Breakroom Chats Episode 73 - Next-Gen Data Services: The DBaaS Revolution with VCF 9 In the high-stakes world of digital infrastructure and modern sovereign cloud strategy, “sovereignty” has become the ultimate prize. Nations are drafting laws to control their digital territories. Enterprises are scrambling to comply, seeking cloud solutions that promise to keep sensitive data safe within jurisdictional lines. In response, hyperscalers have rolled out a red carpet of “sovereign” offerings for Europe — complete with local data centers, EU-resident staff, and billion-dollar investments. But a critical and often deliberately obscured distinction lies at the heart of this new landscape: the difference between data residency and data sovereignty. They are not the same. One is a feature linked to geography; the other is a control mechanism that defines autonomy. Understanding this difference is key to piercing the marketing veil — and recognizing that many so-called “sovereign public clouds” are, in fact, marketing-speak rather than true sovereign cloud compliance. Data Residency Vs. Data Sovereignty: The Core Distinction: Location vs. Customer Control To engage in a meaningful discussion, we must first establish clear definitions for these frequently conflated terms. Data Residency refers exclusively to the physical, geographic location where data is stored.</description></item><item><title>Top 5 hard-earned lessons from the experts on managing Kubernetes</title><link>https://kubermates.org/docs/2025-11-18-top-5-hard-earned-lessons-from-the-experts-on-managing-kubernetes/</link><pubDate>Tue, 18 Nov 2025 15:32:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-18-top-5-hard-earned-lessons-from-the-experts-on-managing-kubernetes/</guid><description>&lt;ol&gt;
&lt;li&gt;Operational overhead catches teams off guard Resources for operational overhead: 2. Hidden corners : Security issues put clusters at risk Default settings Network policy and namespace isolation Containers and the images you run in them Resources for Kubernetes security: 3. Scaling challenges that stall growth and agility The cost of node scaling The right metrics for pod scaling Resources for scaling Kubernetes: 4. Talent acquisition: High talent costs and skill gaps in Kubernetes expertise Resources for talent acquisition: 5. Technical debt piling up faster than teams can manage Ongoing upgrades A shifting tooling landscape Resources for managing tech debt: Bonus lesson: Not every workload belongs on Kubernetes Resources for managing Kubernetes: Bonus lesson: Policy enforcement Resources for policy enforcement: Building reliable, secure, and efficient Kubernetes Posted on November 18, 2025 by Stevie Caldwell, Tech Lead at Fairwinds CNCF projects highlighted in this post Kubernetes has transformed how modern organizations deploy and operate scalable infrastructure, and the hype around automated cloud native orchestration has made its adoption nearly ubiquitous over the past 10+ years. Yet behind the scenes, most teams embarking on their Kubernetes journey quickly encounter operational complexity, configuration challenges, and costly maintenance that few vendors highlight. Drawing from years of real-world experience architecting, building, and maintaining Kubernetes, we recently hosted a webinar sharing five hard-earned lessons to help organizations get started using the container orchestration tool. In this post, we’ve paired each lesson with useful resources and examples of how to navigate managing Kubernetes at scale, whether supporting your own teams, deploying across multiple clusters, or evaluating managed Kubernetes offerings. The Kubernetes community knows that spinning up a cluster is straightforward, especially if you use a managed provider such as AKS , EKS , or GKE. But in reality, running a production environment means managing all the hidden add-ons : DNS controllers, networking, storage, monitoring, logging, secrets, security, and more. Supporting internal users (dev teams, ops, and data scientists) adds significant overhead for any company running Kubernetes.&lt;/li&gt;
&lt;/ol&gt;</description></item><item><title>Kgateway v2.1 is released!</title><link>https://kubermates.org/docs/2025-11-18-kgateway-v2-1-is-released/</link><pubDate>Tue, 18 Nov 2025 15:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-18-kgateway-v2-1-is-released/</guid><description>🌟 What’s new in kgateway 2.1? Agentgateway integration K8s GW API 1.3.0 and Inference Extension 1.0.0 Global policy attachment Deep merging for extauth and extproc policies Additional proxy pod template customization Horizontal Pod Autoscaling Dynamic Forward Proxy Session Affinity Enhanced retries and timeout capabilities Passive health checks with outlier detection New kgateway operations dashboard 🗑️ Deprecated or removed features Release notes Availability Thanks to our contributors! Get Involved Posted on November 18, 2025 by Nina Polshakova, Nadine Spies, &amp;amp; Michael Levan, Solo. io, Aryan Parashar, LFX Mentee CNCF projects highlighted in this post Kgateway is an open source implementation of the Kubernetes Gateway API that unifies ingress, API gateway, service mesh, and AI gateway capabilities in a singular modular control plane. Built for performance and flexibility, it secures and manages traffic across legacy, cloud native, and AI-driven workloads in any environment. We’re excited to announce the release of kgateway v2.1. , a release packed with exciting new features and improvements. Here are a few select updates the kgateway team would like to highlight! This release marks a major milestone — it’s the first version to integrate the open source project agentgateway ! Agentgateway is a highly available, highly scalable data plane that provides AI connectivity for LLMs, MCP tools, AI agents, and inference workloads. As part of this evolution, we’re beginning the deprecation of the Envoy-based AI Gateway and the Envoy-based Inference Extension, since all related functionality is now implemented natively through agentgateway. You can still continue to use Envoy-based Gateways for API Gateway use cases. For this release, agentgateway support is in beta. If you’re trying out the agentgateway GatewayClass , we recommend following the beta release feed to stay up to date with improvements, bug fixes, and breaking changes as the implementation is refined. agentgateway GatewayClass To get started with agentgateway, you simply install kgateway with the following Helm values: agentgateway : enabled : true Then you create a Gateway with the agentgateway GatewayClass as shown here: agentgateway GatewayClass kubectl apply -f- &amp;laquo;EOF kind : Gateway apiVersion : gateway. networking.</description></item><item><title>NVMe Memory Tiering Design and Sizing on VMware Cloud Foundation 9 Part 2: Design for Security, Redundancy, and Scalability</title><link>https://kubermates.org/docs/2025-11-18-nvme-memory-tiering-design-and-sizing-on-vmware-cloud-foundation-9-part-2-design/</link><pubDate>Tue, 18 Nov 2025 12:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-18-nvme-memory-tiering-design-and-sizing-on-vmware-cloud-foundation-9-part-2-design/</guid><description>Security Redundancy Scalability Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Mapping VLAN Tags to Virtual Private Cloud Subnets Unified Authentication in VMware Cloud Foundation SDK 9.0: Seamless authentication across vSphere and vSAN APIs NVMe Memory Tiering Design and Sizing on VMware Cloud Foundation 9 Part 2: Design for Security, Redundancy, and Scalability In Part 1 of this series, we explored some of the pre-requisites for NVMe Memory Tiering such as workload assessment, memory activeness percentage, VM profile limitations, software pre-requisites, and compatibility of NVMe devices. In addition, we highlighted the importance of adopting VMware Cloud Foundation (VCF) 9, which could provide a significant cost reduction in memory, better CPU utilization and greater VM consolidation. But before we can fully deploy this solution it is important we design with security, redundancy, and scalability in mind, and that is what this section is all about. Memory security is not necessarily a super popular topic for admins, and this is because memory is volatile. However attackers can leverage memory to store malicious information on non-volatile media to evade detection, but I digress – this is more a forensics topic (which I enjoy). Once power is no longer present the information on DRAM (volatile) disappears within minutes. So, with NVMe Memory Tiering we are moving pages from volatile (DRAM) to non-volatile media (NVMe). In order to address any security concerns with memory pages being stored on NVMe devices, we have come up with a couple of solutions that our customers can easily implement after initial configuration. With this first release of the Memory Tiering feature, encryption is already part of the package, and ready to be implemented out of the box. In fact, you have the option to encrypt at the VM level (per VM) or at the host level (all VMs in the host). By default, this option is not enabled but can easily be added to your configuration within the vCenter UI. For NVMe Memory Tiering encryption we do not require a Key Management System (KMS) or Native Key Provider (NKP), instead, the key will be randomly generated at the kernel level by each host using AES-XTS encryption.</description></item><item><title>Harbor: Your Enterprise-Ready Container Registry for a Modern Private Cloud</title><link>https://kubermates.org/docs/2025-11-18-harbor-your-enterprise-ready-container-registry-for-a-modern-private-cloud/</link><pubDate>Tue, 18 Nov 2025 09:52:24 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-18-harbor-your-enterprise-ready-container-registry-for-a-modern-private-cloud/</guid><description>What is Harbor Container Registry? Setting Up Harbor Container Registry Prerequisites Deploying Harbor on a VM Using Harbor as an Image Registry Conclusion Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Upgrading VMware Cloud Foundation 5.2 to 9.0: Webinar Takeaways Mapping VLAN Tags to Virtual Private Cloud Subnets VMware Cloud Foundation is the Gold Standard for Virtual Desktop Infrastructure In the evolving landscape of application deployment, containerization with Kubernetes (K8s) has become the new standard. As organizations adopt Kubernetes at scale, public image registries often create challenges, from rate limits to escalating costs and limited control over sensitive data. Harbor bridges this gap as an open-source, enterprise-grade container registry that brings security, performance, and sovereignty to container image management. It seamlessly integrates with existing operational paradigms, providing a robust solution for modern container image management needs. Originally developed by VMware by Broadcom in 2014 and open-sourced in 2016, Harbor joined the Cloud Native Computing Foundation (CNCF) on July 31, 2018, and graduated on June 15, 2020, as its eleventh project – a milestone that reflects both its maturity and its vibrant community. Today, Harbor remains one of the most widely adopted CNCF projects for secure image management across hybrid and private cloud environments. Its active community drives continuous innovation, each release adds new integrations, security features, and performance improvements, ensuring Harbor evolves alongside enterprise cloud-native needs as a container registry. Recent highlights include: ​​v2.13: Integration with CloudNativeAI (CNAI) for AI model management, extended audit logging and enhanced OIDC with PKCE support v2.12: Integration with ACR and ACR EE Registry v2.11: Native SBOM generation and management At its core, Harbor provides a centralized repository for managing container images. Think of it as a private, feature-rich registry for your organization, with robust security and management features built-in. Key features include: Security: Image vulnerability scanning, content trust, and role-based access control. Replication: Replicate images across multiple Harbor instances for disaster recovery and content distribution. Management: A user-friendly UI for managing projects, users, and images.</description></item><item><title>Automation unleashed: Introducing the new Red Hat Certified Ansible Collection amazon.ai for generative AI</title><link>https://kubermates.org/docs/2025-11-18-automation-unleashed-introducing-the-new-red-hat-certified-ansible-collection-am/</link><pubDate>Tue, 18 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-18-automation-unleashed-introducing-the-new-red-hat-certified-ansible-collection-am/</guid><description>Automation unleashed: Introducing the new Red Hat Certified Ansible Collection amazon. ai for generative AI The problem: Manual AI management doesn’t scale Introducing the amazon. ai Collection Core capabilities of Red Hat AnsibleCertified Collection for amazon. ai Automating generative AI with Amazon Bedrock Managing operational performance with DevOps Guru Why Red Hat AnsibleCertified Collection for amazon. ai matters to you Get started with the Red Hat AnsibleCertified Collection for amazon. ai Automation for your AI infrastructure Red Hat Ansible Automation Platform | Product Trial About the author Alina Buzachis More like this Red Hat Lightspeed 2025: From observability to actionable automation Setting up logrotate in Linux Technically Speaking | Taming AI agents with observability Transforming Your Secrets Management | Code Comments Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Generative AI demands infrastructure that’s not only powerful but repeatable, auditable, and scalable. From chat bots and content generation to intelligent automation agents, organizations are deploying AI at scale. But with this innovation comes complexity. In short, deploying generative AI isn’t just about models, it’s about managing the infrastructure and operations behind them reliably. The Red Hat Certified Collection, amazon. ai , addresses this problem by bringing infrastructure-as-code principles to AI and operational monitoring. Even with powerful services like Amazon Bedrock and DevOps Guru, organizations face hurdles: Agent lifecycle complexity : Bedrock Agents can orchestrate multiple models and APIs, but creating, updating, and validating these agents manually is tedious and prone to error.</description></item><item><title>Red Hat Enterprise Linux delivers deterministic performance for time-sensitive networking</title><link>https://kubermates.org/docs/2025-11-18-red-hat-enterprise-linux-delivers-deterministic-performance-for-time-sensitive-n/</link><pubDate>Tue, 18 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-18-red-hat-enterprise-linux-delivers-deterministic-performance-for-time-sensitive-n/</guid><description>Red Hat Enterprise Linux delivers deterministic performance for time-sensitive networking Red Hat Enterprise Linux for real-time performance Our core approach to tackling this challenge is centered on two elements: The testing grounds: Test bed layout and setup Hardware and software The testing concept The data before and after TSN What the data really tells us Test 1B: No TSN Test 2B: With TSN The bottom line for IT and OT leaders Red Hat Enterprise Linux | Product trial About the author David Rapini More like this Enterprise-grade edge virtualization with Red Hat OpenShift and Arctera&amp;rsquo;s InfoScale Create an efficient two-node edge infrastructure with Red Hat OpenShift and Portworx by Pure Storage Open Curiosity | Command Line Heroes What Can Video Games Teach Us About Edge Computing? | Compiler Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The industrial world runs on timing and consistency. In manufacturing and operations, a predictable outcome isn&amp;rsquo;t a nice-to-have; it&amp;rsquo;s the core promise of the industrial system itself. Whether you&amp;rsquo;re managing complex motion control or critical process loops, network communication must be reliable and predictable. Time-sensitive networking (TSN) is the essential evolution of Ethernet that brings determinism and a guaranteed delivery schedule to a standard open industrial network. But here&amp;rsquo;s the reality check for information technology (IT) and operations technology (OT) leaders: TSN is great for the wire and hardware, but it&amp;rsquo;s only as good as the software stack running on the devices at the edge. A jittery operating system can ruin a perfect TSN network before a packet ever leaves the device. Things like hardware interrupts, cache swapping, or even heavy application use (like AI or video management) can significantly impact the OS&amp;rsquo;s ability to build and deliver packets in that crucial, repeatable fashion. At Red Hat, we understand the needs of Industrial and we know how important it is to be at peak performance. So we set up tests to prove it. We recently completed a hands-on technical validation, in collaboration with Intel, to demonstrate how Red Hat Enterprise Linux (RHEL) and Red Hat Device Edge deliver the precise, deterministic performance required for industrial TSN. A real-time kernel: A tuned operating system designed to use a deterministic scheduler to ensure critical tasks execute in fixed time bounds. Guaranteed predictable execution of high-priority workloads by controlling interrupt handling, resource locking, and context switching.</description></item><item><title>Behind the queues: How Kueue reimagines scheduling in Red Hat OpenShift</title><link>https://kubermates.org/docs/2025-11-17-behind-the-queues-how-kueue-reimagines-scheduling-in-red-hat-openshift/</link><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-17-behind-the-queues-how-kueue-reimagines-scheduling-in-red-hat-openshift/</guid><description>Behind the queues: How Kueue reimagines scheduling in Red Hat OpenShift Topology-aware scheduling Kueue with dynamic resource allocation Why it matters Next steps Red Hat OpenShift Container Platform | Product Trial About the authors Pannaga Rao Bhoja Ramamanohara Sohan Kunkerkar More like this NGINX Gateway Fabric certified for Red Hat OpenShift DxOperator from DH2i is now certified for Red Hat OpenShift 4.19 Where Coders Code | Command Line Heroes What Kind of Coder Will You Become? | Command Line Heroes Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share In a modern cluster, the hardest problem isn’t running workloads—it&amp;rsquo;s sharing resources fairly. Red Hat OpenShift clusters are seeing a surge of AI-accelerated workloads, from GPU-intensive training jobs to large batches of inference requests. At the same time, other tenants still need consistent throughput for their everyday CI/CD pipelines and data processing tasks. The result is a constant battle for resources, where some jobs wait too long, others consume more than their fair share, and administrators are left fighting bottlenecks. This is exactly the challenge that Kueue, a Kubernetes-native job queueing and scheduling framework, was built to solve. It introduces structured queues, priorities, and quota enforcement to bring fairness and predictability back into scheduling. With Red Hat Build of Kueue, these upstream innovations are packaged, hardened, and delivered into Red Hat OpenShift as a supported, enterprise-ready solution to enable clusters to run efficiently while giving every workload a fair chance. Once workloads are queued fairly, the next challenge is where they actually land. For distributed jobs, placement can matter as much as allocation: pods that constantly exchange data perform very differently depending on whether they&amp;rsquo;re co-located or scattered across zones. This is where topology-aware scheduling (TAS) comes in. Rather than treating the cluster as a flat pool of machines, TAS considers the physical and logical layout of the infrastructure (racks, blocks, zones) and makes scheduling decisions that optimize communication and efficiency. Workloads that talk a lot can be placed closer together, multi-pod jobs can start in sync through gang scheduling, and fairness across tenants is preserved even as locality is optimized.</description></item><item><title>Introducing OpenShift Service Mesh 3.2 with Istio’s ambient mode</title><link>https://kubermates.org/docs/2025-11-17-introducing-openshift-service-mesh-3-2-with-istio-s-ambient-mode/</link><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-17-introducing-openshift-service-mesh-3-2-with-istio-s-ambient-mode/</guid><description>Introducing OpenShift Service Mesh 3.2 with Istio’s ambient mode Upgrading to OpenShift Service Mesh 3.2 Sidecar-less service mesh: Istio’s ambient mode Ztunnel proxy Waypoint proxy Is Istio’s ambient mode right for you? What are the benefits and trade offs? Significantly reduced resource costs Lightweight zero trust networking Improved scalability Potential performance characteristics Easier adoption Feature support levels Upgrade considerations Getting started with Istio’s ambient mode If I am already using service mesh, can I migrate to ambient mode? Kiali with Istio’s ambient Istio 1.27 updates Gateway API Inference Extensions (GIE) Multicluster with ambient mode Native nftables support in sidecar mode Cert-manager Istio-CSR is generally available Getting started with OpenShift Service Mesh Red Hat Product Security About the author Jamie Longmuir More like this How to join a Linux system to an Active Directory domain How to configure your CA trust list in Linux What Is Product Security? | Compiler Technically Speaking | Security for the AI supply chain Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share We are thrilled to announce the general availability of Red Hat OpenShift Service Mesh 3.2. This release includes the general availability of Istio’s ambient mode —a new way of deploying service mesh without sidecars that significantly lowers the resource costs of using service mesh. This provides a low overhead solution for zero trust networking with lightweight pod-to-pod mTLS encryption and authorization policies based on workload identities, with the ability to add more advanced features as required. Based on the Istio, Envoy, and Kiali projects, this release updates the version of Istio to 1.27 and Kiali to 2.17 , and is supported on Red Hat OpenShift 4.18 and above. If you are running OpenShift Service Mesh 2.6 or earlier releases, you must upgrade to OpenShift Service Mesh 3.0, 3.1, and then 3.2. We recommend migrating to OpenShift Service Mesh 3.0 promptly, because version 2.6 reaches its end of life (EOL) on June 30, 2026 (recently extended from March 12, 2026). An in-depth migration guide is provided in the OpenShift Service Mesh 3.0 documentation , including an analysis of the differences between OpenShift Service Mesh 2.6 and 3.0. This blog post describes using the Kiali console for migrating between OpenShift Service Mesh 2.6 and 3.0. For an example of OpenShift Service Mesh 3 in action, with fully configured metrics and the Kiali console, see this solution pattern. This release brings generally available support for Istio’s ambient mode to OpenShift Service Mesh. Istio’s ambient mode is a significant new feature that enables service mesh features on workloads without the need for sidecar proxies. With Istio’s traditional dataplane architecture, known as “sidecar mode,” each application pod requires a sidecar proxy container to enable service mesh features.</description></item><item><title>VMware Cloud Foundation Named a G2 Leader in IaaS for Fall 2025!</title><link>https://kubermates.org/docs/2025-11-14-vmware-cloud-foundation-named-a-g2-leader-in-iaas-for-fall-2025/</link><pubDate>Fri, 14 Nov 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-14-vmware-cloud-foundation-named-a-g2-leader-in-iaas-for-fall-2025/</guid><description>VMware Cloud Foundation (VCF) Celebrates Continued Leadership Customer Insights and Performance Highlights VMware Cloud Foundation: Recognized Leadership in the Fall 2025 G2 Grid Report What Customers Are Saying Why G2 Recognition Matters Why Customers Choose VMware Cloud Foundation Ready to Modernize Your Private Cloud? Resources Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Upgrading VMware Cloud Foundation 5.2 to 9.0: Webinar Takeaways Operationalizing the Edge with VMware Cloud Foundation VMware Cloud Foundation is the Gold Standard for Virtual Desktop Infrastructure VMware Cloud Foundation (VCF) continues to lead the way in private cloud innovation, earning recognition as a G2 Leader in the “Infrastructure as a Service (IaaS)” category for Fall 2025. This achievement reaffirms our leadership in modern cloud infrastructure and reflects the ongoing trust and satisfaction of our global customers. This recognition underscores VMware’s commitment to delivering a modern, intelligent, and secure private cloud platform that empowers organizations to run traditional and modern workloads consistently across data centers, edge, and sovereign environments. Customer feedback in the Fall 2025 G2 Report underscores the growing momentum and satisfaction behind VMware Cloud Foundation. Over the past six months, customer reviews increased by more than 636% , with 103 verified reviews contributing to the platform’s strong performance and recognition. Nine G2 badges were awarded this season, highlighting excellence in areas such as product satisfaction, user adoption, and overall market momentum. Across key satisfaction categories, VMware Cloud Foundation ranked among the top-rated platforms for ease of administration, reliability, quality of support, and alignment with customer requirements. These results reflect VMware’s continued commitment to delivering a trusted, enterprise-grade private cloud foundation that unifies performance, security, and intelligent operations from core to edge. In the Fall 2025 G2 Grid Report , VMware Cloud Foundation once again demonstrated strong market momentum and customer confidence across multiple categories. Leadership Across Market Segments Leader — Fall 2025 Enterprise Leader — Fall 2025 Mid-Market Leader — Fall 2025 Small Business Leader — Fall 2025 Leader — Fall 2025 Enterprise Leader — Fall 2025 Mid-Market Leader — Fall 2025 Small Business Leader — Fall 2025 Global Recognition Regional Leader — Fall 2025 EMEA Regional Leader — Fall 2025 Asia Pacific Regional Leader — Fall 2025 EMEA Regional Leader — Fall 2025 Asia Pacific Customer Momentum Highest User Adoption — Enterprise (Fall 2025) Momentum Leader — Fall 2025 Highest User Adoption — Enterprise (Fall 2025) Momentum Leader — Fall 2025 These distinctions highlight VMware Cloud Foundation’s continued leadership and innovation across private cloud infrastructure markets worldwide. Organizations of all sizes and regions continue to rely on VMware Cloud Foundation to power their mission-critical workloads across data centers, edge environments, and sovereign clouds. Recent G2 reviews highlight why organizations continue to choose VMware Cloud Foundation as the foundation of their modern cloud strategies: Read more reviews on G2 G2 is the world’s most trusted destination for verified customer reviews and product insights.</description></item><item><title>DxOperator from DH2i is now certified for Red Hat OpenShift 4.19</title><link>https://kubermates.org/docs/2025-11-14-dxoperator-from-dh2i-is-now-certified-for-red-hat-openshift-4-19/</link><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-14-dxoperator-from-dh2i-is-now-certified-for-red-hat-openshift-4-19/</guid><description>DxOperator from DH2i is now certified for Red Hat OpenShift 4.19 Simplifying Kubernetes SQL Server deployment Key features Why certification matters Enterprise-ready capabilities for OpenShift Empowering the modern enterprise Resources Red Hat Ansible Automation Platform | Product Trial About the authors Vivien Wang OJ Ngo More like this NGINX Gateway Fabric certified for Red Hat OpenShift Efficient Scheduling in Red Hat OpenShift with Kueue Press Start | Command Line Heroes Who’s Afraid Of Compilers? | Compiler Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Kubernetes has emerged as a powerful foundation for deploying and managing cloud-native applications, and Red Hat OpenShift operators are the best way to streamline this. DH2i&amp;rsquo;s DxOperator, the SQL Server Operator bundled with DxEnterprise and preferred by Microsoft for Kubernetes deployments, is now officially certified for OpenShift 4.19, and it&amp;rsquo;s available from the Red Hat Ecosystem Catalog. This compliments the recent RHEL 9.6 certification for DxEnterprise. The Openshift certification for DxOperator marks another milestone in delivering enterprise-grade solutions for customers running Microsoft SQL Server in containerized and hybrid cloud environments. Together, Red Hat and DH2i are simplifying how organizations deploy, manage, and scale SQL Server workloads, bringing together the best of two worlds. Customers can now enjoy greater automation, high availability, and operational consistency when using OpenShift in the Microsoft space. DxOperator is the SQL Server Operator for Kubernetes endorsed by Microsoft. It&amp;rsquo;s designed to automate the entire lifecycle of cloud-native Microsoft SQL Server Always-On Availability Group (AG) operations. It allows enterprises to take advantage of the scalability and automation of Kubernetes while maintaining the reliability and performance expected from SQL Server. Cloud-native design: Built on cloud-native principles such as microservice architecture, immutable infrastructure, and declarative configuration. DxOperator integrates easily into OpenShift, enabling standardized deployment pipelines and simplified lifecycle management. Advanced architecture: Leveraging DH2i’s patented clustering technology, DxOperator unlocks true cross-platform database management.</description></item><item><title>Friday Five — November 14, 2025</title><link>https://kubermates.org/docs/2025-11-14-friday-five-november-14-2025/</link><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-14-friday-five-november-14-2025/</guid><description>Friday Five — November 14, 2025 KubeCon + CloudNativeCon Newsroom FastForward - Red Hat’s CTO sees AI as next step for company’s open approach TechTarget - Northrop Grumman, Ford prep AI infrastructure with OpenShift ComputerWeekly - Red Hat Enterprise Linux 10.1 and 9.7: An ‘evolving foundation’ for the stack ZDNet - Why even a US tech giant is launching &amp;lsquo;sovereign support&amp;rsquo; for Europe now About the author Red Hat Corporate Communications More like this Blog post Blog post Original podcast Original podcast Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share At KubeCon + CloudNativeCon, Red Hat is redefining the modern application platform to unite your IT estate: from legacy VMs to next-generation AI workloads. Check out the headlines and dive into our newsroom. Learn more Red Hat&amp;rsquo;s CTO, Chris Wright, talks with FastForward about how AI is the logical next step in the company&amp;rsquo;s long-standing open hybrid cloud strategy. To do this, Red Hat is focusing on leveraging existing open source tools like Linux and Kubernetes, extending them to meet AI requirements, rather than reinventing its entire platform. Learn more See how Northrop Grumman and Ford are leveraging Red Hat OpenShift to build out their environments and prepare for the future of AI innovation. Learn more Red Hat Enterprise Linux 10.1 and 9.7 are now available, building on the innovations of RHEL 10 for a more intelligent and future-ready computing foundation. Learn more In response to the growing push for digital sovereignty in Europe, Red Hat has become the first US company to launch an EU-specific program: Red Hat Confirmed Sovereign Support (RHCSS). By anchoring its support structure within Europe, Red Hat aims to meet strict EU regulations and enhance data control, supply chain transparency, and jurisdictional security for organizations modernizing their cloud and AI infrastructures. Learn more Red Hat is the world’s leading provider of enterprise open source software solutions, using a community-powered approach to deliver reliable and high-performing Linux, hybrid cloud, container, and Kubernetes technologies. Red Hat helps customers integrate new and existing IT applications, develop cloud-native applications, standardize on our industry-leading operating system, and automate, secure, and manage complex environments. Award-winning support, training, and consulting services make Red Hat a trusted adviser to the Fortune 500. As a strategic partner to cloud providers, system integrators, application vendors, customers, and open source communities, Red Hat can help organizations prepare for the digital future.</description></item><item><title>VCF Breakroom Chats Episode 72: From VI Admin to Cloud Provider – Building Your First Self-Service Catalog with VCF 9.0</title><link>https://kubermates.org/docs/2025-11-13-vcf-breakroom-chats-episode-72-from-vi-admin-to-cloud-provider-building-your-fir/</link><pubDate>Thu, 13 Nov 2025 16:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-13-vcf-breakroom-chats-episode-72-from-vi-admin-to-cloud-provider-building-your-fir/</guid><description>About the VCF Breakroom Chat Series Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VCF Breakroom Chats Episode 72: From VI Admin to Cloud Provider - Building Your First Self-Service Catalog with VCF 9.0 From Tickets to Clicks: Driving Real Consumption in Your Self‑Service Private Cloud VCF Breakroom Chats Episode 69 - Beyond vRA + NSX: Delivering Cloud-Native Networking with VCF 9 Virtual Private Clouds Welcome to another exciting episode of the VCF Breakroom Chats! In this latest video feature, we’re joined once again by Vincent Riccio, Product Marketing Engineer for the VCF Division at Broadcom, alongside Alina Thylander. This time, they explore how VI Admins can take the leap into becoming Cloud Providers by building their first self-service catalog with VCF 9.0. Together, they unpack how automation transforms the developer experience—moving away from slow, ticket-driven provisioning to on-demand access where VMs, Kubernetes clusters, and even GPU nodes can be spun up in minutes. You’ll hear how organizations are reporting dramatic reductions in ticket volume, faster release cycles, and reclaimed engineering hours that fuel innovation. Vincent and Alina also dive into why supporting UI, CLI, and API is critical for private cloud adoption, showing how VCF 9.0 empowers both low-code users and cloud-native DevOps teams from a single platform. The conversation continues with a demo of publishing blueprints into the self-service catalog and provisioning them through both the portal and declarative API, before wrapping up with strategies for designing safety guardrails—ensuring broad self-service access without compromising capacity, cost, or compliance. Don’t miss this practical and inspiring discussion on how VCF 9.0 helps you evolve from VI Admin to Cloud Provider while delivering secure, efficient, and flexible cloud services. Want to learn more about VCF Automation? Check out the VCF Automation web page for more resources. Read this blog post: Private Cloud Redefined: Deliver a Unified Cloud Consumption Experience with VMware Cloud Foundation 9.0 Explore VMware Cloud Foundation Automation by the Numbers with the Forrester Total Economic Impact (TEI) Study Download the Self-Service Private Cloud for Dummies Guide This webinar series focuses on VMware Cloud Foundation (VCF). In this series, we share conversations with industry-recognized experts from Broadcom and with solution partners and customers. You’ll gain relevant insights whether you’re an IT practitioner, IT admin, cloud or platform architect, developer, DevOps, senior IT manager, IT executive, or AI/ML professional. If you are new to the series, please check out our previous episodes.</description></item><item><title>Announcing the Next Evolution of VMware Certified Distinguished Expert (VCDX): The Certification for Private Cloud Experts</title><link>https://kubermates.org/docs/2025-11-13-announcing-the-next-evolution-of-vmware-certified-distinguished-expert-vcdx-the-/</link><pubDate>Thu, 13 Nov 2025 14:39:09 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-13-announcing-the-next-evolution-of-vmware-certified-distinguished-expert-vcdx-the-/</guid><description>The Critical Demand for Modern Cloud Expertise The VCDX Advantage: Unlocking Elite Opportunities Accelerating Your Private Cloud Career Your Path to Achieving Cloud Expertise Your Path to Achieving Cloud Expertise Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Announcing the Next Evolution of VMware Certified Distinguished Expert (VCDX): The Certification for Private Cloud Experts VMware Cloud Foundation 9: Now Ready For All Storage From Tickets to Clicks: Driving Real Consumption in Your Self‑Service Private Cloud For over almost two decades, the VMware Certified Design Expert certification has stood as the pinnacle measurement for architects mastering VMware solutions – a globally recognized achievement of elite expertise. But as the private cloud landscape accelerates and our community’s needs evolve, we’re not just adapting – we’re revolutionizing. We are thrilled to announce the evolution of this iconic program, expanding its reach beyond traditional focus on design roles to welcome a broader range of elite VMware professionals: Architects, Administrators, and Support specialists. To reflect this expanded vision, we are proudly renaming it the VMware Certified Distinguished Expert (VCDX). This isn’t just an update, it is a strategic re-imagining of our VCDX program, designed specifically for individuals like you who aspire to actively define the future of the private cloud. It is also an invitation to build on your deep VMware knowledge, push conventional limits, and pioneer the next generation of private cloud environments, and help pioneer the next generation of private cloud environments. “As VCDX #1 and a founding supporter since 2007, I can attest that the VMware VCDX program is not just a certification—it is the definitive, peer-vetted validation for a distinguished expert. ” – John Arrasjid, VCDX #1 The need for specialized modern cloud expertise has never been more crucial. With 76% of enterprises planning to expand cloud services through private cloud, as highlighted in the Flexera 2024 State of the Cloud Report, the demand for profound cloud intelligence is rapidly accelerating. Yet, a critical challenge persists: a pervasive cloud skills gap. A staggering 95% of IT decision-makers said the cloud skills gap negatively impacted their team, leading to slower innovation and increased operational costs. Even more alarming, nearly a third of organizations are actively jeopardizing their bottom line, having missed their financial objectives due to this critical cloud skills gap.</description></item><item><title>Improving modern software supply chain security: From AI models to container images</title><link>https://kubermates.org/docs/2025-11-13-improving-modern-software-supply-chain-security-from-ai-models-to-container-imag/</link><pubDate>Thu, 13 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-13-improving-modern-software-supply-chain-security-from-ai-models-to-container-imag/</guid><description>Improving modern software supply chain security: From AI models to container images The growing complexity of supply chain security Cryptographic integrity with Trusted Artifact Signer 1.3 Securing AI models Enterprise-grade high availability Transparency and monitoring Comprehensive analysis with Trusted Profile Analyzer 2.2 License compliance at scale Streamlined container security AI supply chain visibility The power of integration Looking forward Red Hat Product Security About the author Meg Foley More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The software supply chain has evolved dramatically in recent years. Today&amp;rsquo;s applications integrate countless components—from open source libraries and container images to AI models and training datasets. Each element represents a potential security risk that organizations must understand, verify, and continuously monitor. As supply chain attacks increase in frequency and sophistication, enterprises need comprehensive solutions that provide both artifact integrity and deep visibility into their software dependencies. Red Hat&amp;rsquo;s latest releases of Red Hat Trusted Artifact Signer 1.3 and Red Hat Trusted Profile Analyzer 2.2 deliver a powerful combination of cryptographic signing capabilities and advanced supply chain analysis, addressing a full spectrum of modern software security challenges, including the emerging complexities of AI-powered applications. Modern applications are built from diverse components that create an intricate web of dependencies. Traditional software includes operating system (OS) packages, application libraries, and container base images. But today&amp;rsquo;s AI-powered applications introduce additional complexity with machine learning (ML) models, training datasets, and inference frameworks—each requiring specialized security considerations. This complexity creates multiple attack vectors. Malicious actors can compromise software at build time by injecting code into repositories, at distribution time by replacing legitimate packages with malicious versions, or at deployment time by exploiting weak verification processes. The 2025 supply chain attack on popular npm packages demonstrated that a single compromised component can affect thousands of downstream organizations. Trusted Artifact Signer 1.3 helps to address these challenges by providing enterprise-grade cryptographic signing and verification for all software artifacts.</description></item><item><title>Listening, learning, and leading: How customer feedback shapes the future of Red Hat Learning Subscription</title><link>https://kubermates.org/docs/2025-11-13-listening-learning-and-leading-how-customer-feedback-shapes-the-future-of-red-ha/</link><pubDate>Thu, 13 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-13-listening-learning-and-leading-how-customer-feedback-shapes-the-future-of-red-ha/</guid><description>Listening, learning, and leading: How customer feedback shapes the future of Red Hat Learning Subscription Key takeaways What is next for Red Hat Learning Subscription? Join the conversation Red Hat Learning Subscription | Product Trial About the author Mary Margaret Barnes More like this Blog post Blog post Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share At Red Hat, innovation begins with listening. In October 2025, the Red Hat Learning Think Tank forum brought together a group of passionate learners and leaders to listen, learn, and collaborate on Red Hat Learning Subscription and help shape its roadmap. The forum focused on what learners value most from Red Hat Learning Subscription , the challenges they face, and how Red Hat can better support technical skill development at scale. In this article, you’ll hear about the key takeaways from the meeting, next steps, and the importance of providing feedback to better empower current and future learners. “It’s a single pane of glass for all learning. ” – Martin Kaufmann Red Hat Learning Think Tank participants consistently highlighted how Red Hat Learning Subscription brings clarity and confidence to the certification journey. Red Hat Certification exams help teams focus on what matters most: building practical, job-ready skills. Participants particularly valued: Comprehensive content: Everything you need in one place. Early access and expert extras: Strategic advantages for staying ahead of emerging technologies. Guided learning paths: Confidence-building structure for certification preparation. Hands-on labs and live sessions: Real-world practice that brings concepts to life. Participants also provided feedback on how Red Hat Learning Subscription can improve.</description></item><item><title>GPU Observability: Get Deeper Insights into Your Droplets and DOKS Clusters</title><link>https://kubermates.org/docs/2025-11-12-gpu-observability-get-deeper-insights-into-your-droplets-and-doks-clusters/</link><pubDate>Wed, 12 Nov 2025 20:56:52 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-12-gpu-observability-get-deeper-insights-into-your-droplets-and-doks-clusters/</guid><description>GPU Observability: Get Deeper Insights into Your Droplets and DOKS Clusters Why GPU Observability Matters What’s Included: New Metric Categories Zero Setup, No Extra Cost Benefits of GPU Droplets with DigitalOcean About the author Try DigitalOcean for free Related Articles Image and audio models from fal now available on DigitalOcean Announcing GPU Droplets accelerated by NVIDIA HGX H100 in the EU Announcing cost-efficient storage with Network file storage, cold storage, and usage-based backups By Waverly Swinton Updated: November 12, 2025 2 min read We’re introducing a new set of basic observability metrics for all GPU Droplets and DOKS clusters , giving you a powerful, simple way to monitor and optimize your AI workloads. When running large-scale training, inference, and complex data processing—cluster performance and stability are paramount. Our new observability features are designed to give you the visibility you need to ensure effective utilization of your resources and quickly debug any performance bottlenecks. Get real-time, individual metrics from your NVIDIA and AMD GPUs and their network interfaces on critical factors like utilization, temperature, power consumption, and more—all directly within the DigitalOcean Insights UI, and with zero setup required. We’ve grouped the new metrics into five intuitive categories to provide a comprehensive view of your GPU and DOKS cluster health and performance: Utilization: Understand how busy your GPU cores and memory are. This includes key metrics like GPU Occupancy and Memory Utilization, allowing you to optimize your setup for peak performance live. Utilization: Understand how busy your GPU cores and memory are. This includes key metrics like GPU Occupancy and Memory Utilization, allowing you to optimize your setup for peak performance live. Temperature: Monitor thermal conditions to prevent overheating and ensure stable operation under heavy load. Power: Track power consumption, which is essential for understanding GPU performance and efficiency. Throttle: Identify if your GPU is limiting its performance due to thermal, power, or voltage constraints. This is crucial for debugging sudden performance degradations.</description></item><item><title>Blog: Ingress NGINX Retirement: What You Need to Know</title><link>https://kubermates.org/docs/2025-11-12-blog-ingress-nginx-retirement-what-you-need-to-know/</link><pubDate>Wed, 12 Nov 2025 12:00:00 -0500</pubDate><guid>https://kubermates.org/docs/2025-11-12-blog-ingress-nginx-retirement-what-you-need-to-know/</guid><description>Ingress NGINX Retirement: What You Need to Know About Ingress NGINX History and Challenges Current State and Next Steps To prioritize the safety and security of the ecosystem, Kubernetes SIG Network and the Security Response Committee are announcing the upcoming retirement of Ingress NGINX. Best-effort maintenance will continue until March 2026. Afterward, there will be no further releases, no bugfixes, and no updates to resolve any security vulnerabilities that may be discovered. Existing deployments of Ingress NGINX will continue to function and installation artifacts will remain available. We recommend migrating to one of the many alternatives. Consider migrating to Gateway API , the modern replacement for Ingress. If you must continue using Ingress, many alternative Ingress controllers are listed in the Kubernetes documentation. Continue reading for further information about the history and current state of Ingress NGINX, as well as next steps. Ingress is the original user-friendly way to direct network traffic to workloads running on Kubernetes. ( Gateway API is a newer way to achieve many of the same goals. ) In order for an Ingress to work in your cluster, there must be an Ingress controller running. There are many Ingress controller choices available, which serve the needs of different users and use cases.</description></item><item><title>CNCF Honors Innovators and Defenders with 2025 Community Awards at KubeCon + CloudNativeCon North America</title><link>https://kubermates.org/docs/2025-11-12-cncf-honors-innovators-and-defenders-with-2025-community-awards-at-kubecon-cloud/</link><pubDate>Wed, 12 Nov 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-12-cncf-honors-innovators-and-defenders-with-2025-community-awards-at-kubecon-cloud/</guid><description>Lifetime Achievement Award Top End User Award Top Committer Award Chop Wood Carry Water Award Outstanding Mentor Award Cloud Native Hero TAGGIE End User Case Study Contest Lorem Ipsum Honorees span end users, maintainers, mentors, and long-time contributors, reflecting the strength and diversity of the cloud native ecosystem Key Highlights CNCF announced the KubeCon + CloudNativeCon North America 2025 Community Award winners for technical leadership, community dedication, mentorship, and end-user contributions. These honors reflect the diverse contributions that strengthen the entire cloud native ecosystem, defend open source from patent trolls via the Cloud Native Heroes Challenge, and highlight projects that underpin large-scale AI infrastructure. The global cloud native community benefits, encompassing over 270k contributors and 728 member organizations, as the awards celebrate sustained commitment and build community capacity for long-term health. Winners received their recognition and trophies during the award ceremony at KubeCon + CloudNativeCon North America in Atlanta on November 12, 2025. ATLANTA, KUBECON + CLOUDNATIVECON NORTH AMERICA—November 12, 2025— The Cloud Native Computing Foundation ® (CNCF®), which builds sustainable ecosystems for cloud native software, today announced the winners of the CNCF Community Awards at the annual KubeCon + CloudNativeCon North America. Over the years, the CNCF Community Awards aim to recognize and highlight key contributors across all CNCF projects and Technical Advisory Groups (TAGs). These awards celebrate those committed to shaping the cloud native ecosystem, acknowledging outstanding contributions in areas such as technical leadership, documentation, mentorship, and end-user implementation. Winners were recognized and received their trophies during the event’s award ceremony. “This year’s award recipients reflect what it really takes to go above and beyond in sustaining the cloud native community,” said Chris Aniszczyk, CTO, CNCF. “From mentoring new contributors to maintaining docs to hosting community events, the contributors are doing the hard work that helps our projects grow, stay reliable, and move faster together. ” The Lifetime Achievement Award is presented to longtime contributors who have significantly influenced cloud native technologies and demonstrated sustained commitment to the ecosystem. This award honors individuals whose efforts over the past decade have been instrumental in shaping and supporting the Kubernetes and CNCF communities.</description></item><item><title>Helm Marks 10 Years With Release of Version 4</title><link>https://kubermates.org/docs/2025-11-12-helm-marks-10-years-with-release-of-version-4/</link><pubDate>Wed, 12 Nov 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-12-helm-marks-10-years-with-release-of-version-4/</guid><description>Major update introduces new features while maintaining Helm’s role in Kubernetes application management Key Highlights Helm 4, the first major update in six years to the Kubernetes package manager, is now available, marking Helm’s 10th anniversary. This release simplifies and enhances the safety of deploying and managing applications at scale, addressing challenges such as CI/CD complexity, security, and multi-cluster operations. The target audience includes platform engineers, DevOps teams, Kubernetes administrators, open-source contributors, and enterprise users managing modern cloud-native infrastructure. Helm 4 was announced at KubeCon + CloudNativeCon North America 2025 in Atlanta and is available now. Maintainers will offer a technical deep dive and live demonstrations at Booth 8B in the Project Pavilion. KubeCon + CloudNativeCon North America, ATLANTA, GA – November 12, 2025 – The Cloud Native Computing Foundation® (CNCF®) , which builds sustainable ecosystems for cloud native software, today announced a major new release of Helm , coinciding with the project’s 10th anniversary. Released just weeks after its official anniversary, Helm version 4 marks the first major update to the Kubernetes package manager in six years. Helm 4 arrives at a time when Kubernetes users are managing increasingly complex application deployments, often across multiple clusters and environments. According to the CNCF 2024 Annual Survey , 40% of users cite CI/CD complexity as a top challenge, and 37% report security as a key barrier to adoption. Helm 4 directly addresses these concerns with improvements in testing automation, chart signing, performance, and chart distribution—making it easier for teams to ship software faster and more securely at scale. “Helm 4 is the result of ten years of community insight and real world deployment lessons,” said Chris Aniszczyk, CTO of CNCF. “As teams embrace cloud native scale, AI workloads, and platform engineering, Helm continues to deliver the reliability and flexibility they need to deploy applications with confidence.</description></item><item><title>Introducing Red Hat Enterprise Linux 9.7</title><link>https://kubermates.org/docs/2025-11-12-introducing-red-hat-enterprise-linux-9-7/</link><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-12-introducing-red-hat-enterprise-linux-9-7/</guid><description>Introducing Red Hat Enterprise Linux 9.7 Cryptography for a post-quantum world AI assistance, anywhere Better tooling and less toil for developers Reproducible image builds reduce management complexity Hybrid cloud encryption and new telemetry support Try RHEL 9.7 now Red Hat Enterprise Linux | Product trial About the author Gil Cattelain More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share When we build a new major version of Red Hat Enterprise Linux (RHEL) , our engineering teams learn a lot about what modern IT demands and what customers need to thrive. Those lessons shape the new capabilities and features we tout on the Red Hat Summit stage at launch. After the celebration of a launch, of course, comes the work of bringing those new capabilities to more RHEL customers by building them into previous editions. Today, with the launch of Red Hat Enterprise Linux 9.7, some of the most important security features of RHEL 10 are available to more people who need them. RHEL 10 is the first major Linux distribution fully equipped for post-quantum cryptography (PQC), and now we&amp;rsquo;re introducing PQC algorithms to RHEL 9.7. These algorithms enable secure key exchange, which is crucial for countering future threats from quantum computers. Key exchange enhances data integrity, and building it into RHEL 9.7 prepares your security infrastructure for emerging threats. This is just the start for RHEL and PQC. We plan to continue adding algorithms to future releases, so you can keep up with evolving security practices and compliance mandates. That&amp;rsquo;s not the only improvement coming with RHEL 9.7. It&amp;rsquo;s full of new features and capabilities to help you innovate quickly, improve continuously, and operate Linux more simply. The RHEL command-line assistant has become an essential tool for customers who have struggled to fill the skills gap among their RHEL users.</description></item><item><title>Optimized for Azure HPC: Red Hat delivers an easy HPC RHEL solution</title><link>https://kubermates.org/docs/2025-11-12-optimized-for-azure-hpc-red-hat-delivers-an-easy-hpc-rhel-solution/</link><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-12-optimized-for-azure-hpc-red-hat-delivers-an-easy-hpc-rhel-solution/</guid><description>Optimized for Azure HPC: Red Hat delivers an easy HPC RHEL solution Introducing RHEL HPC system role What are we offering? Where we’re going Unlock your cloud HPC capacity today Red Hat Enterprise Linux | Product trial About the authors Ariel Adam James Huang More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The world of high performance computing (HPC) drives much of the major scientific advances throughout the world. As one of the most trusted enterprise Linux platforms, Red Hat Enterprise Linux (RHEL) serves as the foundation for many of these HPC workloads, serving industries such as automotive, financial services, biomedical, energy, and beyond. Meanwhile, the public cloud has continued to gain traction in the broader compute marketplace, offering tremendous flexibility and dynamic infrastructure. This trend has been emerging as well for HPC, with organizations looking to take advantage of that same flexibility and extra compute capacity in order to scale HPC clusters on demand, shortening their product development or research cycles. This is why we’re excited to launch a new offering: RHEL for HPC on Azure. We’ve partnered closely with Microsoft to identify the technical requirements to accelerate the time-to-deployment for our shared customers. With RHEL for HPC on Azure, you get the automation that installs the tools and libraries required for an accelerated HPC compute environment on Azure infrastructure. The RHEL HPC 9.6 for Azure cloud offering is based on RHEL system roles. The RHEL HPC system role is a Red Hat Ansible Automation Platform role specifically designed to simplify the deployment and configuration of HPC environments. This system role installs necessary third-party components that customers would otherwise have to manually integrate, such as the NVIDIA CUDA Driver, CUDA Toolkit, NVIDIA Collective Communications Library (NCCL), NVIDIA Fabric Manager, NVIDIA RDMA packages, and Open MPI. It is modular, allowing users to selectively install or skip specific packages and offering functionalities such as configuring storage volumes to ensure enough disk space is allocated for these large installations on Azure. You can now select the RHEL HPC image listing in the Azure market place.</description></item><item><title>Prepare for a post-quantum future with RHEL 9.7</title><link>https://kubermates.org/docs/2025-11-12-prepare-for-a-post-quantum-future-with-rhel-9-7/</link><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-12-prepare-for-a-post-quantum-future-with-rhel-9-7/</guid><description>Prepare for a post-quantum future with RHEL 9.7 Critical libraries and applications Disruption on your own terms Take the first step Red Hat Product Security About the authors Dmitry Belyavskiy Emily Fox More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Are you excited to try out post-quantum cryptography in Red Hat Enterprise Linux (RHEL) , but you haven&amp;rsquo;t yet upgraded to RHEL 10? Our efforts to ensure that you&amp;rsquo;re ready to make the switch, and to prepare your organization for &amp;ldquo;Q-Day&amp;rdquo;, now start with RHEL 9.7. By getting started now, you can proactively begin strengthening your security posture and preparing for a seamless transition to RHEL 10. RHEL 9 was released in 2022 and was an important step forward from a security perspective. It was the first version of RHEL that received FIPS 140-3 certification, matching current security requirements. However, a lot has happened since 2022. Security requirements have changed, and the era of post-quantum cryptography has arrived. For the sake of performance and stability, these new cryptographic algorithms can&amp;rsquo;t be backported to earlier versions of most software the way many organizations have come to expect. Upgrading software today to bring continued stability, functionality, and protection in the post-quantum era is a must, and RHEL 9.7 can be your first step. RHEL 9.7 is a crucial stepping stone for your post-quantum transition. It gives you the opportunity to gain practical experience with post-quantum cryptography, and to understand its implications for your specific workloads. You can also identify any necessary application or infrastructure adjustments before you undertake a full upgrade to RHEL 10, which offers the most comprehensive post-quantum cryptography implementation. A sudden, forced transition to post-quantum cryptography when quantum threats become imminent (Q-Day, currently estimated at 2030) could disrupt your business&amp;rsquo;s operations.</description></item><item><title>Stop fighting with Ingress: NGINX Gateway Fabric is now certified for Red Hat OpenShift</title><link>https://kubermates.org/docs/2025-11-12-stop-fighting-with-ingress-nginx-gateway-fabric-is-now-certified-for-red-hat-ope/</link><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-12-stop-fighting-with-ingress-nginx-gateway-fabric-is-now-certified-for-red-hat-ope/</guid><description>Stop fighting with Ingress: NGINX Gateway Fabric is now certified for Red Hat OpenShift Beyond Ingress: How the Gateway API and NGINX change the game A shared model for speed and control Unify your application and AI traffic Get it in the catalog Red Hat OpenShift Container Platform | Product Trial About the author Shane Heroux More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Platform engineering teams know the drill. You need to connect, secure, and route traffic to your applications on OpenShift. Sometimes, it can feel like you’re wrestling with limitations or complexity at scale when managing traditional Ingress. Your platform deserves flexibility without giving up an ounce of control. That’s why we’ve been working closely with F5, and we&amp;rsquo;re happy to share that NGINX Gateway Fabric is now a certified operator for Red Hat OpenShift. For the thousands of teams already running NGINX and Red Hat OpenShift, this is the optimization you&amp;rsquo;ve been waiting for. It brings one of the most popular data planes directly into your trusted OpenShift workflow. You get a high-performance, familiar tool that is now fully supported and validated as part of your enterprise platform, ready to manage everything from simple web apps to critical APIs. The move from Ingress to the Kubernetes Gateway API is more than a technical swap; it&amp;rsquo;s a new, more powerful model for Kubernetes networking. Red Hat already provides a strong, native foundation for the Kubernetes Gateway API in OpenShift. This collaboration with NGINX builds on that foundation by bringing the new NGINX Gateway Fabric to Red Hat OpenShift, giving you a powerful choice. For many organizations, this is an ideal solution that gives you: A trusted data plane: Teams can now run the NGINX data plane they already trust for its battle-tested performance, all managed natively within Red Hat OpenShift.</description></item><item><title>The new and simplified AI accelerator driver experience on Red Hat Enterprise Linux</title><link>https://kubermates.org/docs/2025-11-12-the-new-and-simplified-ai-accelerator-driver-experience-on-red-hat-enterprise-li/</link><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-12-the-new-and-simplified-ai-accelerator-driver-experience-on-red-hat-enterprise-li/</guid><description>The new and simplified AI accelerator driver experience on Red Hat Enterprise Linux The challenge of GPU driver management, and our solution Why this matters for your AI initiatives Easy installation with rhel-drivers Partner validation: Confidence in running AI accelerators on RHEL RHEL Extensions Repository and Supplementary Repository RHEL Extensions Repository Red Hat Supplementary Repository Confidential computing Getting started Prerequisites Single-Command Installation with rhel-drivers Installing NVIDIA Kernel and User Mode Drivers with rhel-drivers Installing AMD kernel and user mode drivers with rhel-drivers Manual driver installation 1. Enable the Extensions and Supplementary Repositories 2. Identify and install the driver packages 3. Reboot your system 4. Verify the installation Intel NPU Kernel Mode Driver: Validating In BaseOS RHEL: the Foundation for building tomorrow&amp;rsquo;s AI applications Red Hat Enterprise Linux | Product trial About the authors James Huang Scott Herold More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Many existing and popular workloads are getting infused and enhanced with AI, and there will likely emerge a new wave of AI applications in the future. This has led to the increasing importance of AI accelerators, including graphics processing units (GPU) and custom training and inference engines. From discrete GPUs to AI acceleration integrated on-die with the traditional CPU, it&amp;rsquo;s clear that specialized, accelerated hardware is required to provide the performance needed to develop and deploy tomorrow&amp;rsquo;s workloads. That&amp;rsquo;s why we&amp;rsquo;re announcing a new, simplified AI accelerator driver experience on Red Hat Enterprise Linux (RHEL). Whether you&amp;rsquo;re a developer building the next ground-breaking AI application, or an IT systems administrator provisioning servers to deploy AI workloads, RHEL provides a seamless experience to get accelerated systems up and running. You can now acquire AI accelerator drivers from NVIDIA and AMD from Red Hat repositories, built and signed by Red Hat using secure software supply chain practices and Secure Boot technologies. In just one command, you can install the latest available accelerator drivers. Historically, installing and maintaining GPU accelerator drivers with enterprise-grade Linux distributions has presented a unique set of challenges.</description></item><item><title>What's new in RHEL 10.1: Offline assistance, convenient AI accelerators, and more</title><link>https://kubermates.org/docs/2025-11-12-what-s-new-in-rhel-10-1-offline-assistance-convenient-ai-accelerators-and-more/</link><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-12-what-s-new-in-rhel-10-1-offline-assistance-convenient-ai-accelerators-and-more/</guid><description>What&amp;rsquo;s new in RHEL 10.1: Offline assistance, convenient AI accelerators, and more AI accelerators close at hand Image mode updates: Soft-reboots and reproducible builds Increasing developer productivity with updated toolsets Fortifying a next-generation foundation Cloud-crossing consistency with simpler image creation Red Hat Enterprise Linux | Product trial About the author Gil Cattelain More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share During the excitement of the Red Hat Enterprise Linux 10 (RHEL) launch at Red Hat Summit, I kept hearing one question from customers and partners: When would an offline version of the RHEL command-line assistant be available? Today I can announce that it&amp;rsquo;s on the way. As part of the RHEL 10.1 update, an offline, locally available command-line assistant is officially in developer preview. For customers with a Red Hat Satellite subscription, it offers AI-powered RHEL guidance based on decades of enterprise Linux experience. Companies and agencies in finance, government, defense, industrial control, and other heightened-security industries will find it particularly useful. A key advantage of this design is its ability to function in completely disconnected, offline, or air-gapped environments, eliminating the need for external network connectivity. This allows users to receive AI-powered guidance and suggestions for a wide range of Red Hat Enterprise Linux tasks, including questions related to the RHEL installation process, troubleshooting, and more, without compromising security or relying on cloud-based services. Access to the offline, locally available command-line assistant requires a Red Hat Satellite subscription. The RHEL command-line assistant gets another improvement in RHEL 10.1 (and RHEL 9.7, also launching today): an increase in its context limit, from 2KB to 32KB. Expanding the command-line assistant&amp;rsquo;s working memory unlocks more powerful, elaborate interactions. Now it can take on larger log files, pipe extensive data streams, and help with bigger tasks. Recent RHEL updates have focused squarely on making it easier for customers to embrace AI. With RHEL 10.1, we&amp;rsquo;re improving a key aspect of the interaction between the operating system and the hardware that supplies the raw computing power AI demands.</description></item><item><title>Your complete guide for getting started with Red Hat OpenShift Virtualization</title><link>https://kubermates.org/docs/2025-11-12-your-complete-guide-for-getting-started-with-red-hat-openshift-virtualization/</link><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-12-your-complete-guide-for-getting-started-with-red-hat-openshift-virtualization/</guid><description>Your complete guide for getting started with Red Hat OpenShift Virtualization Phase 1: Exploration and foundational training Dive in (no cost) Building foundational expertise (associated costs) Phase 2: Real-world application, mentored implementation and day-2 ops Ready to dive deeper? Start with these free resources Your guided path forward, backed by Red Hat Services Phase 3: Operational evolution and platform ROI Guidance you can count on at every stage Red Hat Learning Subscription | Product Trial About the authors Nicole Yee Courtney Grosch More like this Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share To a seasoned virtual machine (VM) administrator, the shift from a familiar environment to Red Hat OpenShift Virtualization can feel daunting. We understand that your primary concern is not just migrating mission-critical workloads, but rapidly gaining the proficiency to manage a new, integrated platform successfully Migrating off your current hypervisor doesn&amp;rsquo;t have to be a sudden leap. It is a gradual evolution, and Red Hat provides you with guided resources at every stage of the journey. That starts with free trials and learning hubs to support your initial exploration. These tools help you build confidence early on, and smoothly transition into expert-led implementation when you&amp;rsquo;re ready to apply your knowledge in real-world scenarios. This ensures a steady, predictable path toward operational success. In this article we provide recommended training and resource paths to make you and your teams successful as you transition to a new platform. Start by familiarizing yourself with basic concepts and gaining hands-on exposure to OpenShift Virtualization. Most resources are free to access, and are designed to help get you comfortable with managing your enterprise VM workloads with OpenShift Virtualization. Experience OpenShift Virtualization Roadshow : A hands-on experience recommended for VM admins and infrastructure architects to learn about using OpenShift Virtualization as a viable alternative for managing enterprise VM workloads. OpenShift Virtualization Learning Hub : Tools and learning materials designed specifically to help users get started with OpenShift Virtualization, organized by task. 60-day OpenShift free trial : Hands-on experience with Red Hat OpenShift.</description></item><item><title>A Decision Framework for Future-Proofing IT Infrastructure</title><link>https://kubermates.org/docs/2025-11-11-a-decision-framework-for-future-proofing-it-infrastructure/</link><pubDate>Tue, 11 Nov 2025 22:53:57 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-11-a-decision-framework-for-future-proofing-it-infrastructure/</guid><description>Discover more from VMware Cloud Foundation (VCF) Blog Related Articles A Decision Framework for Future-Proofing IT Infrastructure Infrastructure Boundaries, Controls, and Policies with VMware Cloud Foundation 9.0 Analyst Insight Series: Virtualization virtue #3: Supporting application modernization With the rise of AI, hybrid cloud environments, and digital transformation initiatives, IT infrastructure is under unprecedented pressure. For IT leaders and enterprise architects, the challenge isn’t just about keeping up—it’s about making strategic, forward-looking decisions while continuing to leverage existing technology investments. In a new paper “ Future-Proofing IT Infrastructure: A Decision Framework for Enterprise Architects ” HyperFRAME Research provides a structured rubric grounded in operational reality to guide this crucial evolution. The framework outlines eight pragmatic steps for infrastructure modernization: Inventory the Current Infrastructure: A successful modernization effort begins by establishing a clear baseline-—a complete picture of existing workloads, deployment patterns, and supporting platforms. This avoids the risk of investing in the wrong solutions. Evaluate Organizational Structure and Team Skillsets: Technology is only as good as the people and processes that support it. Organizations need to rigorously assess their team’s skills and knowledge gaps, and the cost and time required to bridge them. Determine Non-Functional Requirements (NFRs): While functional requirements define what a system does, NFRs—such as performance, scalability, and resilience—dictate how it operates under pressure. Evaluating infrastructure choices with NFRs in mind is essential for ensuring systems can meet business demands. Compare Virtualization and Containerization Options: With a clear understanding of the current environment, team capabilities, and business needs, organizations can determine which compute model best aligns with workload demands and long-term objectives. Evaluate Platform Capabilities and Integration: The enterprise IT environment is complex, with a myriad of individual infrastructure components. Managing it effectively requires a platform that orchestrates these different components into a cohesive, integrated whole, delivering simplicity with agility and flexibility.</description></item><item><title>Infrastructure Boundaries, Controls, and Policies with VMware Cloud Foundation 9.0</title><link>https://kubermates.org/docs/2025-11-11-infrastructure-boundaries-controls-and-policies-with-vmware-cloud-foundation-9-0/</link><pubDate>Tue, 11 Nov 2025 19:54:21 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-11-infrastructure-boundaries-controls-and-policies-with-vmware-cloud-foundation-9-0/</guid><description>Diving Deeper into Self Service VCF Boundaries VCF Policies VCF Automation Policies Discover more from VMware Cloud Foundation (VCF) Blog Related Articles A Decision Framework for Future-Proofing IT Infrastructure Infrastructure Boundaries, Controls, and Policies with VMware Cloud Foundation 9.0 Analyst Insight Series: Virtualization virtue #3: Supporting application modernization As a Solutions Architect here at Broadcom who talks to our customer engineers on the daily, the biggest feedback I get about VMware Cloud Foundation (VCF) 9.0 is a fear (we’ll call it) about having less control with a self service model: “If you want something done right, you have to do it yourself. ” First of all, you will be doing it yourself through the enforcement of boundaries and policies at the onset. You don’t lose control : it is just front-loaded and implemented by applying the policies and boundaries at provisioning. With this approach you actually have more control with less administrative overhead. Second of all, if you are an infrastructure engineer, you are part of “ self” in “ self service” : in a cloud operating model, you will create infrastructure resources the same way everyone else does: using VCF Automation and Operations where your boundaries and restrictions are imposed in a standard and uniform way. Before we dive in, I would be remiss if I didn’t at least mention that VMware infrastructure as a private cloud is not new. Most of the components mentioned below have been around for more than a decade, and they are in the driver’s seat with VCF. If you are a cloud engineer of any sort, you will find yourself right at home with VCF: that’s the whole point. I have talked about this before , albeit indirectly, but like it or not, you provide infrastructure as a service to the people who consume your infrastructure. When you provision a VM, for example, you are providing a virtual machine and everything that goes with it. If it takes days or weeks for that VM to work its way through approvals, or it’s delayed because “we’re waiting on Derek to get back from vacation,” then you’ve already lost. VCF 9.0 provides a cloud native self service model which gives you more control and more security with less effort.</description></item><item><title>VMware Cloud Foundation 9: Now Ready For All Storage</title><link>https://kubermates.org/docs/2025-11-11-vmware-cloud-foundation-9-now-ready-for-all-storage/</link><pubDate>Tue, 11 Nov 2025 19:12:46 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-11-vmware-cloud-foundation-9-now-ready-for-all-storage/</guid><description>VCF 9 is Ready for All vSphere-supported External Storage What about iSCSI, NFS v4.1, and NVMe over Fabrics? Documentation of Support Statement What’s the simplest way to adopt, run and manage storage for VCF? Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VMware Cloud Foundation 9: Now Ready For All Storage From Tickets to Clicks: Driving Real Consumption in Your Self‑Service Private Cloud Analyst Insight Series: Virtualization virtue #3: Supporting application modernization Building a private cloud on VMware Cloud Foundation™ (VCF) 9 necessitates careful consideration of storage options. Customers must weigh the benefits of each storage option based on their specific needs and existing investments to optimize their vSphere environments. Customers looking to rapidly adopt VCF 9 have asked for workflows to support all existing supported vSphere storage types, so in our latest version, Broadcom has significantly expanded storage options, now allowing the use of NFSv3 or Fibre Channel VMFS datastores as principal storage for the management workload domain during greenfield deployments. This enhancement builds upon the robust, automated VMware vSAN deployment capabilities already in place. VMware Cloud Foundation (VCF) 9 supports a variety of storage types as principal storage (see Storage Models in the product documentation for more details). Typically, principal storage is configured during creation of the management or workload domains or when adding new clusters to an existing workload domain. These scenarios are referred to as greenfield deployments. Additional principal storage types such as iSCSI, NFS v4.1, and NVMe over Fabrics (FC or TCP), which are not currently available in a greenfield deployment, can now be configured as principal storage for VCF 9 by converging an existing vSphere environment or by using VCF import. If the default cluster includes multiple datastore types, VCF determines the primary datastore using the following priority: vSAN NFS v3 VMFS NFS 4.1 iSCSI This process is documented in KB 416270​​. This is also detailed in the techdocs under converging an existing vSphere environment into a VCF instance. Specifically “Any supported vSphere storage type” is supported. For a list of supported vSphere 9 supported storage platforms see the Broadcom compatibility guide.</description></item><item><title>Ingress NGINX Retirement: What You Need to Know</title><link>https://kubermates.org/docs/2025-11-11-ingress-nginx-retirement-what-you-need-to-know/</link><pubDate>Tue, 11 Nov 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-11-11-ingress-nginx-retirement-what-you-need-to-know/</guid><description>Ingress NGINX Retirement: What You Need to Know About Ingress NGINX History and Challenges Current State and Next Steps To prioritize the safety and security of the ecosystem, Kubernetes SIG Network and the Security Response Committee are announcing the upcoming retirement of Ingress NGINX. Best-effort maintenance will continue until March 2026. Afterward, there will be no further releases, no bugfixes, and no updates to resolve any security vulnerabilities that may be discovered. Existing deployments of Ingress NGINX will continue to function and installation artifacts will remain available. We recommend migrating to one of the many alternatives. Consider migrating to Gateway API , the modern replacement for Ingress. If you must continue using Ingress, many alternative Ingress controllers are listed in the Kubernetes documentation. Continue reading for further information about the history and current state of Ingress NGINX, as well as next steps. Ingress is the original user-friendly way to direct network traffic to workloads running on Kubernetes. ( Gateway API is a newer way to achieve many of the same goals. ) In order for an Ingress to work in your cluster, there must be an Ingress controller running. There are many Ingress controller choices available, which serve the needs of different users and use cases.</description></item><item><title>From Tickets to Clicks: Driving Real Consumption in Your Self‑Service Private Cloud</title><link>https://kubermates.org/docs/2025-11-11-from-tickets-to-clicks-driving-real-consumption-in-your-self-service-private-clo/</link><pubDate>Tue, 11 Nov 2025 16:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-11-from-tickets-to-clicks-driving-real-consumption-in-your-self-service-private-clo/</guid><description>Build once, consume everywhere Consumption succeeds when it’s simple and safe Automation is the new default Security and governance without slowing adoption Private AI and advanced use cases People, process, and measurable outcomes Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VMware Cloud Foundation 9: Now Ready For All Storage From Tickets to Clicks: Driving Real Consumption in Your Self‑Service Private Cloud Analyst Insight Series: Virtualization virtue #3: Supporting application modernization Tired of slow, ticket‑driven provisioning and fragmented cloud operations? The Self‑Service Private Cloud Dummies Guide shows you how to transform your virtual data center into a speed‑first, policy‑driven private cloud that gives developers instant access to the infrastructure they need — while IT keeps full control. This isn’t just about building infrastructure; it’s about creating a product people actually want to consume. By delivering one portal, one API, and one catalog, you provide a seamless experience that accelerates adoption, reduces friction, and delivers measurable business value. Built on VMware Cloud Foundation , this approach flips the script from managing infrastructure to enabling outcomes. The guide serves as a practical playbook for delivering a unified self‑service experience that cloud admins love to operate — and developers love to use. A modern private cloud needs more than infrastructure — it needs a consumption layer that makes adoption effortless. That begins with a unified experience: one portal, one API, and one catalog that serve all workloads, whether VMs, Kubernetes clusters, or GPU‑enabled AI environments. Curated blueprints provide pre‑approved, reusable templates that embed security and compliance into every deployment. Version‑controlled content, delivered through GitOps‑driven workflows, ensures consistency and trust across teams. A developer‑friendly user experience, supported by low‑code and GUI‑driven infrastructure‑as‑code, lowers barriers and drives adoption. The result: teams launch full environments in minutes, not days — and keep coming back. Adoption grows when the platform feels intuitive and trustworthy.</description></item><item><title>CNCF and SlashData Report Finds Leading AI Tools Gaining Adoption in Cloud Native Ecosystems</title><link>https://kubermates.org/docs/2025-11-11-cncf-and-slashdata-report-finds-leading-ai-tools-gaining-adoption-in-cloud-nativ/</link><pubDate>Tue, 11 Nov 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-11-cncf-and-slashdata-report-finds-leading-ai-tools-gaining-adoption-in-cloud-nativ/</guid><description>New report provides maturity and recommendation scores for tools and projects across AI inference, ML orchestration, and agentic AI platforms Key Highlights: CNCF and SlashData’s new report highlights the top AI tools gaining traction in cloud native ecosystems, with NVIDIA Triton, Metaflow, Airflow, and Model Context Protocol leading in adoption and developer trust. For AI inference tools, NVIDIA Triton, DeepSpeed, TensorFlow Serving, and BentoML are the projects developers cumulatively placed in the adopt position. Further, NVIDIA Triton received the highest ratings for both maturity and usefulness. In the ML orchestration category, Airflow and Metaflow rose to the adopt position. Metaflow earned the highest maturity ratings, while Airflow had the highest usefulness ratings. In agentic AI platforms, Model Context Protocol (MCP) and Llama Stack reached the adopt classification. MCP leads on maturity and usefulness; meanwhile, Agent2Agent posted the highest recommendation score at 94%. ATLANTA, KUBECON + CLOUDNATIVECON NORTH AMERICA. – November 11, 2025 – The Cloud Native Computing Foundation® (CNCF®), which builds sustainable ecosystems for cloud native software, released new findings from the Q4 2025 CNCF Technology Landscape Radar report with SlashData , uncovering insight on developers’ experience and opinions on AI inference tools and engines, agentic AI platforms and projects, and ML orchestration tools. “Organizations building and operating AI systems can’t treat tooling the way they did five years ago,” said Chris Aniszczyk, CTO, CNCF. “What this new research affirms is that cloud native principles from scalable infrastructure and orchestration are foundational not just for backend apps, but for inference pipelines and agentic AI systems. Choosing technologies rated ‘adopt’ helps reduce risk and increase productivity.</description></item><item><title>CNCF and SlashData Survey Finds Cloud Native Ecosystem Surges to 15.6M Developers</title><link>https://kubermates.org/docs/2025-11-11-cncf-and-slashdata-survey-finds-cloud-native-ecosystem-surges-to-15-6m-developer/</link><pubDate>Tue, 11 Nov 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-11-cncf-and-slashdata-survey-finds-cloud-native-ecosystem-surges-to-15-6m-developer/</guid><description>New research reveals 15.6 million developers now use cloud native technologies, with backend and DevOps professionals leading adoption Key Highlights: CNCF and SlashData release State of Cloud Native Development research report. Survey finds cloud native adoption has reached 15.6 million developers globally, with backend and DevOps (58%) professionals leading the way. Research shows 77% of backend developers are using at least one cloud native technology. The report notes cloud infrastructure strategies are shifting: hybrid cloud use has grown to 32% and multi-cloud to 26% among developers, and distributed cloud is emerging among backend teams. ATLANTA, KUBECON + CLOUDNATIVECON NORTH AMERICA – November 11, 2025 – The Cloud Native Computing Foundation® (CNCF®), which builds sustainable ecosystems for cloud native software, today released new insights from its latest State of Cloud Native Development report with SlashData , revealing the evolving footprint of cloud native development across backend, AI and DevOps communities. The findings reflect the growing adoption and diversification of cloud native practices globally and the effects of cloud native in AI. “The data confirms what we’re seeing in the ecosystem: cloud native is expanding far beyond traditional backend and container infrastructure use cases,” said Chris Aniszczyk, CTO, CNCF. “From platform engineering to AI, developers are incorporating cloud native technologies to meet reliability, scalability and operational needs. ” Cloud Native Adoption Reaches Scale Cloud native technologies are now a cornerstone of modern software delivery. The 15.6 million developers adopting these tools represent a critical mass driving efficiency, automation, and resiliency. 77% of backend developers report using at least one cloud native technology, highlighting greater adoption. Backend and DevOps professionals lead adoption at 58%, reflecting cloud native’s deep integration in enterprise operations.</description></item><item><title>CNCF Launches Certified Kubernetes AI Conformance Program to Standardize AI Workloads on Kubernetes</title><link>https://kubermates.org/docs/2025-11-11-cncf-launches-certified-kubernetes-ai-conformance-program-to-standardize-ai-work/</link><pubDate>Tue, 11 Nov 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-11-cncf-launches-certified-kubernetes-ai-conformance-program-to-standardize-ai-work/</guid><description>Supporting Quotes New initiative targets cloud native AI portability and reliability across environments Key Highlights CNCF and the Kubernetes open source community are launching the Certified Kubernetes AI Conformance Program to create open, community-defined standards for running AI workloads on Kubernetes. As organizations increasingly move AI workloads into production, they need consistent and interoperable infrastructure. This initiative helps reduce fragmentation and ensures reliability across environments. Platform vendors, infrastructure teams, enterprise AI practitioners, and open source contributors in Kubernetes and across the cloud native ecosystem need a common foundation for seeking interoperable and production-ready AI deployments. The program is now available and being developed in the open to encourage broader participation. KUBECON + CLOUDNATIVECON NORTH AMERICA, ATLANTA — Nov. 11, 2025 — The Cloud Native Computing Foundation® (CNCF®), which builds sustainable ecosystems for cloud native software, today announced the launch of the Certified Kubernetes AI Conformance Program at KubeCon + CloudNativeCon North America. The new program introduces a community-led effort to define and validate standards for running artificial intelligence (AI) workloads reliably and consistently on Kubernetes. ] The program outlines a minimum set of capabilities and configurations required to run widely used AI and machine learning frameworks on Kubernetes. The initiative seeks to give enterprises confidence in deploying AI on Kubernetes while providing vendors a common baseline for compatibility. Announced in its beta phase at KubeCon + CloudNativeCon Japan in June, the Kubernetes AI Conformance Program has successfully certified its initial participants with a v1.0 release and have started to work on a roadmap for a v2.0 release next year. The growing use of Kubernetes for AI workloads highlights the importance of common standards.</description></item><item><title>CNCF Launches CNPE Certification to Define Enterprise-Scale Platform Engineering Globally</title><link>https://kubermates.org/docs/2025-11-11-cncf-launches-cnpe-certification-to-define-enterprise-scale-platform-engineering/</link><pubDate>Tue, 11 Nov 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-11-cncf-launches-cnpe-certification-to-define-enterprise-scale-platform-engineering/</guid><description>The Certified Cloud Native Platform Engineer (CNPE) joins CNCF’s expanding certification portfolio as it meets growth training demand Key Highlights The Certified Cloud Native Platform Engineer (CNPE) joins CNCF’s certification portfolio and Certified Kubernetes Network Engineer (CKNE) is announced as the next upcoming certification in CNCF’s portfolio. In 2026, CNCF will release a new certification Certified Kubernetes Network Engineer (CKNE), an intermediate-level exam for networking of Kubernetes and cloud native platforms. Since its 2024 launch, CNCF’s Kubestronaut program has recognized more than 2,500 participants across 100+ countries, underscoring worldwide demand for cloud native expertise. KUBECON + CLOUDNATIVECON NORTH AMERICA, ATLANTA, Georgia – November 11, 2025 – The Cloud Native Computing Foundation ® (CNCF®), which builds sustainable ecosystems for cloud native software, today announced the global availability of the Certified Cloud Native Platform Engineer (CNPE) certification—a new credential designed to validate advanced, real-world expertise in architecting and operating enterprise-scale internal developer platforms (IDPs), as the community convenes in Atlanta for KubeCon + CloudNativeCon North America 2025 As organizations worldwide invest in platform engineering to scale developer velocity and reliability, the CNPE offers a clear pathway for professionals advancing from foundational knowledge to strategic leadership roles. Developed in collaboration with Linux Foundation Training &amp;amp; Certification , CNPE certifies proficiency in platform architecture, GitOps, observability, security, and developer experience across production-grade cloud native systems. “Platform engineering teams are quickly becoming the backbone of organizations looking to scale modern infrastructure, and with CNPE, we’re defining what excellence looks like at the open source and enterprise level by providing a career advantage to individuals,” said Chris Aniszczyk, CTO, CNCF. “This launch sets a global standard for the skills needed to design and operate secure and scalable platforms that empower teams and accelerate innovation. ” The CNPE is a performance-based, 120-minute certification exam that builds on the Certified Cloud Native Platform Associate (CNPA). It covers five key domains: Platform Architecture and Infrastructure (15%), GitOps and Continuous Delivery (25%), Platform APIs and Self-Service Capabilities (25%), Observability and Operations (20%), and Security and Policy Enforcement (15%). Additionally, CNCF will have another new certification coming soon called Certified Kubernetes Network Engineer (CKNE). The CKNE certification will be a practical based, intermediate-level exam for networking of Kubernetes and cloud native platforms. The certification exam will be created by members of the cloud native community to reflect best practices without any vendor lock-in.</description></item><item><title>KServe becomes a CNCF incubating project</title><link>https://kubermates.org/docs/2025-11-11-kserve-becomes-a-cncf-incubating-project/</link><pubDate>Tue, 11 Nov 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-11-kserve-becomes-a-cncf-incubating-project/</guid><description>What is KServe? Key Milestones and Ecosystem Growth Integrations Across the Cloud Native Landscape Technical Components Community Highlights Maintainer Perspective From the TOC Looking Ahead Posted on November 11, 2025 by KServe Project Maintainers CNCF projects highlighted in this post The CNCF Technical Oversight Committee (TOC) has voted to accept KServe as a CNCF incubating project. KServe joins a growing ecosystem of technologies tackling real-world challenges at the edge of cloud native infrastructure. KServe is a standardized distributed generative and predictive AI inference platform for scalable, multi-framework deployment on Kubernetes. It is designed as a single, unified platform for both generative and predictive AI inference on Kubernetes. While simple enough for quick deployments, KServe is powerful enough to handle enterprise-scale AI workloads with advanced features. KServe originated in 2019 as a collaborative effort by Google, IBM, Bloomberg, NVIDIA, and Seldon under the Kubeflow project. It was later donated to the LF AI &amp;amp; Data Foundation in February 2022. In September 2022, the project rebranded from KFServing to the standalone KServe, graduating from Kubeflow. KServe then moved to CNCF as an incubator in September 2025. KServe has demonstrated consistent and growing adoption across diverse industries and geographies, with production deployments ranging from large-scale multi-cloud enterprise platforms to specialized internal AI infrastructure. The project is used by major organizations, including Bloomberg, Red Hat, Cloudera, CyberAgent, Nutanix, SAP, NVIDIA, and others, spanning sectors such as enterprise software, cloud infrastructure, online media, gaming, and financial services. Deployments support both generative and predictive AI workloads at scale.</description></item><item><title>Lima becomes a CNCF incubating project</title><link>https://kubermates.org/docs/2025-11-11-lima-becomes-a-cncf-incubating-project/</link><pubDate>Tue, 11 Nov 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-11-lima-becomes-a-cncf-incubating-project/</guid><description>What is Lima? Where Does It Fit in the Cloud Native Landscape? Use cases Lima’s Beginnings and Growth Maintainer Perspective From the TOC Main Components: Notable Milestones Latest release Posted on November 11, 2025 by Lima Project Maintainers CNCF projects highlighted in this post The CNCF Technical Oversight Committee (TOC) has voted to accept Lima as a CNCF incubating project. Lima enables secure, isolated environments for running cloud native and AI workloads. Lima, short for “Linux Machines,” provides Linux virtual machines optimized for running containers in local development environments. Lima comes with built-in integration for the following container engines : containerd [CNCF Graduated] (default) Docker Podman [CNCF Sandbox] Kubernetes [CNCF Graduated] k3s [CNCF Sandbox] k0s [CNCF Sandbox] Usernetes RKE2 k3s [CNCF Sandbox] k0s [CNCF Sandbox] Usernetes RKE2 Apptainer Lima is also known to be useful for a variety of other use cases beyond containerization. One of the most edgy use cases is to run an AI coding agent inside a VM in order to isolate the agent from direct access to host files and commands. This setup ensures that even if an AI agent is deceived by malicious instructions searched from the Internet (e. g. , fake package installations), any potential damage is confined within the VM or limited to files specified to be mounted from the host. The Lima website features several examples of hardening AI agents: Aider Claude Code Codex Gemini GitHub Copilot CLI GitHub Copilot in Visual Studio Code The Lima project was launched in May 2021 by Akihiro Suda, a maintainer of containerd and numerous other projects in the container ecosystem. The project was initially designed as a “containerd machine” aiming to demonstrate and promote containerd, including nerdctl (contaiNERD CTL), to Mac users. Later the project scope was expanded to support other container engines and non-container applications as well. Lima supports non-macOS hosts, such as Linux, NetBSD, and Windows.</description></item><item><title>OpenFGA Becomes a CNCF Incubating Project</title><link>https://kubermates.org/docs/2025-11-11-openfga-becomes-a-cncf-incubating-project/</link><pubDate>Tue, 11 Nov 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-11-openfga-becomes-a-cncf-incubating-project/</guid><description>What is OpenFGA? OpenFGA’s History Maintainer Perspective From the TOC Main Components Notable Milestones Looking Ahead Posted on November 11, 2025 by OpenFGA Project Maintainers CNCF projects highlighted in this post The CNCF Technical Oversight Committee (TOC) has voted to accept OpenFGA as a CNCF incubating project. OpenFGA is an authorization engine that addresses the challenge of implementing complex access control at scale in modern software applications. Inspired by Google’s global access control system, Zanzibar, OpenFGA leverages Relationship-Based Access Control (ReBAC). This allows developers to define permissions based on relationships between users and objects (e. g. , who can view which document). By serving as an external service with an API and multiple SDKs, it centralizes and abstracts the authorization logic out of the application code. This separation of concerns significantly improves developer velocity by simplifying security implementation and ensures that access rules are consistent, scalable, and easy to audit across all services, solving a critical complexity problem for developers building distributed systems. OpenFGA was developed by a group of Okta employees, and is the foundation for the Auth0 FGA commercial offering. The project was accepted as a CNCF Sandbox project in September 2022. Since then, it has been deployed by hundreds of companies and received multiple contributions. Some major moments and updates include: 37 companies publicly acknowledge using it in production.</description></item><item><title>How to Choose the Right Course During Black Friday Sales</title><link>https://kubermates.org/docs/2025-11-11-how-to-choose-the-right-course-during-black-friday-sales/</link><pubDate>Tue, 11 Nov 2025 11:02:11 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-11-how-to-choose-the-right-course-during-black-friday-sales/</guid><description>Why Black Friday Is the Best Time to Invest in Learning Common Mistakes People Make When Buying Courses on Sale 1. Buying Too Many Random Courses 2. Choosing Based Only on Discounts 3. Ignoring the Instructor and Course Reviews 4. Skipping the Basics 5. Forgetting to Plan for Consistent Learning Step-by-Step: How to Choose the Right Course for You Step 1: Define Your Learning Goal Step 2: Look for Structured Learning Paths Step 3: Prioritize Hands-On Labs Over Theory Step 4: Check the Instructor and Course Quality Step 5: Consider Value, Not Just Price Step 6: Plan Your Study Schedule Step 7: Choose Long-Term Learning Ecosystems Top KodeKloud Courses Worth Grabbing This Black Friday Explore KodeKloud’s AI Learning Path - Built for Everyone, Taught Through Doing DevOps Learning: Pick Your Starting Point Master the Cloud - Because Every Modern Infrastructure Runs on It New to IT or Not from a Technical Background? Start Here - Your Foundation Awaits Final Tips for Getting the Best Value 1. Focus on Skills, Not Discounts 2. Choose Hands-On Learning Over Passive Watching 3. Start with One Path - Then Expand 4. Make Learning a Routine 5. Don’t Miss the Deadline Ready to Invest in Yourself? FAQs How to Use the cURL Command to Download Files Freelancing Tips: What I Learned After 100+ Clients Why Tech Leads Are The Most in-Demand Role of Gen AI and Agentic AI Evolution Kubernetes Best Practices in 2025: Scaling, Security, and Cost Optimization Top Kubernetes Certifications in 2025: Which One Should You Choose? Learn how to choose the right course for your goals and avoid buying random, unfinished ones. Explore structured Learning Paths in DevOps, Cloud (AWS, Azure, GCP), AI, and IT Foundations.</description></item><item><title>A deeper look at post-quantum cryptography support in Red Hat OpenShift 4.20 control plane</title><link>https://kubermates.org/docs/2025-11-11-a-deeper-look-at-post-quantum-cryptography-support-in-red-hat-openshift-4-20-con/</link><pubDate>Tue, 11 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-11-a-deeper-look-at-post-quantum-cryptography-support-in-red-hat-openshift-4-20-con/</guid><description>A deeper look at post-quantum cryptography support in Red Hat OpenShift 4.20 control plane The quantum threat PQC in Kubernetes and OpenShift Applying PQC to the OpenShift control plane The Red Hat perspective The Go version mismatch What about etcd? The road ahead Red Hat OpenShift Container Platform | Product Trial About the author JP Jung More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The age of quantum computing is on the horizon, and with its immense processing power comes a significant threat to the cryptographic foundations of our digital world. In this article, we&amp;rsquo;ll explore the emerging support for post-quantum cryptography (PQC) in Red Hat OpenShift 4.20 , focusing on how it enhances the core components of the Kubernetes control plane: the apiserver, kubelet, scheduler, and controller-manager. Missing is etcd, using an older version of Go. Today&amp;rsquo;s widely used public-key cryptosystems, such as RSA and elliptic curve cryptography (ECC), form the foundation of security-enhanced online communication. These systems are vulnerable to attacks from large-scale quantum computers, however, which can solve the mathematical problems underlying these algorithms with alarming speed. This has given rise to attacks in which adversaries record encrypted traffic today to decrypt it in the future once they have access to a powerful quantum computer. The same challenge applies to data at rest if an adversary manages to make a copy now to decrypt later. To counter this threat, the field of PQC has emerged, developing new cryptographic algorithms that are resistant to attacks from both classical and quantum computers. Red Hat OpenShift is a Cloud Native Computing Foundation (CNCF) certified distribution of Kubernetes, and Kubernetes is written in the Go programming language. Thus, the journey to PQC support in OpenShift begins with Go. The Go 1.24 release marked a significant milestone by introducing support for the X25519MLKEM768 hybrid key exchange mechanism. X25519MLKEM768 is a hybrid key exchange that combines classical X25519 (elliptic curve Diffie-Hellman) with ML-KEM-768 (the post-quantum algorithm).</description></item><item><title>Bringing intelligent, efficient routing to open source AI with vLLM Semantic Router</title><link>https://kubermates.org/docs/2025-11-11-bringing-intelligent-efficient-routing-to-open-source-ai-with-vllm-semantic-rout/</link><pubDate>Tue, 11 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-11-bringing-intelligent-efficient-routing-to-open-source-ai-with-vllm-semantic-rout/</guid><description>Bringing intelligent, efficient routing to open source AI with vLLM Semantic Router What is vLLM Semantic Router? vLLM Semantic Router and llm-d Enterprise and community value About the author Huamin Chen More like this Blog post Blog post Original podcast Original podcast Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The speed of innovation in large language models (LLMs) is astounding, but as enterprises move these models into production, the conversation shifts - it’s no longer just about raw scale; it’s about per-token efficiency and smart, targeted compute use. Simply put, not all prompts require the same level of reasoning. If a user has a simple request, like, &amp;ldquo;What is the capital of North Carolina?&amp;rdquo; a multi-step reasoning process required for say, a financial projection, isn’t necessary. If organizations use heavyweight reasoning models for every request, the result is both costly and inefficient. This dilemma is what we call the challenge of implementing reasoning budgets, and it’s why Red Hat developed vLLM Semantic Router, an open source project that intelligently selects the best model for each task, optimizing cost and efficiency while maximizing ease of use. vLLM Semantic Router is an open source system that acts as an intelligent, cost-aware request routing layer for the highly efficient vLLM inference engine. Think of it as the decision-maker for your LLM inference pipeline - it addresses efficiency challenges through dynamic, semantic-aware routing by: Utilizing a lightweight classifier, like ModernBERT or other pre-trained models, to analyze the query’s intent and complexity. Routing simple queries to a smaller, faster LLM or a non-reasoning model to save compute resources. Directing complex requests requiring deep analysis to more powerful, reasoning-enabled models. vLLM Semantic Router’s purpose is to ensure every token generated adds value. Written in Rust and using Hugging Face’s Candle framework, the router delivers low latency and high concurrency and is engineered for high performance. With the power of open source, vLLM Semantic Router promotes model flexibility by offering efficient model switching and semantic-aware routing.</description></item><item><title>Create efficient two-node edge infrastructure with Red Hat OpenShift and Portworx/Pure Storage</title><link>https://kubermates.org/docs/2025-11-11-create-efficient-two-node-edge-infrastructure-with-red-hat-openshift-and-portwor/</link><pubDate>Tue, 11 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-11-create-efficient-two-node-edge-infrastructure-with-red-hat-openshift-and-portwor/</guid><description>Create efficient two-node edge infrastructure with Red Hat OpenShift and Portworx/Pure Storage The edge dilemma: High availability vs. cost optimization Two-node OpenShift with arbiter explained What&amp;rsquo;s the high-availability stance of TNA? Is the arbiter node a regular node? Unified data services at the edge Cost efficiency and resilience for the open hybrid cloud Red Hat OpenShift Container Platform | Product Trial About the authors Paul Lancaster Daniel Froehlich Andy Gower (Pure Storage) More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The demand to extend applications to the edge has never been greater. From retail shops to industrial and manufacturing sites, there&amp;rsquo;s a need to create, consume, and store data at the edge. Deploying applications at the edge comes with a set of physical constraints, but also with the need to deliver a truly cost-efficient and resilient architecture. When building applications at the edge, you must consider the needs of the individual site as well as the cost to deploy, manage, and maintain applications across multiple edge locations. The good news is that Red Hat OpenShift is evolving to meet this demand head-on. With the introduction of the two-node OpenShift with arbiter topology , Red Hat and partners like Portworx by Pure Storage are delivering a cost-efficient and resilient architecture designed specifically for the edge. The primary motivation behind the two-node OpenShift with arbiter (TNA) initiative is simple: Cost for large-scale edge deployments. For mission-critical edge sites, high availability is non-negotiable. Should a node fail, applications must continue running without disruption, which is why the control plane requires a quorum (typically three or more nodes) to prevent split-brain scenarios and maintain consistency (a principle known as the CAP theorem). Two-node OpenShift with arbiter, now generally available (GA) with OpenShift 4.20, solves this by dramatically reducing the hardware footprint while preserving three-node consistency and availability. Two-node OpenShift with arbiter architecture is a specialized, cost-sensitive solution that is technically a three-node cluster.</description></item><item><title>KServe joins CNCF as an incubating project</title><link>https://kubermates.org/docs/2025-11-11-kserve-joins-cncf-as-an-incubating-project/</link><pubDate>Tue, 11 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-11-kserve-joins-cncf-as-an-incubating-project/</guid><description>KServe joins CNCF as an incubating project The critical engine behind Red Hat OpenShift AI Unlocking enterprise AI value Join the Movement! About the author Yuan Tang More like this Blog post Blog post Original podcast Original podcast Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share We are excited to share that KServe , the leading standardized AI inference platform on Kubernetes, has been accepted as an incubating project by the Cloud Native Computing Foundation (CNCF). This milestone validates KServe’s maturity, stability and role as the foundation for scalable, multi-framework model serving in production environments. By moving into the CNCF’s neutral governance, KServe’s development will be driven purely by community needs, accelerating its standardization for serving AI models on Kubernetes. For Red Hat this is a validation of our commitment to delivering open, reliable and standardized AI solutions for the hybrid cloud. At Red Hat, we believe the best AI infrastructure is built on open standards and Kubernetes. KServe is the critical model serving component that powers Red Hat OpenShift AI, helping ensure our customers can transition from model experimentation to production inference seamlessly and at scale. OpenShift AI leverages KServe’s features to solve the biggest enterprise AI challenges, helping enterprises realize: High-performance LLM optimization - KServe is optimized for large language models (LLMs), providing high-performance features like KV cache offloading, distributed inference with vLLM, as well as disaggregated serving, pre-fix caching, intelligent scheduling and variant autoscaling via the integration with llm-d. Advanced autoscaling - In addition to the horizontal pod autoscaling capability from Kubernetes, KServe also supports autoscaling with KEDA (Kubernetes Event-driven Autoscaler), which enables event-driven scaling based on external metrics such as vLLM metrics. Both predictive and generative AI model inference - KServe supports pluggable, reusable, extensible runtimes, ranging from scikit-learn and XGBoost for predictive AI to Hugging Face and vLLM for generative AI model inference. This helps ensure that enterprises can switch to the best runtime for specific use cases. The journey of AI from the lab to the bottom line requires production infrastructure that can handle exponential growth, especially as enterprise usage shifts to widespread generative applications. Now bolstered by the full resources and neutral governance of the CNCF, KServe directly addresses these core operational challenges - from tackling complexity with a unified API to controlling cloud costs through its scale-to-zero capabilities.</description></item><item><title>Red Hat OpenShift 4.20 accelerates virtualization and enterprise AI innovation</title><link>https://kubermates.org/docs/2025-11-11-red-hat-openshift-4-20-accelerates-virtualization-and-enterprise-ai-innovation/</link><pubDate>Tue, 11 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-11-red-hat-openshift-4-20-accelerates-virtualization-and-enterprise-ai-innovation/</guid><description>Red Hat OpenShift 4.20 accelerates virtualization and enterprise AI innovation Distribute and scale AI/ML workloads with LeaderWorkerSet and JobSet Native AI routing with Red Hat OpenShift AI 3 Update AI models without touching your pods Chat with your OpenShift or Kubernetes clusters with natural language Accelerate AI workloads with NVIDIA Bluefield DPUs Streamline identity management with native OIDC support Manage workload identities with Zero Trust on OpenShift Streamline secrets management with External Secrets Operator Eliminate container privilege escalation risks with user namespaces Service mesh without sidecars with Istio’s ambient mode Native BGP support in OpenShift Networking Catch issues before updating your cluster Optimize edge deployments with two node OpenShift Load-aware rebalancing and faster VM migration in Red Hat OpenShift Virtualization 4.20 Scale virtualization workloads in Oracle Cloud Infrastructure Expand OpenShift into Oracle sovereign clouds Try Red Hat OpenShift 4.20 today Red Hat OpenShift Container Platform | Product Trial About the authors Ju Lim Subin Modeel More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Red Hat OpenShift 4.20 is now generally available. It&amp;rsquo;s based on Kubernetes 1.33 and CRI-O 1.33 and, together with Red Hat OpenShift Platform Plus , this release underscores our commitment to provide a trusted, comprehensive, and consistent application platform. On OpenShift, AI workloads, containers, and virtualization seamlessly co-exist, enabling enterprises to innovate faster across the hybrid cloud, without compromising on security. Available in self-managed or fully managed cloud service editions, OpenShift offers an application platform with a complete set of integrated tools and services for cloud-native, AI, virtual and traditional workloads alike. This article highlights the latest OpenShift 4.20 innovations and key enhancements. For a comprehensive list of updates and improvements, refer to the OpenShift 4.20 release notes. This latest release boosts the platform’s capabilities for AI workloads with the general availability of LeaderWorkerSet , Gateway API inference extension, and runtime Open Container Initiative (OCI) image volume source. It also strengthens core functionalities, introducing vital improvements like bring-your-own external identity provider, Zero Trust Workload Identity Manager, user namespace, External Secrets Operator for Red Hat OpenShift , Border Gateway Protocol (BGP) support. Furthermore, OpenShift 4.20 brings key advancements in virtualization, such as CPU load aware rebalancing and faster live migration, providing a more robust and versatile platform for diverse computing needs. OpenShift 4.20 now enables the X25519MLKEM768 hybrid key exchange mechanism with the Go 1.24 release as part of our journey to add support for post-quantum cryptography (PQC). This enhances TLS communications between core Kubernetes control plane components like the API server, kubelet, scheduler, and controller-manager, helping protect them from quantum-based attacks. Red Hat&amp;rsquo;s latest releases of Trusted Artifact Signer 1.3 and Trusted Profile Analyzer 2.2 deliver a powerful combination of cryptographic signing capabilities and advanced supply chain analysis.</description></item><item><title>Red Hat OpenShift 4.20: Expanded Oracle cloud infrastructure support</title><link>https://kubermates.org/docs/2025-11-11-red-hat-openshift-4-20-expanded-oracle-cloud-infrastructure-support/</link><pubDate>Tue, 11 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-11-red-hat-openshift-4-20-expanded-oracle-cloud-infrastructure-support/</guid><description>Red Hat OpenShift 4.20: Expanded Oracle cloud infrastructure support Complete Oracle cloud infrastructure support Oracle Distributed Cloud Services: Oracle OCI Edge Cloud Services: Industries benefitting from OpenShift and Oracle cloud services Consistent installation experience Get started today Looking ahead Red Hat OpenShift Container Platform | Product Trial About the authors Marcos Entenza Linh Nguyen More like this Blog post Blog post Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Red Hat OpenShift 4.20 brings significant expansion of support across Oracle&amp;rsquo;s diverse cloud infrastructure services. This enhancement delivers OpenShift&amp;rsquo;s enterprise-grade container platform to additional Oracle cloud services, providing your organization with greater flexibility and choice in your deployment strategy. OpenShift 4.20 introduces support for five new Oracle cloud infrastructure services: General Availability: EU Sovereign Cloud : Full production support for organizations requiring data sovereignty and regulatory compliance within European borders Technology Preview : Oracle US Government Cloud : Specialized infrastructure designed for US federal agencies and contractors Oracle Cloud for UK Government &amp;amp; Defence : Dedicated cloud services for UK public sector organizations Oracle Cloud Isolated Regions : Enhanced security and isolation for sensitive workloads Oracle Alloy : Oracle&amp;rsquo;s cloud-at-customer offering for maximum control and customization With OpenShift 4.20, organizations can now deploy across Oracle&amp;rsquo;s comprehensive cloud portfolio, including: Commercial Public Cloud Dedicated Region Oracle Cloud Isolated Region (New in OpenShift 4.20) Oracle Alloy (New in OpenShift 4.20) Oracle US Government Cloud (New in OpenShift 4.20) Oracle UK Government Cloud (New in OpenShift 4.20) EU Sovereign Cloud (New in OpenShift 4.20) Private Cloud Appliance Oracle Cloud@Customer This expansion addresses the diverse needs of modern enterprises, including: Government and public sector : Organizations can deploy OpenShift in specialized government clouds that meet strict regulatory and security requirements. Data sovereignty : EU Sovereign Cloud enables European organizations to maintain complete data residency and compliance with regional regulations. Enhanced security : Oracle Cloud Isolated Region provides additional security layers for sensitive workloads requiring maximum isolation. Hybrid flexibility : Oracle Alloy enables organizations to bring Oracle&amp;rsquo;s cloud capabilities on-premises while maintaining OpenShift compatibility. Edge computing : Oracle OCI Edge Cloud services, which includes Private Cloud Appliance and Oracle Cloud@Customer, provide organizations with on-premises and edge deployment options while maintaining cloud-native capabilities. OpenShift 4.20 can be deployed in both connected and disconnected environments across all Oracle cloud services. Organizations with internet connectivity can leverage an interactive installation experience. If organizations are operating in air-gapped or disconnected environments, we provide specialized installation methods designed specifically for enhanced security and isolation. Ready to explore OpenShift 4.20 on Oracle Cloud? Choose from Oracle&amp;rsquo;s full spectrum of cloud services to match your specific requirements, from public cloud deployments to sovereign clouds, government-specific infrastructure, and on-premises solutions. This expansion represents our continued commitment to provide OpenShift users with maximum deployment flexibility across leading cloud platforms.</description></item><item><title>The strategic shift: How Ford and Emirates NBD stopped paying the complexity tax for virtualization</title><link>https://kubermates.org/docs/2025-11-11-the-strategic-shift-how-ford-and-emirates-nbd-stopped-paying-the-complexity-tax-/</link><pubDate>Tue, 11 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-11-the-strategic-shift-how-ford-and-emirates-nbd-stopped-paying-the-complexity-tax-/</guid><description>The strategic shift: How Ford and Emirates NBD stopped paying the complexity tax for virtualization The business problem: When legacy meets agility The solution: One platform to unify the estate Lessons from the migration trenches The strategic results The future is autonomous and AI-ready A final word of advice About the author Ashesh Badani More like this Blog post Blog post Original podcast Original podcast Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share For most large-scale enterprises today, the hybrid cloud isn&amp;rsquo;t a strategy, it’s just the reality. Most organizations are running in both worlds: they have modern, cloud-native applications in containers, and critical, often mission-critical, systems in virtual machines (VMs). The reality is that running two separate virtualization stacks creates silos, complexity, and unnecessary operational cost – what can be called the complexity tax. It slows down your operations and application teams, strains budgets, and ultimately makes it harder to deliver value to the business. We recently spoke with leaders who decided to tackle this fragmentation head-on: Sandeep Kulkarni from Ford Motor Co. and Ali Rey from Emirates NBD. They shared their journey of unifying VMs and containers on one cohesive platform. Their story offers a clear, straightforward playbook for any IT leader facing a similar challenge. Both Ford and Emirates NBD are giants in their respective industries, but they faced the same fundamental challenge: not every application needs to be modernized but they can be future-proofed with a modern platform. Ford , an iconic, 122-year-old brand, runs thousands of applications. While the company has an aggressive modernization journey underway, many core systems may remain in VMs waiting for the application modernization journey or due to their fundamental architecture, compliance, or legacy vendor support. Emirates NBD , one of the largest banks in the Middle East, operates 24/7.</description></item><item><title>Analyst Insight Series: Virtualization virtue #3: Supporting application modernization</title><link>https://kubermates.org/docs/2025-11-10-analyst-insight-series-virtualization-virtue-3-supporting-application-modernizat/</link><pubDate>Mon, 10 Nov 2025 14:24:50 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-10-analyst-insight-series-virtualization-virtue-3-supporting-application-modernizat/</guid><description>Discover more from VMware Cloud Foundation (VCF) Blog Related Articles From Tickets to Clicks: Driving Real Consumption in Your Self‑Service Private Cloud Analyst Insight Series: Virtualization virtue #3: Supporting application modernization VCF Breakroom Chats Episode 69 - Beyond vRA + NSX: Delivering Cloud-Native Networking with VCF 9 Virtual Private Clouds Guest post by Jean Atelsek, Senior Research Analyst,S&amp;amp;P Global Market Intelligence 451 Research This blog is the third in our series on the benefits and trends of virtualization (read blog#1 and blog#2 here) and a companion to the 451 Research Business Impact Brief “ The virtues of virtualization. ” Application modernization means different things to different people, but its universal goal is to make information technology more responsive to business needs. Virtualization – the ability to run multiple operating systems on a single physical machine – planted the seed for the cloud computing paradigm that underlies most modern software development, and it also represents a critical bridge between legacy environments and the container-based and AI-driven deployments revolutionizing how we interact with IT. What does a modern workload look like? Microservices-based architectures, which put software functionality in small, flexible and portable application containers, are one hallmark of cloud-native deployments. 451 Research expects this market to grow at a 15% CAGR to reach $18 billion by 2029. The boom in AI/ML-based workloads is a significant contributor to this forecast. One big challenge of deploying AI workloads is optimizing usage of the powerful graphics processing units (GPUs) needed to efficiently run them. The latest GPUs from NVIDIA, AMD and Intel have been hard to come by due to high demand, and they are expensive when they do become available, with prices reaching up to $40,000 each. The last thing a company wants after provisioning such pricey silicon is to have it sitting idle in a data center waiting to produce a return on the investment. The versatility of virtual machines (VMs) lets organizations “sweat the assets” of their IT environment no matter how heterogeneous the devices supporting it. Decoupling applications from the underlying hardware makes it possible to run software seamlessly across different machines and operating systems. With modern workloads and larger datasets, precious GPU capacity can be accessed and shared more effectively because GPU functionality can be sliced up and distributed at scale to support a variety of programs.</description></item><item><title>Falco Links Real-Time Detection with Forensic-Level Analysis in the Cloud Native Stack</title><link>https://kubermates.org/docs/2025-11-10-falco-links-real-time-detection-with-forensic-level-analysis-in-the-cloud-native/</link><pubDate>Mon, 10 Nov 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-10-falco-links-real-time-detection-with-forensic-level-analysis-in-the-cloud-native/</guid><description>New integration connects Falco alerts to Stratoshark’s forensic tools, delivering Wireshark-style visibility into system call and audit log data Key Highlights Falco, a CNCF graduated project, now integrates with Stratoshark to connect real-time security alerts with forensic-level capture and analysis tools. Security teams can instantly pivot from detection to deep investigation without switching tools, reducing response time and improving root cause analysis. Platform and security teams working across Kubernetes, containerized environments, and hybrid/multicloud infrastructure. Available now; demonstrated live at KubeCon + CloudNativeCon North America 2025 in Atlanta. ATLANTA—KUBECON + CLOUDNATIVECON NORTH AMERICA, Nov. 10, 2025 — The Cloud Native Computing Foundation® (CNCF®), which builds sustainable ecosystems for cloud native software, today announced new integrations between Falco , a graduated project, and Stratoshark , a forensic tool inspired by Wireshark. With this release, Falco alerts can now trigger precise forensic captures, allowing real-time threat detection to be paired with deeper event analysis. Security and platform teams have long faced challenges bringing together real-time threat detection and the detailed forensic visibility needed for effective incident response. Historically, security analysts and incident responders relied on separate tools to manually capture full system-call activity, producing large volumes of unfiltered data that often slowed down investigations and complicated incident resolution. Falco Captures provides on-demand event recording, enabling alerts when a security rule triggers. Each triggered alert comes with an automatically recorded trail of system calls, ready for immediate replay and inspection, equipping security analysts with precise, actionable insight. “We’ve long seen how alerts without context force a time‑costly hunt,” said Leonardo Grasso, core maintainer of Falco.</description></item><item><title>VCF Breakroom Chats Episode 69 – Beyond vRA + NSX: Delivering Cloud-Native Networking with VCF 9 Virtual Private Clouds</title><link>https://kubermates.org/docs/2025-11-10-vcf-breakroom-chats-episode-69-beyond-vra-nsx-delivering-cloud-native-networking/</link><pubDate>Mon, 10 Nov 2025 13:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-10-vcf-breakroom-chats-episode-69-beyond-vra-nsx-delivering-cloud-native-networking/</guid><description>VCF Breakroom Chats Episode 69 About the VCF Breakroom Chat Series Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VCF Breakroom Chats Episode 69 - Beyond vRA + NSX: Delivering Cloud-Native Networking with VCF 9 Virtual Private Clouds SAP HANA and SAP NetWeaver Support for vSphere in VMware Cloud Foundation 9.0 on Intel Xeon 6 CPUs with P-core Systems Scaling VMware Cloud Foundation 9.0 Lab Environments using Holodeck 9.0 Welcome to the next episode of the VCF Breakroom Chats. Today, we are happy to present this vLog with Arkadiy Shapiro and Thomas Vigneron, Technology Product Management at Broadcom. In this episode, Arkadiy Shapiro, Thomas Vigneron, and Taka Uenishi discuss how VMware Cloud Foundation 9 fundamentally changes how IT teams deliver self-service networking to developers and application teams. Want to learn more about VCF Automation? Check out the VCF Automation web page for more resources. Read this blog post: Private Cloud Redefined: Deliver a Unified Cloud Consumption Experience with VMware Cloud Foundation 9.0 Listen to our previous topics. This webinar series focuses on VMware Cloud Foundation (VCF). In this series, we share conversations with industry-recognized experts from Broadcom and with solution partners and customers. You’ll gain relevant insights whether you’re an IT practitioner, IT admin, cloud or platform architect, developer, DevOps, senior IT manager, IT executive, or AI/ML professional. If you are new to the series, please check out our previous episodes.</description></item><item><title>SAP HANA and SAP NetWeaver Support for vSphere in VMware Cloud Foundation 9.0 on Intel Xeon 6 CPUs with P-core Systems</title><link>https://kubermates.org/docs/2025-11-10-sap-hana-and-sap-netweaver-support-for-vsphere-in-vmware-cloud-foundation-9-0-on/</link><pubDate>Mon, 10 Nov 2025 10:37:25 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-10-sap-hana-and-sap-netweaver-support-for-vsphere-in-vmware-cloud-foundation-9-0-on/</guid><description>Broadcom is pleased to announce SAP support for VMware vSphere in VMware Cloud Foundation 9 virtual machines running SAP HANA 2.0 and SAP NetWeaver–based applications on Intel Xeon Processors, including Intel’s latest generation Intel Xeon 6 with P-cores processors. See SAP note 3663150 for details. SAP HANA Support Supported SAP HANA VM Configurations on Intel Xeon 6 (Granite Rapids-SP, 2-socket host): Migration Guidance SAP NetWeaver Support What’s Planned Next References: Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VCF Breakroom Chats Episode 69 - Beyond vRA + NSX: Delivering Cloud-Native Networking with VCF 9 Virtual Private Clouds SAP HANA and SAP NetWeaver Support for vSphere in VMware Cloud Foundation 9.0 on Intel Xeon 6 CPUs with P-core Systems VCF Breakroom Chats | Episode 70: Simplifying Private Cloud with Fleet Management and Lifecycle Management vSphere support in VCF 9 for SAP HANA 2.0 includes support for: Intel Xeon 6 with P-cores processors (Granite Rapids) Intel 5 th Gen Xeon Processor (Emerald Rapids) Intel 3 rd Gen Xeon Processor (Icelake) with SAP HANA VM standard configurations up to 4 TiB of memory and up to 344 vCPUs. EMR with workload based sizing up to 6 TiB of memory. Validation for 2nd Gen Xeon ( Cascade Lake, Cooper Lake ) and 4 th Gen Xeon ( Sapphire Rapids) processors supporting up to 16 TB and 960 vCPUs is currently in progress. Support for these Intel processors will be added later to SAP note 3663150 – SAP HANA on vSphere in VCF 9. *Estimated vaSAPS incorporate a maximal 10% virtualization overhead and are based on benchmark 2025064. Definition of aSAPS. Support for Intel Xeon 6 with P-Cores with vSphere 8 was released at the end of September 2025 (see SAP note 3372365 ), enabling seamless migration from vSphere 8 environments to vSphere in VMware Cloud and vSphere Foundation 9. The Intel Xeon 6 platform, with its support for up to 86 cores per socket and increased memory capacity and VCF 9, is an ideal choice for migrating existing virtualized or bare-metal SAP HANA systems from older SAP appliance hardware using CPU generations, such as Cascade Lake, which typically offered 0.75–1 TB per CPU, without the need for re-sizing or significant effort. For example, a previous 4 -socket server running 2 nd Gen Intel Xeon (Cascade or Cooper Lake) 4 TB SAP HANA systems, can get migrated onto a 2 -socket 4 TB Intel Xeon 6 system without resizing required. The benefits of migrating and consolidating existing virtualized or bare-metal SAP HANA systems are clear: Reduced energy consumption Simplified architecture and operation Reduced downtime by leveraging enhanced vSphere features like vMotion or VMware HA Optimized total cost of ownership (TCO) Basic Migration Steps Verify SAP HANA VM sizing and select the appropriate Granite Rapids server configuration to avoid oversizing.</description></item><item><title>Scaling VMware Cloud Foundation 9.0 Lab Environments using Holodeck 9.0</title><link>https://kubermates.org/docs/2025-11-10-scaling-vmware-cloud-foundation-9-0-lab-environments-using-holodeck-9-0/</link><pubDate>Mon, 10 Nov 2025 07:30:31 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-10-scaling-vmware-cloud-foundation-9-0-lab-environments-using-holodeck-9-0/</guid><description>Adding Additional Hosts Adding Additional Clusters Conclusion References Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VCF Breakroom Chats Episode 69 - Beyond vRA + NSX: Delivering Cloud-Native Networking with VCF 9 Virtual Private Clouds Scaling VMware Cloud Foundation 9.0 Lab Environments using Holodeck 9.0 Enterprise customers can now deploy NVIDIA Run:ai on VMware Cloud Foundation When we started building Holodeck 9.0 , the goal was simple — to automate the provisioning of VMware Cloud Foundation (VCF) lab environments using nested ESXi hosts. What began as a development project soon became a tool we actively use for testing real-world VCF use cases. One of Holodeck’s biggest strengths is its scalability and ease of use thus enabling you to easily expand your lab environment even after the initial deployment. If you’re setting up your lab for the first time, check out the official Holodeck 9.0 documentation for detailed guidance. In this post, I’ll walk you through how to add additional ESXi hosts or clusters to an existing Holodeck 9.0 environment. You can easily scale your environment by provisioning additional nested ESXi hosts using the New-HoloDeckESXiNodes cmdlet. This command automates the entire process of deploying nested ESX hosts, configures, powers on, and connects new hosts to the network based on your specified parameters such as CPU, memory, site (‘a’ or ‘b’), and vSAN mode (‘ESA’ or ‘OSA’). You can easily scale your environment by provisioning additional nested ESXi hosts using the New-HoloDeckESXiNodes cmdlet. This command automates the entire process of deploying nested ESX hosts, configures, powers on, and connects new hosts to the network based on your specified parameters such as CPU, memory, site (‘a’ or ‘b’), and vSAN mode (‘ESA’ or ‘OSA’). Start PowerShell: Run pwsh. Import the Holodeck configuration associated with your Holodeck instance. Run below command to add additional hosts With this flexibility, the new hosts are instantly ready for use.</description></item><item><title>GPU-as-a-Service for AI at scale: Practical strategies with Red Hat OpenShift AI</title><link>https://kubermates.org/docs/2025-11-10-gpu-as-a-service-for-ai-at-scale-practical-strategies-with-red-hat-openshift-ai/</link><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-10-gpu-as-a-service-for-ai-at-scale-practical-strategies-with-red-hat-openshift-ai/</guid><description>GPU-as-a-Service for AI at scale: Practical strategies with Red Hat OpenShift AI The need for GPUaaS on Red Hat OpenShift AI AI workload integration and autoscaling Queue management with Kueue Effective autoscaling with KEDA Observability-driven optimization Conclusion The adaptable enterprise: Why AI readiness is disruption readiness About the authors Ana Biazetti Lindani Phiri More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Stop wasting budget on idle GPUs. Learn how to implement dynamic allocation, multi-tenancy, and effective autoscaling for your AI workloads. For organizations investing heavily in AI, the cost of specialized hardware is a primary concern. GPUs/accelerators are expensive, and if that hardware is unused and sits idle, it leads to significant budget waste, making it more difficult to scale your AI projects. One solution is to adopt GPU-as-a-Service (GPUaaS), an operational model designed to help maximize the return on investment (ROI) of your hardware. Red Hat OpenShift AI is a Kubernetes-based platform that can be used to implement a multi-user GPUaaS solution. While provisioning the hardware is the first step, achieving true GPUaaS requires additional dynamic allocation based on workload demand, so GPUs are more quickly reclaimed to minimize idle time. GPUaaS also necessitates multi-tenancy. This is where advanced queuing tools like Kueue (Kubernetes Elastic Unit Execution) become indispensable. Kueue partitions shared resources and enforces multi-tenancy via quotas, guaranteeing fair, predictable access for multiple teams and projects. Once this governance is in place, the core challenge shifts to creating an autoscaling pipeline for AI workloads. The goal of a GPUaaS platform is to integrate popular AI frameworks and automatically scale resources based on workload demand.</description></item><item><title>Red Hat collaborating with Omnissa to bring Horizon virtual desktops to OpenShift Virtualization</title><link>https://kubermates.org/docs/2025-11-10-red-hat-collaborating-with-omnissa-to-bring-horizon-virtual-desktops-to-openshif/</link><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-10-red-hat-collaborating-with-omnissa-to-bring-horizon-virtual-desktops-to-openshif/</guid><description>Red Hat collaborating with Omnissa to bring Horizon virtual desktops to OpenShift Virtualization Why this matters Looking ahead Learn more 15 reasons to adopt Red Hat OpenShift Virtualization About the author Simon Seagrave More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Choice and flexibility are essential in today&amp;rsquo;s IT environments. That&amp;rsquo;s why Omnissa is collaborating with Red Hat to enable organizations to deploy virtual desktops and applications using Omnissa Horizon on Red Hat OpenShift Virtualization. In the coming months, this collaboration will provide organizations with a modern, Kubernetes-native option for deploying and managing virtual machine-based (VM-based) virtual desktops and published applications alongside modern container-based applications, without having to maintain separate virtualization silos. Organizations continue to rely on virtual machine VM-based applications for critical workloads, even as they accelerate their shift to containers and Kubernetes. Maintaining separate virtualization and container platforms can lead to unnecessary silos, duplicate processes, and higher costs. By enabling Omnissa Horizon on Red Hat OpenShift Virtualization, customers will be able to: Run Horizon-based virtual desktops and apps on OpenShift Virtualization Simplify monitoring and operations with a unified platform for policies, automation, and governance Modernize workloads at the desired pace , supporting current VM workloads while introducing new cloud-native applications, including AI Omnissa is a digital work platform leader, trusted by thousands of organizations worldwide. Omnissa’s integrated platform and solutions deliver exceptional employee experiences that optimize security, IT operations, and cost. Omnissa Horizon, formerly part of VMware’s End-User Computing business, is an industry-leading platform for delivering secure, scalable virtual desktops and applications across hybrid and multi-cloud environments. Horizon includes capabilities such as on-demand application delivery and management via Omnissa App Volumes , Omnissa Dynamic Environment Manager , and secure access gateway using Omnissa Unified Access Gateway. Red Hat OpenShift Virtualization, built on the mature and widely used KVM (Kernel-based Virtual Machine) hypervisor, extends Red Hat OpenShift to run VMs alongside containers. IT teams can manage both types of workloads with a consistent set of management tools and processes, automation, and security policies. By combining Red Hat’s hybrid cloud expertise with Omnissa’s digital workspace capabilities, organizations will be able to gain: Simplified VDI and modern application deployment : With Horizon&amp;rsquo;s VM-based infrastructure, customers can more easily deploy virtual desktops alongside their containerized application workloads using OpenShift Virtualization.</description></item><item><title>Red Hat Developer Hub 1.8 delivers context-aware AI, faster self-service and scalable governance</title><link>https://kubermates.org/docs/2025-11-10-red-hat-developer-hub-1-8-delivers-context-aware-ai-faster-self-service-and-scal/</link><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-10-red-hat-developer-hub-1-8-delivers-context-aware-ai-faster-self-service-and-scal/</guid><description>Red Hat Developer Hub 1.8 delivers context-aware AI, faster self-service and scalable governance Unlocking higher productivity with context-aware AI Governance at scale: Enforcing standards without slowing down Personalized experiences for faster self-service and onboarding An enterprise-grade, turnkey internal developer portal About the author Balaji Sivasubramanian More like this Blog post Blog post Original podcast Original podcast Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The biggest frustration for modern developers isn&amp;rsquo;t the code; it&amp;rsquo;s the friction. It’s the constant context switching, the endless search for the right internal document, and the risk of building on outdated standards. 50% of developers reported losing 10+ hours a week to non-coding tasks, which can equate to a ~$1.6 million loss per 100 engineers per year. Organizations need to empower developers with a self-service experience that is fast, highly personalized and grounded in the company&amp;rsquo;s unique development standards. That&amp;rsquo;s why we’re excited to announce Red Hat Developer Hub 1.8 , the latest version of our enterprise-grade developer portal based on the Backstage project. New features and enhancements in this release provide users: AI tools to boost productivity and integrate AI into applications Lifecycle tools to scale governance, get support more easily, and extend functionality faster A more personalized experience for faster self-service and onboarding To unlock higher productivity with AI-assistance, the AI itself needs to be context-aware. Red Hat Developer Hub 1.8 introduces the foundational architecture to make this happen, turning the platform into an intelligent partner that understands your environment. Beyond the core architecture, we&amp;rsquo;re simplifying how you integrate AI into your applications. Developers shouldn&amp;rsquo;t have to interrupt their workflow to hunt for models. Productivity increases when models live alongside their existing tools. Red Hat Developer Hub 1.8 delivers several key features to establish this new intelligent foundation and streamline AI integration: Red Hat Developer Lightspeed for Red Hat Developer Hub is now powered by Llama Stack : Developer Lightspeed, an assistant that helps developers complete their non-coding tasks faster, is now built on top of the open source Llama Stack framework, an emerging industry standard for agentic AI systems (available as a developer preview). With this more flexible foundation, Developer Lightspeed can now utilize many of the open source project’s abstractions and features, including MCP agents and RAG, which will allow customers to bring their own knowledge in future releases.</description></item><item><title>Red Hat OpenShift is joining the Kueue</title><link>https://kubermates.org/docs/2025-11-10-red-hat-openshift-is-joining-the-kueue/</link><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-10-red-hat-openshift-is-joining-the-kueue/</guid><description>Red Hat OpenShift is joining the Kueue What is Kueue? AI use cases for Kueue Other use cases for Kueue Introducing Red Hat Build of Kueue for OpenShift What next? Red Hat OpenShift Container Platform | Product Trial About the author Duncan Hardie More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Kueue is a community-driven project with the goal to develop a resource management system for Kubernetes that efficiently manages batch workloads. It enables sharing a Kubernetes cluster among teams and users in a fair, cost-effective, and efficient way. This blog post delves into what Kueue is, what its main use cases are, and introduces the Red Hat build of Kueue, a new operator from the Red Hat OpenShift team. Kueue operates by acting as a central controller that queues jobs and decides when and where they should run. It is designed to manage batch jobs effectively by providing advanced queuing, quota management, and resource sharing capabilities. Kueue helps in optimizing cluster utilization and ensuring that resources are allocated in a fair and efficient manner. Kueue is particularly useful in the AI space but there are other use cases. You can&amp;rsquo;t examine Kueue use cases without considering AI. While AI seems to insert itself into every topic and conversation, Kueue really has a highly relevant part to play. Kueue is key in managing and optimizing AI workloads within Kubernetes environments. To put it another way, if you are doing AI related work then you should be using Kueue. Here are some of the main AI use cases: Job scheduling and prioritization: Kueue allows for the efficient scheduling and prioritization of AI training and inference jobs.</description></item><item><title>Announcing the 2025 Steering Committee Election Results</title><link>https://kubermates.org/docs/2025-11-09-announcing-the-2025-steering-committee-election-results/</link><pubDate>Sun, 09 Nov 2025 15:10:00 -0500</pubDate><guid>https://kubermates.org/docs/2025-11-09-announcing-the-2025-steering-committee-election-results/</guid><description>Announcing the 2025 Steering Committee Election Results Results Big thanks! Get involved with the Steering Committee The 2025 Steering Committee Election is now complete. The Kubernetes Steering Committee consists of 7 seats, 4 of which were up for election in 2025. Incoming committee members serve a term of 2 years, and all members are elected by the Kubernetes Community. The Steering Committee oversees the governance of the entire Kubernetes project. With that great power comes great responsibility. You can learn more about the steering committee’s role in their charter. Thank you to everyone who voted in the election; your participation helps support the community’s continued health and success. Congratulations to the elected committee members whose two year terms begin immediately (listed in alphabetical order by GitHub handle): Kat Cosgrove ( @katcosgrove ), Minimus Paco Xu ( @pacoxu ), DaoCloud Rita Zhang ( @ritazh ), Microsoft Maciej Szulik ( @soltysh ), Defense Unicorns They join continuing members: Antonio Ojea ( @aojea ), Google Benjamin Elder ( @BenTheElder ), Google Sascha Grunert ( @saschagrunert ), Red Hat Maciej Szulik and Paco Xu are returning Steering Committee Members. Thank you and congratulations on a successful election to this round’s election officers: Christoph Blecker ( @cblecker ) Nina Polshakova ( @npolshakova ) Sreeram Venkitesh ( @sreeram-venkitesh ) Thanks to the Emeritus Steering Committee Members. Your service is appreciated by the community: Stephen Augustus ( @justaugustus ), Bloomberg Patrick Ohly ( @pohly ), Intel And thank you to all the candidates who came forward to run for election. This governing body, like all of Kubernetes, is open to all. You can follow along with Steering Committee meeting notes and weigh in by filing an issue or creating a PR against their repo.</description></item><item><title>Blog: Announcing the 2025 Steering Committee Election Results</title><link>https://kubermates.org/docs/2025-11-09-blog-announcing-the-2025-steering-committee-election-results/</link><pubDate>Sun, 09 Nov 2025 15:10:00 -0500</pubDate><guid>https://kubermates.org/docs/2025-11-09-blog-announcing-the-2025-steering-committee-election-results/</guid><description>Announcing the 2025 Steering Committee Election Results Results Big thanks! Get involved with the Steering Committee The 2025 Steering Committee Election is now complete. The Kubernetes Steering Committee consists of 7 seats, 4 of which were up for election in 2025. Incoming committee members serve a term of 2 years, and all members are elected by the Kubernetes Community. The Steering Committee oversees the governance of the entire Kubernetes project. With that great power comes great responsibility. You can learn more about the steering committee’s role in their charter. Thank you to everyone who voted in the election; your participation helps support the community’s continued health and success. Congratulations to the elected committee members whose two year terms begin immediately (listed in alphabetical order by GitHub handle): Kat Cosgrove ( @katcosgrove ), Minimus Paco Xu ( @pacoxu ), DaoCloud Rita Zhang ( @ritazh ), Microsoft Maciej Szulik ( @soltysh ), Defense Unicorns They join continuing members: Antonio Ojea ( @aojea ), Google Benjamin Elder ( @BenTheElder ), Google Sascha Grunert ( @saschagrunert ), Red Hat Maciej Szulik and Paco Xu are returning Steering Committee Members. Thank you and congratulations on a successful election to this round’s election officers: Christoph Blecker ( @cblecker ) Nina Polshakova ( @npolshakova ) Sreeram Venkitesh ( @sreeram-venkitesh ) Thanks to the Emeritus Steering Committee Members. Your service is appreciated by the community: Stephen Augustus ( @justaugustus ), Bloomberg Patrick Ohly ( @pohly ), Intel And thank you to all the candidates who came forward to run for election. This governing body, like all of Kubernetes, is open to all. You can follow along with Steering Committee meeting notes and weigh in by filing an issue or creating a PR against their repo.</description></item><item><title>Introducing Calico AI and Istio Ambient Mode</title><link>https://kubermates.org/docs/2025-11-09-introducing-calico-ai-and-istio-ambient-mode/</link><pubDate>Sun, 09 Nov 2025 14:00:02 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-09-introducing-calico-ai-and-istio-ambient-mode/</guid><description>The Complexity of Modern Kubernetes Networking Introducing Istio Ambient Mode in Calico Deep Dive: What is Istio Ambient Mode for Calico? Key Features and Benefits Calico AI: Achieving Operational Simplicity Deep Dive: What Is the Calico AI Assistant? How It Works Key Features and Capabilities Benefits of Calico AI Assistant Availability Intelligent and Unified Security for Kubernetes The Calico Advantage Next Steps: See Calico in Action 👇 Kubernetes has transformed how teams build and scale applications, but it has also introduced new layers of complexity. Platform and DevOps teams must now integrate and manage multiple technologies: CNI, ingress and egress gateways, service mesh, and more across increasingly large and dynamic environments. As more applications are deployed into Kubernetes clusters, the operational burden on these teams continues to grow, especially when maintaining performance, reliability, security, and observability across diverse workloads. To address this complexity and tool sprawl, Tigera is incorporating Istio’s Ambient Service Mesh directly into the Calico Unified Network Security Platform. Service mesh has become the preferred solution for application-level networking, particularly in environments with a large number of services or highly regulated workloads. Among available service meshes, Istio stands out as the most popular and widely adopted, supported by a thriving open-source community. By leveraging the lightweight, sidecarless design of Istio Ambient Mode, Calico delivers all the benefits of service mesh, secure service-to-service communication, mTLS authentication, L7 traffic management, and observability, without the burden of sidecars. Complementing this addition is Calico AI. Calico AI brings intelligence and automation to Kubernetes networking. It addresses the massive operational burden on teams by allowing them to interrogate complex network data and configurations using simple, natural language. It empowers teams to troubleshoot faster, detect anomalies, and optimize configurations using natural language prompts. Together, Calico AI and Istio Ambient Mode will help Kubernetes teams streamline operations, unify their toolchains, and manage complex Kubernetes environments with greater efficiency and confidence.</description></item><item><title>Enterprise customers can now deploy NVIDIA Run:ai on VMware Cloud Foundation</title><link>https://kubermates.org/docs/2025-11-07-enterprise-customers-can-now-deploy-nvidia-run-ai-on-vmware-cloud-foundation/</link><pubDate>Fri, 07 Nov 2025 20:36:44 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-07-enterprise-customers-can-now-deploy-nvidia-run-ai-on-vmware-cloud-foundation/</guid><description>NVIDIA Run:ai on VCF Architecture Overview Architecture Diagram Deployment Scenarios Scenario 1: Installing NVIDIA Run:ai on a vSphere Kubernetes Service enabled VCF instance Scenario 2: Integrating vSphere Kubernetes Service with existing NVIDIA Run:ai Deployments Operational Insights: The “Day 2” VCF Advantage Summary: Bare-Metal vs. VCF for Enterprise AI NVIDIA Run:ai on VCF accelerates Enterprise AI Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VCF Breakroom Chats Episode 69 - Beyond vRA + NSX: Delivering Cloud-Native Networking with VCF 9 Virtual Private Clouds Scaling VMware Cloud Foundation 9.0 Lab Environments using Holodeck 9.0 Enterprise customers can now deploy NVIDIA Run:ai on VMware Cloud Foundation NVIDIA Run:ai accelerates AI operations through dynamic resource orchestration, maximizing GPU utilization, comprehensive AI-lifecycle support, and strategic resource management. By pooling resources across environments and utilizing advanced orchestration, NVIDIA Run:ai significantly enhances GPU efficiency and workload capacity. We recently announced that enterprises can now deploy NVIDIA Run:ai with the built-in VMware vSphere Kubernetes Services (VKS), a standard capability in VMware Cloud Foundation (VCF). This will help enterprises achieve optimum GPU utilization with NVIDIA Run:ai, streamlining Kubernetes deployment, and supporting both container and VM deployments on VCF. This allows for the deployment of AI and traditional workloads on a single platform This blog explores how Broadcom customers can now deploy NVIDIA Run:ai on VCF leveraging VMware Private AI Foundation with NVIDIA to deploy AI Kubernetes clusters to maximize GPU utilization, streamline operations, and unlock GenAI on their private data. While many organizations default to running Kubernetes on bare-metal servers, this “do-it-yourself” approach often results in siloed islands of infrastructure. It forces IT teams to manually build and manage the very services that VCF provides by design, lacking the deep integration, automated lifecycle management, and resilient abstractions for compute, storage, and networking that are critical for production AI. This is where the VMware Cloud Foundation platform provides a decisive advantage. vSphere Kubernetes Service is the Best Way to Deploy Run:ai on VCF The most effective and integrated way to deploy NVIDIA Run:ai on VCF is by using VKS, which provides enterprise-ready, Cloud Native Computing Foundation (CNCF) certified Kubernetes clusters that are fully managed and automated. NVIDIA Run:ai is then deployed onto these VKS clusters, creating a unified, secure, and resilient platform from the hardware up to the AI application. The value is not just in running Kubernetes, but in running it on a platform that solves foundational enterprise challenges: Lower TCO through VCF: Reduce infrastructure silos, leverage existing tools and skill set without retraining and changing processes, and provide unified lifecycle management across infrastructure components.</description></item><item><title>VMware Cloud Foundation Automation – All Apps Organization Configurations</title><link>https://kubermates.org/docs/2025-11-07-vmware-cloud-foundation-automation-all-apps-organization-configurations/</link><pubDate>Fri, 07 Nov 2025 16:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-07-vmware-cloud-foundation-automation-all-apps-organization-configurations/</guid><description>Summary Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VCF Breakroom Chats Episode 69 - Beyond vRA + NSX: Delivering Cloud-Native Networking with VCF 9 Virtual Private Clouds Scaling VMware Cloud Foundation 9.0 Lab Environments using Holodeck 9.0 VMware Cloud Foundation Automation - All Apps Organization Configurations In today’s private cloud world, enterprise IT is evolving from “build and manually manage everything” to “provide agile, self-service infrastructure to application teams,” while still maintaining governance, security, and cost control. With VMware Cloud Foundation Automation , you have a platform delivered by the provider team (or the internal cloud platform team) that hands off a polished, governed, ready-to-use environment to your organization’s administrators, who then enable application teams. In VCF Automation, an organization is a top-level construct created by the provider administrator to represent a tenant or line of business. Each organization operates within its own secure, isolated boundary, ensuring that users can access only the services and resources assigned to them. Organizations consume capacity and cloud services from a shared resource pool, and then subdivide those resources for their application teams — modeled in VCF Automation as Projects and Namespaces. Within each organization, users are assigned roles that define their access and responsibilities, enabling controlled, governed self-service. Think of it like a modern enterprise campus: the provider team has constructed the building — the power, cooling, network backbone, and secure access points. Now the keys are handed over to the organization administrator to define how the space is used — allocating areas for teams, setting access controls, defining shared services, and ensuring everyone can work efficiently within the infrastructure’s boundaries. The foundation is ready — now it’s time to prepare the workspace where applications and teams will thrive. In this blog, we’re focusing on the moment right after the hand-off: you’re the org admin, the region quota has been allocated, the networking plumbing is in place, and your “All Apps” organization is ready for configuration. Why “All Apps Organization”? Because this organization type is designed to support the broadest range of workloads — from traditional VM-based application stacks to containerized microservices platforms— both of which your application teams expect to consume at speed and with consistency. Your role as the organization admin is to set that up once — defining Projects/Namespaces, configuring networking and consumption boundaries, setting identity providers up and user access, and making sure app teams can hit the ground running — all while protecting the underlying infrastructure.</description></item><item><title>Code-level telemetry instrumentation: From “oh hell no” to “worth it”</title><link>https://kubermates.org/docs/2025-11-07-code-level-telemetry-instrumentation-from-oh-hell-no-to-worth-it/</link><pubDate>Fri, 07 Nov 2025 15:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-07-code-level-telemetry-instrumentation-from-oh-hell-no-to-worth-it/</guid><description>A platform engineer’s guide to developer buy-in Posted on November 7, 2025 by Whitney Lee CNCF projects highlighted in this post Originally published on the author’s personal blog, whitneylee. com As platform engineers, we want the holistic system insights that instrumented code can give us – yes, please. With code-level insights plus infrastructure observability, we can connect infrastructure signals to business outcomes. We can prove our value with Service Level Objectives (SLOs) like “99% of checkouts complete within 2 seconds. ” Not to mention the ability to auto-detect dependencies between services, automatically create dependency graphs, and power intelligent scaling. But instrumenting code is often a pain in the ass for developers. How do we motivate them to participate? Well, first off, we build application-adjacent observability collections into the platform as much as possible. What can we observe about our applications without the developer having to lift a finger? Framework-level telemetry : OpenTelemetry provides auto-instrumentation for popular programming languages, enabling your platform to collect trace data without modifying the code. You simply attach an agent, run your app with a startup flag, and it automatically implements common frameworks like HTTP servers, database clients, messaging libraries, Spring Boot, Express, etc. Kernel-level telemetry : The platform can also collect kernel-level insights using an eBPF-powered observability tool, such as Pixie or Cilium. Network-level telemetry : These can come from your service mesh technologies, like Istio, Linkerd, or Cilium. All of this you can do without the developer writing a single line of observability-related code.</description></item><item><title>Self-hosted human and machine identities in Keycloak 26.4</title><link>https://kubermates.org/docs/2025-11-07-self-hosted-human-and-machine-identities-in-keycloak-26-4/</link><pubDate>Fri, 07 Nov 2025 15:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-07-self-hosted-human-and-machine-identities-in-keycloak-26-4/</guid><description>What’s New in 26.4 Passwordless user authentication with Passkeys Tightened OpenID Connect security with FAPI 2 and DPoP Simplified deployments across multiple availability zones Authenticating applications with Kubernetes service account tokens or SPIFFE Looking ahead Posted on November 7, 2025 by Alexander Schwartz CNCF projects highlighted in this post Keycloak is a leading open source solution in the cloud-native ecosystem for Identity and Access Management, a key component of accessing applications and their data. With the release of Keycloak 26.4, we’ve added features for both machine and human identities. New features focus on security enhancement, deeper integration, and improved server administration. See below for the release highlights, or dive deeper in our Keycloak 26.4 release announcement. Keycloak recently surpassed 30k GitHub stars and 1,350 contributors. If you’re attending KubeCon + CloudNativeCon North America in Atlanta, stop by and say hi —we’d love to hear how you’re using Keycloak! Keycloak now offers full support for Passkeys. As secure, passwordless authentication becomes the new standard, we’ve made passkeys simple to configure. For environments that are unable to adopt passkeys, Keycloak continues to support OTP and recovery codes. You can find a passkey walkthrough on the Keycloak blog. Keycloak 26.4 implements the Financial-grade API (FAPI) 2.0 standard, ensuring strong security best practices. This includes support for Demonstrating Proof-of-Possession (DPoP), which is a safer way to handle tokens in public OpenID Connect clients. Deployment across multiple availability zones or data centers is simplified in 26.4: Split-brain detection Full support in the Keycloak Operator Latency optimizations when Keycloak nodes run in different data centers Keycloak docs contain a full step-by-step guide , and we published a blog post on how to scale to 2,000 logins/sec and 10,000 token refreshes/sec.</description></item><item><title>Introducing the Kubeflow SDK: A Pythonic API to Run AI Workloads at Scale</title><link>https://kubermates.org/docs/2025-11-07-introducing-the-kubeflow-sdk-a-pythonic-api-to-run-ai-workloads-at-scale/</link><pubDate>Fri, 07 Nov 2025 00:00:00 -0600</pubDate><guid>https://kubermates.org/docs/2025-11-07-introducing-the-kubeflow-sdk-a-pythonic-api-to-run-ai-workloads-at-scale/</guid><description>Unified SDK Concept Introducing Kubeflow SDK Role in the Kubeflow Ecosystem Key Features Unified Python Interface Trainer Client Optimizer Client Local Execution Mode Local Process Backend: Fastest Iteration Container Backend: Production-Like Environment Kubernetes Backend: Production Scale What’s Next? Get Involved Unified SDK Concept Introducing Kubeflow SDK Role in the Kubeflow Ecosystem Role in the Kubeflow Ecosystem Key Features Unified Python Interface Trainer Client Optimizer Client Local Execution Mode Local Process Backend: Fastest Iteration Container Backend: Production-Like Environment Kubernetes Backend: Production Scale Unified Python Interface Trainer Client Optimizer Client Local Execution Mode Local Process Backend: Fastest Iteration Container Backend: Production-Like Environment Kubernetes Backend: Production Scale Local Process Backend: Fastest Iteration Container Backend: Production-Like Environment Kubernetes Backend: Production Scale What’s Next? Get Involved ⚡ We want your feedback! Help shape the future of Kubeflow SDK by taking our quick survey. Scaling AI workloads shouldn’t require deep expertise in distributed systems and container orchestration. Whether you are prototyping on local hardware or deploying to a production Kubernetes cluster, you need a unified API that abstracts infrastructure complexity while preserving flexibility. That’s exactly what the Kubeflow Python SDK delivers. As an AI Practitioner, you’ve probably experienced this frustrating journey: you start by prototyping locally, training your model on your laptop. When you need more compute power, you have to rewrite everything for distributed training. You containerize your code, rebuild images for every small change, write Kubernetes YAMLs, wrestle with kubectl, and juggle multiple SDKs — one for training, another for hyperparameter tuning, and yet another for pipelines. Each step demands different tools, APIs, and mental models. All this complexity slows down productivity, drains focus, and ultimately holds back AI innovation. What if there was a better way? The Kubeflow community started the Kubeflow SDK &amp;amp; ML Experience Working Group (WG) in order to address these challenges. You can find more information about this WG on our YouTube playlist. The SDK sits on top of the Kubeflow ecosystem as a unified interface layer.</description></item><item><title>Nirmata and ControlPlane Partner to Secure Kubernetes with AI</title><link>https://kubermates.org/docs/2025-11-07-nirmata-and-controlplane-partner-to-secure-kubernetes-with-ai/</link><pubDate>Fri, 07 Nov 2025 02:37:57 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-07-nirmata-and-controlplane-partner-to-secure-kubernetes-with-ai/</guid><description>Nirmata and ControlPlane Partner to Secure Kubernetes with AI What is the Nirmata AI Platform Engineer Assistant? Why Enterprises Need AI-Driven Policy Enforcement Inside the Nirmata AI Control Hub ControlPlane: Kubernetes Security Experts The Synergy: Policy, Practice, and Protection Innovation Without Compromise In today’s fast-paced world of cloud-native development , Kubernetes has become the de facto operating system of the cloud. But with great power comes great complexity —especially around security and governance. That’s why our collaboration with ControlPlane is so critical for modern organizations. While we both tackle distinct areas, combining Nirmata’s policy-as-code platform with ControlPlane’s deep Kubernetes security consulting, GitOps expertise, and defense solutions creates a robust, end-to-end security strategy that helps teams move faster and safer. Nirmata is the AI Platform Engineer’s assistant , powered by Kyverno , the leading open-source Policy-as-Code engine. Nirmata’s core function is to turn policy-as-code into automated governance for security, compliance, cost, and reliability across any infrastructure. It uses AI agents that work together to close the loop from detection to remediation to enforcement. Today’s enterprises face four realities: Scale and Complexity : Hundreds of clusters, thousands of nodes, and sprawling IaC repositories make manual enforcement impossible. Rising Regulatory Pressure : From finance to healthcare, industries must prove compliance across both Kubernetes and IaC environments. Operational Burnout : Platform and security teams are inundated with violations, alerts, and repetitive toil. Inconsistent Rules : Disparate and even contradictory policies Without automation, compliance becomes a bottleneck. AI for platform engineering flips this equation on its head.</description></item><item><title>Friday Five — November 7, 2025</title><link>https://kubermates.org/docs/2025-11-07-friday-five-november-7-2025/</link><pubDate>Fri, 07 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-07-friday-five-november-7-2025/</guid><description>Friday Five — November 7, 2025 Red Hat Introduces Confirmed Sovereign Support for European Union InfoWorld - Agentic AI is complex, not complicated Navigating the industrial edge: How a platform approach unlocks business value Red Hat AI: Modular building blocks for scalable, repeatable model customization SiliconANGLE - Inside the cloud-native AI revolution: Red Hat, Google and the next wave of Kubernetes innovation About the author Red Hat Corporate Communications More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Red Hat Confirmed Sovereign Support is purpose-built to deliver dedicated EU-citizen-driven technical support from within the EU for Red Hat software subscriptions, providing a new level of verifiable local control over critical IT operations. Learn more Scott McCarty, Senior Principal Product Manager for Red Hat Enterprise Linux (RHEL) Server, explains the difference between complex vs. complicated when it comes to agentic AI and what it takes to be successful in this new world of AI. Learn more The world of OT is rapidly transforming. Once built on proprietary, siloed systems, OT environments are now under immense pressure to evolve. Driven by the need for greater efficiency, productivity, and cost reduction, the focus is shifting from simply maintaining operations to building smarter, more agile, and more resilient industrial systems. Learn more Red Hat AI offers a model customization experience, evolving InstructLab into a modular architecture powered by Red Hat&amp;rsquo;s Python libraries. This maintains InstructLab&amp;rsquo;s open, extensible fine-tuning and instruction-following pipeline, providing greater flexibility and scalability for enterprise use. Learn more Ahead of this year’s KubeCon + CloudNativeCon North America event, SiliconANGLE takes a look at how Kubernetes’ continued evolution is powering the next era of open-source, AI-driven computing. Learn more Red Hat is the world’s leading provider of enterprise open source software solutions, using a community-powered approach to deliver reliable and high-performing Linux, hybrid cloud, container, and Kubernetes technologies. Red Hat helps customers integrate new and existing IT applications, develop cloud-native applications, standardize on our industry-leading operating system, and automate, secure, and manage complex environments. Award-winning support, training, and consulting services make Red Hat a trusted adviser to the Fortune 500.</description></item><item><title>What’s New in Calico – Fall 2025 Release</title><link>https://kubermates.org/docs/2025-11-06-what-s-new-in-calico-fall-2025-release/</link><pubDate>Thu, 06 Nov 2025 23:12:37 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-06-what-s-new-in-calico-fall-2025-release/</guid><description>Simplify, Secure, and Scale Your Infrastructure Resilient, High-Performance Networking High Availability (HA) Management Plane for VMs and Bare Metal Hosts Running Calico Official Release (GA) of the Nftables data plane for Calico Support for the Maglev Load Balancing in Calico for More Reliable Session Connectivity Simplified Operations and Visibility Support for Priority Markings in QoS Policies Dashboards and Observability for Calico Ingress Gateway Get Started with Calico Fall 2025 As organizations scale Kubernetes and hybrid infrastructures, many are realizing that more tools don’t mean better security. A recent Microsoft report found that organizations with 16+ point solutions see 2. 8x more data security incidents than those with fewer tools. Yet platform teams are still expected to deliver resilience and performance across containers, VMs, and bare metal, often while juggling fragmented tools that introduce risk, downtime, and complexity. The Fall 2025 release of Calico Enterprise and Calico Cloud cuts through that complexity. Its new features are designed to make your infrastructure more resilient, performant, and observable—right out of the box. From disaster recovery automation to modern data plane support and application traffic handling, these updates empower platform engineers to simplify operations while meeting strict reliability requirements. 1. Resilient, High-Performance Networking and Improved Quality of Service : High availability (HA) management planes for VM and bare metal hosts – Increases resiliency and helps reduce downtime. The official release (GA) of the nftables dataplane -Delivers faster and more scalable rule-processing than iptables. Maglev-Based Session Affinity – Maintain stable network traffic flow during failures or scaling. High availability (HA) management planes for VM and bare metal hosts – Increases resiliency and helps reduce downtime.</description></item><item><title>Meet the World’s First AI Platform Engineering Assistant</title><link>https://kubermates.org/docs/2025-11-06-meet-the-world-s-first-ai-platform-engineering-assistant/</link><pubDate>Thu, 06 Nov 2025 19:03:51 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-06-meet-the-world-s-first-ai-platform-engineering-assistant/</guid><description>Meet the World’s First AI Platform Engineering Assistant AI-Powered Governance, Security, and Compliance Why AI-Driven Platform Engineering Matters Now Experience the Future of Platform Engineering Cloud-native infrastructure, in the age of AI, has never been more powerful or more complex. Today, platform engineering teams manage thousands of clusters, configurations, and compliance requirements across Kubernetes, Infrastructure-as-Code (IaC), and multiple clouds. Enter Nirmata’s AI Platform Engineering Assistant , the first AI-powered solution that brings automation and intelligence to every stage of infrastructure governance. Built on Kyverno , the industry’s leading Policy-as-Code engine, Nirmata turns platform engineering teams into force multipliers without slowing developers down. Nirmata’s AI Platform Engineering Assistant combines Kyverno’s trusted policy engine with a suite of intelligent AI agents and assistants that automate policy authoring, detection, remediation, and continuous compliance. Copilot provides a natural-language interface for platform engineers to describe desired outcomes, automatically generating policies and enforcement actions. NCTL AI (CLI Agent) – Implements natural language assistance in the command line (CLI) to generate policies and scans of resources and violations Policy Authoring Agent (PaC) converts natural-language intent into validated Kyverno policy-as-code, accelerating policy creation and reducing human error. Remediation Agent identifies and applies secure, compliant fixes across clusters and pipelines, minimizing manual intervention. Think of it as your AI copilot for platform engineering. One that can understand natural language, automatically generate YAML policy, enforce guardrails across environments, and even fix policy misconfigurations and violations in real-time. The result? Faster, safer, and more scalable operations, without slowing down your developers. However, if you are a platform engineering purist who loves the command line interface and wants to leverage the same expert AI guidance, then there is Nirmata’s NCTL AI that integrates directly into your CLI for natural language policy-as-code governance and automation.</description></item><item><title>Your Complete Guide to KubeCon + CloudNativeCon North America 2025</title><link>https://kubermates.org/docs/2025-11-06-your-complete-guide-to-kubecon-cloudnativecon-north-america-2025/</link><pubDate>Thu, 06 Nov 2025 18:53:18 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-06-your-complete-guide-to-kubecon-cloudnativecon-north-america-2025/</guid><description>Pack your bags and your strategy The events you don’t want to miss More ways to engage Find your track Making the Most of Your Time After KubeCon and CloudNativeCon Posted on November 6, 2025 by Eliza Power KubeCon + CloudNativeCon North America 2025 is just days away, and we want to help you make the most of it! Whether you’re joining us for the first time or you’re a veteran, a little preparation goes a long way at an event this massive. We’ve compiled your ‘How to KubeCon + CloudNativeCon’ guide to make sure you have everything you need to navigate the premier cloud native gathering. When: November 10-13 Where: Georgia World Congress Center, Atlanta Sched app First things first: download the Sched app. Trust us, this handy little app will be your best friend. Using Sched, you’ll be able to build your personalized schedule , get real-time session updates and set reminders to ensure you don’t miss out. Pack smart And speaking of preparation, let’s talk about packing smart. You’ll be doing a lot of walking between sessions, exhibit halls, and buildings, so here’s what we recommend bringing: Comfortable shoes (we cannot stress this enough!) A reusable water bottle (there are water filling stations throughout the venue) A portable charger Layers for varying room temperatures Business cards or your preferred digital contact method A backpack for all that conference swag Our top tips for success on day one: Arrive early to collect your badge (even on day zero!) Explore the venue Network authentically Be open to unexpected experiences Take notes Project Pavillion and Tours This is one of our favorite spots at KubeCon + CloudNativeCon. Running Tuesday through Thursday during Solutions Showcase hours, the Project Pavilion lets you connect directly with CNCF project maintainers. Whether you’re using Kubernetes, Prometheus, Envoy, or any other CNCF project, this is your chance to ask questions, learn about upcoming features, share your use cases, and get help with implementation challenges. Check the schedule for Project Pavilion Tours. Community Hub Head to the Community Hub to experience the heart of the cloud native community. The Community Hub is a welcoming space for attendees to connect, learn, and celebrate equity, inclusion, and accessibility.</description></item><item><title>Cloud Native Computing Foundation Announces Graduation of Crossplane</title><link>https://kubermates.org/docs/2025-11-06-cloud-native-computing-foundation-announces-graduation-of-crossplane/</link><pubDate>Thu, 06 Nov 2025 17:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-06-cloud-native-computing-foundation-announces-graduation-of-crossplane/</guid><description>Graduation marks Crossplane’s readiness for widespread use and its evolution from a control plane framework to groundwork for intelligent, secure, and scalable cloud operations and platform engineering Key Highlights: Crossplane has officially graduated within the CNCF project ecosystem, recognizing its maturity, wide adoption, and strong community, along with its importance in platform engineering. With over 3,000 contributors from more than 450 organizations, Crossplane ranks in the top 10% of all CNCF projects for contributor engagement and authorship. Crossplane has delivered over 100 releases, including the recent Crossplane v2.0, which introduces a more refined architecture for building full application control planes and enhances support for AI-driven operations. Crossplane is now a graduated CNCF project and is available for production use today. SAN FRANCISCO, Calif. – November 6, 2025 – The Cloud Native Computing Foundation® (CNCF®), which builds sustainable ecosystems for cloud native software, today announced the graduation of Crossplane , recognizing its maturity, wide adoption, and strong community momentum, along with its importance in cloud native platform engineering. Crossplane brings the universal contract of declarative APIs to cloud infrastructure, applications, and services. It turns infrastructure into programmable, policy-driven software, enabling both humans and intelligent agents to request and manage environments safely through APIs rather than scripts or tickets. As AI begins to drive the need for more operations, Crossplane’s architecture offers the predictability and safety required for automation. Its declarative model allows intelligent agents to operate cloud environments by targeting outcomes instead of executing instructions, helping bridge human-operated and AI-operated infrastructure. “Crossplane’s graduation marks a major milestone for cloud native and multi cloud platform engineering,” said Chris Aniszczyk, CTO, CNCF. “Crossplane empowers teams to build secure, scalable internal platforms.</description></item><item><title>Gateway API 1.4: New Features</title><link>https://kubermates.org/docs/2025-11-06-gateway-api-1-4-new-features/</link><pubDate>Thu, 06 Nov 2025 09:00:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-11-06-gateway-api-1-4-new-features/</guid><description>Gateway API 1.4: New Features Graduations to Standard Channel Backend TLS policy Status information about the features that an implementation supports Named rules for Routes Experimental channel changes Enabling external Auth for HTTPRoute Mesh resource Introducing default Gateways Configuring client certificate validation Breaking changes Standard GRPCRoute -. spec field required (technicality) Experimental CORS support in HTTPRoute - breaking change for allowCredentials field Improving the development and usage experience Try it out Get involved Related Kubernetes blog articles Ready to rock your Kubernetes networking? The Kubernetes SIG Network community presented the General Availability (GA) release of Gateway API (v1.4.0)! Released on October 6, 2025, version 1.4.0 reinforces the path for modern, expressive, and extensible service networking in Kubernetes. Gateway API v1.4.0 brings three new features to the Standard channel (Gateway API&amp;rsquo;s GA release channel): BackendTLSPolicy for TLS between gateways and backends supportedFeatures in GatewayClass status supportedFeatures Named rules for Routes and introduces three new experimental features: Mesh resource for service mesh configuration Default gateways to ease configuration burden** externalAuth filter for HTTPRoute externalAuth Leads: Candace Holman , Norwin Schnyder , Katarzyna Łach GEP-1897: BackendTLSPolicy BackendTLSPolicy is a new Gateway API type for specifying the TLS configuration of the connection from the Gateway to backend pod(s). Prior to the introduction of BackendTLSPolicy, there was no API specification that allowed encrypted traffic on the hop from Gateway to backend. The BackendTLSPolicy validation configuration requires a hostname. This hostname serves two purposes. It is used as the SNI header when connecting to the backend and for authentication, the certificate presented by the backend must match this hostname, unless subjectAltNames is explicitly specified. BackendTLSPolicy validation hostname subjectAltNames If subjectAltNames (SANs) are specified, the hostname is only used for SNI, and authentication is performed against the SANs instead. If you still need to authenticate against the hostname value in this case, you MUST add it to the subjectAltNames list. subjectAltNames hostname subjectAltNames BackendTLSPolicy validation configuration also requires either caCertificateRefs or wellKnownCACertificates. caCertificateRefs refer to one or more (up to 8) PEM-encoded TLS certificate bundles. If there are no specific certificates to use, then depending on your implementation, you may use wellKnownCACertificates , set to &amp;ldquo;System&amp;rdquo; to tell the Gateway to use an implementation-specific set of trusted CA Certificates.</description></item><item><title>CNCF Ambassador’s reflections on 10 years of the Cloud Native Computing Foundation</title><link>https://kubermates.org/docs/2025-11-06-cncf-ambassador-s-reflections-on-10-years-of-the-cloud-native-computing-foundati/</link><pubDate>Thu, 06 Nov 2025 15:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-06-cncf-ambassador-s-reflections-on-10-years-of-the-cloud-native-computing-foundati/</guid><description>Posted on November 6, 2025 by Dotan Horovits CNCF projects highlighted in this post Ambassador post originally published on Medium by Dotan Horovits “Cloud Native Computing Foundation (CNCF) is a new open standardization initiative recently formed under the Linux Foundation with the mission of providing standard reference architecture for cloud native applications and services, based on open-source software (OSS). ” That’s how I started my blog post 10 years ago , in August 2015 (back then I blogged on WordPress). And the momentum carried on. Eight months later I was excited to share that Prometheus joined The CNCF. Prometheus has since become a de-facto standard for cloud-native monitoring, and recently reached an important milestone with the long-awaited 3.0 major release. More projects have followed suit and joined the CNCF to augment Kubernetes with needed capabilities for running critical production workloads and managing them at scale, addressing aspects such as observability, security and networking. This ecosystem has fortified Kubernetes as the de-facto standard for container orchestration (remember that 10 years ago we had also Docker swarm, Mesos, Nomad?). In fact, just last year we celebrated 10 years to Kubernetes (or as we called it in the CNCF: kuberTENes). Since then the CNCF has grown to over 200 projects in various maturity stages, including over 30 graduated project. The domains have also expanded from Kubernetes and its immediate circle in the DevOps domain, into far wider topics, ranging from FinOps to Platform Engineering and chaos engineering. In fact, looking at the CNCF landscape these days could be somewhat overwhelming. I got to escort OpenTelemetry, second most active project in the CNCF, since its foundation.</description></item><item><title>Safely managing Cilium network policies in Kubernetes: Testing and simulation techniques</title><link>https://kubermates.org/docs/2025-11-06-safely-managing-cilium-network-policies-in-kubernetes-testing-and-simulation-tec/</link><pubDate>Thu, 06 Nov 2025 15:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-06-safely-managing-cilium-network-policies-in-kubernetes-testing-and-simulation-tec/</guid><description>How do Cilium network policy operations and enforcement modes work? Test environment setup Scenario 1: Applying the first default-deny policy Solution 1. a: Endpoint audit mode Solution 1. b: Policy default-deny mode Scenario 2: Making changes to the existing policies Solution 2. a: L7 allow-all Summary &amp;amp; next steps Posted on November 6, 2025 by Dean Lewis, Isovalent CNCF projects highlighted in this post Network policy changes are among the most frequent operations in a Kubernetes cluster. They are also among the most delicate, as even a small mistake can lead to widespread traffic disruption. This tutorial walks through several methods to make policy management safer, especially in day-2 operations or brownfield deployments where clusters already run critical workloads. It shows how to test and validate changes before enforcing them, helping teams adopt a more reliable approach to policy rollout. By following these steps, you can build confidence in your network policy lifecycle and reduce the likelihood of accidental outages. Cilium network policies build upon Kubernetes NetworkPolicy, extending it with deeper visibility and more flexible rule types. Policies are used to allow or deny traffic based on defined rules, which can apply to ingress, egress, or both. These rules are evaluated at the datapath level, meaning enforcement can take place directly in the kernel through eBPF. Every Cilium-managed endpoint operates under a policy enforcement mode, which defines the default network behavior before and after a policy is applied: Default: All traffic is allowed until an endpoint is selected by a policy.</description></item><item><title>Akamai Builds Cloud Native Resilience: Cloud credits to power CNCF projects</title><link>https://kubermates.org/docs/2025-11-05-akamai-builds-cloud-native-resilience-cloud-credits-to-power-cncf-projects/</link><pubDate>Wed, 05 Nov 2025 16:52:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-05-akamai-builds-cloud-native-resilience-cloud-credits-to-power-cncf-projects/</guid><description>Framing the Challenge Why This Matters Supporting Innovation Through Infrastructure Real-World Impact Conclusion &amp;amp; Next Steps Posted on November 5, 2025 by Ihor Dvoretskyi, Sr. Developer Advocate, Cloud Native Computing Foundation Akamai , a CNCF Gold member since 2023 and a committed supporter of open source infrastructure, is generously donating $1,000,000 in annual cloud credits. The donation will support both the Linux Foundation and Cloud Native Computing Foundation. As cloud native technologies evolve to address AI, edge, and platform engineering demands, the infrastructure required to build, test, and scale these projects grows more complex. CI/CD pipelines, release testing, and performance benchmarking often require significant compute power. Community-led projects can struggle to access the necessary infrastructure without dependable support. Cloud credit donations are critical for enabling sustainable project operations without shifting costs to maintainers. Akamai’s credit contribution will allow CNCF to better support scalable, resilient infrastructure that benefits a range of technical initiatives. “We’re incredibly grateful for Akamai’s infrastructure contribution—it’s the kind of support that helps our community scale—experiment—and push cloud native innovation forward,” said Chris Aniszczyk, CTO of the Cloud Native Computing Foundation. “When our members invest like this, it strengthens the resilience and long-term impact of the entire ecosystem. ” As a Gold member, Akamai has deepened its collaboration with CNCF to ensure that projects can access the cloud infrastructure they need to thrive. We will direct the credits toward supporting project infrastructure needs.</description></item><item><title>Accelerating Kubernetes Innovation through CNCF Contributions</title><link>https://kubermates.org/docs/2025-11-05-accelerating-kubernetes-innovation-through-cncf-contributions/</link><pubDate>Wed, 05 Nov 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-05-accelerating-kubernetes-innovation-through-cncf-contributions/</guid><description>Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VCF Breakroom Chats Episode 69 - Beyond vRA + NSX: Delivering Cloud-Native Networking with VCF 9 Virtual Private Clouds Scaling VMware Cloud Foundation 9.0 Lab Environments using Holodeck 9.0 Enterprise customers can now deploy NVIDIA Run:ai on VMware Cloud Foundation Open-source forms the foundation of modern cloud infrastructure, driving key innovations across the industry. At Broadcom, we are not just consumers of open-source projects, we are active contributors and maintainers helping shape CNCF initiatives. Our engineers help address complex operational challenges through direct upstream contributions that make Kubernetes more reliable, scalable, and secure. Broadcom has long been one of the top five contributors 1 to CNCF projects, demonstrating not just participation but genuine leadership in the open-source ecosystem. This isn’t about using open source simply as a consumer but rather strengthening it as a shared foundation for innovation, a practice we uphold across all projects shown in Figure 1. Broadcom’s sustained investment and contributions reflect deep trust in community-driven progress, where collaboration and transparency ultimately serve customers, partners, and the broader industry. Figure 1: Fostering Innovation through Collaboration Challenges for customers By 2029, more than 95% of global organizations will be running containerized applications in production, up from less than 50% in 2023 2. Kubernetes has played a key role as the container orchestration system with adoption at 80% according to a survey conducted by CNCF in 2024 3. In the same survey, 46% of the respondents indicated that open-source projects, including Kubernetes itself, are difficult to understand or run while an equal percentage expressed concerns about the longevity of these projects, and 29% of respondents expressed concerns about security vulnerabilities. While Kubernetes is incredibly powerful and a standard for modern infrastructure, it is also inherently complex. Kubernetes requires integration with dozens of separate tools to achieve enterprise-grade security, compliance, lifecycle management, and consistent operations at scale. Customers are responsible for integrating and operating a complex ecosystem securely and consistently.</description></item><item><title>Introducing the CNCF End User Contributor Program: Earn Access, Influence, and Recognition</title><link>https://kubermates.org/docs/2025-11-05-introducing-the-cncf-end-user-contributor-program-earn-access-influence-and-reco/</link><pubDate>Wed, 05 Nov 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-05-introducing-the-cncf-end-user-contributor-program-earn-access-influence-and-reco/</guid><description>Program Transition: Retiring the End User Supporter Program Your Two Pathways to End User Engagement The Future is Bright for Contributors Who can join? How to apply Posted on November 5, 2025 by Eliza Power The cloud native ecosystem runs on the contributions from many sources– including vendors, developers, academics, and importantly, end users. The real-world production experience of end-user organizations is essential for project evolution and growth. If your organization is a CNCF End User and is contributing insights, code, case studies, demos, or feedback to the CNCF ecosystem, then this announcement is for you. We’re thrilled to announce the launch of the CNCF End User Contributor Program ; a zero-fees pathway for End Users to connect with our community, influence its technical direction, and earn public recognition. With this launch, we’re retiring the End User Support Program – and we want to thank every organization that participated for their valuable support. For Existing End User Supporters: To ensure a smooth transition, existing End User Supporters can renew for one more year under the existing program as long as their renewal term commences on or before January 1, 2026. But we won’t be accepting any new Supporters going forward. Going forward, End User organizations have two distinct, value-driven options for engagement. Both grant access to our End User community, but they differ in benefits: The End User Contributor Program is the free pathway to ensure your company remains connected to the heart of the CNCF End User community and its programs. As a qualified End User Contributor , your organization gains access to specialized events, programs, and privileges, including: Exclusive access to End User programs: Participate in initiatives and programs tailored specifically for cloud native end users. Attend End User-only events at KubeCon + CloudNativeCon: Connect with peers, project leaders, and CNCF leadership in dedicated sessions and networking events Participate in End User groups: Engage in focused discussions, share challenges, and collaborate on solutions with other End Users. Eligibility for the End User Technical Advisory Board (TAB): Run for and participate in the End User TAB, directly influencing CNCF’s technical vision and project oversight from an end-user perspective.</description></item><item><title>Workload Placement in VMware Cloud Foundation: Smarter VM Deployment for Performance and Efficiency</title><link>https://kubermates.org/docs/2025-11-05-workload-placement-in-vmware-cloud-foundation-smarter-vm-deployment-for-performa/</link><pubDate>Wed, 05 Nov 2025 13:24:27 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-05-workload-placement-in-vmware-cloud-foundation-smarter-vm-deployment-for-performa/</guid><description>Why Workload Placement Matters How It Works: The “Smart Room Planner” Analogy Business and Operational Intent Expressing Intent with Tags A Real-World Example Workload Placement vs. DRS Final Thoughts Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VCF Breakroom Chats Episode 69 - Beyond vRA + NSX: Delivering Cloud-Native Networking with VCF 9 Virtual Private Clouds Scaling VMware Cloud Foundation 9.0 Lab Environments using Holodeck 9.0 VMware Cloud Foundation Automation - All Apps Organization Configurations When deploying virtual machines (VMs), where they land matters — not just for performance, but for efficiency, compliance, and cost. That’s where Workload Placement in VMware Cloud Foundation (VCF) comes in. It finds the ideal cluster or host for each VM by evaluating real-time capacity, performance data, and your organization’s business or operational intent. In short, it helps ensure every workload lands exactly where it should — without the manual guesswork. If you’ve ever had to manually decide where to deploy a new workload, you know how easy it is to create resource hotspots or imbalance within your clusters. Workload Placement eliminates that uncertainty. By continuously analyzing real-time capacity and performance metrics, VCF automatically recommends — or even performs — optimal VM placements. This ensures your workloads meet performance, compliance, and cost objectives while freeing your teams to focus on innovation instead of firefighting. Think of Workload Placement as a smart room planner for your data center. Each VM is a piece of furniture — some large, some small, all with specific needs. The clusters and hosts are rooms with varying space, lighting, and features.</description></item><item><title>From Chat to Control: Why Platform Engineers Need More Than an LLM</title><link>https://kubermates.org/docs/2025-11-05-from-chat-to-control-why-platform-engineers-need-more-than-an-llm/</link><pubDate>Wed, 05 Nov 2025 08:00:43 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-05-from-chat-to-control-why-platform-engineers-need-more-than-an-llm/</guid><description>From Chat to Control: Why Platform Engineers Need More Than an LLM The Productivity Mirage Why LLMs Alone Don’t Deliver Real Platform Productivity No Real Context Short Memory, Long Workflows Blind Confidence Without Validation No Audit, No Trust Operational Reality: Cost, Access, and Real-Time Enforcement From Chat to Control: Enter the Nirmata AI Platform Engineering Assistant How Nirmata Bridges the Gap Built for the Real World — and Regulated Ones Agentic Intelligence, Not Just LLMs The Path Forward Large Language Models (LLMs) like ChatGPT, Claude, and tools like Cursor are transforming how developers write and debug code. They autocomplete YAML, summarize logs, and even generate Kubernetes manifests. But for platform engineers, the mission isn’t just about writing code — it’s about governing, securing, and optimizing the systems that run it. And that’s where today’s LLMs fall short. It’s tempting to believe that adding an LLM to your workflow instantly makes your platform team more productive. But platform engineering requires precision, auditability, and control, not just good guesses. When managing clusters, pipelines, and cloud environments, every action must be traceable, validated, and compliant with security and regulatory standards. General-purpose LLMs weren’t designed for that. Let’s break down why. LLMs understand text — not systems. They don’t have live visibility into your clusters, GitOps pipelines, or policy baselines. So while they can generate YAML or Terraform, they can’t tell if it will actually pass admission control or align with organizational standards.</description></item><item><title>Kubernetes Best Practices in 2025: Scaling, Security, and Cost Optimization</title><link>https://kubermates.org/docs/2025-11-05-kubernetes-best-practices-in-2025-scaling-security-and-cost-optimization/</link><pubDate>Wed, 05 Nov 2025 07:28:41 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-05-kubernetes-best-practices-in-2025-scaling-security-and-cost-optimization/</guid><description>Why Kubernetes Best Practices Matter in 2025 Scaling Kubernetes Applications Efficiently Horizontal Pod Autoscaler (HPA) and Vertical Pod Autoscaler (VPA) Cluster Autoscaler and Resource Quotas Right-Sizing Containers for Performance Using KodeKloud’s Kubernetes Labs to Practice Scaling Securing Your Kubernetes Cluster in 2025 Role-Based Access Control (RBAC) and Secrets Management Implementing Network Policies and Pod Security Standards Supply-Chain Security and Image Scanning Learn Kubernetes Security Hands-On with KodeKloud Labs Kubernetes Cost Optimization Strategies Right-Sizing Clusters and Workloads Leverage Spot Instances and Node Pool Optimization Use Namespaces and Resource Quotas for Cost Control Monitor, Measure, and Reduce Idle Time Practice Kubernetes Cost Optimization with KodeKloud Labs Kubernetes Observability and Performance Tuning Why Observability Is the Foundation of Performance Core Pillars: Metrics, Logs, and Traces Bringing AI into Observability (AIOps) eBPF and OpenTelemetry: Deep Observability for 2025 How KodeKloud Helps You Master Observability Common Kubernetes Mistakes to Avoid in 2025 1. Overprovisioning Nodes and Resources 2. Ignoring Security Policies 3. Skipping Regular Version Upgrades 4. Weak Observability and Reactive Monitoring 5. Overprivileged RBAC Configurations Bonus Insight: Real-Time Security and Compliance Quick Recap Practice These Fixes the KodeKloud Way The Future of Kubernetes Excellence Lies in Practice Final Thought FAQ How to Use the cURL Command to Download Files Freelancing Tips: What I Learned After 100+ Clients Why Tech Leads Are The Most in-Demand Role of Gen AI and Agentic AI Evolution Top Kubernetes Certifications in 2025: Which One Should You Choose? Kubernetes Tutorial for Beginners 2025 CKA Exam Verification Guide Kubernetes Architecture Explained: Nodes, Pods, and Clusters Quick Fixes for Common Kubernetes Issues Guide to AWS Certification What is Kubernetes? The challenge isn’t adoption - it’s optimization and security. Clusters are larger, faster, and business-critical than ever. Scale Smarter, Not Just Bigger Combine HPA, VPA, and Cluster Autoscaler for data-driven scaling. Use resource quotas to keep teams fair and costs predictable. Practice live with KodeKloud scaling labs and dashboards. Secure by Design Apply RBAC , Pod Security Standards , and Network Policies. Scan and sign container images with Trivy and Cosign.</description></item><item><title>Multi-cluster GitOps with the Argo CD Agent Technology Preview</title><link>https://kubermates.org/docs/2025-11-05-multi-cluster-gitops-with-the-argo-cd-agent-technology-preview/</link><pubDate>Wed, 05 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-05-multi-cluster-gitops-with-the-argo-cd-agent-technology-preview/</guid><description>Multi-cluster GitOps with the Argo CD Agent Technology Preview Topologies for Argo CD What is the Argo CD Agent? Centralized visibility and insights Reduce Argo CD resource usage Best of both topologies Integration with Red Hat Advanced Cluster Management The road ahead Getting started Red Hat Ansible Automation Platform | Product Trial About the authors Harriet Lawrence Gerald Nunn Jann Fischer More like this Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Running a single instance of Argo CD on a single cluster used to be common, but now a multi-cluster GitOps architecture has become the norm. This way of working does come with its own set of challenges. Typically organizations have had to make compromises when deploying Argo CD in varying topologies depending on their specific priorities (for example, management or scalability). But multi-cluster deployments just got easier to manage and secure with the Argo CD Agent in Red Hat OpenShift GitOps. A new addition to the GitOps operator, the agent is part of the Red Hat OpenShift Platform Plus (OPP) subscription. Broadly, there are two common topologies for implementing Argo CD. The first is centralized, in which a single Argo CD instance runs in a management cluster and is responsible for deploying resources across a fleet of OpenShift or Kubernetes clusters. This provides for a single pane of glass which simplifies management, but can become challenging to scale as the number of applications and clusters grow. The other common topology is distributed, in which Argo CD is distributed across the fleet of OpenShift or Kubernetes clusters. This provides increased scalability, but also increases the complexity of provisioning and managing those instances. Scale has become the major blocker for large production implementations. It&amp;rsquo;s not just Argo CD&amp;rsquo;s performance that&amp;rsquo;s important.</description></item><item><title>Zero-Trust with Zero-Friction eBPF in Calico v3.31</title><link>https://kubermates.org/docs/2025-11-04-zero-trust-with-zero-friction-ebpf-in-calico-v3-31/</link><pubDate>Tue, 04 Nov 2025 21:28:38 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-04-zero-trust-with-zero-friction-ebpf-in-calico-v3-31/</guid><description>Calico eBPF by default* The Road to Seamless Installation How the Operator Automates Setup High-Availability (HA) Kubernetes API Server Support Manifest-Based Installations Limitations Join the Conversation Calico has used eBPF as one of its dataplanes since version 3.13 , released more than five years ago. At the time, this was an exciting step forward, introducing a new, innovative data plane that quickly gained traction within the Calico community. Since then, there have been many changes and continued evolution, all thanks to the many adopters of the then-new data plane. However, there has been one persistent challenge in the installation process since day one: bootstrapping the eBPF data plane required a manual setup step. This extra friction point often frustrated operators and slowed adoption. With the launch of Calico v3.31, that hurdle to using the eBPF data plane has finally been removed. For many environments (see Limitations section below), you can now install Calico with eBPF enabled right out of the box with no manual setup required. Simply use the provided installation manifest ( custom-resources-bpf. yaml ), which comes preconfigured with the data plane option set to eBPF. To get started, follow the instructions in Install Calico networking and network policy for on-premises deployments to enjoy a much smoother installation experience. See Calico eBPF in action with this short demo, highlighting how version 3.31 makes setup easier and increases performance: The Calico eBPF data plane includes a kube-proxy replacement, enabling faster service implementation than the upstream version of kube-proxy for iptables and better integration with the rest of the data plane for overall superior performance. However, a kube-proxy replacement requires knowing how to reach the Kubernetes API server.</description></item><item><title>Announcing Vitess 23.0.0</title><link>https://kubermates.org/docs/2025-11-04-announcing-vitess-23-0-0/</link><pubDate>Tue, 04 Nov 2025 19:13:35 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-04-announcing-vitess-23-0-0/</guid><description>Why This Release Matters What’s New in Vitess 23 New Default Versions New &amp;amp; Improved Metrics Deprecations &amp;amp; Removals Topology &amp;amp; VTOrc Enhancements VTTablet &amp;amp; CLI / Docker Updates Upgrade Notes What’s Next Thanks &amp;amp; Acknowledgements Posted on November 4, 2025 by The Vitess Team CNCF projects highlighted in this post The Vitess team is excited to release Vitess 23.0.0 — the latest major version of Vitess — bringing new defaults, better operational tooling, and refined metrics. This release builds on the strong foundation of version 22 and is designed to make deployment and observability smoother, while continuing to scale MySQL workloads horizontally with confidence For production users of Vitess, this release is meaningful in several ways: Upgrading defaults: Moving to MySQL 8.4 as default future-proofs deployments and signals forward compatibility. Better metrics: The added observability enables deeper insights into transaction routing, shard behavior, and recovery actions — making debugging and alerting more precise. Clean-ups &amp;amp; deprecations: Removing legacy metrics and APIs simplifies monitoring and avoids confusion. Operational strength: Enhanced VTOrc and topology controls reduce risk in large-scale fleets and tighten security boundaries. Here are some of the standout changes you should know about: The default MySQL version for the vitess/lite:latest image has been bumped from 8.0.40 to 8.4.6. → PR #18569 vitess/lite:latest VTGate now advertises MySQL version 8.4.6 by default (instead of 8.0.40). If your backend uses a different version, set the mysql_server_version flag accordingly. → PR #18568 mysql_server_version Important upgrade detail for operator users: When upgrading from MySQL 8.0 → 8.4 with the Vitess Operator, you must: Add innodb_fast_shutdown=0 to your extra. cnf in the YAML. Apply the file and wait until all pods are healthy. Switch the image to vitess/lite:v23.0.0.</description></item><item><title>VCF Breakroom Chats | Episode 70: Simplifying Private Cloud with Fleet Management and Lifecycle Management</title><link>https://kubermates.org/docs/2025-11-04-vcf-breakroom-chats-episode-70-simplifying-private-cloud-with-fleet-management-a/</link><pubDate>Tue, 04 Nov 2025 15:16:14 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-04-vcf-breakroom-chats-episode-70-simplifying-private-cloud-with-fleet-management-a/</guid><description>Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VCF Breakroom Chats Episode 69 - Beyond vRA + NSX: Delivering Cloud-Native Networking with VCF 9 Virtual Private Clouds SAP HANA and SAP NetWeaver Support for vSphere in VMware Cloud Foundation 9.0 on Intel Xeon 6 CPUs with P-core Systems VCF Breakroom Chats | Episode 70: Simplifying Private Cloud with Fleet Management and Lifecycle Management Are you looking to improve consistency, eliminate manual upgrade processes, and reduce maintenance downtime to help scale your private cloud deployment? In episode 70 of the VCF Breakroom Chats , Product Marketing Engineer Sehjung Hah and Marc Umeno from Product Management discuss infrastructure fleet management. Marc explains how VMware Cloud Foundation (VCF) 9.0 automates the process of keeping your private cloud environment updated with the latest patched versions, improving upgrading and patching software or Lifecycle Management. This ensures consistency and eliminates manual upgrade processes and potential discrepancies. Key features include: A single VCF Operations console for environment-wide upgrades, replacing manual individual vCenter upgrades. Pre-checks to minimize surprises during fleet-wide upgrades. Deployment of security patches via an end-to-end process across the fleet for both connected and disconnected environments. Utilization of quick boot and live patch to minimize downtime. Scheduled upgrades and patches to enhance administrator efficiency. Watch VCF Breakroom Chat episode 70 and learn how these innovations provide customers with scalable operations and management in VCF 9.0. About the VCF Breakroom Chat Series This webinar series focuses on VMware Cloud Foundation (VCF). ​In this series, we share conversations with industry-recognized experts from Broadcom and with solution partners and customers. You’ll gain relevant insights whether you’re an IT practitioner, IT admin, cloud or platform architect, developer, DevOps, senior IT manager, IT executive, or AI/ML professional.</description></item><item><title>NVMe Memory Tiering Design and Sizing on VMware Cloud Foundation 9 – Part 1: Pre-requisites and Hardware Compatibility</title><link>https://kubermates.org/docs/2025-11-04-nvme-memory-tiering-design-and-sizing-on-vmware-cloud-foundation-9-part-1-pre-re/</link><pubDate>Tue, 04 Nov 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-04-nvme-memory-tiering-design-and-sizing-on-vmware-cloud-foundation-9-part-1-pre-re/</guid><description>PART 1: Prerequisites and Hardware Compatibility Workload Assessment Software Pre-requisites NVMe Compatibility Discover more from VMware Cloud Foundation (VCF) Blog Related Articles SAP HANA and SAP NetWeaver Support for vSphere in VMware Cloud Foundation 9.0 on Intel Xeon 6 CPUs with P-core Systems Scaling VMware Cloud Foundation 9.0 Lab Environments using Holodeck 9.0 VMware Cloud Foundation Automation - All Apps Organization Configurations At VMware Explore 2025 in Las Vegas, a plethora of announcements were made as well as many deep dives into the new features and enhancements included in VMware Cloud Foundation (VCF) 9, including the popular NVMe Memory Tiering feature. Although this feature is available at the compute component of VCF (vSphere), we refer to it in the context of VCF given the deep integration with other components such as VCF Operations, which we will refer to in later blogs. Memory Tiering is a new feature included with VMware Cloud Foundation and was one of the main topics of conversation for most of my sessions at VMware Explore 2025. I saw a lot of interest, and a lot of great questions came from our customers on adoption, use cases, etc. This is a multi-part blog series where I intend to help answer a lot of the common questions coming from our customers, partners, and internal staff. For a high level understanding of what Memory Tiering is, please refer to this blog. Before enabling Memory Tiering, a thorough assessment of your environment is crucial. Start by evaluating workloads in your datacenter and pay special attention to memory. One of the key measures we will look at is the active memory of the workload. For workloads to be an optimal candidate for Memory Tiering, we want the total active memory to be 50% or less of the DRAM capacity. Why 50% you ask? Well, the default configuration of Memory Tiering is to provide you with 100% more memory or 2x your memory. So, after enabling Memory Tiering, half of the memory will be coming from DRAM (Tier 0) and the other half comes from NVMe (Tier 1).</description></item><item><title>vSAN Data Protection in VMware Cloud Foundation – The Solution You Already Own</title><link>https://kubermates.org/docs/2025-11-04-vsan-data-protection-in-vmware-cloud-foundation-the-solution-you-already-own/</link><pubDate>Tue, 04 Nov 2025 13:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-04-vsan-data-protection-in-vmware-cloud-foundation-the-solution-you-already-own/</guid><description>Local Protection the Easy Way Simple and Flexible Recovery Fast and Flexible Cloning How to Get Started Install the Virtual Appliance used for vSAN Data Protection Configure Protection Groups Extended Functionality in VCF 9.0 Summary Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Scaling VMware Cloud Foundation 9.0 Lab Environments using Holodeck 9.0 VMware Cloud Foundation Automation - All Apps Organization Configurations Workload Placement in VMware Cloud Foundation: Smarter VM Deployment for Performance and Efficiency With a flurry of announcements and new capabilities offered in VMware Cloud Foundation (VCF) 9.0, it is sometimes easy to overlook relatively new features hidden in plain sight. Protecting data in a private cloud has been a hot topic as of late, extending well beyond just typical data recovery requirements. Customers are looking to devise practical strategies to protect themselves against ransomware attacks and disaster recovery using solutions that are easy to manage at scale. vSAN Data Protection initially debuted in vSAN 8 U3, as a part of VMware Cloud Foundation 5.2. Perhaps the most overlooked aspect of vSAN Data Protection is that it is a part of the VCF license! Why is this so important? If you are running vSAN ESA in your VCF environment, you have everything you need to locally protect your workloads using vSAN Data Protection. It can serve as a terrific way to augment your existing protection strategies or serve as the foundation for more comprehensive protection. Let’s take a brief look at what this local protection can do for you, and how you can adopt it in a simple and scalable way. As a part of your VCF license, vSAN Data Protection gives you the ability to use snapshots the way you always envisioned. Using vSAN ESA’s native snapshotting engine, it allows you to: Easily define groups of VMs and their protection and retention schedules — retaining up to 200 snapshots per VM. Create crash-consistent snapshots of VMs at regular intervals with little to no impact on performance. Easily restore one or more VMs directly in vCenter Server using the vSphere Client, even if they have been removed from inventory. Since vSAN Data Protection protects at the VM level, protecting and restoring discrete VMDKs within a VM is not possible at this time.</description></item><item><title>Leading the Cloud With Curiosity : Spotlight on Pranav Nambiar, SVP, AI/ML &amp; PaaS</title><link>https://kubermates.org/docs/2025-11-04-leading-the-cloud-with-curiosity-spotlight-on-pranav-nambiar-svp-ai-ml-paas/</link><pubDate>Tue, 04 Nov 2025 04:01:20 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-04-leading-the-cloud-with-curiosity-spotlight-on-pranav-nambiar-svp-ai-ml-paas/</guid><description>Leading the Cloud With Curiosity : Spotlight on Pranav Nambiar, SVP, AI/ML &amp;amp; PaaS What made DigitalOcean the right place for you? How is your team shaping the future for our customers? What makes DigitalOcean a special place to build and innovate? Dive into the future with DigitalOcean About the author Try DigitalOcean for free Related Articles Is DigitalOcean Your Next Career Spot? A 5-Year Insider on Why It Should Be Sharks of DigitalOcean: Archana Kamath, Senior Director, IaaS Sharks of DigitalOcean: Darian Wilkin, Senior Manager, Solutions Engineering By Sujatha R Technical Writer Published: November 4, 2025 3 min read Pranav Nambiar is no stranger to big tech, but what inspired him to join DigitalOcean was to be closer to the builders. He also wanted to work in a place where ideas move fast, people stay humble, and real breakthroughs happen. Today, he leads our AI/ML and PaaS portfolios with curiosity, empathy, and a belief that innovation should lift others up. “Having spent a lot of time in large companies, I was eager to experience a smaller, more agile company and broaden my horizons. DigitalOcean stood out for three main reasons: First , the fundamentals are incredibly strong. A company that moves from startup to IPO and now serves more than 640,000 customers—that’s an incredible achievement. It clearly signals strong fundamentals at DigitalOcean. Second , the opportunity to make a big impact is real. With generative AI transforming how innovation happens, and developers becoming the new drivers of change, DigitalOcean’s developer-first product heritage and simplified cloud focus position us to make massive waves. Last but not least , the culture is what brought me here. People build amazing products with a strong focus on simplicity and sophistication—this is what DigitalOcean is about. All of these combined made me feel this was the right place to join the journey and create a meaningful impact.</description></item><item><title>SRE vs Platform Engineer: Evolving Roles in the Age of AI-Driven Platforms</title><link>https://kubermates.org/docs/2025-11-04-sre-vs-platform-engineer-evolving-roles-in-the-age-of-ai-driven-platforms/</link><pubDate>Tue, 04 Nov 2025 01:00:38 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-04-sre-vs-platform-engineer-evolving-roles-in-the-age-of-ai-driven-platforms/</guid><description>SRE vs Platform Engineer: Evolving Roles in the Age of AI-Driven Platforms Core Focus Goals Tools Used Skills &amp;amp; Responsibilities What Type of Organizations Need These Roles (and Salaries) The Rise of AI in Platform Engineering and SRE How Nirmata’s AI Platform Engineer Assistant Helps Platform Engineers How Nirmata’s AI Platform Engineer Assistant Helps SREs The Future: Convergence of SRE and Platform Engineering Summary Next Steps Modern cloud-native organizations rely on two critical disciplines to achieve operational excellence — Site Reliability Engineering (SRE) and Platform Engineering. While both roles share a focus on automation, reliability, and scalability, their core missions, tools, and metrics for success differ. With the rise of AI-assisted infrastructure management, these roles are converging faster than ever. SREs ensure system reliability, performance, and uptime through automation and incident management. Platform Engineers build self-service, secure, and compliant platforms that empower developers to deploy applications efficiently. SREs focus on maintaining system reliability and meeting SLAs, while Platform Engineers enable developers with secure, reusable, and automated workflows. SREs measure success through uptime and error budgets, while Platform Engineers focus on developer productivity and compliance consistency. SREs rely on Prometheus , Grafana , PagerDuty , and ChaosMesh for observability and incident response. Platform Engineers leverage ArgoCD , Kyverno , Terraform , and Nirmata Control Hub for automation, governance, and Policy-as-Code enforcement. SREs specialize in observability, capacity planning, and performance optimization, while Platform Engineers focus on building Internal Developer Platforms (IDPs), enforcing security policies, and managing infrastructure-as-code. Startups often employ hybrid DevOps or SRE roles. Mid-size SaaS companies need Platform Engineers to standardize deployments and policies.</description></item><item><title>Introducing Red Hat Satellite 6.18: New AI, management, and system health capabilities</title><link>https://kubermates.org/docs/2025-11-04-introducing-red-hat-satellite-6-18-new-ai-management-and-system-health-capabilit/</link><pubDate>Tue, 04 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-04-introducing-red-hat-satellite-6-18-new-ai-management-and-system-health-capabilit/</guid><description>Introducing Red Hat Satellite 6.18: New AI, management, and system health capabilities Enhance efficiency with offline AI Expanded management capabilities on-premises Improved subscription reporting Optimized content delivery Learn more The adaptable enterprise: Why AI readiness is disruption readiness About the author Mary Mackey More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Red Hat Satellite 6.18 is now generally available (GA), with new AI and system monitoring and management capabilities to streamline IT operations and simplify Linux administration. This release continues to build on our commitment to providing a unified management solution across on-premise, hybrid cloud, and public cloud environments. Key updates in Red Hat Satellite 6.18 focus on three areas: AI-powered tools, system health, and content management features. Satellite 6.18 introduces new AI-powered features designed to streamline Linux administration. These functionalities aim to increase efficiency for all IT professionals, regardless of your experience level. The new model context protocol (MCP) server, released as a technology preview, allows you to query Satellite for information using natural language. This eliminates the need for specific query syntax, or significant manual effort. You can efficiently generate reports on patching requirements, categorize patches, and filter lists by environment with a natural language query to an MCP client connected to the MCP server. The command-line assistant, first introduced in Red Hat Enterprise Linux (RHEL) 10 and launched earlier in 2025, offers natural language answers directly from the command line. Included with Satellite 6.18 is the developer preview offline version of the command-line assistant. This offline command-line assistant is packaged and runs as containers, enabling local queries without the need for external network connectivity. This is particularly beneficial for organizations in sectors such as government, finance, and industrial control, where cloud access is restricted.</description></item><item><title>Red Hat Insights is now Red Hat Lightspeed: Accelerating AI-powered management</title><link>https://kubermates.org/docs/2025-11-04-red-hat-insights-is-now-red-hat-lightspeed-accelerating-ai-powered-management/</link><pubDate>Tue, 04 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-04-red-hat-insights-is-now-red-hat-lightspeed-accelerating-ai-powered-management/</guid><description>Red Hat Insights is now Red Hat Lightspeed: Accelerating AI-powered management What does this mean for you? What to expect The adaptable enterprise: Why AI readiness is disruption readiness About the author Brent Midwood More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Red Hat Insights is now Red Hat Lightspeed, reflecting our next step in bringing AI-powered management to Red Hat platforms. This change builds on the foundation we&amp;rsquo;ve established with Red Hat Insights, and extends our commitment to integrating intelligence, predictive analytics, and risk management across the Red Hat portfolio. For years, Red Hat Insights has been a core part of every Red Hat subscription, helping you manage your environments across Red Hat Enterprise Linux, Red Hat OpenShift, and Red Hat Ansible Automation Platform. Red Hat Lightspeed continues to provide you with powerful, predictive analytics that help you work faster, smarter, and with greater security and expands the reach of these capabilities with AI-driven insights that help you operate more efficiently and securely at scale. The capabilities you already depend on are still here, built into your Red Hat subscription and accessible through the same workflows and interfaces you use today. We&amp;rsquo;re enhancing the level of intelligence and automation behind them by introducing AI-enhanced context, recommendations, and automation that make Red Hat Lightspeed more efficient and more adaptive to your environment. Move faster : Identify and resolve issues sooner with AI-assisted analysis that anticipates risks and accelerates remediation. Gain deeper clarity : Combine years of operational data with pattern recognition to surface insights that help you make informed decisions across infrastructure, cost, and performance. Strengthen your security posture : Extend existing vulnerability and compliance capabilities with AI-driven prioritization that helps you focus on the most critical exposures first. Work seamlessly within your workflows : Continue using your existing integrations with Red Hat Ansible Automation Platform, ServiceNow, Splunk, and others, now augmented with more intelligent recommendations and automation paths that fit naturally into your daily operations. With Red Hat Lightspeed, you&amp;rsquo;ll continue using the same workflows, console entry points, and integrations that you rely on today. All existing capabilities remain available, and there&amp;rsquo;s no action required on your part to keep using them.</description></item><item><title>Navigating the industrial edge: How a platform approach unlocks business value</title><link>https://kubermates.org/docs/2025-11-03-navigating-the-industrial-edge-how-a-platform-approach-unlocks-business-value/</link><pubDate>Mon, 03 Nov 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-03-navigating-the-industrial-edge-how-a-platform-approach-unlocks-business-value/</guid><description>Navigating the industrial edge: How a platform approach unlocks business value Breaking down silos is key Key benefits of Red Hat&amp;rsquo;s advanced compute platform approach Collaborating for your success About the author Kelly Switt More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share When I talk to industrial customers, a consistent theme emerges: the world of operational technology (OT) is undergoing a rapid, often-daunting transformation. For decades, OT environments have been built on proprietary, siloed systems, prized for their stability and longevity. But as we see technology power improvements in efficiency, productivity, and cost reduction, the pressure to evolve is immense. It&amp;rsquo;s no longer just about keeping the factory running; it&amp;rsquo;s about making it smarter, more agile, and more resilient. This shift presents a unique challenge for IT managers and business leaders. How do you bring the benefits of modern IT—like automation, security, and data analytics—into a world built on a completely different set of principles, all without disrupting mission-critical operations? As a software company with deep IT roots, Red Hat knew that in order to truly have relevance in this space, we needed to not only bring in experts in the OT space, but also to partner with trusted brands like ABB, Schneider Electric, and more that could provide guidance on how to move into the OT space. Red Hat is focused on helping industries navigate this transformation by providing a more consistent advanced compute platform designed to simplify deployment, maintenance, and operations. Our open source solutions are designed to lower operating costs, accelerate time-to-market, and deliver reliable innovation with greater security capabilities. The answer isn&amp;rsquo;t about replacing everything overnight. It&amp;rsquo;s about building a bridge and this is where a comprehensive, platform-centric approach becomes so powerful. Breaking down silos is key. In my experience, the biggest roadblock isn&amp;rsquo;t the technology itself—it&amp;rsquo;s the historical and necessary distinction between IT and OT.</description></item><item><title>Drift Detection for Kubernetes: The Missing Link in Secure GitOps</title><link>https://kubermates.org/docs/2025-11-01-drift-detection-for-kubernetes-the-missing-link-in-secure-gitops/</link><pubDate>Sat, 01 Nov 2025 21:50:42 +0000</pubDate><guid>https://kubermates.org/docs/2025-11-01-drift-detection-for-kubernetes-the-missing-link-in-secure-gitops/</guid><description>Drift Detection for Kubernetes: The Missing Link in Secure GitOps What is Drift Detection? Implications for Security Why Drift Detection Matters — Even with GitOps Customer Use Case: Detecting Unauthorized RBAC Drift Policy-as-Code to the Rescue Example Policy: Detect Unauthorized Changes to ClusterRoles and ClusterRoleBindings How It Works Testing the Policy Beyond RBAC: Other Drift Detection Possibilities Summary Next Steps Modern Kubernetes environments are meant to be declarative — what you define in Git is what should be running in production. But in reality, things drift. Resources change without review, permissions are updated manually, or configuration baselines diverge. This phenomenon — known as configuration drift — silently erodes the reliability and security of even the most well-engineered platforms. In this post, we’ll explore what drift detection means for Kubernetes, why it’s critical even in GitOps workflows, and how Policy-as-Code can help teams prevent and detect it automatically. Drift detection is the process of identifying when the actual state of a Kubernetes resource deviates from its declared or desired state. Drift can occur in many ways: A user manually edits a resource ( kubectl edit/apply ) outside of GitOps workflows. An automated tool or controller changes a setting unexpectedly. A misconfigured Helm or CI/CD pipeline overwrites values. When drift occurs, your clusters may no longer match compliance, security, or operational expectations — and those changes often go unnoticed until an incident occurs. Uncontrolled drift can create shadow configurations — resources that bypass review and violate security policies. For example: An altered ClusterRole granting unintended privileges.</description></item><item><title>Forrester Opportunity Snapshot: A Single Platform Approach to Managing Containers and VMs</title><link>https://kubermates.org/docs/2025-10-31-forrester-opportunity-snapshot-a-single-platform-approach-to-managing-containers/</link><pubDate>Fri, 31 Oct 2025 23:27:45 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-31-forrester-opportunity-snapshot-a-single-platform-approach-to-managing-containers/</guid><description>Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Forrester Opportunity Snapshot: A Single Platform Approach to Managing Containers and VMs VCF Breakroom Chats Episode 71: Maximizing VCF Resource Prudency through Capacity Management IDC Spotlight: Overcoming Kubernetes Complexity with Modern Private Cloud Today, technology modernization isn’t a choice—it’s a necessity. IT leaders seeking a competitive edge are prioritizing a modern, resilient infrastructure that can support advanced applications and drive innovation. This new focus is squarely on cloud-native ecosystems, with both containers and virtual machines (VMs) seen as core elements of an enterprise modernization strategy. The challenge, as always, lies in managing increasing complexity. Organizations are running more container and VM management solutions than ever before, often facing issues with siloed infrastructure and operational inefficiencies. Broadcom recently commissioned a Forrester Opportunity Snapshot – “ Modernize Or Fall Behind: Rethinking IT Infrastructure For A Competitive Edge ” — that surveyed 216 IT decision-makers from the US, EMEA, and APAC with influence over their organization’s IT infrastructure strategy and architecture. 84% reported they were struggling to oversee the different tools and platforms used to manage their server virtualization and container management solutions. The demand for a simpler approach is clear: 85% of respondents want a single, unified platform to manage both containers and VMs. This unified approach promises significant benefits beyond simplified management. More than 80% of IT decision makers expect a single platform to increase scalability, streamline infrastructure management, and enhance resource utilization. But this isn’t just about central IT; a unified platform also accelerates DevOps processes and boosts application team agility. Read the report to learn how your peers are thinking about how to address the challenges of deploying and managing both containers and VMs, and how a single, unified platform such as VMware Cloud Foundation can help.</description></item><item><title>Building a unified hybrid cloud with Infrastructure as Code at RBC</title><link>https://kubermates.org/docs/2025-10-31-building-a-unified-hybrid-cloud-with-infrastructure-as-code-at-rbc/</link><pubDate>Fri, 31 Oct 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-31-building-a-unified-hybrid-cloud-with-infrastructure-as-code-at-rbc/</guid><description>Hybrid cloud strategy: A unified approach Secure and controlled deployments Standardization and GitOps best practices Solving the on-premises infrastructure challenge Building a custom IaC provider for on-premises environment Achievements and future plans Final words Posted on October 31, 2025 by John Nixon, Royal Bank of Canada Managing infrastructure across a hybrid cloud environment—spanning public platforms and private data centers—presents a major challenge. Organizations must balance compliance, cost control, and developer experience while delivering consistency at scale. At RBC, we addressed this by building a secure and scalable Infrastructure as Code (IaC) strategy tailored for hybrid environments. Our ambition extends beyond being a leading financial institution—we’re developing the internal capabilities, engineering practices, and developer platforms to operate like a modern technology company. To deliver a consistent infrastructure experience, RBC embraced three guiding principles: compliance, cost management, and developer convenience. Security and regulatory requirements were prioritized, cloud spending was closely monitored and optimized, and developer productivity was enhanced by making infrastructure management seamless. A centralized IaC platform enabled uniform workflows. By deploying execution agents within each cloud or on-premises zone, deployments became faster, localized, and more secure—keeping sensitive data within appropriate boundaries. This was a foundational step in our broader transformation: elevating infrastructure as a first-class product and treating platform engineering as a core competency, not a support function. It’s how we’re enabling RBC to scale, while preserving the security and trust expected of a global financial institution. Operating in a regulated industry means security is non-negotiable. RBC embedded policy-as-code into every stage of the infrastructure lifecycle.</description></item><item><title>Tool descriptions are eating up all your AI tokens (but they don’t have to)</title><link>https://kubermates.org/docs/2025-10-31-tool-descriptions-are-eating-up-all-your-ai-tokens-but-they-don-t-have-to/</link><pubDate>Fri, 31 Oct 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-31-tool-descriptions-are-eating-up-all-your-ai-tokens-but-they-don-t-have-to/</guid><description>Where the waste comes from Reducing token waste with smarter tool selection Community-driven innovation Looking ahead Posted on October 31, 2025 by Craig McLuckie, Stacklok The vast majority of developers now use AI coding assistants daily. As these tools become more advanced and widely adopted, usage quotas and rate limits have also become a familiar frustration. Many providers enforce weekly or monthly usage caps to manage compute costs. Once you hit a limit, you might be blocked, throttled, or shifted to slower processing queues — halting productivity and disrupting your workflow. But there’s a surprising reason behind much of this token consumption: wasted tool metadata. As contributors exploring the Model Context Protocol (MCP) have found, the vast majority of tokens consumed by AI coding assistants come not from your prompts or code, but from unnecessary tool descriptions that get bundled into each request. Let’s say you’ve installed MCP servers for GitHub, Grafana, and Notion. You ask your AI coding assistant to: “List the 10 most recent issues from my GitHub repo. ” That simple prompt uses 102,000 tokens, not because the task is complex, but because the model receives metadata for 114 tools, most of which have nothing to do with the request. Other common prompts create similar waste: “Summarize my meeting notes from October 19, 2025” uses 240,600 tokens, again with 114 tools injected, even though only the Notion server is relevant “Search dashboards related to RDS” consumes 93,600 tokens In each case, only a small fraction of those tokens are relevant to the task. Even saying “hello” burns more than 46,000 tokens. Multiply that across even a few dozen prompts per day, and you’re burning millions of tokens on context the model doesn’t need.</description></item><item><title>Top Kubernetes Certifications in 2025: Which One Should You Choose?</title><link>https://kubermates.org/docs/2025-10-31-top-kubernetes-certifications-in-2025-which-one-should-you-choose/</link><pubDate>Fri, 31 Oct 2025 05:52:44 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-31-top-kubernetes-certifications-in-2025-which-one-should-you-choose/</guid><description>Introduction - Why Kubernetes Certifications Matter in 2025 The CNCF and Its Role in Kubernetes Certifications The Value of Getting Kubernetes Certified 1. Boosts Your Job Prospects and Credibility 2. Demonstrates Real, Practical Skills 3. Opens Doors to Cloud and DevOps Roles 4. Keeps You Competitive in a Growing Market Understanding the Kubernetes Certification Path KCNA → CKA → CKAD → KCSA (Optional: CKS for advanced security) KCNA - Kubernetes and Cloud Native Associate (Beginner Level) Who It’s For What It Covers Exam Format Why It’s Worth It Next Step CKA - Certified Kubernetes Administrator (Intermediate Level) Who It’s For What It Covers Exam Format Why It’s Worth It CKAD - Certified Kubernetes Application Developer (Developer Track) Who It’s For What It Covers Exam Format CKA vs CKAD - What’s the Difference? Why It’s Worth It KCSA - Kubernetes and Cloud Security Associate (Security Track) Who It’s For What It Covers Exam Format Why It’s Worth It CKS - Certified Kubernetes Security Specialist (Advanced Security Level) Who It’s For What It Covers Exam Format Why It’s Worth It How to Choose the Right Kubernetes Certification Exam Details and Preparation Tips (2025 Edition) Preparation Tips for Success The CNCF Kubestronaut Learning Journey What Is the CNCF Kubestronaut Journey? How KodeKloud Helps You Become a Kubestronaut Common Mistakes to Avoid Before the Exam 1. Skipping Hands-On Practice 2. Poor Time Management During the Exam 3. Focusing on Memorization Instead of Concepts 4. Neglecting kubectl Shortcuts and YAML Syntax 5. Ignoring Security and Context 6. Waiting Too Long to Schedule the Exam Final Thoughts - Your Kubernetes Career Roadmap for 2025 FAQs Kubernetes Tutorial for Beginners 2025 CKA Exam Verification Guide Kubernetes Architecture Explained: Nodes, Pods, and Clusters Quick Fixes for Common Kubernetes Issues Guide to AWS Certification What is Kubernetes? A Beginner’s Guide to Container Orchestration DevOps Tutorials 2025: Step-by-Step Learning Resources for Beginners Ubuntu: Set Timezone How to Enable SSH on Ubuntu Certifications in DevOps: Which Are Worth Your Time in 2025? CNCF (Cloud Native Computing Foundation) governs all official Kubernetes certifications and ensures global recognition and regular updates. Certifications validate real-world, practical Kubernetes skills , not just theoretical knowledge.</description></item><item><title>Efficient and reproducible LLM inference: Inside Red Hat’s MLPerf Inference v5.1 submissions</title><link>https://kubermates.org/docs/2025-10-31-efficient-and-reproducible-llm-inference-inside-red-hat-s-mlperf-inference-v5-1-/</link><pubDate>Fri, 31 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-31-efficient-and-reproducible-llm-inference-inside-red-hat-s-mlperf-inference-v5-1-/</guid><description>Efficient and reproducible LLM inference: Inside Red Hat’s MLPerf Inference v5.1 submissions Executive summary Introduction MLPerf test scenarios and vLLM harness MLPerf test scenarios vLLM harness Benchmarking setup and results Model and dataset Performance tuning: Autotune Results Future outlook and concluding remarks The adaptable enterprise: Why AI readiness is disruption readiness About the authors Naveen Miriyalu Diane Feddema Michey Mehta Keith Valin Michael Goin Ashish Kamra Jean Hsiao More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share As generative AI (gen AI) workloads become central to enterprise applications, benchmarking their inference performance has never been more critical for understanding the limits of their capabilities. In MLPerf Inference v5.1, Meta’s Llama 3.1-8B was featured for the first time. This post presents Red Hat’s submissions using the FP8 quantized llama 3.1-8b model with vLLM 0.10.0 on Red Hat Enterprise Linux (RHEL) 9.6, outlining our approach to achieving reproducible, high-throughput LLM inference. Our configuration delivered competitive single-GPU performance, achieving 5777 tokens/sec (Offline) and 5103 tokens/sec (Server) on a single H100 GPU, and 1642 tokens/sec (Offline) and 1207 tokens/sec (Server) on a single L40S GPU. In this article, you’ll hear about MLPerf test scenarios, benchmark setup, tuning methodology, and results, and learn how these efforts align with Red Hat’s broader strategy to deliver open, scalable, and production-grade AI inference platforms. The accelerated pace of innovation in large language models (LLMs) and their associated ecosystems has increased widespread adoption across virtually every industry. This momentum is fueled by continual advancements in hardware accelerators, including modern GPUs and custom AI Application-Specific Integrated Circuits (ASICs) that unlock an unprecedented level of parallelism and computational throughput. Complementing these advancements, software-layer innovations are transforming how efficiently these hardware capabilities are harnessed. At the heart of this software evolution are inference engines, which serve as the execution backbone for deploying foundation models. These engines optimize how model weights, activation patterns, and memory flows are mapped to underlying hardware, achieving substantial gains in latency, throughput, and energy efficiency. Modern inference engines dynamically adapt to heterogeneous environments across cloud, edge, and on-premise deployments. They also integrate advanced scheduling, quantization, caching, and compilation techniques to accelerate end-to-end inference pipelines.</description></item><item><title>Friday Five — October 31, 2025</title><link>https://kubermates.org/docs/2025-10-31-friday-five-october-31-2025/</link><pubDate>Fri, 31 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-31-friday-five-october-31-2025/</guid><description>Friday Five — October 31, 2025 AIM Media House : “Customers want an OpenAI-like service they control,” says Red Hat’s Tushar Katarki Headed to KubeCon? Join us for Red Hat OpenShift Commons SiliconANGLE : Red Hat tightens Nvidia ties with BlueField integration and native CUDA support Technology Magazine : Red Hat on how to build durable, AI‑ready enterprises The State of Missouri scales automation, unlocks time for innovation About the author Red Hat Corporate Communications More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share When enterprises first began experimenting with generative AI, many raced to cloud-based frontier models. It didn’t take long for the reality of cost, privacy and lock-in to set in. For many, the promise of AI independence felt out of reach. Red Hat thinks it doesn’t have to be. Learn more Red Hat OpenShift Commons Gathering is taking place November 10th alongside KubeCon NA in Atlanta. Join OpenShift users, partners, customers and contributors to hear real-world insights, use cases and lessons learned from customers like Northrop Grumman, Morgan Stanley, Ford, Banco do Brasil, Dell, DTCC and more! Save your seat today! Learn more Red Hat is simplifying and securing AI workload deployment across commercial, government, and hybrid clouds with key updates. These include Red Hat OpenShift support on Nvidia BlueField DPUs, a hardened container image for government use with Nvidia GPUs, and an agreement to distribute the Nvidia Cuda Toolkit across Red Hat&amp;rsquo;s software. Learn more Red Hat’s Michael Ferris explains why resilience alone won’t cut it and how AI readiness demands adaptability built into culture and operations. Learn more The State of Missouri serves millions digitally every year, offering a range of services from motor vehicle registrations and various state licenses to health and social benefits and resident support. These citizen services require significant operational rigor, driven by an IT organization focused on delivering technology with speed, consistency and efficiency. Learn more Red Hat is the world’s leading provider of enterprise open source software solutions, using a community-powered approach to deliver reliable and high-performing Linux, hybrid cloud, container, and Kubernetes technologies. Red Hat helps customers integrate new and existing IT applications, develop cloud-native applications, standardize on our industry-leading operating system, and automate, secure, and manage complex environments.</description></item><item><title>Technical Guide: Nirmata Terraform Cloud (TFC) Run Task Integration</title><link>https://kubermates.org/docs/2025-10-30-technical-guide-nirmata-terraform-cloud-tfc-run-task-integration/</link><pubDate>Thu, 30 Oct 2025 16:55:17 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-30-technical-guide-nirmata-terraform-cloud-tfc-run-task-integration/</guid><description>Technical Guide: Nirmata Terraform Cloud (TFC) Run Task Integration Overview 1. Prerequisites 2. Architecture Overview 3. Key Capabilities 4. Configuring the Integration Step 1: Enable Integration in Nirmata Control Hub Step 2: Create a Run Task in Terraform Cloud Step 3: Trigger and Validate a Terraform Run 5. Continuous Compliance (Post-Provision) 6. Advanced Use Case: Golden Paths for Secure Self-Service 7. Summary 8. Next Steps The Nirmata and HashiCorp Terraform Cloud (TFC) integration enables policy-as-code validation and continuous governance across your entire Infrastructure-as-Code (IaC) lifecycle. Terraform defines what infrastructure to provision; Nirmata governs how that infrastructure is configured and managed after provisioning. Together, they enable a secure-by-design , fully automated workflow that spans from provision to policy. This guide explains how to configure, test, and operationalize the Nirmata Terraform Cloud (TFC) Run Task integration for continuous compliance enforcement.</description></item><item><title>Calico Whisker in Action: Reading and Understanding Policy Traces</title><link>https://kubermates.org/docs/2025-10-30-calico-whisker-in-action-reading-and-understanding-policy-traces/</link><pubDate>Thu, 30 Oct 2025 15:00:09 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-30-calico-whisker-in-action-reading-and-understanding-policy-traces/</guid><description>Kubernetes Network Policy Behaviour Flow Reporter Field Policy Trace Output Examples 1. Kubernetes Default Behaviour 2. Staged Allow 3. Staged Deny 4. Enforced Allow 5. Enforced Deny Mastering Network Policies with Calico Whisker Ready to take control of your Kubernetes security? Kubernetes adoption is growing, and managing secure and efficient network communication is becoming increasingly complex. With this growth, organizations need to enforce network policies with greater precision and care. However, implementing these policies without disrupting operations can be challenging. That’s where Calico Whisker comes in. It helps teams implement network policies that follow the principle of least privilege, ensuring workloads communicate only as intended. Since most organizations introduce network policies after applications are already running, safe and incremental rollout is essential. To support this, Calico Whisker offers staged network policies , which allow teams to preview a policy’s effect in a live environment before enforcing it.</description></item><item><title>Don’t just attend KubeCon + CloudNativeCon, Merge Forward your experience!</title><link>https://kubermates.org/docs/2025-10-30-don-t-just-attend-kubecon-cloudnativecon-merge-forward-your-experience/</link><pubDate>Thu, 30 Oct 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-30-don-t-just-attend-kubecon-cloudnativecon-merge-forward-your-experience/</guid><description>What’s Merge Forward? Where to find us Let’s connect at the Community Hub! Meet us at the Merge Forward Kiosk in the Project Pavilion! Must-attend sessions to add to your calendar: Get a glimpse of the amazing energy at KubeCon + CloudNativeCon Get a head start on the KubeCon + CloudNativeCon fun! Posted on October 30, 2025 by Catherine Paganini, Merge Forward Co-Founder We are so excited that the Merge Forward team will be at KubeCon + CloudNativeCon Atlanta this year. If you part of an underrepresented group, an ally, or just love meeting people from all walks of life, then be sure to add these Merge Forward sessions to your calendar. It will be fun, diverse, and inclusive, with many opportunities to connect with peers and make new KubeCon + CloudNativeCon friends. Since there’s a lot going on, we’ve put together this Merge Forward Community Guide to KubeCon + CloudNativeCon so you don’t miss any of our great diversity and inclusion activities and sessions. Launched just two months ago, Merge Forward is all about fostering a more inclusive and welcoming cloud native community. It is the new home to new and old community groups such as the Deaf and Hard of Hearing WG, Women in Cloud Native, Neurodiversity, and many more. If you haven’t done so yet, learn more about our mission and how to join our growing community on the Merge Forward Community page. KubeCon + CloudNativeCon is more than just sessions. It’s a great opportunity to connect with like-minded peers and allies. Below you’ll find some opportunities to build your network, learn about accessibility, attend Merge Forward community member talks, and make new friends. The Community Hub is a dedicated space for people from diverse backgrounds and their allies. Swing by to meet your peers, expand your professional network, and learn from shared experiences.</description></item><item><title>Securing the software supply chain: How distroless containers defend against npm malware attacks</title><link>https://kubermates.org/docs/2025-10-30-securing-the-software-supply-chain-how-distroless-containers-defend-against-npm-/</link><pubDate>Thu, 30 Oct 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-30-securing-the-software-supply-chain-how-distroless-containers-defend-against-npm-/</guid><description>The wake-up call: npm ‘is’ package compromise Why traditional containers failed Distroless: Security through minimalism Taking distroless further: Secure, minimal containers for cloud native workloads Measurable impact Why it matters Bottom line Posted on October 30, 2025 by Dhanush VM, CleanStart In July 2025, the npm package “is” —downloaded millions of times each week—was quietly hijacked. A simple phishing email to its maintainer opened the door for attackers to inject malicious code into the software supply chain, embedding backdoors into thousands of downstream applications. This incident is more than a one-off breach; it’s a warning shot for the entire open source ecosystem. Even trusted dependencies can become vectors for sophisticated supply-chain attacks. As development pipelines grow increasingly automated, traditional defenses are no longer enough. To stay secure, organizations must rethink the foundation of their containers themselves. That’s where distroless containers come in—stripping away unnecessary components to eliminate entire classes of vulnerabilities before they can be exploited. Traditional containers are built like miniature operating systems. They include shells, package managers, network tools, and other system utilities — many of which are unnecessary for the application but are ideal targets for attackers. When the malicious ‘is’ package was executed in these environments, it had access to tools to download more payloads, connect to remote servers, and persist in the system. In essence, developers unintentionally shipped a hacker’s toolkit into production. Distroless containers flip this paradigm by including only the essentials required to run an application — nothing more.</description></item><item><title>Not your grandfather's VMs: Renewing backup for Red Hat OpenShift Virtualization</title><link>https://kubermates.org/docs/2025-10-30-not-your-grandfather-s-vms-renewing-backup-for-red-hat-openshift-virtualization/</link><pubDate>Thu, 30 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-30-not-your-grandfather-s-vms-renewing-backup-for-red-hat-openshift-virtualization/</guid><description>Not your grandfather&amp;rsquo;s VMs: Renewing backup for Red Hat OpenShift Virtualization Why old backup models break A solution for a modern platform Protect your investment, achieve resilience Further discovery 15 reasons to adopt Red Hat OpenShift Virtualization About the author Shane Heroux More like this Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Maybe you’re planning your migration right now, or you’ve done it (congratulations!) Joining industry leaders in the strategic move to Red Hat OpenShift Virtualization gives you the best of both worlds: The operational familiarity of a virtual machine (VM) combined with the agility and scalability of a Kubernetes-native platform. You&amp;rsquo;re running critical workloads in a modern, efficient way. But have you stopped to ask the crucial question: &amp;ldquo;How are we protecting them?&amp;rdquo; If your answer involves traditional, hypervisor-level backup tools, you might have a significant gap in your data protection strategy. The simple truth is that a VM running on OpenShift is fundamentally different from one on a legacy hypervisor, and it demands a modern approach to backup and recovery. In a traditional virtualization stack, a VM is a relatively self-contained unit. Backing it up usually meant taking an agent-based or hypervisor-level snapshot of a VMDK or VHDX file on a datastore. Simple enough. But in OpenShift Virtualization, a VM is much more than a virtual disk file. It&amp;rsquo;s a composite application defined by a collection of interdependent Kubernetes resources. This includes the VirtualMachineInstance (VMI) itself, its DataVolumes and PersistentVolumeClaims (PVCs), along with associated ConfigMaps , Secrets , and network services that allow it to function. VirtualMachineInstance DataVolumes PersistentVolumeClaims ConfigMaps Secrets Backing up only the persistent volume is like saving your car&amp;rsquo;s engine but throwing away the chassis, wheels, and control system. You have a critical component, but you can&amp;rsquo;t actually rebuild the car.</description></item><item><title>Build, Deploy, and Scale with Confidence: vSphere Kubernetes Service 3.5 is Now Live with 24-Month Support</title><link>https://kubermates.org/docs/2025-10-29-build-deploy-and-scale-with-confidence-vsphere-kubernetes-service-3-5-is-now-liv/</link><pubDate>Wed, 29 Oct 2025 16:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-29-build-deploy-and-scale-with-confidence-vsphere-kubernetes-service-3-5-is-now-liv/</guid><description>vSphere Kubernetes release(VKr) 1.34 Advanced Cluster Configuration for Kubernetes Components Cluster API v1beta2 for Enhanced Kubernetes Cluster Management Prevent Upgrade Failures Due to Misconfigured PodDisruptionBudget Streamlined Operations: CLI, Add-On Management, and Support Tools Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Build, Deploy, and Scale with Confidence: vSphere Kubernetes Service 3.5 is Now Live with 24-Month Support VCF Breakroom Chats Episode 71: Maximizing VCF Resource Prudency through Capacity Management Government-Ready NVIDIA AI Enterprise Containers Now Available for Customers of VMware Private AI Foundation with NVIDIA VMware vSphere Kubernetes Service (VKS) 3.5 is now generally available and this new release makes 24-month support available to each Kubernetes minor version, starting with VKr 1.34. We previously announced 24 months of support for vSphere Kubernetes release (VKr) 1.33 in June 2025. This change provides platform teams with greater stability and flexibility when planning upgrades. VKS 3.5 also brings several improvements to operational consistency and lifecycle management, including fine-grained configuration controls for core Kubernetes components, a declarative add-on management model, and integrated support-bundle generation directly from the VCF CLI. Additionally, new upgrade guardrails—such as PodDisruptionBudget validation and compatibility checks—help ensure safer, more predictable cluster upgrades. Together, these enhancements improve reliability, operational efficiency, and Day-2 management for Kubernetes environments at scale. VKS 3.5 enables support for VKr 1.34. Every vSphere Kubernetes minor version is now supported for 24 months from its release date. This reduces upgrade pressure, stabilizes environments, and frees up teams to focus on delivering value instead of constantly planning upgrades. Teams that want quick access to the latest Kubernetes features can stay current with new releases. Those that prefer to run on a stable version for longer can now do so with confidence. VKS 3.5 supports both operating models.</description></item><item><title>VCF Breakroom Chats Episode 71: Maximizing VCF Resource Prudency through Capacity Management</title><link>https://kubermates.org/docs/2025-10-29-vcf-breakroom-chats-episode-71-maximizing-vcf-resource-prudency-through-capacity/</link><pubDate>Wed, 29 Oct 2025 15:21:11 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-29-vcf-breakroom-chats-episode-71-maximizing-vcf-resource-prudency-through-capacity/</guid><description>Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Forrester Opportunity Snapshot: A Single Platform Approach to Managing Containers and VMs Build, Deploy, and Scale with Confidence: vSphere Kubernetes Service 3.5 is Now Live with 24-Month Support VCF Breakroom Chats Episode 71: Maximizing VCF Resource Prudency through Capacity Management Welcome to the next episode of the VCF Breakroom Chats. Today, we are happy to present this vLog with Sowmya Srinivasa, Technology Product Management, VCF Division, Broadcom. In this episode, Sowmya Srinivasa and Sachin Alex discuss the importance of Capacity Management for enterprises, how VMware Cloud Foundation (VCF) helps in assessing, planning and optimizing their VCF infrastructure and what the future holds for the capability. Want to learn more about Capacity Management in VCF Operations? Check out the VCF Operations web page for more resources. Read this blog post – Capacity Management: The IT Balancing Act You Can’t Ignore About the VCF Breakroom Chat Series This webinar series is the successor to the vSphere Breakroom Chat, with a renewed focus on VMware Cloud Foundation (VCF). ​ In this series, we share vlogs with industry-recognized experts from VMware and VMware partners &amp;amp; customers. These vlogs are concise, like meeting in a breakroom and having a quick conversation to get great information quickly. This series is for you if you are an IT practitioner- IT admin, cloud or platform architect, developer, DevOps, senior IT manager, IT executive, Artificial Intelligence, or a Machine Learning professional.</description></item><item><title>Helping Startups Build Faster with an AI Startup Ecosystem</title><link>https://kubermates.org/docs/2025-10-29-helping-startups-build-faster-with-an-ai-startup-ecosystem/</link><pubDate>Wed, 29 Oct 2025 13:59:30 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-29-helping-startups-build-faster-with-an-ai-startup-ecosystem/</guid><description>Helping Startups Build Faster with an AI Startup Ecosystem How ecosystem partners can help accelerate AI startup growth About the author Try DigitalOcean for free Related Articles Image and audio models from fal now available on DigitalOcean OAuth App Based Workload Identity for Droplets Is DigitalOcean Your Next Career Spot? A 5-Year Insider on Why It Should Be By Courtney Gras Director, Startup Ecosystem Published: October 29, 2025 3 min read At DigitalOcean, weâve always been committed to supporting cutting-edge startups by providing them with accessible cloud solutions tailored to the needs of fast growing businesses. Some of the AI startups already growing alongside DigitalOcean include Uxify, who is building an AI-enabled website performance solution using DigitalOcean Gradientâ¢ AI GPU Droplets and other products. They value how DigitalOceanâs straightforward products enable them to focus on building their business, saying âAs a startup, time to market is essential and the simplicity of DigitalOcean cut all the noise and removed a lot of bureaucracy compared to some of the other providers. â As AI has become an integral part of todayâs leading startups, weâre pleased to announce the evolution of our offerings for startups building with AI. Recognizing startups need more than just tools to build and launch amazing products, DigitalOceanâs new AI Startup Ecosystem brings together established technology partners, vibrant community organizations, AI developer platforms, and university entrepreneurship programs to help founders build, launch and scale faster on DigitalOceanâs comprehensive agentic cloud. The DigitalOcean AI Startup Ecosystem provides members with discounted infrastructure and tooling to help them build, while offering access to technical support and go-to-market resources to help them grow. Working with founding partners like Techstars, the AI Startup Ecosystem serves as a trusted partner to founders, removing barriers, reducing costs, and providing a scalable cloud platform that grows alongside its members. Startups, apply today through this link. As we continue to grow our AI startup offerings, weâre excited to be deepening our existing partnerships for greater impact. For example, working with Techstars weâll support founders throughout their entire startup journey, from supporting early-stage ideas with hackathons, to offering AI startup packages including access to GPU discounts, infrastructure credits and hands-on technical support. The partnership with Techstars highlights our dedication to supporting startups from idea to growth. âDigitalOcean understands the startup journey because theyâve lived it as one of our own alumni companies.</description></item><item><title>HPE Alletra Storage MP B10000 for Red Hat OpenShift</title><link>https://kubermates.org/docs/2025-10-29-hpe-alletra-storage-mp-b10000-for-red-hat-openshift/</link><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-29-hpe-alletra-storage-mp-b10000-for-red-hat-openshift/</guid><description>HPE Alletra Storage MP B10000 for Red Hat OpenShift A comprehensive CSI driver CSI specification and CSI extensions Advanced uses cases OpenShift Virtualization Convert VMware vVols to PersistentVolumes Campus stretch cluster disaster recovery Learn more Red Hat OpenShift Container Platform | Product Trial About the author Michael Mattsson (HPE) More like this Blog post Blog post Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share HPE Alletra Storage MP B10000 offers the industry’s only disaggregated, scale-out block and file storage with a 100% data availability guarantee. It delivers an AI-driven cloud management, efficient scale, and extreme resiliency and performance for many Enterprise workloads as a Leader in the 2025 Gartner® Magic Quadrant™ for Enterprise storage platforms. The B10000 is built on the Alletra Storage MP platform, which offers the full gamut of block, file and object tailored to specific data OpenShift workloads. HPE GreenLake for File Storage offers scale-out unstructured data with high throughput and capacity over NFS. HPE Alletra Storage MP X10000 provides high performance object storage tailored for modern AI workloads and data protection at scale. Red Hat OpenShift is the industry’s leading hybrid cloud application platform powered by Kubernetes. Using OpenShift simplifies and accelerates the development, delivery, and lifecycle management of a hybrid mix of applications, delivering more consistency across on-premise, public clouds, and at the edge. This blog highlights the HPE Alletra Storage MP B10000 and the capabilities enabled for Red Hat OpenShift using the HPE CSI Operator for OpenShift. HPE GreenLake for File Storage and HPE Alletra Storage MP X10000 are both platforms that also may be used directly with OpenShift using native protocols. Several efforts are underway to unlock integration points for OpenShift customers to take advantage of the entire portfolio in a more seamless manner through CSI and COSI drivers in the future. The HPE CSI driver for Kubernetes is a comprehensive CSI driver packaged in the Red Hat certified HPE CSI operator for OpenShift. It features all the relevant capabilities for private cloud OpenShift and OpenShift Virtualization, while providing several innovations outside the CSI spec to differentiate the B10000 from being just a storage solution using CSI extensions and customer resource definitions (CRD).</description></item><item><title>Median time from submission to publication: 2.5 weeks From banking success, to a new future in tech: Bradley’s Red Hat journey so far</title><link>https://kubermates.org/docs/2025-10-29-median-time-from-submission-to-publication-2-5-weeks-from-banking-success-to-a-n/</link><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-29-median-time-from-submission-to-publication-2-5-weeks-from-banking-success-to-a-n/</guid><description>Median time from submission to publication: 2.5 weeks From banking success, to a new future in tech: Bradley’s Red Hat journey so far Red Hat Learning Subscription | Product Trial About the authors Zoe Chu Bradley Bennett Holly Bailey More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Not every journey into technology is linear. Just ask Bradley, a non-traditional student at the University of North Carolina at Chapel Hill and a business analyst intern on the IT Operations Analytics team. Bradley started his career at Red Hat during the summer of 2024, but getting here has been an adventure in itself. We sat down with him to talk about unique career paths, transferable skills, and his experience with Red Hat’s emerging talent program. After graduating high school, Bradley pursued a career in banking instead of following the traditional college route. Over the next 5 years, he worked his way up from a banking associate to a senior role. Bradley was progressing well in his career, but felt like something was missing. “Everything was going pretty well for me,” says Bradley, “to the point that I had the freedom to make a different decision in life, to go do what I always wanted to do. And that was to work in technology in some way or another. ” Bradley began this transition by attending community college at night. “Weirdly enough, the astronomy course I took in community college was the reason I wanted to pursue a higher-level education at UNC,” he reflects. His astronomy professor was passionate about the subject and connected it to students’ daily lives.</description></item><item><title>The State of Missouri Scales Automation, Unlocks Time for Innovation</title><link>https://kubermates.org/docs/2025-10-29-the-state-of-missouri-scales-automation-unlocks-time-for-innovation/</link><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-29-the-state-of-missouri-scales-automation-unlocks-time-for-innovation/</guid><description>The State of Missouri Scales Automation, Unlocks Time for Innovation More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The State of Missouri serves millions of people digitally every year, offering a range of services from motor vehicle registrations and various state licenses to health and social benefits and resident support. These citizen services require significant operational rigor, driven by an IT organization focused on delivering technology with speed, consistency, and efficiency. Before adopting Red Hat Ansible Automation Platform, the State of Missouri’s IT landscape was a mix of disparate IT automation products, leading to a sprawling and disconnected environment and a number of resulting pain points. For the engineering team, it meant dealing with a variety of tools, a lack of standardization, and an inability to easily share and collaborate on work across the organization. Across departments, critical tasks could take hours or even days to complete manually, increasing the risk of human error and delaying the delivery of services. Recognizing that this approach wasn&amp;rsquo;t sustainable, the state adopted Ansible Automation Platform to centralize automation efforts with a dedicated team to support it. As the team began rolling out this platform, certain requirements had to be met. First, the team tackled a widespread concern of how to integrate a sensitive password and certificate management solutions into the new automation. Instead of creating a new process, they built a custom Ansible Content Collection that connects to their existing secrets repository that includes modules and roles to more securely call sensitive data. They approved and stored this new custom collection in their private automation hub instance – allowing them to organize and control recommended automation assets like this for use across the IT organization. With testing complete, they were ready to use it. This readily available custom Ansible Content Collection made a huge impact in their organization, providing other teams with a sense of comfort and overcoming their hesitancy to adopt automation.</description></item><item><title>Remediator Agent for Kubernetes – AI-Powered Policy Remediation</title><link>https://kubermates.org/docs/2025-10-28-remediator-agent-for-kubernetes-ai-powered-policy-remediation/</link><pubDate>Tue, 28 Oct 2025 22:08:46 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-28-remediator-agent-for-kubernetes-ai-powered-policy-remediation/</guid><description>Remediator Agent for Kubernetes – AI-Powered Policy Remediation What Is the Remediator Agent? Why We Built Remediator How The Remediator Agent Works Flexible Deployment Options What Makes the Remediator Agent Different Try Remediator Agent Today Kubernetes gives teams incredible power and flexibility—but it’s also noisy. Every day, platform and security teams encounter a flood of policy violations, including missing resource limits, insecure container settings, deprecated APIs, and more. Fixing them typically means opening a ticket, chasing down the right developer, or manually editing manifests—wasting time and increasing mean time to remediate (MTTR). What if instead of endless firefighting, your system could spot a violation and propose the fix—automatically ? That’s precisely what the Remediator Agent does. The Remediator Agent is an AI-powered remediation engine for Kubernetes that integrates directly with Kyverno and Nirmata Control Hub (NCH). It bridges the gap between policy detection and automated action , helping teams reduce manual work while maintaining their clusters’ compliance and security. With Kyverno and NCH, you already get powerful policy-as-code and cluster-wide visibility into policy violations. But visibility alone doesn’t solve the problem; it just exposes it. Teams kept asking: “Can you just fix this for me?” “Can you open the PR so I don’t have to?” “Can we run this across multiple clusters without breaking GitOps?” The Remediator Agent answers these with AI-assisted remediation that works hand-in-hand with your existing GitOps workflows – and most importantly, helps teams dramatically reduce MTTR. Think of Remediator as the missing link between detection and action: Watch: The agent continuously detects policy violations. Understand: AI interprets the violation, the resource, and the policy intent. Propose: It generates a clean diff, the actual lines to change, and explains why.</description></item><item><title>Government-Ready NVIDIA AI Enterprise Containers Now Available for Customers of VMware Private AI Foundation with NVIDIA</title><link>https://kubermates.org/docs/2025-10-28-government-ready-nvidia-ai-enterprise-containers-now-available-for-customers-of-/</link><pubDate>Tue, 28 Oct 2025 18:31:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-28-government-ready-nvidia-ai-enterprise-containers-now-available-for-customers-of-/</guid><description>Why This Matters What It Enables STIG &amp;amp; FIPS Capabilities in VCF 9.0 STIG Compliance FIPS 140 Support Ready to get started on your AI and ML journey? Check out these helpful resources: Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Build, Deploy, and Scale with Confidence: vSphere Kubernetes Service 3.5 is Now Live with 24-Month Support VCF Breakroom Chats Episode 71: Maximizing VCF Resource Prudency through Capacity Management Government-Ready NVIDIA AI Enterprise Containers Now Available for Customers of VMware Private AI Foundation with NVIDIA We’re excited to announce a major milestone in our mission to bring secure, enterprise-grade AI to regulated industries. Starting today, VMware Private AI Foundation with NVIDIA is part of the NVIDIA AI Factory for Government reference design. This reference design is based on NVIDIA accelerated compute and NVIDIA AI Enterprise , and now includes government-ready software that meets security requirements for use within high assurance deployments. With this new reference design, VMware Private AI Foundation with NVIDIA customers can deploy GenAI workloads more securely to meet the requirements of regulated industries. Generative AI is reshaping industries; however, government agencies, defense, healthcare and financial institutions face unique challenges: Sensitive data can be locked in secure enclaves governed by strict compliance frameworks such as FedRAMP, FISMA, and NIST 800-53. Deploying Generative AI (GenAI) agents (e. g. , research assistants, chatbots) in these environments requires bespoke engineering, manual hardening, and non-repeatable processes. These barriers prevent regulated users from realizing the full benefits of GenAI and agentic AI. To support AI deployments for the public sector and other regulated industries, the NVIDIA AI Factory for Government reference design which includes the latest NVIDIA AI Enterprise release, provides guidance to deploy AI workloads. It is: Built on STIG hardened configurations and requirements for containers Uses FIPS validated cryptographic modules and related capabilities Is subject to ongoing monitoring and remediation of vulnerabilities including CISA KEVs By leveraging this secure, performant reference design through VMware Private AI Foundation with NVIDIA, customers can confidently harness the power of AI to achieve their goals faster and more securely. VMware Private AI Foundation with NVIDIA , a joint AI platform between Broadcom and NVIDIA, enables enterprises to run RAG workflows, fine-tune and customize LLM models, and run inference workloads in their data centers, addressing privacy, choice, cost, performance and compliance concerns.</description></item><item><title>Extending GPU Fractionalization and Orchestration to the edge with NVIDIA Run:ai and Amazon EKS</title><link>https://kubermates.org/docs/2025-10-28-extending-gpu-fractionalization-and-orchestration-to-the-edge-with-nvidia-run-ai/</link><pubDate>Tue, 28 Oct 2025 18:05:27 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-28-extending-gpu-fractionalization-and-orchestration-to-the-edge-with-nvidia-run-ai/</guid><description>Extending GPU Fractionalization and Orchestration to the edge with NVIDIA Run:ai and Amazon EKS Extensive and performant global cloud infrastructure Challenges in bringing AI workloads to the edge Training Inference Run:ai support for AWS Hybrid and Edge services Best practices for Run:ai at the edge Conclusion About the authors As organizations of all sizes have rapidly embraced the opportunity to pair foundation models (FMs) with AI agents to streamline complex workflows and processes, the demand for artificial intelligence and machine learning capabilities across distributed locations has never been stronger. For example, some organizations need to run custom, in-house language models within a specific geographic boundary to meet data residency requirements, while others require processing data locally to serve latency-sensitive edge inference requests. These distributed processing needs often require running AI/ML workloads in local metro Points of Presence (PoPs), customer premises, and beyond – especially when an AWS Region is not close enough to meet performance or compliance requirements. Managing these distributed workloads introduces another challenge: the need for efficient and topology-aware GPU resource management becomes critical, particularly at the distributed edge, where capacity is often limited and requires optimal allocation. Building on these emerging needs for distributed AI/ML capabilities and efficient GPU resource management, Amazon Web Services (AWS) and NVIDIA have been working together to explore solutions native to the environments that customers most frequently use for model training and inference, such as Amazon Elastic Kubernetes Service. In a previous blog post , we showcased how NVIDIA Run:ai addresses key challenges in GPU resource management, including static allocation limitations, resource competition, and inefficient sharing in GPU clusters. The blog post detailed the implementation of NVIDIA Run:ai on Amazon EKS, which featured dynamic GPU fractions, node-level scheduling, and priority-based sharing. Since then, we have released NVIDIA Run:ai in AWS Marketplace , allowing customers to deploy the Run:ai control plane to their Amazon EKS clusters without having to manage the deployment of individual Helm Charts. Building on this collaboration, today we are extending this flexibility to the entire AWS cloud continuum, enabling you to optimize GPU resources wherever your workloads need to run – in an AWS Region , on-premises , or at the edge. That is why we are excited to announce native Run:ai support for Amazon EKS in AWS Local Zones (including Dedicated Local Zones), Amazon EKS on AWS Outposts , and Amazon EKS Hybrid Nodes. As part of this launch, you can now extend Run:ai environments to support a cluster of GPUs separated by hundreds (if not thousands) of miles across these AWS Hybrid and Edge services. This architectural pattern enables you to create powerful high availability and disaster recovery strategies while maximizing cost efficiency and complying with local data residency requirements.</description></item><item><title>5 Essential Steps to Strengthen Kubernetes Egress Security</title><link>https://kubermates.org/docs/2025-10-28-5-essential-steps-to-strengthen-kubernetes-egress-security/</link><pubDate>Tue, 28 Oct 2025 16:51:33 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-28-5-essential-steps-to-strengthen-kubernetes-egress-security/</guid><description>Your Kubernetes Egress Security Checklist Step 1: Establish a Strong Default Security Baseline Step 2: Build Scalable, Precise Policies Step 3: Manage Outbound IPs with Egress Gateways Step 4: Govern, Validate, and Monitor Policies Step 5: Take the Next Step Securing what comes into your Kubernetes cluster often gets top billing. But what leaves your cluster, outbound or egress traffic, can be just as risky. A single compromised pod can exfiltrate data, connect to malicious servers, or propagate threats across your network. Without proper egress controls, workloads can reach untrusted destinations, creating serious security and compliance risks. This guide breaks down five practical steps to strengthen Kubernetes egress security, helping teams protect data, enforce policies, and maintain visibility across clusters. By default, Kubernetes allows unrestricted outbound communication, meaning any pod can reach any external destination and dramatically increase the attack surface. Implementing egress controls ensures pods can communicate only with explicitly trusted services, containing the impact of a compromised workload and preventing unauthorized data exfiltration or lateral movement. Granular egress controls are also essential for meeting security and compliance mandates, providing authorization, logging, and monitoring for all external connections. To help teams tackle this challenge, we’ve put together a Kubernetes Egress Security Checklist , based on best practices from real-world environments. Whether you’re just beginning to define your egress policies or looking to strengthen your existing posture, these 5 steps will help you reduce risk and improve visibility. [ ] Implement Global Default-Deny: Establish a global default-deny policy for all Ingress and egress traffic as a first-order security and compliance requirement. Ensure it explicitly excludes critical system pods and allows necessary DNS traffic.</description></item><item><title>Analyst Insight Series: Consolidated, Consistent and Simplified Operations Deliver on The Promise of Platform Engineering</title><link>https://kubermates.org/docs/2025-10-28-analyst-insight-series-consolidated-consistent-and-simplified-operations-deliver/</link><pubDate>Tue, 28 Oct 2025 16:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-28-analyst-insight-series-consolidated-consistent-and-simplified-operations-deliver/</guid><description>Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Build, Deploy, and Scale with Confidence: vSphere Kubernetes Service 3.5 is Now Live with 24-Month Support VCF Breakroom Chats Episode 71: Maximizing VCF Resource Prudency through Capacity Management Government-Ready NVIDIA AI Enterprise Containers Now Available for Customers of VMware Private AI Foundation with NVIDIA Guest post by Jay Lyman , Senior Research Analyst, S&amp;amp;P Global Market Intelligence 451 Research This blog is the second in our series, “Effective Platform Engineering: Priorities, Pitfalls and Success in Taming Complexity and Managing Today’s Enterprise Infrastructure and Applications. ”(read the first blog here ) and a companion to the analyst report, “ Cost, Productivity, Security Shape Today’s Platform Engineering ” To implement effective platform engineering, enterprise teams must address significant challenges related to complexity and integration across infrastructures, applications, tools and teams. Key to overcoming these challenges are consolidated, consistent and simplified operations that enhance developer productivity, increase cost efficiency, and improve quality and security. Tackling integration and complexity IT teams commonly cite integration and complexity as hurdles to effective platform engineering and DevOps, as highlighted in our Voice of the Enterprise survey (see figure below). Challenges in adopting platform engineering Q. What challenges, if any, do you expect (or have you encountered) in adopting platform engineering? (Select top 3). Base: Respondents whose organizations have platform engineering initiatives. Source: 451 Research’s Voice of the Enterprise: Cloud Native, Platform Engineering 2024. Simplified operations focused on unified lifecycle management can help reduce the complexity of modern enterprise infrastructure and applications, including rapidly evolving tools and platforms such as Kubernetes. While Kubernetes promises effective management of applications and infrastructure at massive scale, it can also introduce complexity due to its various open-source components for orchestration, monitoring, observability and security. These components require centralized and consistent support to enhance operational efficiency, collaboration, governance and control. The platform approach Enterprises can simplify operations by consolidating and ensuring consistency across compute, storage, networking and security resources through a unified platform that can integrate with other tools and platforms across software development and deployment processes.</description></item><item><title>Application Versus Infrastructure-Level High Availability with vSAN in VMware Cloud Foundation</title><link>https://kubermates.org/docs/2025-10-28-application-versus-infrastructure-level-high-availability-with-vsan-in-vmware-cl/</link><pubDate>Tue, 28 Oct 2025 16:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-28-application-versus-infrastructure-level-high-availability-with-vsan-in-vmware-cl/</guid><description>Accommodating for Failure Availability and Recovery of Applications and Data The Evolution of Options in High Availability Application-Level High Availability for Apps and Data Infrastructure-Level High Availability for Apps and Data Different Approaches to Achieve Data Consistency Differences in Recovery Time Objectives (RTO) Which One is Right for You? Summary Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Government-Ready NVIDIA AI Enterprise Containers Now Available for Customers of VMware Private AI Foundation with NVIDIA Application Versus Infrastructure-Level High Availability with vSAN in VMware Cloud Foundation The FinOps Journey: From Visibility to Business Value Maintaining availability of data and the applications that produce or consume that data might be the most important responsibility of data center administrators. Capabilities like high performance or special data services mean very little if the applications and the data they produce or consume is not readily available. Ensuring availability is a complex topic, as application availability and data availability use different techniques to achieve the desired result. Sometimes availability requirements are achieved using infrastructure-level mechanisms, while others may use application-centric solutions. What is best for your environment depends heavily on the requirements and capabilities of your infrastructure. While VMware Cloud Foundation (VCF) can deliver high levels of data and application availability in a simple way , this post will look at the differences in providing high availability of applications and data using application-level technologies versus inherent infrastructure-level technologies in VCF. We will also look at how VMware Data Services Manager (DSM) can play a part in simplifying some of these decisions. Protecting applications and data require an understanding of what typical failures look like, and what a system can do to accommodate for failure. For example, failures in a physical infrastructure may include: Centralized storage solutions like storage arrays Discrete storage devices in distributed solutions Hosts Network interface cards (NICs) Network switching fabrics, causing partitions Site/zone-level failures Failures like these noted above could impact the data, the applications, or both. Failures come in a variety of ways, with some explicitly identified, while others only through absence. Some failures are temporary, while others permanent. Solutions must be sophisticated enough to automatically handle these failure and recovery scenarios.</description></item><item><title>Red Hat AI: Modular building blocks for scalable, repeatable model customization</title><link>https://kubermates.org/docs/2025-10-28-red-hat-ai-modular-building-blocks-for-scalable-repeatable-model-customization/</link><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-28-red-hat-ai-modular-building-blocks-for-scalable-repeatable-model-customization/</guid><description>Red Hat AI: Modular building blocks for scalable, repeatable model customization Docling: Document intelligence SDG hub: Collection of high quality synthetic data pipelines Conclusion The adaptable enterprise: Why AI readiness is disruption readiness About the authors Aditi Saluja Jehlum Vitasta Pandit Ana Biazetti More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Taking generative AI (gen AI) from experimentation to enterprise deployment is never one-size-fits-all. Healthcare, finance, manufacturing, and retail each have their own vocabularies, data quirks, and compliance challenges. These complexities can’t be addressed with generic workflows because real, business-critical deployments demand depth, control, and precision. Red Hat AI offers a model customization experience that builds on the success of InstructLab , evolving it into a modular architecture powered by Python libraries created by Red Hat. This approach preserves InstructLab’s core strengths—its open, extensible pipeline for fine-tuning and instruction-following—while enabling greater flexibility and scalability for enterprise environments. With this foundation, AI experts and ML practitioners can adapt methods, orchestrate pipelines, and scale model training without losing the agility to evolve as techniques improve. The result is a more sophisticated path to enterprise-grade model customization—one that delivers adaptability and control without sacrificing speed. At the heart of this evolution are 3 core components that enable experts to fine-tune models and integrate them effectively with retrieval-augmented generation (RAG). Docling for data processing SDG hub for synthetic data generation (SDG) Training hub for fine-tuning and continual learning Since these components are modular, you can use them independently or connect them end-to-end. We will also provide more supported notebooks and AI/data science pipelines that teach you the technology as well as how to adapt it to your use case, your data, your model, and run it reliably by combining data processing, synthetic data generation, and fine-tuning techniques. Docling is our supported solution for data processing. It’s the number one open source repository for document intelligence.</description></item><item><title>Red Hat to distribute NVIDIA CUDA across Red Hat AI, RHEL and OpenShift</title><link>https://kubermates.org/docs/2025-10-28-red-hat-to-distribute-nvidia-cuda-across-red-hat-ai-rhel-and-openshift/</link><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-28-red-hat-to-distribute-nvidia-cuda-across-red-hat-ai-rhel-and-openshift/</guid><description>Red Hat to distribute NVIDIA CUDA across Red Hat AI, RHEL and OpenShift Why this matters: Simplicity and consistency Our open source approach to AI About the author Ryan King More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share For decades, Red Hat has been focused on providing the foundation for enterprise technology — a flexible, more consistent, and open platform. Today, as AI moves from a science experiment to a core business driver, that mission is more critical than ever. The challenge isn&amp;rsquo;t just about building AI models and AI-enabled applications; it’s about making sure the underlying infrastructure is ready to support them at scale, from the datacenter to the edge. This is why I&amp;rsquo;m so enthusiastic about the collaboration between Red Hat and NVIDIA. We&amp;rsquo;ve long worked together to bring our technologies to the open hybrid cloud, and our new agreement to distribute the NVIDIA CUDA Toolkit across the Red Hat portfolio is a testament to that collaboration. This isn&amp;rsquo;t just another collaboration; it&amp;rsquo;s about making it simpler for you to innovate with AI, no matter where you are on your journey. Today, one of the most significant barriers to AI adoption isn&amp;rsquo;t a lack of models or compute power, but rather the operational complexity of getting it all to work together. Engineers and data scientists shouldn&amp;rsquo;t have to spend their time managing dependencies, hunting for compatible drivers, or figuring out how to get their workloads running reliably on different systems. Our new agreement with NVIDIA addresses this head-on. By distributing the NVIDIA CUDA Toolkit directly within our platforms, we&amp;rsquo;re removing a major point of friction for developers and IT teams. You will be able to get the essential tools for GPU-accelerated computing from a single, trusted source. This means: A streamlined developer experience.</description></item><item><title>Take a fail-fast approach for developing RHEL upgrade automation</title><link>https://kubermates.org/docs/2025-10-28-take-a-fail-fast-approach-for-developing-rhel-upgrade-automation/</link><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-28-take-a-fail-fast-approach-for-developing-rhel-upgrade-automation/</guid><description>Take a fail-fast approach for developing RHEL upgrade automation The challenge The solution 1. Automate Everything 2. Snapshot with rollback 3. Custom Modules 4. Reporting dashboard (optional, but VERY useful) Lessons learned from automating a million RHEL in-place upgrades Learn more about RHEL upgrades We&amp;rsquo;re here to help Red Hat Ansible Automation Platform | Product Trial About the authors Bob Mader Bob Handlin More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share It&amp;rsquo;s been just over two years since we wrote about automating in-place upgrades for Red Hat Enterprise Linux (RHEL). During that time, we&amp;rsquo;ve seen dozens of customers upgrade hundreds of thousands of systems using our prescriptive, automated approach to make RHEL upgrades happen at scale. In this article, we’ll do a quick review of the key features that help accelerate the roll out of RHEL upgrade automation. We’ll look at what’s worked well, but also at some of the challenges and lessons learned. The key learning: Fail fast, iterate, and try again. The most important thing this accomplishes is making the upgrade process less scary, allowing quick recovery to the original state when things don&amp;rsquo;t go perfectly right away. Many of our biggest customers have large RHEL environments that have grown and evolved through the decades since enterprise adoption of Linux took off in the early 2000s. Organizations have tried to virtualize and containerize with the best intentions of modernizing how they deploy and manage application workloads, but some still have vast numbers of RHEL hosts that haven&amp;rsquo;t caught up.</description></item><item><title>The power of confidential containers on Red Hat OpenShift with NVIDIA GPUs</title><link>https://kubermates.org/docs/2025-10-28-the-power-of-confidential-containers-on-red-hat-openshift-with-nvidia-gpus/</link><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-28-the-power-of-confidential-containers-on-red-hat-openshift-with-nvidia-gpus/</guid><description>The power of confidential containers on Red Hat OpenShift with NVIDIA GPUs Confidential containers on bare metal Unlocking enterprise AI with NVIDIA&amp;rsquo;s Blackwell GPUs and confidential containers Confidential containers: Securing the cloud-native AI stack End-to-end attestation: Building a chain of trust Red Hat AI Inference Server Demo: AI inferencing with CoCo bare metal and confidential GPUs Final thoughts The adaptable enterprise: Why AI readiness is disruption readiness About the authors Ariel Adam Pradipta Banerjee Jens Freimann Emanuele Giuseppe Esposito More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Artificial intelligence (AI) is rapidly moving from a theoretical concept to a central engine of enterprise value, transforming sectors from healthcare to finance. The capacity of AI to analyze, predict, and automate makes it an indispensable asset for modern innovation. Yet, this widespread adoption introduces a significant security imperative: as AI workloads scale, so does the risk of unauthorized access to proprietary AI models and the sensitive data they handle. These AI models are significant assets for organizations, representing substantial investments in research, training, and inferencing. Protecting them requires a robust security strategy that goes beyond traditional measures for data at rest (in storage) and in transit (over networks). The most vulnerable state is data in use, the moment it&amp;rsquo;s actively being processed in memory. This is where confidential computing emerges as a game-changer. By providing a trusted execution environment (TEE), it strengthens application security through isolation, encryption, and attestation, protecting data and code as they are being used. This approach enables a comprehensive defense-in-depth strategy, which is crucial for regulated industries handling sensitive information. By integrating these security features with a scalable, high-performance AI and machine learning (ML) ecosystem, companies can leverage AI with confidence, without sacrificing security for speed. In this blog, we will explore the usage of Red Hat OpenShift confidential containers along with NVIDIA confidential GPUs to protect AI workloads, focusing on Red Hat AI Inference Server as the workload. Confidential containers (CoCo) are a cloud-native implementation of confidential computing that brings the security of a TEE to a standard containerized workload (such as Intel TDX or AMD SEV-SNP).</description></item><item><title>Pros and Cons of Crowdfunding Your Startup | DigitalOcean (migrated)</title><link>https://kubermates.org/docs/2025-10-27-pros-and-cons-of-crowdfunding-your-startup-digitalocean-migrated/</link><pubDate>Mon, 27 Oct 2025 20:37:59 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-27-pros-and-cons-of-crowdfunding-your-startup-digitalocean-migrated/</guid><description>Redirecting. Click here if you are not redirected.</description></item><item><title>The Policy-as-Code AI Agent: Smarter Kubernetes Governance &amp; Security</title><link>https://kubermates.org/docs/2025-10-27-the-policy-as-code-ai-agent-smarter-kubernetes-governance-security/</link><pubDate>Mon, 27 Oct 2025 16:17:44 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-27-the-policy-as-code-ai-agent-smarter-kubernetes-governance-security/</guid><description>The Policy-as-Code AI Agent: Smarter Kubernetes Governance &amp;amp; Security What is the Policy-as-Code Agent? Why the Policy-as-Code Agent Matters Your Personal Policy Assistant Key Outcomes Why Now Try the Policy-as-Code Agent Today Policies are the backbone of Kubernetes governance. They enforce security, compliance, and operational best practices. However, for most teams, policy authoring feels like a source of friction: endless YAML, fragmented documentation, and excessive trial-and-error. The Policy-as-Code (PaC) Agent changes that. Instead of wrestling with syntax, you describe your intent in plain English. The agent generates the right Kyverno policies, scaffolds matching test cases, and even prepares Git-ready outputs. It’s not about writing YAML faster – it’s about making governance effortless, scalable, and developer-friendly. The PaC Agent is an AI-powered assistant for authoring and governing Kubernetes policies. It helps teams translate natural language intent into validated, testable Kyverno policies , bridging the gap between developer velocity and security rigor. With the PaC Agent, anyone — from developers to platform engineers — can create, test, and package policies with confidence. Policies aren’t just a compliance exercise; they directly shape the developer experience. When governance is slow or inconsistent, it creates bottlenecks in the system.</description></item><item><title>Announcing the Certified Meshery Contributor (CMC)</title><link>https://kubermates.org/docs/2025-10-27-announcing-the-certified-meshery-contributor-cmc/</link><pubDate>Mon, 27 Oct 2025 13:02:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-27-announcing-the-certified-meshery-contributor-cmc/</guid><description>About the Certification Introducing the Meshery Certification Program Certification Tracks, Tiers and Exam Structure Developers Track Certification Exams An Addition to Meshery’s Existing Recognition Program Posted on October 27, 2025 by CNCF Ambassador, Lee Calcote A CNCF-first of its kind Open source projects thrive or die based in large part on their community of contributors. It behooves maintainers to make opportunities for recognition and support of their contributors abundant. As a tool to allow contributors to showcase and validate their expertise, Meshery maintainers are thrilled to announce the launch of the Certified Meshery Contributor (CMC) certification — the project’s first certification and a first of its kind for the CNCF — crafted to acknowledge and authenticate the proficiency of developers actively shaping the Meshery ecosystem. Meshery, recognized as the CNCF’s sixth highest-velocity project, offers a thoughtfully designed contributor onboarding experience. This includes weekly newcomer meetings, self-paced training, and now a new certification program to validate contributors’ expertise. The Certified Meshery Contributor (CMC) certification validates technical proficiency in contributing to the Meshery open source project through written assessments. The certification consists of five distinct exams, each dedicated to one of Meshery’s major architectural domains : Meshery Server, Meshery CLI, Meshery UI, Meshery Models, and Meshery Extensibility. Tailored for developers possessing intermediate skills in Go, React, and OpenAPI schemas, and with hands-on experience in Meshery’s codebase, this certification affirms an individual’s capability to make substantial contributions to project development. This includes adept code implementation, thorough testing, seamless integration, and strategic optimization. Track: Developers Target Audience : Developers, technical writers, and community members. Prerequisites: Proficiency in Git and GitHub, familiar with Meshery’s architecture, and healthy understanding of open source collaboration. Key Focus: Ability to competently participate as an open source contributor across the core collection of Meshery’s architectural domains, demonstrating understanding of code (Go, JavaScript), documentation, and CI-based project enhancements.</description></item><item><title>IDC Spotlight: Overcoming Kubernetes Complexity with Modern Private Cloud</title><link>https://kubermates.org/docs/2025-10-27-idc-spotlight-overcoming-kubernetes-complexity-with-modern-private-cloud/</link><pubDate>Mon, 27 Oct 2025 13:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-27-idc-spotlight-overcoming-kubernetes-complexity-with-modern-private-cloud/</guid><description>Take the Next Step Discover more from VMware Cloud Foundation (VCF) Blog Related Articles IDC Spotlight: Overcoming Kubernetes Complexity with Modern Private Cloud Introducing the Private AI Sizing Guide VCF Breakroom Chats Episode 68: Counting down to KubeCon North America 2025 – Let’s Chat! Platform engineers, are you grappling with the inherent complexities of Kubernetes at scale? The promise of modern software delivery often collides with the realities of intricate configurations, distributed operations, and the sheer volume of new applications. If you’re seeking a more streamlined, secure, and efficient way to manage your Kubernetes environments, then the IDC Spotlight paper, “ Enabling Platform Engineers to Overcome Kubernetes Complexity with a Modern Private Cloud Platform ,” is a must-read. This Broadcom-sponsored IDC paper dives deep into the challenges and opportunities facing platform engineers today. It offers a third-party perspective on the critical need for modern platform engineering approaches that empower developers with self-service capabilities, while ensuring robust governance, cost control, and security. The paper explores key trends, including: The tidal wave of new applications – With IDC projecting over 1 billion net-new applications by 2028, the demand for efficient container orchestration is skyrocketing. The advent of Kubernetes – Kubernetes has become crucial for modern software delivery, including new AI application delivery, but its inherent complexity limits scalability, especially when managing distributed operations and ensuring security and governance. The rise of platform engineering – This discipline is crucial for removing developer toil but requires better tools and a more robust platform to manage the rising complexity of Kubernetes at scale. The shift to private clouds – A growing number of organizations are repatriating workloads from public clouds to on-premises private clouds, driven by performance and security needs, especially for AI-powered applications. A key takeaway from the IDC paper is the emergence of modern private cloud platforms. These platforms are designed to assist platform engineering teams in effectively managing Kubernetes at scale across their full spectrum of enterprise applications. It details how solutions like VMware Cloud Foundation (VCF) provide an integrated stack of capabilities designed to automate deployment, configuration, and life-cycle management in a pre-validated, pre-integrated private cloud platform. For platform engineers, VCF offers : Operational simplicity &amp;amp; scalability – Enterprise-grade scaling for production Kubernetes workloads and simplified provisioning environments.</description></item><item><title>Unlocking telecommunication transformation with Red Hat OpenShift Service on AWS</title><link>https://kubermates.org/docs/2025-10-27-unlocking-telecommunication-transformation-with-red-hat-openshift-service-on-aws/</link><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-27-unlocking-telecommunication-transformation-with-red-hat-openshift-service-on-aws/</guid><description>Unlocking telecommunication transformation with Red Hat OpenShift Service on AWS Current landscape of public cloud adoption for service providers The strategic imperative of Red Hat OpenShift Service on AWS How an APJ service provider cut estimated TCO by 26% and deployed in just 2 days Key use cases for service providers on Red Hat OpenShift Service on AWS Conclusion The adaptable enterprise: Why AI readiness is disruption readiness About the authors Vishal Jagtap Subrat Gupta Manu Joy More like this Blog post Blog post Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share For the telecommunications (telco) industry, the time to modernize is now. Telco service providers face intense pressure to deliver new digital services faster, while improving agility and cutting cost. By modernizing with advanced network systems and automation, telcos can remain competitive while increasing revenue, optimizing costs, and enhancing security and resilience. However, every telco service provider’s cloud journey is unique, with workload requirements like latency, throughput, and data locality dictating whether an application should run on premise or in the cloud. The collaboration between Red Hat, AWS, and Nokia creates a powerful solution to support these unique requirements. Together, they provide a clear framework to help service providers decide which workloads belong in the public cloud and which should remain on premise. Red Hat OpenShift Service on AWS delivers a managed application platform, tailored for the diverse and demanding telco industry, making it easier to shift applications to the cloud and support continuous innovation. Additionally, it provides the flexibility and consistency telcos need to manage workloads across diverse hybrid environments, significantly reducing their total cost of ownership (TCO) and accelerating innovation. This blog explores the primary motivators for cloud adoption, the benefits of Red Hat OpenShift Service on AWS, and a real-world success story that demonstrates its impact on achieving a clear path for infrastructure modernization and strategic goals, including improved cost savings, agility, and resilience. Service providers are using public cloud solutions to meet critical goals, including: Cost optimization: Shifting from capital-heavy hardware to an operational expenditure model cuts infrastructure and maintenance costs. Scalability and elasticity: Dynamic resource scaling handles demand spikes. Agility and speed: Quick deployment of services accelerates innovation and market responsiveness.</description></item><item><title>Connecting distributed Kubernetes with Cilium and SD-WAN: Building an intelligent network fabric</title><link>https://kubermates.org/docs/2025-10-25-connecting-distributed-kubernetes-with-cilium-and-sd-wan-building-an-intelligent/</link><pubDate>Sat, 25 Oct 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-25-connecting-distributed-kubernetes-with-cilium-and-sd-wan-building-an-intelligent/</guid><description>The challenge of distributed Kubernetes networking Towards an intelligent network fabric Bridging Kubernetes and SD-WAN with Cilium The role of a Kubernetes operator Enforcing traffic policies with Cilium and Cisco Catalyst SD-WAN End-to-end policy enforcement example Future directions: Observability and SLO awareness Conclusion Learn more Posted on October 25, 2025 by Gábor Rétvári and Tamás Lévai, Cisco CNCF projects highlighted in this post Learn how Kubernetes-native traffic management and SD-WAN integration can deliver consistent security, observability, and performance across distributed clusters. Modern businesses are rapidly adopting distributed architectures to meet growing demands for performance, resilience, and global reach. This shift is driven by emerging workloads that demand distributed infrastructure: AI/ML model training distributed across GPU clusters, real-time edge analytics processing IoT data streams, and global enterprise operations that require seamless connectivity across on-premises workloads, data centers, cloud providers, and edge locations. Today businesses are increasingly struggling to ensure secure, reliable and high-performance global connectivity while maintaining visibility across this distributed infrastructure. How do you maintain consistent end-to-end policies when applications traverse multiple network boundaries? How do you optimize performance for latency-sensitive critical applications when they could be running anywhere? And how do you gain clear visibility into application communication across this complex, multi-cluster, multi-cloud landscape? This is where a modern, integrated approach to networking becomes essential, one that understands both the intricacies of Kubernetes and the demands of wide-area connectivity. Let’s explore a proposal for seamlessly bridging your Kubernetes clusters, regardless of location, while intelligently managing the underlying network paths. Such an integrated approach solves several critical business needs: Unified security posture : Consistent policy enforcement from the wide-area network down to individual microservices. Optimized performance : Intelligent traffic routing that adapts to real-time conditions and application requirements. Global visibility : End-to-end observability across all layers of the network stack. In this post we discuss how to interconnect Cilium with a Software-Defined Wide Area Network (SD-WAN) fabric to extend Kubernetes-native traffic management and security policies into the underlying network interconnect. Learn how such integration simplifies operations while delivering the performance and security modern distributed workloads demand. Imagine a globally distributed service deployed across dozens of locations worldwide.</description></item><item><title>What’s New in Calico v3.31: eBPF, NFTables, and More</title><link>https://kubermates.org/docs/2025-10-25-what-s-new-in-calico-v3-31-ebpf-nftables-and-more/</link><pubDate>Sat, 25 Oct 2025 01:59:07 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-25-what-s-new-in-calico-v3-31-ebpf-nftables-and-more/</guid><description>Calico NFTables is now Generally Available (GA)! Calico eBPF data plane Enhancements eBPF Based Installation Resource Configurable cgroupv2 path Calico Whisker (Calico Observability Stack) Networking Calico Bandwidth Management Quality of Service (QoS) Traffic Classification How it works: Why it matters: Encapsulation Routes Programmed Direct From Felix Granular Control Over natOutgoing (A Community Contribution!) BGP Enhancements Performance &amp;amp; Usability We’re excited to announce the release of Calico v3.31, 🎉 which brings a wave of new features and improvements. For a quick look, here are the key updates and improvements in this release: Calico NFTables Dataplane is now Generally Available Calico eBPF Dataplane Enhancements Simplified installation: new template defaults to eBPF , automatically disables kube-proxy via kubeProxyManagement field, and adds bpfNetworkBootstrap for auto API endpoint detection. Configurable cgroupv2 path: support for immutable OSes (e. g. , Talos). Simplified installation: new template defaults to eBPF , automatically disables kube-proxy via kubeProxyManagement field, and adds bpfNetworkBootstrap for auto API endpoint detection. eBPF kube-proxy kubeProxyManagement bpfNetworkBootstrap Configurable cgroupv2 path: support for immutable OSes (e. g. , Talos). Calico Whisker (Observability Stack) Improved UI and performance in Calico v3.31. New policy trace categories: Enforced vs Pending. Lower memory use, IPv6 fixes, and more efficient flow streaming.</description></item><item><title>Introducing the Private AI Sizing Guide</title><link>https://kubermates.org/docs/2025-10-24-introducing-the-private-ai-sizing-guide/</link><pubDate>Fri, 24 Oct 2025 14:03:23 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-24-introducing-the-private-ai-sizing-guide/</guid><description>Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Introducing the Private AI Sizing Guide VCF Breakroom Chats Episode 68: Counting down to KubeCon North America 2025 – Let’s Chat! VCF Breakroom Chats Episode 67 - The Language of Cloud: Decoding Namespaces, Projects, and Orgs in VMware Cloud Foundation An essential resource for VMware Private AI Foundation with NVIDIA deployments Enterprises are racing to embed AI capabilities, from predictive analytics to conversational agents, yet they grapple with a unique set of hurdles: changing AI Application requirements, strict data‑privacy regulations, security risks, new LLMs coming to the market, escalating infrastructure costs, and the need for governance that balances innovation with security. To cut through this complexity, enterprises need guidance for right‑sizing compute, storage, and networking resources. Accurate resource planning can help organizations avoid over‑provisioning, stay compliant, and accelerate time‑to‑value, ensuring that their AI investments are both scalable and sustainable. To meet our customers where they are today, we are excited to announce the availability of our Private AI Sizing Guide, a resource designed to help you size your VMware Private AI Foundation with NVIDIA deployments with confidence. This guide provides a clear and detailed framework for estimating total resource requirements in the management domain and workload domain, so you can optimize your infrastructure for your organization’s specific needs. By leveraging this guide, you can reduce the risk of overprovisioning or underprovisioning, helping ensure a robust and scalable infrastructure for your private AI deployments. Key benefits: Estimate total requirements with confidence Optimize infrastructure for specific needs Reduce risk of overprovisioning or underprovisioning Get started with the Sizing Guide today and take the first step towards a successful private AI deployment! Ready to get started with AI and ML? Check out these helpful resources: Complete this form to contact us. Read the VMware Private AI Foundation with NVIDIA solution brief. Learn more about VMware Private AI Foundation with NVIDIA. Connect with us on Twitter at @VMwareVCF and on LinkedIn at VMware VCF.</description></item><item><title>Cloud Native Sustainability Month 2025: A global community movement for greener tech</title><link>https://kubermates.org/docs/2025-10-24-cloud-native-sustainability-month-2025-a-global-community-movement-for-greener-t/</link><pubDate>Fri, 24 Oct 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-24-cloud-native-sustainability-month-2025-a-global-community-movement-for-greener-t/</guid><description>What is Sustainability Month? Why participate? How to get involved Let’s build a greener cloud native future Posted on October 24, 2025 by TAG Operational Resilience Following the success of previous years, CNCF’s Sustainability Month is back — bigger and greener than ever. Organized by the TAG Operational Resilience community, this month-long global initiative brings together practitioners, developers, and advocates to explore sustainable cloud native practices and drive real change. After the big success from the last couple of years, with 20+ meetups around the world in 2024 and a global livestream, we’re back – now as CNCF Cloud Native Sustainability Month, extending beyond a week to allow more time for deeper global conversations on sustainability! The global event, supported by the CNCF Technical Advisory Group Operational Resilience, aims to bring together local communities to keep the discussion open regarding sustainable technology and a more sustainable cloud native future. Sustainability Month is your opportunity to: Dive into the latest sustainable cloud native technologies Connect with like-minded experts and peers who are driving environmental change in tech Learn practical and the newest strategies for reducing your cloud infrastructure’s carbon footprint Join the global conversation about building a greener, more responsible technology industry By attending or hosting a session, you can help drive the conversation forward, sharing ideas and best practices that will shape the future of sustainable cloud native infrastructure. Let’s work together to make a meaningful impact and create a greener, more responsible tech ecosystem! We need your energy and creativity to make this month a success! Whether you’re a local community organizer, a sustainability advocate, or someone simply curious about green tech, there’s a way to participate. When: November 2025 How: Add your event or meetup to the global list here Stay connected: Join the CNCF Slack channel #tag-operational-resilience for updates and coordination Organizers and speakers will receive CNCF recognition badges for their contributions. Sustainability Month is more than an event — it’s a global movement. By sharing local insights and collaborating across borders, we can make cloud native technology both innovative and environmentally responsible. Together, we can ensure that cloud innovation goes hand in hand with sustainability. Let’s make this year’s Sustainability Month the most impactful one yet! Join the CNCF Slack and connect in the #tag-operational-resilience channel to stay up to date and get involved. Share.</description></item><item><title>VCF Breakroom Chats Episode 68: Counting down to KubeCon North America 2025 – Let’s Chat!</title><link>https://kubermates.org/docs/2025-10-24-vcf-breakroom-chats-episode-68-counting-down-to-kubecon-north-america-2025-let-s/</link><pubDate>Fri, 24 Oct 2025 13:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-24-vcf-breakroom-chats-episode-68-counting-down-to-kubecon-north-america-2025-let-s/</guid><description>Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Introducing the Private AI Sizing Guide VCF Breakroom Chats Episode 68: Counting down to KubeCon North America 2025 – Let’s Chat! VCF Breakroom Chats Episode 67 - The Language of Cloud: Decoding Namespaces, Projects, and Orgs in VMware Cloud Foundation Welcome to Episode 68 of the VCF Breakroom Chats. Today, we are happy to present this video with Natalie Fisher, Principal Product Manager at Broadcom. In this episode, Jay Thontakudi and Natalie Fisher talk about all the exciting events and activities planned at KubeCon North America taking place in Atlanta between November 10th through November 13th. We will talk about our contributions, activities, important sessions, and the engaging demos planned for this event. Stop by our booth #1010 to meet our engineers and watch live demos. We can’t wait to talk about all things Kubernetes with you! Learn more about running and managing modern applications on VMware Cloud Foundation 9.0: Check out vSphere Kubernetes Service (VKS) Explore more resource on VMware Cloud Foundation Website About the VCF Breakroom Chat Series This webinar series is the successor to the vSphere Breakroom Chat, with a renewed focus on VMware Cloud Foundation (VCF). ​In this series, we share vlogs with industry-recognized experts from Broadcom and VMware solution partners and customers. These vlogs are concise, like meeting in a breakroom and having a quick conversation to get great information quickly. This series is for IT practitioners, IT admins, cloud and platform architects, developers, DevOps, senior IT managers, IT executives, and AI/ML professionals.</description></item><item><title>10 essential articles to guide your hybrid cloud modernization playbook</title><link>https://kubermates.org/docs/2025-10-24-10-essential-articles-to-guide-your-hybrid-cloud-modernization-playbook/</link><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-24-10-essential-articles-to-guide-your-hybrid-cloud-modernization-playbook/</guid><description>10 essential articles to guide your hybrid cloud modernization playbook What’s next? The adaptable enterprise: Why AI readiness is disruption readiness About the author Isabel Lee More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The technology roundup is being redefined by a confluence of critical priorities: Securing the software supply chain, finding a definitive playbook for virtualization, and making the strategic leap into enterprise AI governance. This roundup highlights the articles that have proven most essential for executing these priorities. We’ve collected the most strategically relevant insights covering everything from new AI assistant features in Red Hat Enterprise Linux (RHEL) and Red Hat Ansible Automation Platform to the governance models needed for GitOps and infrastructure modernization. Use this list as your guide to making well-informed decisions across your hybrid cloud footprint. The evolution of Red Hat Ansible Lightspeed AI isn’t just for code generation anymore. This article announces the formal release of the Red Hat Ansible Lightspeed intelligent assistant, a gen AI service embedded right into the platform’s user interface (UI). It serves as an instant subject matter expert (SME) for administrators and operators, using Red Hat documentation to streamline troubleshooting and onboarding processes. Read how it cuts down on operational friction by answering critical questions like “How do I manage user access?” and provides transparency with referenced links—all without leaving the automation platform. Fedora 43 Beta now available The release of Fedora 43 Beta provides a crucial early look at the innovations that will ultimately shape the RHEL family. Learn more about how this release is focused on improving the day-to-day experience for sysadmins and developers. Key updates include moving the installer to the modern Anaconda WebUI by default and the introduction of DNF5 for package management. For developers, the updated GNU toolchain and new language versions like Python 3.14 and Golang 1.25 provide a reliable platform for building modern applications.</description></item><item><title>Friday Five — October 24, 2025</title><link>https://kubermates.org/docs/2025-10-24-friday-five-october-24-2025/</link><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-24-friday-five-october-24-2025/</guid><description>Friday Five — October 24, 2025 SiliconANGLE - Red Hat aims new Developer Lightspeed AI features at application migration The open source engine driving AI from experiment to production and why inference is everything Automation unlocks 5G&amp;rsquo;s full potential: One New Zealand&amp;rsquo;s journey TechStrong - Red Hat delivers distributed computing platform for running AI applications NetApp and Red Hat strengthen collaboration to drive IT modernization with Red Hat OpenShift Virtualization About the author Red Hat Corporate Communications More like this Blog post Blog post Original podcast Original podcast Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share This week Red Hat launched Developer Lightspeed, a new generative AI tool, and MTA 8. The combined solution integrates AI-generated refactoring and automated replatforming to accelerate developers&amp;rsquo; migration of applications to the Red Hat OpenShift platform. Learn more In this blog, Red Hat addresses the challenge of scaling AI to production by focusing on efficient inference. Learn more Red Hat showcases how its automation portfolio, including Red Hat Ansible Automation Platform and OpenShift, unlocked the full potential of 5G for One New Zealand. Learn more Red Hat AI 3 integrates RHEL AI, Red Hat OpenShift AI, Red Hat AI Inference Server, and llm-d into a unified solution, offering customers a scalable Model-as-a-Service environment across the hybrid cloud. Learn more Red Hat and NetApp expand joint development efforts using Red Hat OpenShift Virtualization and NetApp storage solutions to support customer migration and modernization efforts. Learn more Red Hat is the world’s leading provider of enterprise open source software solutions, using a community-powered approach to deliver reliable and high-performing Linux, hybrid cloud, container, and Kubernetes technologies. Red Hat helps customers integrate new and existing IT applications, develop cloud-native applications, standardize on our industry-leading operating system, and automate, secure, and manage complex environments. Award-winning support, training, and consulting services make Red Hat a trusted adviser to the Fortune 500. As a strategic partner to cloud providers, system integrators, application vendors, customers, and open source communities, Red Hat can help organizations prepare for the digital future. The latest on IT automation for tech, teams, and environments Updates on the platforms that free customers to run AI workloads anywhere Explore how we build a more flexible future with hybrid cloud The latest on how we reduce risks across environments and technologies Updates on the platforms that simplify operations at the edge The latest on the world’s leading enterprise Linux platform Inside our solutions to the toughest application challenges The future of enterprise virtualization for your workloads on-premise or across clouds.</description></item><item><title>Red Hat Performance and Scale Engineering</title><link>https://kubermates.org/docs/2025-10-24-red-hat-performance-and-scale-engineering/</link><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-24-red-hat-performance-and-scale-engineering/</guid><description>Red Hat Performance and Scale Engineering Krkn-AI: A feedback-driven approach to chaos engineering Network performance in distributed training: Maximizing GPU utilization on OpenShift How Red Hat has redefined continuous performance testing Extending the Chaos: A Guide to Building Custom Scenarios for Krkn vLLM or llama. cpp: Choosing the right LLM inference engine for your use case LFX Mentorship: CNCF - Krkn: Chaos scenario rollback feature (2025 Term 2) BGP dynamic routing with Fast Data Path on RHOSO 18 Unleash controlled chaos with krknctl Enhancing system resilience with Krkn chaos dashboard Ollama vs. vLLM: A deep dive into performance benchmarking Scaling OpenShift Network Policies: Results and Takeaways Scaling OpenShift Network Policies: Our Journey in Developing a Robust Workload Testing Tool Improving performance of multiple I/O threads for OpenShift Virtualization OpenShift LACP bonding performance expectations Feature Introduction: Multiple IOthreads for OpenShift Virtualization How we improved AI inference on macOS Podman containers Dynamic VM CPU Workload Rebalancing with Load Aware Descheduler Red Hat Enterprise Linux Performance Results on Intel® Xeon® 6 processors How to run performance and scale validation for OpenShift AI Performance boosts in vLLM 0.8.1: Switching to the V1 engine Evaluating memory overcommitment in OpenShift Virtualization Boost OpenShift database VM density with memory overcommit Supercharge Your AI with OpenShift AI and Redis: Unleash speed and scalability RHEL for Real Time: CPU throttling and risks Scalable Database Performance with OpenShift Virtualization, Out-of-the-Box Unlocking the Effective Context Length: Benchmarking the Granite-3.1-8b Model Monitoring Red Hat Ansible Automation Platform using Performance Co-Pilot RoCE multi-node AI training on Red Hat OpenShift Performance of Terraform provider to manage Openshift fleets A step by step guide to setting up OCP Virtualization on hyperconverged ODF and deploy 10K VMs Virtualized database I/O performance improvements in RHEL 9.4 Use kube-burner to measure Red Hat OpenShift VM and storage deployment at scale Scaling virtio-blk disk I/O with IOThread Virtqueue Mapping Generative AI fine-tuning of LLMs: Red Hat and Supermicro showcase outstanding results for efficient Llama-2-70b fine tuning using LoRA in MLPerf Training v4.0 Unleashing 100GbE network efficiency: SR-IOV in Red Hat OpenShift on OpenStack Scaling Red Hat OpenStack Platform 17.1 to more than 1000+ virtual nodes Sharing is caring: How to make the most of your GPUs (part 1 - time-slicing) Scale testing image-based upgrades for single node OpenShift How to create and scale 6,000 virtual machines in 7 hours with Red Hat OpenShift Virtualization Egress IP Scale Testing in OpenShift Container Platform IPsec Performance on Red Hat Enterprise Linux 9: A Performance Analysis of AES-GCM Ensure a scalable and performant environment for ROSA with hosted control planes Accelerating generative AI adoption: Red Hat OpenShift AI achieves impressive results in MLPerf inference benchmarks with vLLM runtime Red Hat Enterprise Linux Performance Results on 5th Gen Intel® Xeon® Scalable Processors Optimizing Quay/Clair: Database profiling results Optimizing Quay/Clair: Profiling, performance, and efficiency Save memory with OpenShift Virtualization using Free Page Reporting Test Kubernetes performance and scale with kube-burner 5 ways we work to optimize Red Hat Satellite Best practices for OpenShift Data Foundation disaster recovery resource planning DPDK latency in OpenShift - Part II Correlating QPS rate with resource utilization in self-managed Red Hat OpenShift with Hosted Control Planes Continuous performance and scale validation of Red Hat OpenShift AI model-serving stack Kube-burner: Fanning the flames of innovation in the CNCF Sandbox Evaluating LLM inference performance on Red Hat OpenShift AI Operating Tekton at scale: 10 lessons learned from Red Hat Trusted Application Pipeline Behind the scenes: Introducing OpenShift Virtualization Performance and Scale KrknChaos is joining CNCF Sandbox Supercharging chaos testing using AI Quantifying performance of Red Hat OpenShift for Machine Learning (ML) Training on Supermicro A+ Servers with MLPerf Training v3.1 OpenShift Cluster Manager API: Load-testing, breaking, and improving it Data Plane Development Kit (DPDK) latency in Red Hat OpenShift - Part I Running 2500 pods per node on OCP 4.13 Bulk API in Automation Controller Red Hat Enterprise Linux achieves significant performance gains with Intel&amp;rsquo;s 4th Generation Xeon Scalable Processors OpenShift/Kubernetes Chaos Stories Enhancing/Maximizing your Scaling capability with Automation Controller 2.3 Red Hat new Benchmark results on AMD EPYC4 (Genoa) processors A Guide to Scaling OpenShift Data Science to Hundreds of Users and Notebooks Run Windows workloads on OpenShift Container Platform A Guide to Functional and Performance Testing of the NVIDIA DGX A100 Scaling Automation Controller for API Driven Workloads Performance Improvements in Automation Controller 4.1 The Curious Case of the CPU Eating Gunicorn Entitlement-Free Deployment of the NVIDIA GPU Operator on OpenShift Red Hat collaborates with NVIDIA to deliver record-breaking STAC-A2 Market Risk benchmark Red Hat Satellite 6.9 with Puma Web Server Using NVIDIA A100’s Multi-Instance GPU to Run Multiple Workloads in Parallel on a Single GPU Multi-Instance GPU Support with the GPU Operator v1.7.0 Making Chaos Part of Kubernetes/OpenShift Performance and Scalability Tests Demonstrating Performance Capabilities of Red Hat OpenShift for Running Scientific HPC Workloads A Complete Guide for Running Specfem Scientific HPC Workload on Red Hat OpenShift Running HPC workloads with Red Hat OpenShift Using MPI and Lustre Filesystem Introduction to Kraken, a Chaos Tool for OpenShift/Kubernetes About the author Red Hat Performance Team More like this Blog post Blog post Original podcast Original podcast Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Red Hat&amp;rsquo;s most recent posts about Performance, Scale, Chaos and more. Chaos engineering is the practice of deliberately introducing controlled failures into a system to uncover weaknesses before they affect end users. By continuously running chaos experiments, teams can build greater confidence in their systems and identify real performance bottlenecks. However, applying chaos in real-world environments can be challenging due to the complex, dynamic nature of applications and infrastructure, especially in environments like Kubernetes. read more We compared two IBM Cloud GPU clusters—one with NVIDIA L40S GPUs and one with H100 GPUs—head‑to‑head to see what really drives distributed training performance. The key finding is that for distributed training, the choice of network architecture is the most significant factor in performance, far outweighing the capabilities of the default container networking. Our tests conclusively show that using the standard Red Hat OpenShift pod network for internode communication creates a severe performance bottleneck that prevents expensive GPU resources from being fully utilized. read more Continuous performance testing (CPT) is a critical aspect of modern software development, especially considering the mission-critical applications and diverse infrastructure on which Red Hat OpenShift can run. In this article, we will discuss the importance of continuous performance testing, the challenges the OpenShift Performance and Scale Team discovered, and how shifting-left has increased our team velocity. read more Your system is resilient… until it’s not.</description></item><item><title>Sovereign AI in action</title><link>https://kubermates.org/docs/2025-10-24-sovereign-ai-in-action/</link><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-24-sovereign-ai-in-action/</guid><description>Sovereign AI in action The imperative for sovereign AI in India Open source: The cornerstone of trust and independence NxtGen SpeedCloud: India&amp;rsquo;s sovereign AI in action How Red Hat can help A shared vision for India&amp;rsquo;s AI future The adaptable enterprise: Why AI readiness is disruption readiness About the author Vincent Caldeira More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The recent Open Source Summit Hyderabad offered a platform to discuss a significant shift in the global AI landscape: the move toward a distributed, trusted, and self-reliant AI model. For too long, the narrative around AI has been dominated by a few centralized players and proprietary platforms. That era is quickly ending. India is now part of this movement with its unique, development-centered approach to AI. I had the pleasure of presenting with Rajgopal A S, MD and CEO at NxtGen Cloud Technologies. Our joint presentation, &amp;ldquo;Sovereign AI in Action,&amp;rdquo; showcased how this vision is being realized through the powerful combination of NxtGen&amp;rsquo;s SpeedCloud and Red Hat&amp;rsquo;s open source technologies. Organizations and nations increasingly recognize the critical need for digital sovereignty—the ability to independently control and safeguard essential digital infrastructure, data, operations, technology, and supply chains in alignment with their policies, values, and strategic objectives. This is particularly urgent in India due to several interconnected challenges: Data control and protection: Governments see sovereign AI as vital for protecting data privacy and safeguarding national security. This requires controlling where data is stored, who can access it, and how it’s used, often insisting on data staying within specific national or regional borders. The growth of data sovereignty laws and data protection rules worldwide means companies are having to adjust quickly. Technological independence: There&amp;rsquo;s also a strong desire to reduce reliance on external providers&amp;rsquo; infrastructure and technologies amid geopolitical uncertainties. Currently, 95% of accelerator-enabled public cloud regions use US-owned accelerators, showing a concentration of control.</description></item><item><title>CNCF embraces LFX Self Service for calendar management</title><link>https://kubermates.org/docs/2025-10-23-cncf-embraces-lfx-self-service-for-calendar-management/</link><pubDate>Thu, 23 Oct 2025 14:33:19 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-23-cncf-embraces-lfx-self-service-for-calendar-management/</guid><description>A new era of automation and autonomy Streamlined access and enhanced features Transition and sunset plan Posted on October 23, 2025 by Riaan Kleinhans, CNCF CNCF projects highlighted in this post The Cloud Native Computing Foundation ( CNCF ) has experienced remarkable growth since its inception, welcoming its first project, Kubernetes , on March 10, 2016. By 2025, the CNCF proudly supports over 200 active projects , generating thousands of meetings annually. This rapid expansion, while a testament to the thriving cloud native ecosystem, also presented a unique set of operational challenges, particularly in managing project calendars. Historically, CNCF staff members manually managed a central Google Calendar for all projects, handling requests for meeting additions, changes, and removals through a high-volume service desk ticketing system. For anyone who has struggled to keep even a couple of personal calendars synchronized, the complexity of managing over 200 project calendars on behalf of diverse communities is easily imaginable. This labor-intensive process, while functional, was not scalable and often led to bottlenecks for both projects and staff. The CNCF Projects team is continuously seeking innovative ways to enhance the project experience through automation and increased autonomy. Recognizing the need for a more efficient and empowering calendar solution, the team initiated a significant transition in June 2025. Projects are now being migrated to the new CNCF calendar system, a robust platform backed by the Linux Foundation’s LFX infrastructure. This new system introduces self-managed calendars for every project, empowering maintainers with direct control over their scheduling. The foundation of this new calendar system is the Linux Foundation Project Control Center (PCC). PCC serves as the single source of truth for project lifecycle information, dynamically pulling essential details such as project logos, calendar links, and GitHub links, and displaying them directly on the new calendar interface.</description></item><item><title>Highlights from CNCF’s first Open Observability Summit</title><link>https://kubermates.org/docs/2025-10-23-highlights-from-cncf-s-first-open-observability-summit/</link><pubDate>Thu, 23 Oct 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-23-highlights-from-cncf-s-first-open-observability-summit/</guid><description>The first foundation-led conference on open observability Fluent Bit vs. OpenTelemetry Collector: A benchmark face-off Real-world scale: Observability at eBay Bringing OpenTelemetry to Android with Kotlin SDK Deep-dive on tuning the OpenTelemetry Collector Broadcom’s end-to-end observability: From mobile to mainframe Spotify’s metric migration: From in-house TSDB to VictoriaMetrics and Prometheus Introducing Rotel: A Rust-based alternative to the OpenTelemetry Collector AI is a hot topic in observability Posted on October 23, 2025 by Dotan Horovits CNCF projects highlighted in this post Ambassador post originally published on Medium by Dotan Horovits It’s about time open observability had its own industry-wide, vendor-neutral event. This year, the Cloud Native Computing Foundation (CNCF) finally made it happen with the inaugural Open Observability Summit , bringing together contributors, practitioners, and end users for a packed day of learning and collaboration. I was honored to deliver a keynote at this important event, as well as have my podcast OpenObservability Talks as a partner of the event. I dedicated my latest episode to covering the highlights from the event, and invited two fellow observability veterans and speakers of the event — Alok Bhide , member of the summit’s content committee and Head of Product Innovation at Chronosphere, and Henrik Rexed , Developer Advocate at Dynatrace, CNCF Ambassador, and host of the Is It Observable podcast. Together, we unpacked the summit highlights. Here’s a recap, and you can find the full recording on all the podcast apps, with links bellow. Where’s the community’s meeting point to discuss open source observability? There are quite a few conferences out there on observability, but they are largely owned by vendors in this space. And with Monitorma currently on pause, it leaves us with no good place to come together. The CNCF has done great job in bringing everyone together, dovetailing Open Source Summit North America. Alok, who’s been on the program committeee and got to review and select the talks on the Call For Papers, shared on the balance of the talks and breadth of the projects covered: “combination of foundational talks and the shining new things… and across the different projects: OTel, FluentBit, Jaeger, Prometheus, everything. ” This resulted in some great talks, which are already available online on the CNCF’s YouTube channel.</description></item><item><title>VCF Breakroom Chats Episode 67 – The Language of Cloud: Decoding Namespaces, Projects, and Orgs in VMware Cloud Foundation</title><link>https://kubermates.org/docs/2025-10-23-vcf-breakroom-chats-episode-67-the-language-of-cloud-decoding-namespaces-project/</link><pubDate>Thu, 23 Oct 2025 13:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-23-vcf-breakroom-chats-episode-67-the-language-of-cloud-decoding-namespaces-project/</guid><description>VCF Breakroom Chats Episode 67 About the VCF Breakroom Chat Series Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Introducing the Private AI Sizing Guide VCF Breakroom Chats Episode 68: Counting down to KubeCon North America 2025 – Let’s Chat! VCF Breakroom Chats Episode 67 - The Language of Cloud: Decoding Namespaces, Projects, and Orgs in VMware Cloud Foundation Welcome to the next episode of VCF Breakroom Chats. Today, we are happy to present this vLog with Vincent Riccio, Product Marketing Engineer at Broadcom. In this episode, Vincent Riccio and Taka Uenishi explore the various governance layers in VCF Automation, explaining the governance constructs (from clusters to namespaces) every VI Admin must know for becoming a Cloud Admin. Want to learn more about VCF Automation? Check out the VCF Automation web page for more resources. Read this blog post: Private Cloud Redefined: Deliver a Unified Cloud Consumption Experience with VMware Cloud Foundation 9.0 Listen to our previous topics. This webinar series focuses on VMware Cloud Foundation (VCF). In this series, we share conversations with industry-recognized experts from Broadcom and with solution partners and customers. You’ll gain relevant insights whether you’re an IT practitioner, IT admin, cloud or platform architect, developer, DevOps, senior IT manager, IT executive, or AI/ML professional. If you are new to the series, please check out our previous episodes.</description></item><item><title>Image and audio models from fal now available on DigitalOcean</title><link>https://kubermates.org/docs/2025-10-23-image-and-audio-models-from-fal-now-available-on-digitalocean/</link><pubDate>Thu, 23 Oct 2025 12:30:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-23-image-and-audio-models-from-fal-now-available-on-digitalocean/</guid><description>Image and audio models from fal now available on DigitalOcean Explore the new models Try it out Bring your ideas to life with fal on DigitalOcean About the author Try DigitalOcean for free Related Articles Announcing GPU Droplets accelerated by NVIDIA HGX H100 in the EU Announcing cost-efficient storage with Network file storage, cold storage, and usage-based backups Announcing per-sec billing, new Droplet plans, BYOIP, and NAT gateway preview to reduce scaling costs By Grace Morgan Updated: October 24, 2025 4 min read Weâre excited to announce the launch of four multimodal AI models from fal on the DigitalOcean Gradientâ¢ AI Platform , now available in public preview through Serverless Inference. These models allow you to generate images and audio directly via API, without worrying about infrastructure, scaling, or vendor management. With this release, building AI-powered applications that include visual and audio content is easier than ever. The fal models, now in public preview, cover a variety of modalities, enabling you to experiment, prototype, and deploy multimodal AI features quickly: Image generation: Stable Diffusion XL fast ( fal-ai/fast-sdxl ) â High-resolution image generation Stable Diffusion XL fast ( fal-ai/fast-sdxl ) â High-resolution image generation fal-ai/fast-sdxl FLUX. 1 (schnell) ( fal-ai/flux/schnell ) â Fast image generation for quick prototyping FLUX. 1 (schnell) ( fal-ai/flux/schnell ) â Fast image generation for quick prototyping fal-ai/flux/schnell Audio generation: Stable Audio ( fal-ai/stable-audio-25/text-to-audio ) â Convert text into natural-sounding audio Stable Audio ( fal-ai/stable-audio-25/text-to-audio ) â Convert text into natural-sounding audio fal-ai/stable-audio-25/text-to-audio ElevenLabs TTS Multilingual v2 9 ( fal-ai/elevenlabs/tts/multilingual-v2 ) â Multilingual text-to-speech ElevenLabs TTS Multilingual v2 9 ( fal-ai/elevenlabs/tts/multilingual-v2 ) â Multilingual text-to-speech fal-ai/elevenlabs/tts/multilingual-v2 These models are available via Serverless Inference, letting you generate images and audio through the same simple API-driven workflow you already use on Gradient AI Platform. You can start using these models through the Serverless Inference API ( https://inference. do-ai. run ) after opting in to the public preview in the DigitalOcean console. Hereâs a quick look at how to interact with them: https://inference. do-ai. run First, opt in to the public preview to access the fal models on the Gradient AI Platform.</description></item><item><title>Kubernetes Tutorial for Beginners 2025</title><link>https://kubermates.org/docs/2025-10-23-kubernetes-tutorial-for-beginners-2025/</link><pubDate>Thu, 23 Oct 2025 07:42:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-23-kubernetes-tutorial-for-beginners-2025/</guid><description>Introduction - Why Learn Kubernetes in 2025 Why Kubernetes Is Worth Your Time Why 2025 Is the Perfect Time to Learn What Is Kubernetes? (Quick Refresher) Understanding the Basics How Kubernetes Solves Real Problems Ready to Get Hands-On? Understanding Kubernetes Architecture (The Foundation) The Control Plane (The Brain of Kubernetes) The Worker Nodes (Where Your Apps Live) Putting It All Together Learn Visually Your First Kubernetes Setup (Getting Started the Right Way) Option 1: Run Kubernetes Locally (Minikube or Kind) Option 2: Learn Kubernetes Online Instantly (No Setup Needed) Option 3: Cloud-Based Kubernetes (Intermediate Path) Best Practice for 2025 Beginners Step-by-Step Kubernetes Tutorial for Beginners 2025 Step 1: Create a Namespace (Optional but Recommended) Step 2: Deploy Your First Pod Step 3: Expose the Pod as a Service Step 4: Scale Your Deployment Step 5: Clean Up (Always Good Practice) What You Just Learned Best Way to Learn Kubernetes Online Start with the Fundamentals Follow a Beginner-Friendly Roadmap Why KodeKloud Is the Best Place to Learn Kubernetes Online Pro Tip: Combine Study + Practice From Beginner to Certified - Your Next Step Why Get Certified in Kubernetes Popular Kubernetes Certifications How KodeKloud Helps You Get Certified Common Mistakes Beginners Make (and How to Avoid Them) 1. Skipping the Fundamentals 2. Avoiding YAML and Declarative Configuration 3. Overcomplicating the Setup 4. Ignoring the ‘Why’ Behind the Commands 5. Not Practicing Troubleshooting Key Takeaway Free &amp;amp; Paid Resources to Learn Kubernetes 1. Free Kubernetes Resources 2. Paid Kubernetes Resources 3. When to Move from Free to Paid Learning Summary - Why 2025 Is the Best Time to Start Why Start Learning Now Final Thought FAQ CKA Exam Verification Guide Kubernetes Architecture Explained: Nodes, Pods, and Clusters Quick Fixes for Common Kubernetes Issues Guide to AWS Certification What is Kubernetes? A Beginner’s Guide to Container Orchestration DevOps Tutorials 2025: Step-by-Step Learning Resources for Beginners Ubuntu: Set Timezone How to Enable SSH on Ubuntu Certifications in DevOps: Which Are Worth Your Time in 2025? What is DevOps? Understand Kubernetes architecture step by step - Control Plane, Nodes, Pods, and Clusters. Start instantly with KodeKloud Free Kubernetes Labs - no installation or setup needed. Learn Kubernetes online through KodeKloud Kubernetes courses , visual lessons, and browser-based labs. Follow a proven learning roadmap covering YAML, Deployments, Services, and troubleshooting.</description></item><item><title>Migration toolkit for applications 8: Bringing modernized applications to market faster</title><link>https://kubermates.org/docs/2025-10-23-migration-toolkit-for-applications-8-bringing-modernized-applications-to-market-/</link><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-23-migration-toolkit-for-applications-8-bringing-modernized-applications-to-market-/</guid><description>Migration toolkit for applications 8: Bringing modernized applications to market faster Automating replatforming to Red Hat OpenShift Combining automation with intelligence Looking ahead About the authors Shaaf Syed Ramón Román Nissen Jonathan Recinos More like this Blog post Blog post Original podcast Original podcast Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Application modernization is a continuous journey for enterprises, driven by the need for greater business agility, enhanced security, and cost optimization. While the benefits are clear, large-scale modernization projects can be complex, time-consuming, and require significant upfront investment. With the introduction of migration toolkit for applications 8 (MTA 8), Red Hat is helping organizations overcome these challenges by automating key parts of the modernization journey. The migration toolkit for applications has long provided tools for containerization readiness, source-code analysis, and project management. MTA 8 builds on this foundation by introducing two major new capabilities: automated replatforming and intelligent refactoring. Replatforming—migrating an application from one platform to another with minimal code changes—is a common modernization strategy that can deliver significant benefits like improved scalability, security, and cost-efficiency. However, even this approach can be time-consuming due to a number of manual, error-prone tasks. MTA 8 automates this process by generating the necessary deployment artifacts to run an application on Red Hat OpenShift. The first supported migration path is from Cloud Foundry to OpenShift, enabling users to: Discover and analyze applications: MTA connects to the source platform to identify applications and extract crucial deployment and runtime information. Generate deployment assets: Using this normalized data, MTA translates the configuration into the target platform format by leveraging Helm Charts to create deployment manifests for OpenShift. Streamline deployment: The resulting artifacts are automatically placed in a target repository, which can be picked up by CI/CD pipelines for automated deployment. This automation helps to reduce manual work and improve the ROI of application modernization projects.</description></item><item><title>OAuth App Based Workload Identity for Droplets</title><link>https://kubermates.org/docs/2025-10-22-oauth-app-based-workload-identity-for-droplets/</link><pubDate>Wed, 22 Oct 2025 18:51:32 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-22-oauth-app-based-workload-identity-for-droplets/</guid><description>OAuth App Based Workload Identity for Droplets What is workload identity federation? Security properties Workload identity solution architecture Next steps About the author Try DigitalOcean for free Related Articles How DigitalOcean Uses Semgrep to Fortify Security: A Highlight From Our Toolset Contextual Vulnerability Management With Security Risk As Debt Regresshion vulnerability: Recommended actions and steps we&amp;rsquo;ve taken By John Andersen Senior Product Security Engineer Published: October 22, 2025 8 min read This post is the first entry in a three part series on workload identity federation : Part 1: Architecture (this post) Part 1: Architecture (this post) Part 2: Deployment and Configuration Part 2: Deployment and Configuration Part 3: Usage from Droplets and GitHub Actions Part 3: Usage from Droplets and GitHub Actions This entry will cover what workload identity federation is and how it can be implemented leveraging DigitalOceanâs OAuth API. In the following entries in this series, weâll deploy an open source Proof of Concept (PoC) , configure roles and policies for workload identity access control, spin up a Droplet, write a GitHub Actions workflow, and access databases and Spaces keys from them using their respective workload identity tokens. Workload identity is used to reduce the amount of secrets involved in deploying and administrating software systems. Instead of authentication being done based on something a workload knows, for example passwords or API tokens, authentication is done based on what the workload is. The heart of workload identity federation is asymmetric cryptography. By leveraging public / private key pairs, tokens can be issued to workloads, such as Droplets, and used for authentication and authorization to APIs exposed by resource servers. Workload identity tokens are exchanged for domain specific access tokens, or grant access to resources directly. This series showcases how we can use DigitalOceanâs OAuth API and fine grained permission scopes to implement and leverage workload identity federation using OpenID Connect (OIDC) protocol tokens. Weâll enable secretless access to DigitalOcean hosted databases and Spaces buckets from Droplets and GitHub Actions workflows. Eliminating the need to provision static, long-lived credentials for databases and Spaces buckets for those environments. Authentication based on what the workload is requires that the infrastructure orchestrating the workload be able to make verifiable claims about a workloadâs properties. To do this, the infrastructure responsible for running the workload enables issuance of workload-specific tokens containing these claims.</description></item><item><title>VMware Cloud on AWS: VMC Console UI Migration to Broadcom</title><link>https://kubermates.org/docs/2025-10-22-vmware-cloud-on-aws-vmc-console-ui-migration-to-broadcom/</link><pubDate>Wed, 22 Oct 2025 18:36:18 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-22-vmware-cloud-on-aws-vmc-console-ui-migration-to-broadcom/</guid><description>Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VMware Cloud on AWS: VMC Console UI Migration to Broadcom VCF Breakroom Chats Episode 66 - VCF Automation Org Types Demystified: Debunking Misconceptions about VM &amp;amp; All Apps Orgs VMware Workstation &amp;amp; Fusion 25H2: Embracing Calendar Versioning and New Features Last Update: October 22, 2025 Editorial Note: This is a living blog featuring important service updates regarding the migration of the VMware Cloud on AWS Console from VMware to Broadcom. Please check back frequently for the latest updates and additional answered FAQs. As this feature is in active development, all information in this document is subject to change. As part of our commitment to improving the VMware Cloud on AWS (VMC) customer experience, we are announcing that the current VMC Console at https://vmc. vmware. com is being migrated to a new user interface at https://vmc. broadcom. com ! This change will provide a more unified and modern product experience. Customers will be automatically redirected; there is no need to change settings. The rollout has already started for select customers, and the VMware Cloud on AWS team will begin the general rollout on October 29, 2025 and continue for several weeks. Customers will receive a separate email notifying them the specific date they will be enabled. What is Changing? Customers will observe the following changes after they are enabled: At the new URL, customers will have access to the improved VMC Console user interface Navigating to the VMC Console at https://vmc.</description></item><item><title>VCF Breakroom Chats Episode 66 – VCF Automation Org Types Demystified: Debunking Misconceptions about VM &amp; All Apps Orgs</title><link>https://kubermates.org/docs/2025-10-22-vcf-breakroom-chats-episode-66-vcf-automation-org-types-demystified-debunking-mi/</link><pubDate>Wed, 22 Oct 2025 17:15:19 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-22-vcf-breakroom-chats-episode-66-vcf-automation-org-types-demystified-debunking-mi/</guid><description>VCF Breakroom Chats Episode 66 About the VCF Breakroom Chat Series Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VMware Cloud on AWS: VMC Console UI Migration to Broadcom VCF Breakroom Chats Episode 66 - VCF Automation Org Types Demystified: Debunking Misconceptions about VM &amp;amp; All Apps Orgs Busting Cyber Resilience Myths with VMware Cloud Foundation Welcome to the next episode of VCF Breakroom Chats. Today, we are happy to present this vLog with Maher AlAsfar, Product Marketing Engineer at Broadcom. In this episode, Maher AlAsfar and Taka Uenishi demystify and debunk misconceptions around the different VCF Automation organization types that are available in VCF 9.0. Want to learn more about VCF Automation? Check out the VCF Automation web page for more resources. Read this blog post: Private Cloud Redefined: Deliver a Unified Cloud Consumption Experience with VMware Cloud Foundation 9.0 Listen to our previous topics. This webinar series focuses on VMware Cloud Foundation (VCF). In this series, we share conversations with industry-recognized experts from Broadcom and with solution partners and customers. You’ll gain relevant insights whether you’re an IT practitioner, IT admin, cloud or platform architect, developer, DevOps, senior IT manager, IT executive, or AI/ML professional. If you are new to the series, please check out our previous episodes.</description></item><item><title>LFX Insights: A new way to understand open source projects</title><link>https://kubermates.org/docs/2025-10-22-lfx-insights-a-new-way-to-understand-open-source-projects/</link><pubDate>Wed, 22 Oct 2025 16:38:23 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-22-lfx-insights-a-new-way-to-understand-open-source-projects/</guid><description>What is Insights? Why Insights matters for end users 5 Key features for end users LF Open Source Index Coverage beyond Linux Foundation projects Project health score Contributor and organization attribution Report faulty or incomplete data How you can use Insights Explore LFX Insights Posted on October 22, 2025 by Ricardo Rocha, Cern Open source forms the backbone of modern technology ecosystems. From orchestration and observability to frameworks and developer tools, today’s technology choices depend on projects we may not control but rely on every day. The challenge: not all projects are equal. Some are maintained by large, diverse contributor bases. Others hinge on a handful of individuals. Some projects are responsive to security issues, while others leave risks unaddressed. Until now, it has been hard to see which projects are truly healthy. That’s the gap LFX Insights is built to close. ​​LFX Insights, developed by the Linux Foundation, helps organizations make informed decisions about the open source projects they depend on. Instead of relying on surface-level metrics like GitHub stars, Insights helps you answer deeper questions such as: Is this project actively maintained? Is there a healthy mix of contributors and organizations? How quickly are issues and pull requests being resolved? Does the project follow good security and governance practices? It gives you the information you need to select, adopt, and invest in open source projects with confidence. End users carry real risk when dependencies aren’t healthy. We’ve all seen the consequences: left-pad’s removal, the Log4Shell vulnerability, or the recent XZ backdoor attempt.</description></item><item><title>Cloud Native Maturity Model 4.0 (Beta): Reflecting what’s next for cloud native — and we want your input</title><link>https://kubermates.org/docs/2025-10-22-cloud-native-maturity-model-4-0-beta-reflecting-what-s-next-for-cloud-native-and/</link><pubDate>Wed, 22 Oct 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-22-cloud-native-maturity-model-4-0-beta-reflecting-what-s-next-for-cloud-native-and/</guid><description>What’s new in version 4.0 Why this matters What the community is saying Join the conversation Posted on October 22, 2025 by Danielle Cook, Simon Forster, Robert Glenn The CNCF Cartografos Working Group is excited to announce the beta release of the Cloud Native Maturity Model 4.0. This version expands the framework to include AI, FinOps, and evolving cultural practices. We invite the community to review the model and help shape the final release. The Cloud Native Maturity Model (CNMM) was created to support you, whether you’re just beginning your cloud native journey, leading a team, or already an expert. It helps identify where you may need to invest in tools, processes, people, or policies. Most importantly, it connects technical goals to business outcomes, giving you a framework to communicate the value of cloud native strategy to leadership. Since its initial launch in 2021 (with updates in 2022 and 2023), the model has helped organizations navigate adoption and maturity. Now, the Cartografos Working Group is excited to share the beta release of Version 4.0 —and we want your input. Key updates in CNMM 4.0 (Beta): Expanded focus on AI, emerging technologies, and FinOps Business-first maturity levels aligning outcomes with technical progress Shift-left approach to policy and governance Direct links to CNCF TAGs, working groups, and architectural references Cloud native has evolved rapidly, and CNMM 4.0 reflects those changes. This release expands coverage of AI, emerging technologies, and the cultural shifts required for success. It continues to emphasize that cloud native maturity must serve the business – not just technology for technology’s sake. Too often, conversations focus on Kubernetes or infrastructure in isolation.</description></item><item><title>From tokens to caches: How llm-d improves LLM observability in Red Hat OpenShift AI 3.0</title><link>https://kubermates.org/docs/2025-10-22-from-tokens-to-caches-how-llm-d-improves-llm-observability-in-red-hat-openshift-/</link><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-22-from-tokens-to-caches-how-llm-d-improves-llm-observability-in-red-hat-openshift-/</guid><description>From tokens to caches: How llm-d improves LLM observability in Red Hat OpenShift AI 3.0 New service level objectives (SLO) in the age of LLMs The challenge of managing these new SLOs What is llm-d? How llm-d Helps solve the observability gap Example PromQL queries Example dashboard Conclusion The adaptable enterprise: Why AI readiness is disruption readiness About the authors Christopher Nuland Sally O&amp;rsquo;Malley More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share As enterprises scale large language models (LLMs) into production, site reliability engineers (SREs) and platform operators face a new set of challenges. Traditional application metrics—CPU usage, request throughput, memory consumption—are no longer enough. With LLMs, reliability and efficacy are defined by entirely new dynamics—token-level performance, cache efficiency, and inference pipeline latency. This article explores how llm-d , an open source project co-developed with the leading AI vendors (Red Hat, Google, IBM, etc. ) and integrated into Red Hat OpenShift AI 3.0, redefines observability for LLM workloads. Users expect responsive applications, including those enhanced with AI, and enterprises require consistent performance to turn a pilot project into a profitable production application at scale. While SLOs in traditional microservice architectures are usually framed around request latency and error rate , the user experience for LLMs depends on more nuanced measures, including: Time to first token (TTFT): Measures the delay before the initial token of a response is streamed to the user. A lower TTFT is needed to provide a responsive and immediate user experience, especially in interactive applications. Time per output token (TPOT): Indicates the speed at which tokens are generated after the process begins. A consistent and low TPOT provides a smooth and efficient streaming of the complete response to the user. Cache hit rate: Represents the proportion of requests that can use previously computed context stored in GPU memory. A high cache hit rate significantly reduces computational overhead and improves overall system throughput by avoiding redundant processing.</description></item><item><title>Modernize: Migrate from SUSE Rancher RKE1 to Red Hat OpenShift</title><link>https://kubermates.org/docs/2025-10-22-modernize-migrate-from-suse-rancher-rke1-to-red-hat-openshift/</link><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-22-modernize-migrate-from-suse-rancher-rke1-to-red-hat-openshift/</guid><description>Modernize: Migrate from SUSE Rancher RKE1 to Red Hat OpenShift Why Red Hat OpenShift is the right choice for your business Enhanced security and consistency A rich developer experience and ecosystem Enterprise support How to migrate from RKE1 to Red Hat OpenShift Assessment and planning (Pre-migration) Platform and configuration migration Workload migration and validation Cutover and decommissioning Why choose Red Hat? Red Hat OpenShift Container Platform | Product Trial About the author Jonathan Gershater More like this Blog post Blog post Original podcast Original podcast Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share SUSE Rancher RKE1 reached its end-of-life (EOL) on July 31, 2025. Continuing to use it could leave your production workloads vulnerable to security risks, without vendor support, and facing compatibility challenges. If you’re considering a change, Red Hat OpenShift offers an enterprise-grade application platform that can support your Kubernetes container strategy today and scale for the future. Red Hat OpenShift is a complete, enterprise-focused container application platform built on a hardened, consistent Red Hat Enterprise Linux (RHEL) foundation. Red Hat OpenShift offers the following benefits for your organization: Integrated security - Red Hat OpenShift provides built-in, proactive security features, including Security Context Constraints (SCCs) that are more restrictive than standard Kubernetes role-based access control (RBAC), and a foundation built on Red Hat Enterprise Linux CoreOS for immutable, managed-host operating systems (OSs). Consistent platform - Red Hat OpenShift delivers a consistent experience across on-premise, public cloud, and edge deployments. This consistency simplifies operations, troubleshooting, and compliance. Cluster security - Integrating Red Hat OpenShift compliance operator and applying enterprise security policies achieves a hardened security cluster. Integrated developer tools - Red Hat OpenShift features an integrated web console, built-in CI/CD tools, and ArgoCD for GitOps-based continuous deployment and developer tools. Red Hat OpenShift Pipelines and Source-to-Image (S2I) workflows boost developer productivity. The operator framework - Red Hat OpenShift takes advantage of operators (software extensions that manage the lifecycle of an application)—from deployment to upgrades, backups, and scaling—making Day 2 operations simple and reliable. Integration with existing DevOps toolchains, including GitHub, GitLab, and Jira.</description></item><item><title>The Path to Digital Sovereignty: Why an Open Ecosystem is the Key for Europe</title><link>https://kubermates.org/docs/2025-10-22-the-path-to-digital-sovereignty-why-an-open-ecosystem-is-the-key-for-europe/</link><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-22-the-path-to-digital-sovereignty-why-an-open-ecosystem-is-the-key-for-europe/</guid><description>The Path to Digital Sovereignty: Why an Open Ecosystem is the Key for Europe About the author Penny Philpot More like this Blog post Blog post Original podcast Original podcast Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share In an increasingly complex technology landscape, digital sovereignty has moved from a theoretical concept to an urgent strategic imperative for European organizations and governments. Recent global events—from supply chain disruptions to geopolitical conflicts—have underscored the critical need for greater control over their technology. Digital sovereignty is a strategic effort for organizations to build greater resilience, choice and confidence into IT environments. This isn’t a move towards isolation, with true sovereignty requiring more than just local data centers. Businesses are re-evaluating their provider dependencies and are increasingly looking for a flexible approach that allows them to define their own digital destiny through a collaborative and open ecosystem of partners. While data residency is crucial, it&amp;rsquo;s just one part of a larger, more complex picture. An effective sovereign cloud solution is built on four interconnected pillars: Assurance Sovereignty: Verifying that processes and security standards are met, and verifying that platforms are built to the required standards and certified for regulated industries like banking and government. Operational Sovereignty: Maintaining control over system management, monitoring, and who deploys and maintains the systems. Data Sovereignty: Having control over where data is stored and processed, and maintaining a high level of protection from extra-territorial influences. Technical Sovereignty: Exerting full control of IT environments by using open standards and technology to prevent dependencies on specific providers. An open source approach is fundamental to a sovereignty strategy because it provides a transparent foundation where customers can inspect and trust the underlying technology. However, technology alone isn&amp;rsquo;t enough; a strong partner ecosystem—particularly in Europe—is essential to act as a multiplier, helping organizations to meet their sovereignty goals.</description></item><item><title>Announcing the General Availability of Holodeck 9.0.1.0</title><link>https://kubermates.org/docs/2025-10-21-announcing-the-general-availability-of-holodeck-9-0-1-0/</link><pubDate>Tue, 21 Oct 2025 21:28:43 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-21-announcing-the-general-availability-of-holodeck-9-0-1-0/</guid><description>Project Contributors Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Announcing the General Availability of Holodeck 9.0.1.0 From Spend to Value: The Advantage of Private Cloud VCF Breakroom Chats Episode 65: VMware Cloud Foundation with Ubuntu OS - Smarter Choice for Modern Apps We’re excited to announce the General Availability of Holodeck 9.0.1.0, building upon the success of Holodeck 9.0 to further empower your teams in deploying standardized and automated nested VMware Cloud Foundation (VCF) environments! A sincere thank you from the team to the thousands of folks that downloaded and tried out Holodeck 9.0, we appreciate the engagement and feedback. It’s helped us further shape and grow the tool! In today’s fast-paced technological landscape, the ability to rapidly and reliably test new technical capabilities is paramount. While deploying nested VCF environments has always been possible, it often involves time-consuming manual configurations and significant hardware overhead. Holodeck continues to change the game by providing a self-contained, automated, and repeatable solution that dramatically reduces complexity and resource requirements, now with even more robust features. Some highlights: New-HoloDeckConfig: This cmdlet now validates the integrity of the config. json template. If the template file’s integrity does not match, the user will see a warning before a new config file is created for the Holodeck instance. Ideally, users are not expected to modify the template config file; however, there may be edge cases where a customer chooses to do so. In such cases, the warning ensures they are informed of the change. Remove-HoloDeckInstance: This cmdlet has a new optional parameter to support the automated deletion of the holodeck instance. If not specified, then the instance ID will be captured from the global config file. Also, the prompt for user confirmation is removed, instead, the user will have 15 seconds to abort the operation by pressing any key on the console.</description></item><item><title>When to Use BGP, VXLAN, or IP-in-IP: A Practical Guide for Kubernetes Networking</title><link>https://kubermates.org/docs/2025-10-21-when-to-use-bgp-vxlan-or-ip-in-ip-a-practical-guide-for-kubernetes-networking/</link><pubDate>Tue, 21 Oct 2025 19:29:35 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-21-when-to-use-bgp-vxlan-or-ip-in-ip-a-practical-guide-for-kubernetes-networking/</guid><description>The Cost of a Mismatched Network Mode A Technical Breakdown of Networking Modes VXLAN (Virtual Extensible LAN) IP-in-IP BGP (Border Gateway Protocol) Decision Matrix Simplifying Operations Across Network Modes Final Verdict: Selecting the Right Networking Mode for Your Workload Ready to Simplify Kubernetes Networking? When deploying a Kubernetes cluster, a critical architectural decision is how pods on different nodes communicate. The choice of networking mode directly impacts performance, scalability, and operational overhead. Selecting the wrong mode for your environment can lead to persistent performance issues, troubleshooting complexity, and scalability bottlenecks. The core problem is that pod IPs are virtual. The underlying physical or cloud network has no native awareness of how to route traffic to a pod’s IP address, like 10.244.1.5 It only knows how to route traffic between the nodes themselves. This gap is precisely what the Container Network Interface (CNI) must bridge. 10.244.1.5 The CNI employs two primary methods to solve this problem: Overlay Networking (Encapsulation): This method wraps a pod’s packet inside another packet that the underlying network understands. The outer packet is addressed between nodes, effectively creating a tunnel. VXLAN and IP-in-IP are common encapsulation protocols. Underlay Networking (Routing): This method teaches the network fabric itself how to route traffic directly to pods. It uses a routing protocol like BGP to advertise pod IP routes to the physical network, making pods routable without encapsulation. This guide dives into the technical differences of these options to help you select the right mode for your environment.</description></item><item><title>Enhancing container security in Amazon EKS Auto Mode with KubeArmor</title><link>https://kubermates.org/docs/2025-10-21-enhancing-container-security-in-amazon-eks-auto-mode-with-kubearmor/</link><pubDate>Tue, 21 Oct 2025 19:25:41 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-21-enhancing-container-security-in-amazon-eks-auto-mode-with-kubearmor/</guid><description>Enhancing container security in Amazon EKS Auto Mode with KubeArmor The runtime security challenge in Kubernetes Architecture Key components Prerequisites How does KubeArmor improve EKS Auto Mode security? 1. System call-level protection 2. Granular process and file access control 3. Supply chain security enhancement 4. Zero-day vulnerability mitigation 5. Compliance support Real world use case Deployment steps Step 1: Create an EKS Auto Mode cluster Step 2: Install KubeArmor using Helm Step 3: Verify KubeArmor installation Step 4: Apply file integrity monitoring/protection policy Implementing security policies with KubeArmor Use case 1: Web application hardening Use case 2: Crypto mining prevention Progressive policy implementation Integrating with AWS security services Benefits of integration Amazon CloudWatch integration Enhanced threat response with AWS GuardDuty integration Unified security dashboard Cleaning up Step 1: Remove KubeArmor policies Step 2: Uninstall KubeArmor components Step 3: Remove CloudWatch integration resources Step 4: Delete the EKS Auto Mode cluster Step 5: Remove IAM resources (if created) Step 6: Remove local configuration Security Best Practices to consider Conclusion Resources About the authors This post was written with Rahul Jadhav from Accuknox. As organizations adopt Amazon Elastic Kubernetes Service (Amazon EKS) Auto Mode for its streamlined Kubernetes cluster management, security remains a shared responsibility. Although EKS Auto Mode automates control plane operations, container runtime security needs more attention. In this post, we explore how KubeArmor , an open source container-aware security enforcement system, enhances the security posture of containerized workloads running on EKS Auto Mode clusters. Although EKS Auto Mode significantly streamlines cluster management by automating control plane and node operations, securing the workloads running within the cluster remains a critical user responsibility. Traditional security tools often struggle to provide granular visibility and control at the container runtime level, leaving potential gaps for sophisticated attacks. Container runtime security presents unique challenges that traditional security approaches don’t fully address.</description></item><item><title>Applying RBAC to databases on Kubernetes: Practical, real-world examples</title><link>https://kubermates.org/docs/2025-10-21-applying-rbac-to-databases-on-kubernetes-practical-real-world-examples/</link><pubDate>Tue, 21 Oct 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-21-applying-rbac-to-databases-on-kubernetes-practical-real-world-examples/</guid><description>Introduction Why RBAC is important for managing databases How RBAC works in Kubernetes-based platforms Enabling RBAC Why Everest implements its own RBAC layer? Inside RBAC policy (Everest Example) Common RBAC roles in practice Validating and testing policies Best practices for RBAC in stateful workloads Conclusion Posted on October 21, 2025 by Edith Puclla, Percona CNCF projects highlighted in this post Role-Based Access Control (RBAC) is one of the most important security features in any cloud native platform. It determines who can do what inside the Kubernetes Cluster , helping teams give the right access to the right people, keep systems safer, and make permissions easy to manage. Learn how to apply Role-Based Access Control (RBAC) to manage database workloads safely and effectively on Kubernetes. Using Percona Everest as an example, this post demonstrates how to implement roles for developers, DBAs, and operators — with step-by-step policies and best practices. In the Kubernetes ecosystem, RBAC is usually discussed in the context of pods, nodes, or cluster-level resources, and when it comes to stateful workloads, especially databases, RBAC becomes even more critical. Databases carry sensitive data, backups, and credentials, so you want developers, DBAs, and operators to have the right level of access, no more, no less. In this post, I’ll use a database management platform running on Kubernetes as a case study to walk through RBAC concepts. Specifically, I’ll use Percona Everest , an open-source platform for running and managing databases on Kubernetes. Everest provides a UI, CLI, and API to create and manage PostgreSQL , MongoDB , and MySQL clusters. I will show examples using Everest’s RBAC, but the principles apply to any Kubernetes database platform. Note: Kubernetes already includes RBAC to manage access to cluster resources like pods and nodes. Platform like Percona Everest build on top of this their own RBAC layer for database-specific actions (clusters, backups, restores), and this post focuses on that layer.</description></item><item><title>How Discover cut $1.4 million from its annual AWS budget in two game days</title><link>https://kubermates.org/docs/2025-10-21-how-discover-cut-1-4-million-from-its-annual-aws-budget-in-two-game-days/</link><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-21-how-discover-cut-1-4-million-from-its-annual-aws-budget-in-two-game-days/</guid><description>How Discover cut $1.4 million from its annual AWS budget in two game days 2 Days, $1.4 million Tagging workloads Technology and developer culture Red Hat OpenShift Container Platform | Product Trial About the author Alex Handy More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Do you remember shared cell phone plans? At the turn of the century, a whole family might share the same pool of mobile phone minutes, and it only took a few hours of idle chatter on the phone to drain the account of all remaining minutes of talk time for everyone. Well, a similar phenomenon is currently happening in the cloud today. Anyone who&amp;rsquo;s ever touched a public cloud has likely encountered an accidental overage payment. That&amp;rsquo;s a problem, but it&amp;rsquo;s even more problematic that it&amp;rsquo;s deceptively easy to overlook a single instance of this when you&amp;rsquo;re dealing in hundreds at a time. At Discover, the financial services company recently acquired by Capital One, developers found a way to solve their overage problems—and implement serious cost savings. It’s not surprising that a company offering credit cards would understand the value of cost optimization in the cloud. What is surprising, however, is that in those two days, they managed to cut their cloud bill by $1.4 million each year. Discover moved onto Red Hat OpenShift in 2022. Prior to that, the company had been attempting to solidify on CloudFoundry, the Platform-as-a-Service often promoted by Pivotal, VMware, and Broadcom. Craig Katz, OpenShift Director at Discover, said that Discover now uses OpenShift to manage its container infrastructure inside of Amazon Web Services. In order to migrate their existing 1,200 workloads to OpenShift and Amazon Web Services, the decision was made early on to focus on the speed and quality of the transition, and to push cost concerns to the side. As Katz put it, &amp;ldquo;You can choose two: fast, cheap, or good.</description></item><item><title>Learn Red Hat OpenShift Virtualization concepts from a VMware Admin Background with this learning path</title><link>https://kubermates.org/docs/2025-10-21-learn-red-hat-openshift-virtualization-concepts-from-a-vmware-admin-background-w/</link><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-21-learn-red-hat-openshift-virtualization-concepts-from-a-vmware-admin-background-w/</guid><description>Learn Red Hat OpenShift Virtualization concepts from a VMware Admin Background with this learning path Benefits of the VMware OpenShift Virtualization learning path How to use the path effectively A new chapter: Daily virtualization tasks Why this matters now Red Hat Learning Subscription | Product Trial About the authors Carolyn May Ryan Capra More like this Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share If you’re a virtual machine (VM) administrator who’s spent years managing VMs in vSphere and are now considering how Red Hat OpenShift Virtualization might fit into your operational world, we’ve built something just for you. The high-level guide to Red Hat OpenShift Virtualization as a VMware admin learning path is designed to bridge the learning gap many face when moving from traditional virtualization solutions to Kubernetes workflows. This blog provides a glimpse into what&amp;rsquo;s included in the path so you can take on OpenShift Virtualization with confidence. This learning path enables VMware experts to quickly become productive with OpenShift Virtualization by providing direct, chapter-based translations of key technical concepts. With the OpenShift Virtualization learning path, VM admins can: Translate familiar VMware concepts into OpenShift paradigms. Through storage, networking, compute, observability, and VM component chapters, the path offers direct “translations” of how functions work in OpenShift Virtualization. Learn fast with manageable chapters. Chapters focus on key themes, including storage, networking, compute, observability, and VM components, making up 30 minutes of content in total. Understand differences and similarities. Some tasks map directly; some require new ways of thinking. The path helps you understand when to take advantage of what you know and when to explore new concepts. Reduce risk in transition.</description></item><item><title>Overcoming the cost and complexity of AI inference at scale</title><link>https://kubermates.org/docs/2025-10-21-overcoming-the-cost-and-complexity-of-ai-inference-at-scale/</link><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-21-overcoming-the-cost-and-complexity-of-ai-inference-at-scale/</guid><description>Overcoming the cost and complexity of AI inference at scale The full-stack approach to AI performance Optimizing the AI model Optimizing the inferece runtime Enabling distributed, large-scale AI How Red Hat simplifies AI at scale Red Hat AI Learn more Get started with AI Inference About the author Brian Stevens More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Operationalizing AI models at scale is a critical challenge for IT leaders. While the initial cost of training a large language model (LLM) can be significant, the real and often underestimated expense is tied to inference. AI inference —the process of using a trained model to generate an output—is the most resource-intensive and costly part of an AI application, especially because it happens constantly during production. Inefficient inference can compromise an AI project&amp;rsquo;s potential return on investment (ROI) and negatively impact customer experience due to high latency. Effectively serving LLMs at scale requires a strategic, full-stack approach that addresses both the model itself and the serving runtime. A single approach is not enough. Achieving high performance and cost efficiency requires a dual focus—managing resource consumption and maximizing throughput. A strategic part of this approach is model compression, which reduces a model&amp;rsquo;s size and resource requirements without compromising accuracy. Quantization is a key technique for model optimization. It reduces the precision of a model&amp;rsquo;s numerical values—like its weights and activations—from standard 16-bit to lower formats such as 8-bit or 4-bit. This significantly shrinks the model’s memory footprint, allowing it to run on less hardware. Sparsity is another effective method, which makes models more efficient by removing unnecessary connections (weights).</description></item><item><title>7 Common Kubernetes Pitfalls (and How I Learned to Avoid Them)</title><link>https://kubermates.org/docs/2025-10-20-7-common-kubernetes-pitfalls-and-how-i-learned-to-avoid-them/</link><pubDate>Mon, 20 Oct 2025 08:30:00 -0700</pubDate><guid>https://kubermates.org/docs/2025-10-20-7-common-kubernetes-pitfalls-and-how-i-learned-to-avoid-them/</guid><description>7 Common Kubernetes Pitfalls (and How I Learned to Avoid Them) 1. Skipping resource requests and limits How to avoid it: 2. Underestimating liveness and readiness probes How to avoid it: 3. “We’ll just look at container logs” (famous last words) How to avoid it: 4. Treating dev and prod exactly the same How to avoid it: 5. Leaving old stuff floating around How to avoid it: 6. Diving too deep into networking too soon How to avoid it: 7. Going too light on security and RBAC How to avoid it: Final thoughts It’s no secret that Kubernetes can be both powerful and frustrating at times. When I first started dabbling with container orchestration, I made more than my fair share of mistakes enough to compile a whole list of pitfalls. In this post, I want to walk through seven big gotchas I’ve encountered (or seen others run into) and share some tips on how to avoid them. Whether you’re just kicking the tires on Kubernetes or already managing production clusters, I hope these insights help you steer clear of a little extra stress. The pitfall : Not specifying CPU and memory requirements in Pod specifications.</description></item><item><title>Adding distributed tracing to AI Gateway: My LFX mentorship journey</title><link>https://kubermates.org/docs/2025-10-20-adding-distributed-tracing-to-ai-gateway-my-lfx-mentorship-journey/</link><pubDate>Mon, 20 Oct 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-20-adding-distributed-tracing-to-ai-gateway-my-lfx-mentorship-journey/</guid><description>My background and preparation Application strategy: Contribute first, apply later Deep product experience Proactive problem solving Why this approach is important Project core: Adding distributed tracing to AI gateway The problem to solve Project goals Design approach Important lessons from the development process Critical testing strategy lessons Key takeaways Community collaboration Project results and value Implemented features How to experience the distributed tracing feature Personal gains Advice for students who want to participate in open source LFX mentorship application strategy Project execution advice Conclusion Posted on October 20, 2025 by Zhengke Zhou CNCF projects highlighted in this post In today’s rapidly evolving AI landscape, effectively monitoring and debugging AI Gateways has become a critical challenge. This article shares my complete experience through the LFX Mentorship program, where I added OpenTelemetry distributed tracing support to kgateway’s AI Gateway functionality. From application strategies for LFX Mentorship to challenges and insights during project implementation, I hope this provides a valuable reference for students who want to participate in open source projects. Before applying for LFX Mentorship, I had already been exposed to [OpenTelemetry](https://opentelemetry. io/) during my internship, gaining foundational knowledge in the observability domain. More importantly, I participated in the Jaeger community’s development work to add Clickhouse support for traces ([PR #6935](https://github. com/jaegertracing/jaeger/pull/6935)), which gave me practical experience with distributed tracing. These experiences made me feel that the LFX Mentorship project about AI Gateway distributed tracing was an excellent opportunity to deepen my learning and contribute to the open source community. I know everyone gets excited when they see a project they’re interested in and can’t wait to apply. I adopted a different strategy: first deeply understand the project, actively participate in the community, and then submit the application. Instead of rushing to submit my application, I first went to actually experience the product involved in the project: [AI Gateway](https://kgateway. dev/docs/ai/about/).</description></item><item><title>CKA Exam Verification Guide</title><link>https://kubermates.org/docs/2025-10-19-cka-exam-verification-guide/</link><pubDate>Sun, 19 Oct 2025 11:51:01 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-19-cka-exam-verification-guide/</guid><description>Exam Setup - The Speed Booster (Optional, But Highly Recommended!) Should you set this up? Good news about autocomplete: The time-saving aliases: Why these specific aliases? Configure vim for YAML editing: Author&amp;rsquo;s recommendation: 1. Storage (10% of the exam) StorageClass Verification Dynamic Provisioning Test PV/PVC Binding Access Modes &amp;amp; Reclaim Policies 2. Troubleshooting (30% of exam) Cluster Health Node Diagnostics Control Plane Components etcd Health Resource Monitoring Container Logs Service Troubleshooting 3. Workloads &amp;amp; Scheduling (15% of exam) Deployments Rolling Updates &amp;amp; Rollbacks ConfigMaps &amp;amp; Secrets HPA (Horizontal Pod Autoscaler) Probes Resource Limits Node Affinity &amp;amp; Taints Cluster Architecture (25% of exam) RBAC Infrastructure Preparation kubeadm Cluster Cluster Upgrade High Availability Helm &amp;amp; Kustomize Extension Interfaces Services &amp;amp; Networking (20% of exam) Pod Connectivity Service Validation Service Types Network Policies Ingress Gateway API DNS Imperative Commands &amp;amp; Time-Savers Generate YAML (Most Important!) kubectl explain (2x faster than docs!) Quick Creation Patterns Exam Workflow Step-by-step process: Time Management Real-World Troubleshooting Scenarios Scenario 1: Pod Stuck in Pending Scenario 2: CrashLoopBackOff Scenario 3: Service Not Accessible Scenario 4: Node NotReady Scenario 5: Deployment Rollout Stuck Scenario 6: PVC Stuck in Pending Common Verification Patterns Quick Reference Card Success Checklist Final Preparation Tips Common Mistakes to Avoid FAQ Kubernetes Architecture Explained: Nodes, Pods, and Clusters Quick Fixes for Common Kubernetes Issues Guide to AWS Certification What is Kubernetes? A Beginner’s Guide to Container Orchestration DevOps Tutorials 2025: Step-by-Step Learning Resources for Beginners Ubuntu: Set Timezone How to Enable SSH on Ubuntu Certifications in DevOps: Which Are Worth Your Time in 2025? What is DevOps? Setup: Use aliases ( k , $do , $now ) only if you’ve practiced them. k $do $now YAML Tip: Generate with &amp;ndash;dry-run=client -o yaml ; never write from scratch. &amp;ndash;dry-run=client -o yaml Docs Shortcut: Use kubectl explain , not web docs. kubectl explain Verify Everything: Always get , describe , logs , and check Events. get describe logs Storage: Default StorageClass + PV/PVC binding must work. Troubleshooting: Start from nodes → system pods → workloads. RBAC &amp;amp; Security: Test permissions with kubectl auth can-i. kubectl auth can-i Upgrade Flow: Control plane first, then kubelet restart. Networking: Endpoints tell truth; check DNS/CoreDNS if service fails.</description></item><item><title>Blog: Spotlight on Policy Working Group</title><link>https://kubermates.org/docs/2025-10-18-blog-spotlight-on-policy-working-group/</link><pubDate>Sat, 18 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-18-blog-spotlight-on-policy-working-group/</guid><description>Spotlight on Policy Working Group Introduction About Working Groups Policy WG Challenges (Note: The Policy Working Group has completed its mission and is no longer active. This article reflects its work, accomplishments, and insights into how a working group operates. ) In the complex world of Kubernetes, policies play a crucial role in managing and securing clusters. But have you ever wondered how these policies are developed, implemented, and standardized across the Kubernetes ecosystem? To answer that, let’s take a look back at the work of the Policy Working Group. The Policy Working Group was dedicated to a critical mission: providing an overall architecture that encompasses both current policy-related implementations and future policy proposals in Kubernetes. Their goal was both ambitious and essential: to develop a universal policy architecture that benefits developers and end-users alike. Through collaborative methods, this working group strove to bring clarity and consistency to the often complex world of Kubernetes policies. By focusing on both existing implementations and future proposals, they ensured that the policy landscape in Kubernetes remains coherent and accessible as the technology evolves. This blog post dives deeper into the work of the Policy Working Group, guided by insights from its former co-chairs: Jim Bugwadia Poonam Lamba Andy Suderman Interviewed by Arujjwal Negi. These co-chairs explained what the Policy Working Group was all about. Hello, thank you for the time! Let’s start with some introductions, could you tell us a bit about yourself, your role, and how you got involved in Kubernetes? Jim Bugwadia : My name is Jim Bugwadia, and I am a co-founder and the CEO at Nirmata which provides solutions that automate security and compliance for cloud-native workloads. At Nirmata, we have been working with Kubernetes since it started in 2014.</description></item><item><title>Spotlight on Policy Working Group</title><link>https://kubermates.org/docs/2025-10-18-spotlight-on-policy-working-group/</link><pubDate>Sat, 18 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-18-spotlight-on-policy-working-group/</guid><description>Spotlight on Policy Working Group Introduction About Working Groups Policy WG Challenges (Note: The Policy Working Group has completed its mission and is no longer active. This article reflects its work, accomplishments, and insights into how a working group operates. ) In the complex world of Kubernetes, policies play a crucial role in managing and securing clusters. But have you ever wondered how these policies are developed, implemented, and standardized across the Kubernetes ecosystem? To answer that, let&amp;rsquo;s take a look back at the work of the Policy Working Group. The Policy Working Group was dedicated to a critical mission: providing an overall architecture that encompasses both current policy-related implementations and future policy proposals in Kubernetes. Their goal was both ambitious and essential: to develop a universal policy architecture that benefits developers and end-users alike. Through collaborative methods, this working group strove to bring clarity and consistency to the often complex world of Kubernetes policies. By focusing on both existing implementations and future proposals, they ensured that the policy landscape in Kubernetes remains coherent and accessible as the technology evolves. This blog post dives deeper into the work of the Policy Working Group, guided by insights from its former co-chairs: Jim Bugwadia Poonam Lamba Andy Suderman Interviewed by Arujjwal Negi. These co-chairs explained what the Policy Working Group was all about. Hello, thank you for the time! Let’s start with some introductions, could you tell us a bit about yourself, your role, and how you got involved in Kubernetes? Jim Bugwadia : My name is Jim Bugwadia, and I am a co-founder and the CEO at Nirmata which provides solutions that automate security and compliance for cloud-native workloads. At Nirmata, we have been working with Kubernetes since it started in 2014.</description></item><item><title>Extending EKS with Hybrid Nodes: IAM Roles Anywhere and HashiCorp Vault</title><link>https://kubermates.org/docs/2025-10-17-extending-eks-with-hybrid-nodes-iam-roles-anywhere-and-hashicorp-vault/</link><pubDate>Fri, 17 Oct 2025 20:30:49 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-17-extending-eks-with-hybrid-nodes-iam-roles-anywhere-and-hashicorp-vault/</guid><description>Extending EKS with Hybrid Nodes: IAM Roles Anywhere and HashiCorp Vault Solution overview Prerequisites PKI architecture IAM Roles Anywhere configuration Vault certificate management EKS Hybrid Nodes configuration Results Cleanup Conclusion About the author Amazon EKS Hybrid Nodes allows businesses to flexibly make use of compute resources outside of AWS by extending an Amazon Elastic Kubernetes Service (Amazon EKS) data plane beyond the AWS Cloud boundary. Use cases for EKS Hybrid Nodes include businesses who have goals or requirements focusing on data sovereignty, low latency communication, and government or industry regulations. In this blog post, we’ll explore how to use AWS Identity and Access Management (IAM) Roles Anywhere , supported by HashiCorp Vault PKI , to facilitate joining EKS Hybrid Nodes to an Amazon EKS Cluster. When a node joins an EKS cluster, it uses metadata from the cluster – such as the cluster certificate bundle – to authenticate. Permission to retrieve this metadata is granted by IAM via the eks:DescribeCluster operation, which can be attached to an IAM Role via an IAM Policy. eks:DescribeCluster Since EKS Hybrid Nodes reside outside of AWS, they cannot inherit IAM Policies directly, and a different mechanism is required to retrieve the cluster certificate bundle. One recommended option is to use AWS Systems Manager (SSM) to provide nodes with temporary IAM credentials and permissions, including eks:DescribeCluster. Another option to accomplish the same outcome is to make use of an existing Public Key Infrastructure (PKI) and AWS IAM Roles Anywhere , which will be the focus of this blog post. eks:DescribeCluster IAM Roles Anywhere supports temporary credential validity periods from the default value of one hour up to a maximum of twelve hours. For this solution, you should have the following prerequisites: An AWS account An Amazon EKS Cluster ( prerequisites ) One or more Linux servers that will become EKS Hybrid Nodes ( compatibility ) A HashiCorp Vault Server or Cluster Figure 1 – Vault PKI Architecture Diagram If you’re already using HashiCorp Vault to manage secrets and protect sensitive data, then you already have a PKI! Vault natively supports PKI as part of its secrets engine. All you must do is to enable it. vault secrets enable pki vault secrets enable pki Because you’ll establish a Trust between IAM Roles Anywhere (IAM-RA) and the Vault Certificate Authority (CA), it is important to be aware that the default Time To Live (TTL) for the Vault CA is 30 days.</description></item><item><title>Why Autonomous Infrastructure is the future: From intent to self-operating systems</title><link>https://kubermates.org/docs/2025-10-17-why-autonomous-infrastructure-is-the-future-from-intent-to-self-operating-system/</link><pubDate>Fri, 17 Oct 2025 18:03:45 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-17-why-autonomous-infrastructure-is-the-future-from-intent-to-self-operating-system/</guid><description>When AI development meets AI operations Beyond automation: The three phases of infrastructure evolution Phase 1: Manual infrastructure (2010-2020) Phase 2: Automated infrastructure (2020-2025) Phase 3: Autonomous infrastructure (2025-2030) The deterministic + probabilistic convergence The end of infrastructure as a limiting factor From reactive fire-fighting to proactive intelligence Theulti-agent infrastructure lifecycle What makes infrastructure truly autonomous The five levels of infrastructure autonomy Beyond today: The infrastructure AGI vision Building the future together Making the transition: From vision to reality Posted on October 17, 2025 by Asif Awan, StackGen Executive summary: We’re at an inflection point where AI-generated code meets AI-managed infrastructure, creating truly self-sustaining systems. This convergence transforms infrastructure from static pipelines to autonomous systems that build, govern, heal, and optimize themselves. Organizations have a narrow window to establish competitive advantage through autonomous infrastructure adoption—particularly in technology-driven industries where infrastructure agility directly impacts market responsiveness. AI is increasingly being applied in infrastructure, moving from assistive tools toward systems that can make autonomous decisions. This shift builds on earlier conversations in the community around intent-to-infrastructure approaches , such as those highlighted at PlatformCon this year, where platform engineers discussed using AI to address bottlenecks. In this post , we describe one implementation of that idea: infrastructure that can operate with minimal human intervention, guided by intent-driven inputs. We’re approaching an inflection point where AI-generated code meets AI-managed infrastructure. This convergence is transforming infrastructure from static pipelines to intelligent systems that can generate, adapt, and evolve continuously. While AI has accelerated development tasks by 2-3x , infrastructure bottlenecks continue to drain $2.5 million annually per 100 developers in lost productivity. But this isn’t just about efficiency—it’s about a fundamental transformation in how we conceive of infrastructure itself. The shift from traditional automation to autonomous systems represents the next phase of infrastructure evolution. Where conventional automation manages what already exists, autonomous infrastructure builds, governs, heals, and optimizes itself.</description></item><item><title>KubeCon + CloudNativeCon North America 2025 Co-Located Event Deep Dive: Platform Engineering Day</title><link>https://kubermates.org/docs/2025-10-17-kubecon-cloudnativecon-north-america-2025-co-located-event-deep-dive-platform-en/</link><pubDate>Fri, 17 Oct 2025 13:55:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-17-kubecon-cloudnativecon-north-america-2025-co-located-event-deep-dive-platform-en/</guid><description>Who will get the most out of attending this event? What is new and different this year? What will the day look like? Should I do any homework first? Posted on October 17, 2025 by Co-Chairs | Paula Kennedy, Stacey Potter, and Matt Menzenski This marks the fourth edition of Platform Engineering Day, following successful events in Paris (2024), Salt Lake City (2024), and London (2025). We’re excited to continue exploring case studies and deep technical dives as platform engineering practices mature. In Atlanta, we’ll continue the expanded two-track format introduced in London earlier this year, giving attendees even more opportunities to explore topics most relevant to them. This event is really aimed at anyone who is building platforms, using platforms or trying to learn more about platform engineering, wherever they are on their journey. There will be content from end-user organizations and updates from the Platform Engineering Community Group, ensuring something valuable for attendees at every level. This year we’re aiming for content that goes deeper into the topics of platform maturity and more use cases of lessons that organisations have learnt. Expect even more real-world insights from practitioners who are shaping the next phase of platform engineering. The day will feature two parallel tracks covering topics such as developer experience, API-driven infrastructure, and the evolving future of platform engineering. We’ll start and end the day together, giving attendees a shared sense of community while still allowing freedom to choose sessions that match their interests. No pre-work is needed—just bring your curiosity! We have an incredible lineup of speakers who will be happy to answer questions and share insights. We can’t wait to see you there—join us for an inspiring day of learning, connection, and platform innovation. Share.</description></item><item><title>Friday Five — October 17, 2025</title><link>https://kubermates.org/docs/2025-10-17-friday-five-october-17-2025/</link><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-17-friday-five-october-17-2025/</guid><description>Friday Five — October 17, 2025 Red Hat Brings Distributed AI Inference to Production AI Workloads with Red Hat AI 3 SiliconANGLE - Red Hat AI 3 targets production inference and agents Beyond the model: Why intelligent infrastructure is the next AI frontier Spotlighting partners who put innovation into action Channel Futures - NetApp Updates Enterprise Storage Lineup, Teams with Red Hat About the author Red Hat Corporate Communications More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Introducing Red Hat AI 3, a unified, open enterprise AI platform that enables distributed LLM inference with llm-d across hybrid environments and sets the stage for scalable agentic AI. Learn more Red Hat&amp;rsquo;s Joe Fernandes discusses Red Hat AI 3, with enhancements across Red Hat AI Inference Server, Red Hat Enterprise Linux AI, and Red Hat OpenShift AI, into one platform. Learn more Scaling AI from a successful LLM proof of concept to production-grade, distributed inference requires “intelligent infrastructure”—an AI-aware control plane that manages workloads, optimizes resources, and supports llm-d for efficient, scalable enterprise AI. Learn more Designed to celebrate partners who are driving innovation and delivering exceptional results for our joint customers across key focus areas such as AI, virtualization, and automation, the Ecosystem Innovation Awards nomination window is officially open. Submit today before December 12. Learn more NetApp has unveiled several new updates to its portfolio of enterprise storage offerings for businesses of any size, and a new partnership with Red Hat to help customers and partners streamline and accelerate enterprise application development and management. Learn more Red Hat is the world’s leading provider of enterprise open source software solutions, using a community-powered approach to deliver reliable and high-performing Linux, hybrid cloud, container, and Kubernetes technologies. Red Hat helps customers integrate new and existing IT applications, develop cloud-native applications, standardize on our industry-leading operating system, and automate, secure, and manage complex environments. Award-winning support, training, and consulting services make Red Hat a trusted adviser to the Fortune 500. As a strategic partner to cloud providers, system integrators, application vendors, customers, and open source communities, Red Hat can help organizations prepare for the digital future. What is agentic AI? Article Predictive AI vs. generative AI Article Top considerations for building a production-ready AI/ML environment E-book Generative AI, the Ansible way Video Innovate and transform with a modern application platform E-book The latest on IT automation for tech, teams, and environments Updates on the platforms that free customers to run AI workloads anywhere Explore how we build a more flexible future with hybrid cloud The latest on how we reduce risks across environments and technologies Updates on the platforms that simplify operations at the edge The latest on the world’s leading enterprise Linux platform Inside our solutions to the toughest application challenges The future of enterprise virtualization for your workloads on-premise or across clouds.</description></item><item><title>Optimizing energy efficiency on Red Hat Enterprise Linux</title><link>https://kubermates.org/docs/2025-10-17-optimizing-energy-efficiency-on-red-hat-enterprise-linux/</link><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-17-optimizing-energy-efficiency-on-red-hat-enterprise-linux/</guid><description>Optimizing energy efficiency on Red Hat Enterprise Linux Testing methodology Measuring network throughput Measuring CPU utilization Computational efficiency Network-intensive process operating regimes Single process Multi-process Saturated Optimizing workloads for hardware locality NUMA-aware resource allocation for network-intensive processes Pinning interrupts and processes Hardware environment in the test Evaluating computer system power consumption External power meter System utility Platform management Normalizing performance against power consumption A note about power supplies Mitigation strategies Use lower-rated power supplies Enable hot standby mode Measure DC output directly Benefits of considering PSU efficiency Conclusion Red Hat Ansible Automation Platform | Product Trial About the authors Adam Okuliar Otto Šabart More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The energy efficiency of data centers and high-performance systems is becoming increasingly important. Energy costs can make up a substantial portion of the operating expenditure (OPEX) budget, potentially serving as a critical factor in the success or failure of a business. This article breaks down the issue of energy efficiency in modern server platforms, presenting tools and an example methodology for measuring power consumption and performance. As a case study, we present measurement results from two server platforms with different CPU architectures. We will emphasize network performance (including throughput measurements), CPU utilization, and power consumption, with a particular emphasis on achieving the highest possible throughput per watt of energy consumed. Evaluating the relationship between a computer’s performance and its power requirements can be broken down into three key steps: Assess the computer’s performance Measure the computer’s power requirements Normalize performance relative to power consumption We measure system performance in terms of gigabits per second of achievable throughput per physical CPU core. It is important to note that with modern, high-speed (100 Gbps and above) Ethernet adapters, it&amp;rsquo;s virtually impossible for a single user-space process to utilize all available bandwidth. Moreover, network throughput is only part of the performance story. Handling network traffic ideally consumes minimal resources, leaving the majority of the system available for creating or processing the data transmitted over the network. Ideally, the CPU is saturated with as few cores as possible. To quantify these requirements, we monitor the following metrics: Maximum achievable throughput in gigabits per second. Total CPU utilization, measured in cores.</description></item><item><title>New Amazon EKS Auto Mode features for enhanced security, network control, and performance</title><link>https://kubermates.org/docs/2025-10-16-new-amazon-eks-auto-mode-features-for-enhanced-security-network-control-and-perf/</link><pubDate>Thu, 16 Oct 2025 20:36:39 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-16-new-amazon-eks-auto-mode-features-for-enhanced-security-network-control-and-perf/</guid><description>New Amazon EKS Auto Mode features for enhanced security, network control, and performance Recent feature highlights Performance and capacity management Advanced networking capabilities Security and compliance Why these features matter Getting started with EKS Auto Mode Conclusion About the authors When we launched Amazon Elastic Kubernetes Service (Amazon EKS) Auto Mode at Amazon Web Services (AWS) re:Invent 2024, we introduced a transformative approach to Kubernetes cluster management on AWS. EKS Auto Mode fully automates Kubernetes cluster management for compute, storage, and networking, so that you can focus on building applications that drive innovation rather than managing cluster infrastructure. EKS Auto Mode addresses a fundamental challenge: although users choose Amazon EKS for the open standards of Kubernetes combined with the capabilities of AWS, they often struggle with the operational complexity of production-grade infrastructure. Managing compute provisioning, maintaining essential plugins for autoscaling, storage, and networking, handling OS patches, and optimizing resource utilization all require significant ongoing effort that takes focus away from the kind of innovation that their users want. EKS Auto Mode handles the complete Kubernetes cluster infrastructure lifecycle through five key capabilities: Automated compute management : Intelligent node provisioning, scaling, and lifecycle management that adapts to workload demands without manual intervention. Integrated networking and storage : Pre-configured Amazon Virtual Private Cloud (Amazon VPC) CNI, Amazon Elastic Block Store (Amazon EBS ) CSI drivers, and load balancer controllers that work together seamlessly from day one Secure and optimized compute runtime: Enforced encryption, automated patching, and secure access management that maintains compliance without operational overhead. Built-in cost optimization : Automatic instance type selection, Spot integration, and right-sizing that continuously optimizes spend based on actual usage patterns. Simplified cluster lifecycle : One-click cluster creation and updates that handle the complexity of Kubernetes version management and component compatibility This foundation has enabled many customers, from large enterprises to scaling startups, to shift cluster operations to AWS, freeing up valuable time and energy on their teams while retaining full Kubernetes interoperability. Teams can deploy production workloads faster, achieve better performance outcomes, and reduce infrastructure costs without needing specialized Kubernetes expertise. Since launch, we’ve been expanding Auto Mode’s capabilities based on our customers’ feedback and real-world usage patterns. This post covers the features and improvements we’ve delivered to address specific operational challenges and extend the functionality of EKS Auto Mode for diverse workloads. Over the past 10 months, we’ve delivered a steady stream of enhancements that expand the capabilities of EKS Auto Mode and address diverse customer requirements: Optimized node lifecycle management : EKS Auto Mode has significantly improved cluster scaling performance through several key enhancements.</description></item><item><title>Busting Cyber Resilience Myths with VMware Cloud Foundation</title><link>https://kubermates.org/docs/2025-10-16-busting-cyber-resilience-myths-with-vmware-cloud-foundation/</link><pubDate>Thu, 16 Oct 2025 19:07:43 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-16-busting-cyber-resilience-myths-with-vmware-cloud-foundation/</guid><description>Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Busting Cyber Resilience Myths with VMware Cloud Foundation VMware at KubeCon North America 2025: Innovation at the Core of Cloud Native VCF Breakroom Chats Episode 64 - AI Made Simple: Empowering VI Admins to Offer Private AI with VCF 9.0 “What you don’t know, won’t hurt you” is a dangerous assumption when it comes to cyber resilience. Lack of awareness of what is needed to enforce successful outcomes is the main reason why organizations continue to struggle, even as they bolster their defenses after being victims to cyberattacks. Cyber resilience is built into the VCF platform to enable customers to confidently stop, detect, mitigate and recover from any threat that could compromise their ability to operate. Our approach to cyber resilience spans across out-of-the-box hardened infrastructure, strong distributed lateral security and confident cyber recovery. Combined, these capabilities deliver unparalleled platform-level value that removes the need for manual integration of point products from different vendors, simplifies lifecycle management and ultimately delivers superior protection for our customers’ mission-critical data. Our newly-released Cyber Resilience Mythbusters video series uncovers key insights to help our customers assess the maturity of their current strategy and know what key questions to ask to enforce true cyber resilience across all levels of the infrastructure. Infrastructure Hardening Myths, Debunked Lateral Security Myths, Debunked Cyber Recovery Myths, Debunked With all this in mind, ask yourself – is your organization truly cyber resilient? And are you confident it will continue to be as the threat landscape evolves? Cyber resilience is a never-ending marathon, and we want to support you every step of the way. If you’d like to know more about how we are engineering VMware Cloud Foundation to deliver unparalleled, platform-based cyber resilience to futureproof the way organizations worldwide operate, visit our webpage or reach out to your Sales or Partner representative.</description></item><item><title>Kubernetes Architecture Explained: Nodes, Pods, and Clusters</title><link>https://kubermates.org/docs/2025-10-16-kubernetes-architecture-explained-nodes-pods-and-clusters/</link><pubDate>Thu, 16 Oct 2025 16:53:51 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-16-kubernetes-architecture-explained-nodes-pods-and-clusters/</guid><description>Introduction: Understanding the Heart of Kubernetes What Is Kubernetes Architecture? (Big Picture Overview) The Kubernetes Cluster - The Foundation How the Cluster Works Key Traits of a Kubernetes Cluster Why the Cluster Matters Control Plane Components - The Brain of Kubernetes 1. API Server - The Front Door of the Cluster 2. etcd - The Cluster’s Memory 3. Scheduler - The Decision Maker 4. Controller Manager - The Cluster’s Automation Engine How These Components Work Together Worker Node Components - The Hands of Kubernetes 1. Kubelet - The Node’s Local Agent 2. Kube-Proxy - The Network Manager 3. Container Runtime - The Engine Running the Containers How Worker Nodes Fit into Kubernetes Architecture Why Worker Nodes Matter Pods - The Smallest Unit of Deployment What Is a Pod? How Pods Work Pods and Networking Pods, Nodes, and the Bigger Picture Why Pods Matter How Kubernetes Works - Step-by-Step Workflow Step 1: Defining the Desired State Step 2: API Server Processes the Request Step 3: Scheduler Assigns Pods to Nodes Step 4: Kubelet Creates and Monitors Pods Step 5: Kube-Proxy Manages Networking Step 6: Controller Manager Ensures Desired State Step 7: Scaling, Updates, and Maintenance In Short: How Kubernetes Works Master Node vs Worker Node - Key Differences Master Node - The Control Center Worker Node - The Execution Layer Master Node vs Worker Node - Comparison Table How They Work Together Why This Design Matters Visualizing Kubernetes Architecture (KodeKloud Tip) Visualizing the Cluster Structure Learn by Seeing and Doing FAQs Quick Fixes for Common Kubernetes Issues Guide to AWS Certification What is Kubernetes? A Beginner’s Guide to Container Orchestration DevOps Tutorials 2025: Step-by-Step Learning Resources for Beginners Ubuntu: Set Timezone How to Enable SSH on Ubuntu Certifications in DevOps: Which Are Worth Your Time in 2025? What is DevOps? Understand the difference between master node vs worker node and their unique roles. Explore how Kubernetes components like API Server, Scheduler, and Kubelet communicate. Discover how Kubernetes works to deploy, scale, and self-heal applications automatically. Visualize the entire Kubernetes cluster architecture - nodes, Pods, and communication flow. Master the fundamentals of container orchestration through KodeKloud’s free Kubernetes labs.</description></item><item><title>Efficient autoscaling: Keeping performance, reliability, and cost in mind with open source projects</title><link>https://kubermates.org/docs/2025-10-16-efficient-autoscaling-keeping-performance-reliability-and-cost-in-mind-with-open/</link><pubDate>Thu, 16 Oct 2025 16:30:52 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-16-efficient-autoscaling-keeping-performance-reliability-and-cost-in-mind-with-open/</guid><description>Kubernetes autoscaling comes with trade-offs Performance Cost Reliability It’s all about trade-offs Author bio Posted on October 16, 2025 by Christian Melendez, AWS CNCF projects highlighted in this post During ContainerDays in Hamburg, Kelsey Hightower posed a simple but powerful question: “Why are we still talking about containers?” His point resonated with me deeply — even in the AI era, the cloud-native community is still refining the fundamentals of container orchestration, scalability, and efficiency. In this post, I’ll explore how open source projects like KEDA and Karpenter can help you balance performance, reliability, and cost in Kubernetes autoscaling. When we talk about Kubernetes autoscaling , it’s not just about adding replicas or nodes when demand grows and removing them when it shrinks. You have to balance performance , reliability , and cost — three forces that constantly pull against each other. The way I like to think about these three pillars is as a triangle , like in the following figure. These three pillars create natural tension. Before your application’s performance degrades, you need to add resources — which increases cost. To save on cost, you scale down resources — which can impact reliability. So how do we find the right balance? Let’s explore tools and recommendations for each pillar. Before you start scaling, you need to understand what truly impacts the performance of your application — what matters to your users and stakeholders. Most of the time, they don’t care about CPU or memory usage directly. These may be indicators, but they don’t always tell the full story.</description></item><item><title>Kyverno vs Kubernetes policies: How Kyverno complements and completes Kubernetes policy types</title><link>https://kubermates.org/docs/2025-10-16-kyverno-vs-kubernetes-policies-how-kyverno-complements-and-completes-kubernetes-/</link><pubDate>Thu, 16 Oct 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-16-kyverno-vs-kubernetes-policies-how-kyverno-complements-and-completes-kubernetes-/</guid><description>How Kyverno extends and integrates with Kubernetes policies Introduction 1. Applying policies on existing resources 2. Reapplying policies on changes 3. Applying policies pff cluster (shift-left) 4. Testing policy as code 5. Reporting policy results 6. Managing fine-grained policy exceptions 7. Complex policy logic 8. Image verification 9. Policy-based automation 10. Kyverno everywhere Conclusion Posted on October 16, 2025 by Jim Bugwadia, Nirmata CNCF projects highlighted in this post Originally posted on Nirmata. com on October 1, 2025 With the addition of ValidatingAdmissionPolicy and MutatingAdmissionPolicy in Kubernetes, do you still need Kyverno? This post answers the question by providing ten reasons why Kyverno is essential even when you are using Kubernetes policy types.</description></item><item><title>From Spend to Value: The Advantage of Private Cloud</title><link>https://kubermates.org/docs/2025-10-16-from-spend-to-value-the-advantage-of-private-cloud/</link><pubDate>Thu, 16 Oct 2025 02:35:46 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-16-from-spend-to-value-the-advantage-of-private-cloud/</guid><description>Why Private Cloud ? 1. Predictable Costs That Don’t Spiral 2. The Repatriation Wave 3. More Value, Built Into a Platform 4. Improved Compliance and Control Value of VMware Cloud Foundation 1. Deploy a unified Private Cloud Platform 2. Automation and Operations to reduce labor 3. Optimize Costs 4. Stop Paying for Every Feature Added 5. Demonstrate ROI, Track Progress 6. Reinvest in Innovation Conclusion: Scaling Smart with VCF Discover more from VMware Cloud Foundation (VCF) Blog Related Articles From Spend to Value: The Advantage of Private Cloud VCF Breakroom Chats Episode 65: VMware Cloud Foundation with Ubuntu OS - Smarter Choice for Modern Apps VCF Breakroom Chats Episode 64 - AI Made Simple: Empowering VI Admins to Offer Private AI with VCF 9.0 Cloud is the cornerstone of enterprise IT strategy, delivering agility, speed, and scalability that transforms how businesses innovate. As organizations mature, many are now gravitating toward private and hybrid models for new initiatives.</description></item><item><title>Simplified patching with Red Hat Enterprise Linux and Red Hat Insights</title><link>https://kubermates.org/docs/2025-10-16-simplified-patching-with-red-hat-enterprise-linux-and-red-hat-insights/</link><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-16-simplified-patching-with-red-hat-enterprise-linux-and-red-hat-insights/</guid><description>Simplified patching with Red Hat Enterprise Linux and Red Hat Insights Define, instruct, and patch What is a content template? Connect to Red Hat Insights Red Hat Enterprise Linux | Product trial About the authors Shane McDowell Anthony Johnson More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The most common task facing system administrators is patching infrastructure. It&amp;rsquo;s time consuming, it requires coordination with application teams and stakeholders, and it often must happen in segments over time. These complications make it difficult to maintain environmental consistency, which in turn can lead to instability, performance issues, and more time spent by operations staff. Using Red Hat Insights content templates to patch Red Hat Enterprise Linux (RHEL) helps limit the complexity of these activities while also increasing consistency across an IT estate. Using Red Hat Insights, the process is simplified and more prescriptive: Define the software you want in your environment, instruct your Red Hat Insights-registered systems to pull updates from that set of software, and patch. It doesn&amp;rsquo;t matter whether you patch your RHEL system today or three weeks from now, you can have greater confidence that the updates installed in your environment are the ones you want, and nothing more. For each subsequent patch cycle, you update your definition, which automatically instructs your systems to pull updates from this new set of updates, and you patch. In Red Hat Insights, this set of software is called a content template. You can create a content template on the Hybrid Cloud Console content template page. A content template is defined by a date and a set of package repositories, which can include those provided by Red Hat, partners, or custom repositories created by you. Any system you associate with that content template is automatically limited to the set of updates from those repositories on the date you&amp;rsquo;ve selected. You can then update your systems using the schedule and method of your choosing, knowing that the end result will be systems with a more predictable and consistent set of software installed.</description></item><item><title>How NRP Scales Global Scientific Research with Calico</title><link>https://kubermates.org/docs/2025-10-15-how-nrp-scales-global-scientific-research-with-calico/</link><pubDate>Wed, 15 Oct 2025 15:53:20 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-15-how-nrp-scales-global-scientific-research-with-calico/</guid><description>Challenges Complex Network Visibility and Debugging Balancing Performance with Granular Security at Scale Managing Advanced Heterogeneous Networking Solutions Enhanced Observability with Calico Telemetry Performant Network Policy Enforcement Flexible Kubernetes Networking Results Customer Perspective What’s Next The National Research Platform (NRP) operates a globally distributed, high-performance computing and networking environment, with an average of 15,000 pods across 450 nodes supporting more than 3,000 scientific project namespaces. With its head node in San Diego, NRP connects research institutions and data centers worldwide via links ranging from 10 to 400 Gbps, serving more than 5,000 users in 70+ locations. Non-profit company Uses Calico Open Source NRP is a partnership of more than 50 institutions, led by researchers at UC San Diego, University of Nebraska-Lincoln, and Massachusetts Green High Performance Computing Center and includes contributions by the National Science Foundation, the Department of Energy, the Department of Defense, and many research universities and R&amp;amp;E networking organizations in the US and around the world. NRP needed a way to diagnose connectivity problems across globally distributed storage nodes. Frequent changes to edge network configurations, ACLs, firewalls, and static routes caused blocked ports, forcing manual troubleshooting with tools such as nmap and iperf. This process slowed down root-cause analysis and problem resolution. Scientific workflows demanded maximum throughput over 100/400 Gbps links and jumbo frames. Traditional host firewalls introduced unacceptable performance penalties, preventing researchers from mounting data and forcing the team to disable them. NRP required a centralized, high-performance network security solution that could enforce fine-grained policies at scale without degrading throughput. NRP’s workflows relied on advanced capabilities such as Layer 2 paths over WAN using ESnet Sense and AutoGOLE, dual-stack IPv4/IPv6 support, and experiments with multipath BGP. They also needed integration with specialized hardware such as FPGAs and smartNICs for P4 packet processing. Managing this mix of protocols, services, and hardware required a flexible CNI that could support complex, multi-layer orchestration.</description></item><item><title>Automating stateful apps with Kubernetes Operators</title><link>https://kubermates.org/docs/2025-10-15-automating-stateful-apps-with-kubernetes-operators/</link><pubDate>Wed, 15 Oct 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-15-automating-stateful-apps-with-kubernetes-operators/</guid><description>What are Kubernetes Operators? Difference between traditional Kubernetes Controllers and Operators How Kubernetes Operators work 1. Set up a Custom Resource 2. Deploy the Operator into the Kubernetes cluster 3. Continuous monitoring of CR by the Operator. 4. Reconciliation loop: Operator compares desired vs actual state 5. Error handling Use cases of Kubernetes Operators Handling stateful apps Messaging systems Monitoring and logging stacks Automating infrastructure Why do we need Operators? Challenges without Operators Kubernetes Operators benefits Conclusion Posted on October 15, 2025 by Keval Bhogayata, Middleware CNCF projects highlighted in this post Member post originally published on the Middleware blog by Keval Bhogayata , covering Automating Stateful Apps with Kubernetes Operators. If you’ve ever had issues with scaling databases or automating upgrades in Kubernetes, Operators can help by saving you time and effort. Handling complex Kubernetes applications like databases, message queues, and distributed systems can be really difficult. Kubernetes handles simple workloads well, while big apps suffer with failover, scaling, backups, and automated updates. These activities often demand operational expertise. If you’re new to container orchestration, you may want to start by understanding the difference between Kubernetes vs Docker This is where you need Kubernetes Operators, which we’ll cover in this article.</description></item><item><title>Automation unlocks 5G's full potential: One New Zealand's journey</title><link>https://kubermates.org/docs/2025-10-15-automation-unlocks-5g-s-full-potential-one-new-zealand-s-journey/</link><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-15-automation-unlocks-5g-s-full-potential-one-new-zealand-s-journey/</guid><description>Automation unlocks 5G&amp;rsquo;s full potential: One New Zealand&amp;rsquo;s journey One New Zealand&amp;rsquo;s journey to automated 5G Key takeaways Red Hat Ansible Automation Platform | Product Trial About the authors Volker Tegtmeyer Sam Sun Poohmipat Sripukdee (Art) More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The telecommunications (telco) industry stands at the precipice of a new era. With 5G networks rapidly expanding, service providers face both immense opportunities and significant challenges. The promise of 5G for ultra-fast speeds, low latency, and massive connectivity hinges on the ability to operationalize these complex networks with unprecedented efficiency and rigorous security measures. Automation is not just a buzzword in this landscape; it’s how to unlock the full value of 5G. Traditional manual processes simply cannot keep pace with the dynamic demands of modern networks. As network functions become increasingly containerized and cloud-native, the need for agile, consistent, and secure operations across hybrid and multicloud environments increases exponentially. Service providers want to invest in innovation to efficiently grow revenue, and automation is key to lowering costs and accelerating time to value on the journey to autonomous networks. Data and AI also deliver value across business and network aspects of telcos to mitigate risk, lower operational costs, and optimize resources. One New Zealand and Red Hat recently hosted a webinar featuring Sam Sun, Account Solution Architect at Red Hat, and Art Sripukdee, Principal Architect from One New Zealand. They shared a compelling story of how One New Zealand transformed its 5G vision into reality, achieving economies of scale, efficiency, and building a network designed for growth. One New Zealand, a leading telco provider, has consistently been at the forefront of mobile technology, from 2G to 5G, and recently launched text messaging over satellite. Their 5G rollout is not merely about new radio sites; it’s completely transforming network architecture.</description></item><item><title>From bottleneck to breakthrough: How Citizens Bank modernized integration with Red Hat OpenShift</title><link>https://kubermates.org/docs/2025-10-15-from-bottleneck-to-breakthrough-how-citizens-bank-modernized-integration-with-re/</link><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-15-from-bottleneck-to-breakthrough-how-citizens-bank-modernized-integration-with-re/</guid><description>From bottleneck to breakthrough: How Citizens Bank modernized integration with Red Hat OpenShift The adaptable enterprise: Why AI readiness is disruption readiness About the author Jaleh Reeves More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Tikhnadhi Kamlakshya , vice president and manager of software engineering at Citizens Bank, saw a clear challenge: his team had a critical disconnect between their modern Salesforce platform and their legacy loan processing system, Black Knight. This gap created a significant bottleneck, hindering the bank&amp;rsquo;s ability to scale its mortgage operations and forcing loan officers to spend hours manually entering data. The main issue stemmed from communication difficulties between the 2 essential systems they relied on for their tasks. On one side, there was the modern, agile Salesforce platform, the bank&amp;rsquo;s system for point of sale. On the other, the Black Knight system, a business-critical, legacy application that handled all loan processing and governance. The 2 systems couldn&amp;rsquo;t talk to each other, and with millions of records to process, the bank was facing a scalability crisis. &amp;ldquo;We were under enormous pressure,&amp;rdquo; Kamlakshya recalls. &amp;ldquo;We had successfully migrated to Salesforce, but the job wasn&amp;rsquo;t finished. The fact that Salesforce couldn&amp;rsquo;t talk to our legacy system was a curveball we hadn&amp;rsquo;t anticipated. &amp;quot; The search for a solution The team at Citizens Bank explored several integration tools, but none could handle the scale and authentication requirements of the project. The team was running out of options, and the pressure was mounting. That&amp;rsquo;s when Kamlakshya and his team explored Red Hat OpenShift.</description></item><item><title>Measure impact and unlock greater value with Red Hat Ansible Automation Dashboard</title><link>https://kubermates.org/docs/2025-10-15-measure-impact-and-unlock-greater-value-with-red-hat-ansible-automation-dashboar/</link><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-15-measure-impact-and-unlock-greater-value-with-red-hat-ansible-automation-dashboar/</guid><description>Measure impact and unlock greater value with Red Hat Ansible Automation Dashboard Smarter, data-driven decisions From IT wins to business proof No more budget battles Privacy and authentication Why it matters Learn more Red Hat Ansible Automation Platform | Product Trial About the author Courtney Cydylo More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share When IT teams adopt automation, the benefits are often felt right away: Faster deployments, fewer manual errors, and more time to innovate. But when it comes time to visualizing and proving that value to business leaders and other stakeholders, the story often falls short. That changes with the new automation dashboard, available with Red Hat Ansible Automation Platform 2.6. Automation dashboard delivers real-time, actionable insights to help guide your day-to-day operations. You gain greater visibility across the entire automation footprint, both direct and indirect nodes, helping identify over-utilization and under-utilization of your Ansible Automation Platform subscriptions. That means you can better optimize and right-size your automation deployments, while executives can more readily see efficiency gains translated into business outcomes. The automation dashboard acts as a business case generator that runs 24/7. It continuously measures: Job success rates : A clear picture of what’s running smoothly. Time savings : Hours returned to teams across the business. Return on investment : Actual dollars saved by automation. With exportable PDF and CSV reports, IT leaders and automation teams can show exactly how much automation is saving, whether that’s proving ROI to a CFO, sharing successes with stakeholders, or identifying opportunities to optimize. Renewal cycles often trigger tough budget conversations.</description></item><item><title>Open source and AI-assisted development: navigating the legal issues</title><link>https://kubermates.org/docs/2025-10-15-open-source-and-ai-assisted-development-navigating-the-legal-issues/</link><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-15-open-source-and-ai-assisted-development-navigating-the-legal-issues/</guid><description>Open source and AI-assisted development: navigating the legal issues Attribution and marking Copyright and licensing formalities Are AI tools “plagiarism machines”? AI-assisted contributions and the DCO Establishing trust Looking ahead The adaptable enterprise: Why AI readiness is disruption readiness About the authors Chris Wright Richard Fontana More like this Blog post Blog post Original podcast Original podcast Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share In our previous post in this series, we talked about how AI is beginning to change the way software is developed. In this follow-up, we focus on some of the main legal (or quasi-legal) issues that open source developers themselves have been raising regarding AI-assisted development. This isn’t a comprehensive overview of every legal issue connected to AI. We aren’t addressing, for example, customer concerns about compliance with AI regulations or liability issues relating to contracts for AI-powered products. Instead, we’re focusing on issues that are being actively debated inside open source communities. Our views on these issues reflect our commitment to responsible use of AI technologies and our “default to open” philosophy. We believe that collaborative and transparent approaches are the best ways to address these concerns constructively. Attribution is a core legal and cultural norm in open source. Licenses generally require you to preserve copyright and authorship notices, and to avoid misleading claims of authorship. AI-assisted development complicates this. Because AI systems are not considered “authors” under copyright law, there is technically no one to credit. But it would still be misleading for a developer to present substantial AI-generated output as purely their own work.</description></item><item><title>Running Slurm on Amazon EKS with Slinky</title><link>https://kubermates.org/docs/2025-10-14-running-slurm-on-amazon-eks-with-slinky/</link><pubDate>Tue, 14 Oct 2025 16:58:24 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-14-running-slurm-on-amazon-eks-with-slinky/</guid><description>Running Slurm on Amazon EKS with Slinky A primer on Slurm The Slinky Project The use case for Slinky Architecture overview of Slurm on EKS with Slinky Slinky Slurm cluster components Benefits of running Slurm on EKS with Slinky Alternatives for running Slurm on AWS AWS ParallelCluster AWS Parallel Computing Service Amazon SageMaker HyperPod Alternative Kubernetes native job schedulers Volcano Apache YuniKorn Kueue When Slurm on EKS is right for you About the authors When building an AI infrastructure stack for pre-training, fine-tuning, or inference workloads, both Slurm and Kubernetes can be used as compute orchestration platforms to meet the needs of different teams and address different stages of the AI development lifecycle. However, traditionally this would result in managing disparate clusters of accelerated compute capacity, potentially duplicating operational overhead and risking resource underuse. But what if you could deploy a Slurm cluster as a Kubernetes service to get the best of both worlds? Think of Kubernetes as a large, modern office building providing shared resources (for example, electricity, internet, security, HVAC) for its tenants. When a specialized lab moves in, needing dedicated resources such as specific power and temperature control, you don’t build a new building. Instead, you integrate the lab into the existing building infrastructure, allowing it to use shared services while maintaining its own precise controls for high-performance work. In the same way, Slurm can be ran inside a Kubernetes environment such as Amazon Elastic Kubernetes Service (Amazon EKS) using the open source Slinky Project. In this post, we introduce the Slinky Project, discuss its benefits, explore some alternatives, and leave you with a bit of homework to go deploy the Slurm on EKS blueprint , which uses the Slinky Slurm operator. Slurm is an open source, highly scalable workload manager and job scheduler designed for managing compute resources on compute clusters of all sizes. It provides three core functions: allocating access to compute resources, providing a framework for launching and monitoring parallel computing jobs, and managing queues of pending work to resolve resource contention. Slurm is widely used in traditional High-Performance Computing (HPC) environments and in AI training to manage and schedule large-scale accelerated compute workloads across multi-node clusters. Slurm allows researchers and engineers to efficiently allocate CPU, GPU, and memory resources for distributed training jobs with fine-grained control over resource types and job priorities. Slurm’s reliability, advanced scheduling features, and integration with both on-premises and cloud environments make it a preferred choice for handling the scale, throughput, and reproducibility that modern AI research and industry demand.</description></item><item><title>The tools for overcoming the top 10 DevOps challenges</title><link>https://kubermates.org/docs/2025-10-14-the-tools-for-overcoming-the-top-10-devops-challenges/</link><pubDate>Tue, 14 Oct 2025 16:55:26 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-14-the-tools-for-overcoming-the-top-10-devops-challenges/</guid><description>The DevOps infinity loop Top challenges in DevOps Environment inconsistencies Team silos &amp;amp; skill gaps Outdated practices Monitoring blind spots CI/CD performance bottlenecks Automation compatibility issues Security vulnerabilities Test infrastructure scalability Unclear debugging reports Decision-making bottlenecks How to overcome DevOps challenges (and why communication is key) Build cross-functional pods Foster psychological safety Standardize environments Tune CI/CD and testing for performance Ensure continuous monitoring &amp;amp; security Improve report readability What a successful DevOps culture looks like The bottom line: talk is DevOps’ greatest strength Posted on October 14, 2025 by Anna Cyganik, Software Mind CNCF projects highlighted in this post DevOps is a way of working that reduces waste. It uses smart tools and practices to build, test, and ship software faster. It makes teams quicker, systems stronger and problems smaller when done right. It’s not just one thing – it’s about making the whole machine run better. But this means that DevOps is not just a toolset or process. It’s a way of thinking and a culture born from the need to fix something broken: the wall between developers and operations. Companies understand the value DevOps brings to projects – which explains why its market value is growing so fast. In 2020, it was worth about $4.3 billion. A year later, its value rose to $5.1 billion. If the pace holds, it will hit $12.2 billion by 2026. That’s almost tripled in six years. Teams understand what DevOps brings to projects.</description></item><item><title>VMware Workstation &amp; Fusion 25H2: Embracing Calendar Versioning and New Features</title><link>https://kubermates.org/docs/2025-10-14-vmware-workstation-fusion-25h2-embracing-calendar-versioning-and-new-features/</link><pubDate>Tue, 14 Oct 2025 13:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-14-vmware-workstation-fusion-25h2-embracing-calendar-versioning-and-new-features/</guid><description>A Clearer Versioning Model Key New Features Expanded CPU and OS Support Bug Fixes and Improvements Customer Questions &amp;amp; Support Global Availability Looking Ahead Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VMware Workstation &amp;amp; Fusion 25H2: Embracing Calendar Versioning and New Features VCF Breakroom Chats Episode 65: VMware Cloud Foundation with Ubuntu OS - Smarter Choice for Modern Apps VCF Breakroom Chats Episode 64 - AI Made Simple: Empowering VI Admins to Offer Private AI with VCF 9.0 We’re excited to announce VMware Workstation 25H2 and VMware Fusion 25H2 , the latest updates to our desktop hypervisors. These releases introduce a new versioning model, deliver new features, and expand support for the latest operating systems and hardware — all designed to make virtualization more seamless, powerful, and up-to-date. Staying current with your tools should be simple. With calendar versioning, it’s now easier to understand when your release was published and plan upgrades accordingly. We’re moving away from traditional version numbers (e. g. , Workstation 17.6. x, Fusion 13.6. x) and adopting a new naming format — 25H2 — that reflects the year (2025) and the half of the year (H2). This provides consistency across releases and ensures clarity for customers. The 25H2 release brings new tools and capabilities that simplify automation, enhance compatibility, and make day-to-day tasks faster and more reliable. dictTool (Workstation &amp;amp; Fusion) A new command-line utility that allows customers to inspect and edit VMware configuration files (.</description></item><item><title>“Sovereign” Label Doesn’t Translate to Sovereignty</title><link>https://kubermates.org/docs/2025-10-14-sovereign-label-doesn-t-translate-to-sovereignty/</link><pubDate>Tue, 14 Oct 2025 07:22:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-14-sovereign-label-doesn-t-translate-to-sovereignty/</guid><description>Discover more from VMware Cloud Foundation (VCF) Blog Related Articles “Sovereign” Label Doesn’t Translate to Sovereignty Unlocking TCO Advantage with vSAN in VMware Cloud Foundation 9.0: A Game-Changer for VMware Cloud Service Providers Private Cloud Beyond the Data Center: The True Scope of VMware Cloud Foundation In 2022, VMware warned that sovereignty depended on the needs of the customer , more than marketing, more than a logo on a cloud product. At the time, it felt early. Now, the evidence is undeniable: hearings, regulatory framework being prioritized , and market pivots prove that what mattered then matters even more now. Sovereignty has gone from an abstract concept to a key customer consideration. Establishing a compliance requirement around sovereignty is becoming a key discussion point in Europe. This blog builds on that foundation. We’ll examine why “sovereign by label” clouds fall short, point to real market signals that expose the gap, and lay out a practical checklist that buyers — and vendors — should demand. Because in 2025, the conversation is no longer about who can market sovereignty, but who can prove it. *** Many traditional public cloud vendors now sell “sovereign” clouds. But a label and local data centers do not erase legal exposure, control-plane dependencies, or auditability gaps. Recent admissions and market moves show the problem : when push comes to shove, control, whether in the form of jurisdiction or on data flows matters more than a marketing name. Let’s unpack the different aspects of this reality, point to concrete market signals, and discuss a practical checklist that buyers and platform vendors should demand if they mean “sovereign” in anything but name only.</description></item><item><title>Beyond the model: Why intelligent infrastructure is the next AI frontier</title><link>https://kubermates.org/docs/2025-10-14-beyond-the-model-why-intelligent-infrastructure-is-the-next-ai-frontier/</link><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-14-beyond-the-model-why-intelligent-infrastructure-is-the-next-ai-frontier/</guid><description>Beyond the model: Why intelligent infrastructure is the next AI frontier The new challenge: Distributed AI inference A shared vision for a shared problem llm-d: A blueprint for production-grade AI Final thoughts The adaptable enterprise: Why AI readiness is disruption readiness About the author Chris Wright More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Your large language model (LLM) proof of concept (PoC) was a success. Now what? The jump from a single server to production-grade, distributed AI inference is where most enterprises hit a wall. The infrastructure that got you this far just can&amp;rsquo;t keep up. As discussed in a recent episode of the Technically Speaking podcast , most organizations&amp;rsquo; AI journey and PoCs begin with deploying a model on a single server—a manageable task. But the next step often requires a massive leap to distributed, production-grade AI inference. This is not simply a matter of adding more machines—we believe this requires a new kind of intelligence within the infrastructure itself—an AI-aware control plane that can help manage the complexity of these unique and dynamic workloads. Deploying LLMs at scale introduces a set of challenges that traditional infrastructure isn&amp;rsquo;t designed to handle. A standard web server, for example, processes uniform requests. In contrast, an AI inference request can be unpredictable and resource-intensive, with variable demands on compute, memory, and networking. Think of it like modern logistics. Moving a small package from one city to another is straightforward. But coordinating a global supply chain requires intelligent logistics management—a system that can track thousands of shipments, dynamically route different types of cargo, and tweak scheduling so everything arrives on time.</description></item><item><title>Deploy with confidence: Announcing the latest Red Hat AI validated models</title><link>https://kubermates.org/docs/2025-10-14-deploy-with-confidence-announcing-the-latest-red-hat-ai-validated-models/</link><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-14-deploy-with-confidence-announcing-the-latest-red-hat-ai-validated-models/</guid><description>Deploy with confidence: Announcing the latest Red Hat AI validated models What are validated models? Red Hat’s model optimization capabilities Meet the most recent validated models Get started today Coming soon The adaptable enterprise: Why AI readiness is disruption readiness About the author Rob Greenberg More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share We are excited to introduce our most recent validated models , designed to empower your deployments. At Red Hat, our goal is to provide the confidence, predictability, and flexibility organizations need to deploy third-party gen AI models across the Red Hat AI platform. This release expands our collection of performance-benchmarked and accuracy-evaluated optimized models, helping you accelerate time to value and select the perfect fit for your enterprise use case. Red Hat AI’s validated models go beyond a simple list, providing efficient, enterprise-ready AI. We combine rigorous performance benchmarking and accuracy testing with a comprehensive packaging process designed to deploy with security and simplicity in mind. Each model is scanned for vulnerabilities and integrated into a managed software lifecycle, helping ensure you receive a high-performing and resource-optimized asset that is focused on security, easy to manage, and ready for long-term updates. The world of large language models (LLMs) is expanding rapidly, making it difficult for enterprises to choose the right one. Organizations often struggle with AI resource capacity planning and ensuring that a model&amp;rsquo;s performance can be reliably reproduced. That&amp;rsquo;s where Red Hat&amp;rsquo;s validated models come in. We provide access to a set of ready-to-use, third-party models that run efficiently on vLLM within our platform. We simplify the selection process by performing extensive testing for you. Our model validation process includes: Performance benchmarking using GuideLLM to assess resource requirements and cost on various hardware configurations.</description></item><item><title>How Red Hat partners are powering the next wave of enterprise AI</title><link>https://kubermates.org/docs/2025-10-14-how-red-hat-partners-are-powering-the-next-wave-of-enterprise-ai/</link><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-14-how-red-hat-partners-are-powering-the-next-wave-of-enterprise-ai/</guid><description>How Red Hat partners are powering the next wave of enterprise AI Powering fast, flexible and efficient inference Accelerating agentic AI deployments Scaling AI across the hybrid cloud Delivering a leading AI ecosystem About the author Ryan King More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The pace of AI innovation is accelerating, and with the launch of Red Hat AI 3 , we&amp;rsquo;re reminded that turning this potential into enterprise reality requires a robust, open ecosystem built on choice and collaboration. Our goal has always been to provide a consistent, powerful platform for AI that works with any model, on any accelerator, and across the hybrid cloud. Today, we&amp;rsquo;re thrilled to highlight the momentum from our partners, who are working alongside us to build out the future of open, hybrid AI on Red Hat. The Red Hat partner ecosystem is the engine that will deliver the generative AI (gen AI) and agentic capabilities that customers need for broad market adoption. It’s about bringing together the best in hardware, software, and services to create a whole that is far greater than the sum of its parts. The launch of Red Hat AI 3 is centered on driving enterprise AI inference, expanding model choice and enabling open models for optimized cost and flexibility – so organizations can go from training to “doing. ” And Red Hat partners play a critical role in making this happen. To deliver AI inference at scale using vLLM and Kubernetes, the llm-d open source project is now generally available as part of Red Hat OpenShift AI 3.0, powered by a coalition of leading gen AI model providers, AI accelerators and premier AI cloud platforms. Founding contributors include CoreWeave, Google Cloud, IBM Research, and NVIDIA, with additional support from partners like AMD, Cisco, Hugging Face, Intel, Lambda, and Mistral AI. Since the project’s introduction earlier this year, Microsoft, Oracle and WEKA have also become active contributing members. As Large Language Models (LLMs) become the foundation for a wide range of gen AI applications, Red Hat is introducing the Partner Model Validation Guide to empower our partners and provide greater choice to customers. This guide outlines a standardized, step-by-step process for Red Hat partners to benchmark their LLMs for inclusion in the Red Hat OpenShift AI model catalog.</description></item><item><title>Introducing AI hub and gen AI studio: The new command center for enterprise gen AI in Red Hat OpenShift AI</title><link>https://kubermates.org/docs/2025-10-14-introducing-ai-hub-and-gen-ai-studio-the-new-command-center-for-enterprise-gen-a/</link><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-14-introducing-ai-hub-and-gen-ai-studio-the-new-command-center-for-enterprise-gen-a/</guid><description>Introducing AI hub and gen AI studio: The new command center for enterprise gen AI in Red Hat OpenShift AI What&amp;rsquo;s new: A closer look Real-world value for the enterprise How to get started What’s next? The adaptable enterprise: Why AI readiness is disruption readiness About the authors Rob Greenberg Peter Double More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The world of gen AI is moving at lightning speed. For enterprises, navigating the flood of new large language models (LLMs), tools (like Model Context Protocol (MCP) servers), and frameworks can feel overwhelming. How do you choose the right model? How do you empower your teams to experiment and build with the latest innovations in AI without creating organizational barriers? At Red Hat, we believe the future of AI is open, accessible, and manageable at scale. That&amp;rsquo;s why we’re excited to announce 2 new consolidated dashboard experiences in Red Hat OpenShift AI 3.0: the AI Hub and the Gen AI studio. These experiences are designed to streamline the entire gen AI lifecycle for enterprises by providing tailored components for the key personas innovating with AI within organizations: platform engineers and AI engineers. AI hub and gen AI studio work together to create a cohesive, end-to-end workflow for building production-ready AI solutions on a trusted, consistent platform. The AI hub is the central point for the management and governance of gen AI assets within OpenShift AI. It empowers platform engineers to discover, deploy, and manage the foundational components their teams need. Key components include: Catalog: A curated library where platform engineers can discover, compare, and evaluate a wide variety of models. This helps overcome &amp;ldquo;model selection paralysis&amp;rdquo; by providing the data needed to choose the optimal model for any use case. Registry: A central repository to register, version, and manage the lifecycle of AI models before they are configured for deployment. Deployments: An administrative page to configure, deploy, and monitor the status of models running on the cluster.</description></item><item><title>Red Hat AI 3 delivers speed, accelerated delivery, and scale</title><link>https://kubermates.org/docs/2025-10-14-red-hat-ai-3-delivers-speed-accelerated-delivery-and-scale/</link><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-14-red-hat-ai-3-delivers-speed-accelerated-delivery-and-scale/</guid><description>Red Hat AI 3 delivers speed, accelerated delivery, and scale 1. Achieve new levels of efficiency with SLA-aware inference 2. Accelerate agentic AI innovation 3. Connecting models to your private data 4. Scaling AI across the hybrid cloud A new approach to enterprise AI The adaptable enterprise: Why AI readiness is disruption readiness About the authors Jennifer Vargas Carlos Condado Will McGrath Robbie Jerrom Younes Ben Brahim Ornkanya Sinonpat (Aom) More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share This past May at Red Hat Summit, we made several announcements across the Red Hat AI portfolio, including the introduction of Red Hat AI Inference Server and Red Hat AI third-party validated models, the integration of Llama Stack and Model Context Protocol (MCP) APIs as a developer preview, and the establishment of the llm-d community project. The portfolio’s latest iteration, Red Hat AI 3 , delivers many of these production-ready capabilities for enterprises. Additionally, we’re providing more tools and services to empower teams to increase efficiency, collaborate more effectively, and deploy anywhere. Let’s explore what Red Hat AI 3 means for your business. Red Hat’s strategy is to serve any model across any accelerator and any environment. The latest inferencing improvements offer features to meet Service Level Agreements (SLAs) of generative AI (gen AI) applications, support for additional hardware accelerators, and an expanded catalog of validated and optimized third-party models. Some highlights include: llm-d is now generally available in Red Hat OpenShift AI 3.0. llm-d provides kubernetes-native distributed inference, which is essential for scaling and managing the unpredictable nature of large language models (LLMs).</description></item><item><title>Why standardization is the key to agentic AI success: How a unified platform spurs innovation</title><link>https://kubermates.org/docs/2025-10-14-why-standardization-is-the-key-to-agentic-ai-success-how-a-unified-platform-spur/</link><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-14-why-standardization-is-the-key-to-agentic-ai-success-how-a-unified-platform-spur/</guid><description>Why standardization is the key to agentic AI success: How a unified platform spurs innovation Understanding the building blocks of an intelligent agent An example of agentic AI in action A streamlined approach for development and production Gaining operational consistency for AI workloads Learn more The adaptable enterprise: Why AI readiness is disruption readiness About the authors Younes Ben Brahim Carlos Condado Will McGrath Roberto Carratalá Philip Hayes Cedric Clyburn More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share You&amp;rsquo;re tasked with bringing AI into your organization, but the journey from a proof-of-concept to a production-ready application is often a maze of fragmented tools and complex integrations, especially for agentic AI use cases. The real challenge isn&amp;rsquo;t just the technology—it&amp;rsquo;s building an AI strategy that is scalable, reliable, and manageable. Standardization is a primary factor in meeting this challenge, primarily by reducing complexity and increasing efficiency. By combining Model Context Protocol (MCP) and Llama Stack on a platform like Red Hat OpenShift AI , you can create a unified and portable environment for your AI applications. OpenShift AI can also support the integration of any other agentic frameworks and components you like, including tools like LangChain, LangGraph, and CrewAI, among others. Red Hat&amp;rsquo;s approach can help simplify development, streamline operations, and give your enterprise the tools it needs to empower your multiple teams, like AI developers and data scientists to turn your AI vision into a reality within a trusted and consistent environment. To build resilient AI-enabled applications, you need a foundation that enables intelligent agents to access and use your enterprise&amp;rsquo;s specific tools and data. This requires 2 key components: a standardized way to expose your services and a unified framework for interacting with them. This is the essence of agentic AI , where an autonomous system can determine the best course of action and tools needed to achieve a goal. The combination of MCP and Llama Stack provides the framework for these agents to operate effectively. MCP is an open protocol that standardizes how your AI systems integrate with external tools and data sources. It has been described as being a &amp;ldquo;USB-C port for AI applications.</description></item><item><title>How to manage EKS Pod Identities at scale using Argo CD and AWS ACK</title><link>https://kubermates.org/docs/2025-10-13-how-to-manage-eks-pod-identities-at-scale-using-argo-cd-and-aws-ack/</link><pubDate>Mon, 13 Oct 2025 21:16:16 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-13-how-to-manage-eks-pod-identities-at-scale-using-argo-cd-and-aws-ack/</guid><description>How to manage EKS Pod Identities at scale using Argo CD and AWS ACK What is GitOps? EKS Pod Identity: Simplifying IAM Permissions for Kubernetes Applications Overview of Amazon Controllers for Kubernetes (ACK) Prerequisites High-level overview of the steps Install Amazon EKS Pod Identity Agent Install the AWS Controllers for Kubernetes (ACK) 1. Creation of a role for our ACK controller and the EKS Pod Identity association 2. Install the ACK controller for EKS with Helm, edit region as needed Accessing Argo CD Deploy the sample application highlighting the problem 1. Clone the code repository for the sample application project 2. Create the IAM role to be used by the application 3. Open application. yaml file, the following parameters can be replaced with your own 4. Create the Argo CD application 5. Confirm the role from inside our container Validate the correct role is passed via a job Alternative Solution: Changing ARGOCD_SYNC_WAVE_DELAY Cleanup Conclusion About the authors In today’s blog post, we’ll explore how to manage at scale the association of Kubernetes service accounts with IAM roles through the Amazon Elastic Kubernetes Service (EKS) Pod Identity association. We will be using Argo CD , a popular GitOps delivery tool, and the AWS Controllers for Kubernetes (ACK) to automate the association, but as we will see this sometimes causes a critical challenge: the EKS Pod Identity API is eventually consistent. We need to verify the association is available for the credentials before the application pods are deployed. Our focus will be on correct deployment, thus addressing that challenge, without causing side effects.</description></item><item><title>VMware at KubeCon North America 2025: Innovation at the Core of Cloud Native</title><link>https://kubermates.org/docs/2025-10-13-vmware-at-kubecon-north-america-2025-innovation-at-the-core-of-cloud-native/</link><pubDate>Mon, 13 Oct 2025 16:54:21 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-13-vmware-at-kubecon-north-america-2025-innovation-at-the-core-of-cloud-native/</guid><description>What to Expect at the VMware Booth (#1010) Check Out the Demo Theater Session Sign up for Expert-led KubeCon Community Breakout Sessions Dive Deeper into VKS Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VMware at KubeCon North America 2025: Innovation at the Core of Cloud Native VCF Breakroom Chats Episode 64 - AI Made Simple: Empowering VI Admins to Offer Private AI with VCF 9.0 VCF Breakroom Chats Episode 63 - No Experience Needed: Demystifying Kubernetes for Aspiring Cloud Admins with VCF 9.0 KubeCon + CloudNativeCon returns to North America November 10–13, 2025 in Atlanta, and VMware by Broadcom is excited to be a Platinum Sponsor of this important community gathering. VMware has been a top contributor to Open Source Kubernetes projects and the community, helping to drive innovation, collaboration, and ecosystem growth and we’re excited to see the innovations and ideas from the community coming up at KubeCon in Atlanta. Whether you’re a developer, platform engineer, Kubernetes maintainer, or open source contributor, we’d love to chat with you onsite at Booth #1010. Plus, we’re participating in several community-led sessions and have booth demos you won’t want to miss — plus plenty of time to connect in person at our booth. Discover why VMware vSphere Kubernetes Service (VKS) is the runtime of choice for the private cloud: CNCF certified Kubernetes runtime, which runs all apps LTS for Kubernetes minor releases Built-in integration with common platform engineering tools, such as Argo, Helm Multi-cluster lifecycle management, policy and backup at scale Single API across a robust set of building-block services, including AI CNCF certified Kubernetes runtime, which runs all apps LTS for Kubernetes minor releases Built-in integration with common platform engineering tools, such as Argo, Helm Multi-cluster lifecycle management, policy and backup at scale Single API across a robust set of building-block services, including AI Our booth is your hub for all things VMware + Kubernetes: Watch demos highlighting Configure Kubernetes in Minutes – Fast deploy, on-demand versions, scalability Deploy Workloads in a Powerful Ecosystem – Conformant Kubernetes with packages, flexible container choices Operate Real VMs with Kubernetes – Operate VMs through Kubernetes APIs with VM service Advance CNCF Together – VMware’s contributions across CNCF projects Meet our experts who are contributing to important open-source projects such as Cluster API, Contour, etcd, Harbor, Containerd , and others. Our engineers are available for 1:1 conversations to discuss open-source solutions that simplify, secure, and scale modern applications Ask questions about VMware + Kubernetes convergence, etcd reliability, resilient infrastructure, and more Discuss all about VKS – upcoming capabilities and roadmap directions Schedule deeper dives with our technical experts Watch demos highlighting Configure Kubernetes in Minutes – Fast deploy, on-demand versions, scalability Deploy Workloads in a Powerful Ecosystem – Conformant Kubernetes with packages, flexible container choices Operate Real VMs with Kubernetes – Operate VMs through Kubernetes APIs with VM service Advance CNCF Together – VMware’s contributions across CNCF projects Configure Kubernetes in Minutes – Fast deploy, on-demand versions, scalability Deploy Workloads in a Powerful Ecosystem – Conformant Kubernetes with packages, flexible container choices Operate Real VMs with Kubernetes – Operate VMs through Kubernetes APIs with VM service Advance CNCF Together – VMware’s contributions across CNCF projects Meet our experts who are contributing to important open-source projects such as Cluster API, Contour, etcd, Harbor, Containerd , and others. Our engineers are available for 1:1 conversations to discuss open-source solutions that simplify, secure, and scale modern applications Ask questions about VMware + Kubernetes convergence, etcd reliability, resilient infrastructure, and more Discuss all about VKS – upcoming capabilities and roadmap directions Schedule deeper dives with our technical experts Join us for the Demo Theater Session: Title: Running etcd in Production: Best Practices &amp;amp; Rescue Recipes Time: November 12, 3:30PM Location: Demo Theater, show floor location #1160 We will explore best practices to keep your clusters healthy and recover fast when things go wrong. Running etcd in production can be challenging with unexpected leader elections or network problems. Watch us demonstrate proven strategies for deploying and operating etcd at scale and share “rescue recipes” in action to address database bloat, instability, and failed upgrades. For those who want to get a head start or dive deeper before the event, we encourage you to explore our recent blog post: Kubernetes on VMware Cloud Foundation 9.0: A Single Platform for All Workloads. Join us at KubeCon + CloudNativeCon North America 2025 in Atlanta, connect with our engineers and maintainers at Booth #1010 , and be part of the conversations shaping the next decade of cloud native infrastructure.</description></item><item><title>KubeCon + CloudNativeCon North America 2025 Co-Located Event Deep Dive: Cloud Native + Kubernetes AI Day</title><link>https://kubermates.org/docs/2025-10-13-kubecon-cloudnativecon-north-america-2025-co-located-event-deep-dive-cloud-nativ/</link><pubDate>Mon, 13 Oct 2025 14:23:36 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-13-kubecon-cloudnativecon-north-america-2025-co-located-event-deep-dive-cloud-nativ/</guid><description>Who will get the most out of attending this event? What is new and different this year? What will the day look like? Should I do any homework first? A special note from the program chairs Posted on October 13, 2025 by Yuzhui Liu, Rajas Kakodkar, Ricardo Rocha, Yuan Tang | Co-Chairs Cloud Native &amp;amp; Kubernetes AI Day is welcoming the AI/ML and High Performance Computing (HPC) communities. Since 2022 there have been multiple dedicated events (Batch / HPC and Cloud Native AI days) but given the overlap in requirements, projects and end user interests it became clear we all fit better together. Cloud Native &amp;amp; Kubernetes AI Day brings together a diverse range of technical enthusiasts, open source contributors, practitioners, researchers and end users. All united in a common goal: enhancing Kubernetes as the ultimate infrastructure management tool for research and AI/ML workloads. The Cloud Native &amp;amp; Kubernetes AI Day is aimed at seasoned practitioners as well as those new to the batch computing and MLOps worlds. Anyone looking for solutions and best practices to provide cost effective and efficient infrastructure to scale out batch computing, training and inference workloads, make the best use of scarce and expensive hardware accelerators and efficiently manage LLMs and agentic infrastructure. This event will also help practitioners of MLOps interact with maintainers of Cloud Native AI projects and foster collaboration between the two worlds. The theme this year is ‘agent,’ with multiple references and reports on Agentic AI and the cloud native infrastructure supporting it. This event will be a unique opportunity to connect and network with people driving this new generation infrastructure. We will have a full day with 10 full sessions and 4 lightning talks and enough time for questions during the sessions and discussion in the break outs. We will be hearing from researchers, project maintainers and many end users reporting on successes and challenges of running AI/ML workloads on top of cloud native infrastructure. While the sessions will be engaging, there will be ample time during coffee breaks and lunch for hallway tracks and networking sessions, helping attendees engage with speakers, maintainers of projects and end users.</description></item><item><title>How Red Hat, NetApp, and Cisco simplify IT modernization</title><link>https://kubermates.org/docs/2025-10-13-how-red-hat-netapp-and-cisco-simplify-it-modernization/</link><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-13-how-red-hat-netapp-and-cisco-simplify-it-modernization/</guid><description>How Red Hat, NetApp, and Cisco simplify IT modernization Modernization is too important for trial and error Application modernization Application development Modern virtualization Simplifying operations Optimizing costs Enabling hybrid cloud Robust data protection AI and ML Use cases with OpenShift AI on FlexPod AI Conclusion Red Hat Learning Subscription | Product Trial About the author George James More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share For enterprise IT, modernizing infrastructure isn&amp;rsquo;t about following trends, it&amp;rsquo;s the way to maximize efficiency and ensure continued support. The ideal path to modernization combines compute, storage, and networking into a single, cohesive system that&amp;rsquo;s easy to deploy. It uses enterprise-grade storage for intelligent data management, and runs on an enterprise-tested application platform that handles containers and virtual machines (VM) in a unified environment. It allows you to develop and deploy applications in your data center, and then move them to the cloud as needed, without retooling. Ultimately, modernization done well helps your developers deploy applications faster, lets your IT teams (from network to storage to sysadmins) coordinate, and saves you time and money. Enterprise IT cannot rely on trial and error. Consider hardware, for instance. It would take weeks (if not months) to piece together compute, storage, and networking from multiple vendors. Then there&amp;rsquo;s integration testing, stress and load testing. Then there&amp;rsquo;s the constant worry about whether you deployed the most efficient and cost effective solution. And that&amp;rsquo;s before you even get to the application stack. Are you trying to modernize from a legacy monolithic architecture to a more modern, cloud-native approach? Do you need a hypervisor for virtual machines? Can you scale and migrate applications from on-premises to a cloud hyperscaler without refactoring? A solution to this problem is needed now, more than ever.</description></item><item><title>What to know before you install or upgrade to Red Hat Ansible Automation Platform 2.6</title><link>https://kubermates.org/docs/2025-10-13-what-to-know-before-you-install-or-upgrade-to-red-hat-ansible-automation-platfor/</link><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-13-what-to-know-before-you-install-or-upgrade-to-red-hat-ansible-automation-platfor/</guid><description>What to know before you install or upgrade to Red Hat Ansible Automation Platform 2.6 New installations of Ansible Automation Platform 2.6 Installing on RHEL RHEL containerized installation (recommended) RHEL RPM installation (deprecated) Installing on Red Hat OpenShift Upgrade and migration paths to Ansible Automation Platform 2.6 Deployment type migrations: Moving between RPM, container, and operator Migrating Ansible Automation Platform across RHEL versions Migrating across PostgreSQL versions Upgrading from Ansible Automation Platform 2.4 to 2.6 Upgrading from Ansible Automation Platform 2.5 to 2.6 Additional resources Red Hat Ansible Automation Platform | Product Trial About the author Leonardo Gallego More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Red Hat Ansible Automation Platform 2.6 introduces powerful capabilities for managing, scaling, and deploying automation across your enterprise. Whether you&amp;rsquo;re a new user planning your first deployment or an existing customer upgrading from a previous version, understanding the available installation and upgrade methods is critical. This guide provides a high-level overview of the supported paths to Ansible Automation Platform 2.6. Ansible Automation Platform offers flexible installation methods to suit your infrastructure, with support for deployments on Red Hat Enterprise Linux (RHEL) and Red Hat OpenShift. On RHEL, there are two primary methods for deploying Ansible Automation Platform, though one of the options is deprecated. Containerized installations are the recommended approach for new installations and deployments on RHEL. The Ansible Automation Platform 2.6 containerized installer requires either RHEL 9 or RHEL 10. Read the documentation to learn more. The Ansible Automation Platform 2.6 RPM-based install is only available on RHEL 9. We suggest using the RHEL RPM install method only for specific upgrade scenarios, not for net new installations. Read the documentation to learn more. When we released Ansible Automation Platform 2.5, we announced that the RPM install method was being deprecated.</description></item><item><title>SaaS deployment architectures with Amazon EKS</title><link>https://kubermates.org/docs/2025-10-10-saas-deployment-architectures-with-amazon-eks/</link><pubDate>Fri, 10 Oct 2025 21:13:56 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-10-saas-deployment-architectures-with-amazon-eks/</guid><description>SaaS deployment architectures with Amazon EKS Patterns for managing remote environment with Amazon EKS in its core Shared responsibility Building distributed SaaS on Kubernetes Application packaging and deployment SaaS Provider Hosted Remote Application Plane Customer Hosted Data Plane with EKS Hybrid Nodes Conclusion About the authors As companies scale their software as a service (SaaS) offerings, they’re expanding their market reach by offering flexible deployment options directly within their customers’ environments. This versatile deployment model enables organizations to maintain data sovereignty, meet compliance standards, achieve optimal performance through reduced latency, and maximize efficiency by running applications close to existing customer datasets. SaaS providers can embrace this approach to serve a broader range of industries and unlock new business opportunities, particularly in highly regulated sectors and performance-sensitive markets. This method of extending the deployment environments into the tenant’s owned environments has been labelled “SaaS Anywhere” in this post. Although SaaS Anywhere solves important challenges related to managing remote customers’ environments, they introduce significant complexity in management and operation. SaaS providers must develop robust systems for provisioning and maintaining their application stack across numerous customer environments, implement cross-account monitoring solutions, manage distributed lifecycle updates, and provide consistent security controls. All of this is done while maintaining operational excellence at scale. In this post we explore patterns and practices for building and operating these distributed Amazon Elastic Kubernetes Service (Amazon EKS )-based applications effectively. When designing SaaS solutions, organizations typically employ one of three deployment models, each offering distinct advantages for specific use cases: SaaS Provider Hosted : Both data and control planes reside in the provider’s Amazon Web Services (AWS) account, optimizing for operational efficiency and rapid customer onboarding. Although lightweight agents may exist in customer environments for telemetry, all core processing remains provider hosted. Remote Application Plane : The data plane runs in the customer’s environment while the control plane stays in the provider’s account. This model balances compliance needs with operational efficiency, allowing customers to maintain data sovereignty while using AWS services.</description></item><item><title>KubeCon + CloudNativeCon North America 2025 Co-Located Event Deep Dive: Data on Kubernetes Day</title><link>https://kubermates.org/docs/2025-10-10-kubecon-cloudnativecon-north-america-2025-co-located-event-deep-dive-data-on-kub/</link><pubDate>Fri, 10 Oct 2025 18:33:21 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-10-kubecon-cloudnativecon-north-america-2025-co-located-event-deep-dive-data-on-kub/</guid><description>What is new and different this year? What will the day look like? Should I do any homework first? Find your community! A note from the program chairs Posted on October 10, 2025 by Adam Durr and Melissa Logan | Co-Chairs Data on Kubernetes Day (DoK Day) began as a virtual event in 2021 and became an official co-located event for KubeCon + Cloud Native Con in 2023. Since then, it has been a staple at both the European and North American events, so we’re excited to bring it back for our community in Atlanta. Running stateful workloads like databases, streaming, AI/ML, and analytics on Kubernetes is no longer a fringe practice—it’s becoming the standard for modern infrastructure. Organizations like Etsy, Grab, and Chick-fil-A have demonstrated what’s possible. Our goal for DoK Day is to provide you with the real-world resources, best practices, and use cases you need to confidently run data workloads on Kubernetes and accelerate your journey. This year, we have seen a significant increase in content submissions focusing on AI and LLMs, reflecting the industry trend of Kubernetes becoming the foundation for AI infrastructure. Our schedule will feature several new talks on this topic, from leveraging GPUs to managing AI data pipelines on Kubernetes. We are also proud to have our most diverse group of speakers and submissions to date, representing a wide range of backgrounds and experiences from the community. DoK Day is for practitioners and decision-makers who are pushing Kubernetes beyond stateless applications. Platform engineers, SREs, data engineers, architects, and CTOs who want to understand how to run stateful workloads at scale will find immense value. Whether you’re just starting to explore running DoK or are already operating mission-critical workloads in production, you’ll walk away with practical knowledge and a stronger network of peers facing similar challenges. No formal prep is required, but we recommend attendees familiarize themselves with the Data on Kubernetes Community (DoKC) resources—such as our white papers, recorded talks, and Slack discussions.</description></item><item><title>What is Kubernetes? A Beginner’s Guide to Container Orchestration</title><link>https://kubermates.org/docs/2025-10-10-what-is-kubernetes-a-beginner-s-guide-to-container-orchestration/</link><pubDate>Fri, 10 Oct 2025 16:19:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-10-what-is-kubernetes-a-beginner-s-guide-to-container-orchestration/</guid><description>Introduction: Why Everyone’s Talking About Kubernetes Understanding Containers - The Foundation of Kubernetes What Are Containers? Containers vs. Virtual Machines (VMs) Why Containers Changed Everything Learn Containers the Hands-On Way The Problem Before Kubernetes - When Containers Got Out of Hand When Containers Multiply The Need for Orchestration The Birth of Kubernetes Before and After Kubernetes (At a Glance) What is Kubernetes? Kubernetes in Simple Terms Kubernetes as a Declarative System Core Building Blocks of Kubernetes A Quick Example How Kubernetes Works - The Big Picture The Control Plane - The Brain of Kubernetes The Worker Nodes - The Hands of Kubernetes How It All Works Together (Simplified Flow) Core Kubernetes Concepts (Explained Simply) 1. Pods - The Smallest Unit of Work 2. ReplicaSets - Keeping the Right Number of Pods Alive 3. Deployments - The Smart Way to Manage Pods 4. Services - Connecting Everything Together 5. ConfigMaps and Secrets - Managing App Configurations 6. Namespaces - Organizing Your Cluster 7. Ingress (Bonus for Beginners) Why Use Kubernetes? (Key Benefits Explained Simply) 1. Automated Scaling - Grow or Shrink on Demand 2. Self-Healing - Applications That Fix Themselves 3. Seamless Updates and Rollbacks 4.</description></item><item><title>A blueprint for zero-trust AI on Kubernetes</title><link>https://kubermates.org/docs/2025-10-10-a-blueprint-for-zero-trust-ai-on-kubernetes/</link><pubDate>Fri, 10 Oct 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-10-a-blueprint-for-zero-trust-ai-on-kubernetes/</guid><description>Are these AI security challenges unique? A blueprint for securing AI on Kubernetes Securing ingress to your endpoints (secure the front door) Control the traffic (Sorry this cluster is RSVP only) Observability (What is actually happening inside the house) The bottom line Posted on October 10, 2025 by Tigera CNCF projects highlighted in this post LLMs and AI are everywhere these days. Everyone wants to build the next big thing, ship it fast, and maybe even cash out and chill for the rest of their lives. The problem? Most open source AI projects are shared as is. They’re created with the best intentions, but their developers aren’t losing sleep over things like security guardrails or production hardening, that part is left for you to figure out. If that sounds like the boat you’re in, you’re in the right place. In this blog, we’ll look at how running AI on Kubernetes introduces some very real networking and security challenges, and what you can do about them before your experiment turns into a liability. Because here’s the thing: AI systems are complicated, sometimes even the people building them admit they don’t fully know why a model makes the decision it does. But at the end of the day, it’s still just bits moving around computers and network cables, and securing those bits, whether they’re API keys, training data, or model endpoints, is easier than you might think if you know the right patterns. Yes and no. On the one hand, the issues AI workloads face, unrestricted network traffic, exposed endpoints, leaked credentials, are the same classic security problems we’ve been wrestling with in IT for decades. On the other hand, they don’t just move data around, they handle sensitive training datasets, expensive inference workloads, and powerful APIs that can be abused in seconds. A stolen API key doesn’t just mean a small breach; it could mean thousands of dollars in cloud bills or a model that starts spilling the beans for the wrong confidant.</description></item><item><title>Friday Five — October 10, 2025</title><link>https://kubermates.org/docs/2025-10-10-friday-five-october-10-2025/</link><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-10-friday-five-october-10-2025/</guid><description>Friday Five — October 10, 2025 What&amp;rsquo;s new in Ansible Automation Platform 2.6 Forward Future Live | 10/3/25 What is model context protocol (MCP)? Optimize and deploy LLMs for production with OpenShift AI Red Hat OpenStack VMware Migration toolkit deep-dive About the author Red Hat Corporate Communications More like this Blog post Blog post Original podcast Original podcast Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Red Hat Ansible Automation Platform (AAP) 2.6 is now generally available, delivering new capabilities, platform enhancements, and strategic integrations to help you build resilient and trusted foundations for the next generation of IT operations, because you aren&amp;rsquo;t just preparing for the future — you are automating for it. Learn more Matt Hicks joined the hosts of Forward Future to discuss the power of open source and collaboration. Learn more An open source protocol, or set of instructions, is like a recipe for code that’s freely available to use and contribute to. MCP provides a simplified and reliable way for AI systems to virtually “plug in” to different data sources and tools. Learn more Organizations that want to run large language models (LLMs) on their own infrastructure—whether in private data centers or in the cloud—often face significant challenges related to GPU availability, capacity, and cost. Learn how to address these challenges with OpenShift AI. Learn more Dive deeper into migrating VMware-based workloads to Red Hat OpenStack Services on OpenShift. By following this guide, you’ll gain the essential knowledge and tools needed to plan and execute a successful, efficient migration. Learn more Red Hat is the world’s leading provider of enterprise open source software solutions, using a community-powered approach to deliver reliable and high-performing Linux, hybrid cloud, container, and Kubernetes technologies. Red Hat helps customers integrate new and existing IT applications, develop cloud-native applications, standardize on our industry-leading operating system, and automate, secure, and manage complex environments. Award-winning support, training, and consulting services make Red Hat a trusted adviser to the Fortune 500. As a strategic partner to cloud providers, system integrators, application vendors, customers, and open source communities, Red Hat can help organizations prepare for the digital future.</description></item><item><title>Reducing cognitive load in enterprise Kubernetes: 5 strategic tenets</title><link>https://kubermates.org/docs/2025-10-10-reducing-cognitive-load-in-enterprise-kubernetes-5-strategic-tenets/</link><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-10-reducing-cognitive-load-in-enterprise-kubernetes-5-strategic-tenets/</guid><description>Reducing cognitive load in enterprise Kubernetes: 5 strategic tenets 1. Centralizing namespace discovery with Red Hat Advanced Cluster Security 2. Centralizing secrets: Eliminating secret sprawl with HashiCorp Vault 3. Service-to-service communication: Location-agnostic connectivity 4. Migrating from ArgoCD to OpenShift GitOps: Standardization through mandate 5. Storage portability: Data persistence for the future The compound effect: Strategic alignment The adaptable enterprise: Why AI readiness is disruption readiness About the author Meg Foley More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share In the rapidly evolving landscape of enterprise Kubernetes deployments, managing cognitive load and operational complexity is a significant challenge. As platform teams scale to support hundreds of development teams across multiple clusters, they face the mental overhead of managing disparate tools, inconsistent processes, and fragmented workflows. After extensive evaluation and real-world testing, a leading insurance organization has refined their approach using 5 key tenets that significantly reduce cognitive load and operational complexity while improving security, reliability, and developer experience. This blog post outlines how they transformed their platform strategy and the impact of choosing specific tools. The Challenge: Developers were spending considerable time determining namespace availability across our multicluster environment. The manual process of checking multiple clusters, understanding naming conventions, and avoiding conflicts drained productivity significantly. Solution: Red Hat Advanced Cluster Security Why Red Hat Advanced Cluster Security? Initially adopted for security compliance, Red Hat Advanced Cluster Security demonstrates an unexpected benefit: its comprehensive visibility into cluster resources provided the precise, cluster-wide visibility needed for namespace management.</description></item><item><title>The open source engine driving AI from experiment to production and why inference is everything</title><link>https://kubermates.org/docs/2025-10-10-the-open-source-engine-driving-ai-from-experiment-to-production-and-why-inferenc/</link><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-10-the-open-source-engine-driving-ai-from-experiment-to-production-and-why-inferenc/</guid><description>The open source engine driving AI from experiment to production and why inference is everything The open source answer to the inference challenge Hardening community innovation for the enterprise The essential open model for AI About the author Brian Stevens More like this Blog post Blog post Original podcast Original podcast Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share This blog is adapted from a recent conversation I had with University of California, Berkeley’s Ion Stoica, featured in Red Hat Research Quarterly’s article, From silos to startups: Why universities must be a part of industry’s AI growth. Read our full conversation here. For the last several years, the narrative around artificial intelligence (AI) has been dominated by large language models (LLMs) and the monumental effort of training them. The technology industry has been focused on the discovery phase—but that era is rapidly shifting. The conversation is moving from, &amp;ldquo;How do we build the model?&amp;rdquo; to, &amp;ldquo;How do we actually run the model in production at scale?&amp;rdquo; This shift is more than a technical detail; it’s the new center of gravity for enterprise AI. When AI leaves the research lab and becomes a core business capability, the focus lands squarely on inference—the firing synapses in a trained model’s “brain” before it generates an answer or takes action. And in the enterprise, inference must be fast, cost-effective, and fully controlled. Moving AI from a proof-of-concept into a reliable, production-grade service introduces significant complexity, cost, and control challenges for IT leaders. Firstly, the hardware required to run these models—especially at the scale the enterprise needs—is expensive and often scarce. Secondly, demand is unpredictable. You might have bursts of high usage followed by long periods of low activity, which can be compounded across hundreds of variants of domain-purposed models. This variability makes it extremely difficult to maximize resource utilization and protect those critical investments.</description></item><item><title>VCF Breakroom Chats Episode 65: VMware Cloud Foundation with Ubuntu OS – Smarter Choice for Modern Apps</title><link>https://kubermates.org/docs/2025-10-09-vcf-breakroom-chats-episode-65-vmware-cloud-foundation-with-ubuntu-os-smarter-ch/</link><pubDate>Thu, 09 Oct 2025 21:23:15 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-09-vcf-breakroom-chats-episode-65-vmware-cloud-foundation-with-ubuntu-os-smarter-ch/</guid><description>Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VCF Breakroom Chats Episode 65: VMware Cloud Foundation with Ubuntu OS - Smarter Choice for Modern Apps VCF Breakroom Chats Episode 64 - AI Made Simple: Empowering VI Admins to Offer Private AI with VCF 9.0 VCF Breakroom Chats Episode 63 - No Experience Needed: Demystifying Kubernetes for Aspiring Cloud Admins with VCF 9.0 Welcome to episode 65 of the VCF Breakroom Chats. Today, we are happy to present this video with Timmy Carr, Director of Product Management at Broadcom. In this episode, Jay Thontakudi and Timmy Carr talk about the Canonical partnership we announced at Explore Las Vegas. We will highlight the benefits and the details of this technology integration. Learn more about running and managing modern applications on VMware Cloud Foundation 9.0: Check out the Kubernetes on VCF 9.0 launch blog Explore more resource on VMware Cloud Foundation Website About the VCF Breakroom Chat Series This webinar series is the successor to the vSphere Breakroom Chat, with a renewed focus on VMware Cloud Foundation (VCF). ​In this series, we share vlogs with industry-recognized experts from Broadcom and VMware solution partners and customers. These vlogs are concise, like meeting in a breakroom and having a quick conversation to get great information quickly. This series is for IT practitioners, IT admins, cloud and platform architects, developers, DevOps, senior IT managers, IT executives, and AI/ML professionals.</description></item><item><title>VCF Breakroom Chats Episode 64 – AI Made Simple: Empowering VI Admins to Offer Private AI with VCF 9.0</title><link>https://kubermates.org/docs/2025-10-09-vcf-breakroom-chats-episode-64-ai-made-simple-empowering-vi-admins-to-offer-priv/</link><pubDate>Thu, 09 Oct 2025 15:42:55 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-09-vcf-breakroom-chats-episode-64-ai-made-simple-empowering-vi-admins-to-offer-priv/</guid><description>VCF Breakroom Chats Episode 64 About the VCF Breakroom Chat Series Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VCF Breakroom Chats Episode 65: VMware Cloud Foundation with Ubuntu OS - Smarter Choice for Modern Apps VCF Breakroom Chats Episode 64 - AI Made Simple: Empowering VI Admins to Offer Private AI with VCF 9.0 VCF Breakroom Chats Episode 63 - No Experience Needed: Demystifying Kubernetes for Aspiring Cloud Admins with VCF 9.0 Welcome to another episode of VCF Breakroom Chats. Today’s vLog features Justin Murray, Product Marketing Engineer at Broadcom. In this episode, Justin Murray and Taka Uenishi discuss how VCF 9.0 makes it easier for VI Admins to get started with Private AI as they grow into Cloud Admin roles. Want to learn more about VCF Automation? Check out the VCF Automation web page for more resources. Read this blog post: Private Cloud Redefined: Deliver a Unified Cloud Consumption Experience with VMware Cloud Foundation 9.0 This webinar series focuses on VMware Cloud Foundation (VCF). In this series, we share conversations with industry-recognized experts from Broadcom and with solution partners and customers. You’ll gain relevant insights whether you’re an IT practitioner, IT admin, cloud or platform architect, developer, DevOps, senior IT manager, IT executive, or AI/ML professional. If you are new to the series, please check out our previous episodes.</description></item><item><title>Testing asynchronous workflows using OpenTelemetry and Istio</title><link>https://kubermates.org/docs/2025-10-09-testing-asynchronous-workflows-using-opentelemetry-and-istio/</link><pubDate>Thu, 09 Oct 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-09-testing-asynchronous-workflows-using-opentelemetry-and-istio/</guid><description>Introduction Challenges in testing asynchronous systems Three approaches to test environment isolation Implementing request-level isolation Central RouteService for tenant mapping Using OpenTelemetry for context propagation Leveraging Istio for traffic routing Implementation considerations Conclusion Posted on October 9, 2025 by Arjun Iyer, SignaDot CNCF projects highlighted in this post Learn how to test complex asynchronous workflows in cloud native applications using OpenTelemetry for context propagation and Istio for traffic routing. Explore cost-effective approaches to isolate test environments without duplicating infrastructure. Asynchronous architectures have become a cornerstone of modern cloud native applications, enabling services to operate independently while maintaining system resilience and scalability. These architectures typically rely on message queues and event-driven communication patterns to decouple services, allowing them to handle varying loads and failures gracefully. Popular message systems in the cloud native ecosystem include Apache Kafka, RabbitMQ, Redis Streams, Google Cloud Pub/Sub, AWS SQS, and Azure Service Bus. Each offers unique capabilities for different use cases, from high-throughput streaming to reliable message delivery. Regardless of which system you choose, testing end-to-end workflows that span multiple services and asynchronous boundaries presents unique challenges that traditional testing approaches struggle to address effectively. This article explores how two CNCF projects—OpenTelemetry for distributed tracing and context propagation, and Istio for traffic management—can work together to create cost-effective, scalable testing environments for asynchronous workflows without the overhead of duplicating entire infrastructure stacks. Testing asynchronous systems introduces several complex challenges not found in synchronous, request-response architectures: Environment setup complexity : Asynchronous systems require multiple coordinated components—brokers, producers, consumers, and often additional infrastructure like schema registries or monitoring tools. Setting up these components correctly with proper security, replication, and partitioning requires significant expertise and time. Test isolation : Unlike synchronous calls where requests can be easily isolated, asynchronous messages in shared systems can interfere with each other. Ensuring that test data from one scenario doesn’t impact another requires careful coordination.</description></item><item><title>Is DigitalOcean Your Next Career Spot? A 5-Year Insider on Why It Should Be</title><link>https://kubermates.org/docs/2025-10-09-is-digitalocean-your-next-career-spot-a-5-year-insider-on-why-it-should-be/</link><pubDate>Thu, 09 Oct 2025 04:41:34 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-09-is-digitalocean-your-next-career-spot-a-5-year-insider-on-why-it-should-be/</guid><description>Is DigitalOcean Your Next Career Spot? A 5-Year Insider on Why It Should Be What makes DigitalOcean the right place for you? How does customer-centricity shape life at DO? How would you describe the pace of innovation at DO? What words best capture DOâs culture? How would you describe DOâs style of working? Dive into the future with DigitalOcean About the author Try DigitalOcean for free Related Articles Sharks of DigitalOcean: Archana Kamath, Senior Director, IaaS Sharks of DigitalOcean: Darian Wilkin, Senior Manager, Solutions Engineering Sharks of DigitalOcean: Laura Schaffer, VP, Growth By Sujatha R Technical Writer Published: October 9, 2025 2 min read Akanksha Kapoor, Manager of Growth Account Management, stands as a great example of how DigitalOcean values come to life. She has been with DO for five years. In this spotlight, she shares how sheâs grown her career at DO, customer-first culture, and the collaborative environment that make DigitalOcean unique. âDigitalOcean opened doors for me with endless opportunities and support. Over the years, Iâve developed not just as a professional but as a person. What I value most here are the people, theyâre smart, humane, and genuinely awesome. â ð¥ Have a look at Akanksha Kapoorâs full conversation â¬ï¸ âAt DigitalOcean, the customer is at the core of everything we do. Their needs guide our priorities, and their feedback drives new products and features. As part of Customer Success, this means striving to deliver a world-class experience while embracing the chance for me and my team to learn and live by these values every day. â âThe word âfastâ doesnât even do it justice, itâs more like lightspeed. Customers ask for something, and within a quarter or two, sometimes even just weeks, itâs brought to life. The ability to innovate at this pace is truly exciting.</description></item><item><title>Boost developer productivity and modernization with a Red Hat developer experience assessment</title><link>https://kubermates.org/docs/2025-10-09-boost-developer-productivity-and-modernization-with-a-red-hat-developer-experien/</link><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-09-boost-developer-productivity-and-modernization-with-a-red-hat-developer-experien/</guid><description>Boost developer productivity and modernization with a Red Hat developer experience assessment Fast-track to a developer platform Exclusive promotion: Developer Experience Assessment Learn more The adaptable enterprise: Why AI readiness is disruption readiness About the authors James Labocki More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share A seamless developer experience (DevEx) isn&amp;rsquo;t just a luxury, it&amp;rsquo;s a necessity. But the reality for many organizations is that developers are spending a significant amount of their valuable time on tasks other than coding. Research from a recent ShiftMag survey reveals that developers lose an average of 8 hours each week to inefficiencies such as technical debt and insufficient documentation. These frustrations are not just a nuisance, they&amp;rsquo;re a drain on productivity that can lead to burnout and a substantial loss of potential output for the business. This is why platform engineering has emerged as a strategic imperative for businesses. According to Gartner , an estimated 80% of large software engineering organizations will establish platform engineering teams by 2026 to provide reusable services and tools for application delivery. The path to building a successful internal developer platform can be daunting. With so many tools and processes to consider, it can be difficult to know where to begin. This is precisely why Red Hat created the Developer Experience Assessment workshop. This two-week interactive workshop provides an increased understanding of the current developer experience, platform engineering teams, and DevOps principles through a classroom-style discovery workshop. This workshop is designed to help teams uncover hidden friction points between teams and map out the application onboarding and development lifecycle. The outcome is a tailored roadmap, or recipe, for how to improve your DevEx and scale platform adoption.</description></item><item><title>Enterprise AI survey: ambition, the value gap, and the importance of open source</title><link>https://kubermates.org/docs/2025-10-09-enterprise-ai-survey-ambition-the-value-gap-and-the-importance-of-open-source/</link><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-09-enterprise-ai-survey-ambition-the-value-gap-and-the-importance-of-open-source/</guid><description>Enterprise AI survey: ambition, the value gap, and the importance of open source About the author Hans Roth More like this Blog post Blog post Original podcast Original podcast Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share EMEA organizations are primed for widespread AI adoption, and see enterprise open source as an important success factor; however, their ability to scale is challenged by skills gaps, high costs and shadow AI. That’s the headline finding of our latest survey of over 900 IT leaders and AI engineers in nine countries 1. According to the survey AI is a core strategic priority for 72% 2 of organizations in Europe, the Middle East, and Africa, and they have plans to increase their AI investments by an average of 32% by 2026 3. The focus on AI is clear, yet the data also reveals a stark gap between ambition and reality: only 7% of organisations report “driving customer value” at scale from their AI investments today 4. This is a crucial challenge for leaders who need to translate significant investment into tangible business outcomes. So what is preventing organisations moving from small-scale pilots to enterprise-wide value? Tackling the barriers to scale AI Our survey data outlines the extent of the talent shortage faced, with 70% of respondents agreeing there is an urgent AI skills gap 3. The most significant gaps cited by those respondents are in the practical application of AI, such as connecting AI to enterprise data (49%), and making efficient use of AI capabilities (48%). Nearly as many (47%) report difficulty educating the business to use AI effectively, which complicates organisation-wide adoption. As the role of AI in the workplace is established, another layer of complexity emerges. ‘Shadow AI’, the use of unsanctioned AI tools by employees, is becoming a significant challenge to effective governance and scalable adoption. The vast majority of responding EMEA organisations (91% 5 ) admit their organisation is experiencing a shadow AI problem. This should serve as a wake-up call: AI is being adopted with or without IT teams’ authorisation.</description></item><item><title>Simplify your cloud-native transition with Oracle WebLogic Server on Red Hat OpenShift Virtualization</title><link>https://kubermates.org/docs/2025-10-09-simplify-your-cloud-native-transition-with-oracle-weblogic-server-on-red-hat-ope/</link><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-09-simplify-your-cloud-native-transition-with-oracle-weblogic-server-on-red-hat-ope/</guid><description>Simplify your cloud-native transition with Oracle WebLogic Server on Red Hat OpenShift Virtualization Collaboration for greater flexibility The power of a unified platform Red Hat Learning Subscription | Product Trial About the authors Mark Longwell Rob McManus Ju Lim More like this Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Organizations today are under increasing pressure to modernize critical applications while maintaining the reliability, security, and performance of their existing IT investments. Many business-critical workloads, including financial systems and telecommunications services, run on Oracle WebLogic Server. At the same time, organizations are embracing cloud-native platforms like Red Hat OpenShift to gain the agility, scalability, and consistency needed in hybrid and multicloud cloud environments. Working with the Oracle WebLogic team, Oracle&amp;rsquo;s validation of WebLogic Server on Red Hat OpenShift Virtualization allows your organization to leverage current investments as you move to fully containerized workloads. In addition, Oracle also supports WebLogic Server on Red Hat OpenShift. Oracle has validated WebLogic Server on Red Hat OpenShift Virtualization , enabling organizations to modernize applications by bridging existing investments with cloud-native environments. This validation, alongside support for WebLogic Server on Red Hat OpenShift , empowers you to consolidate your infrastructure and manage both virtual machines and containers on a unified platform. Red Hat&amp;rsquo;s collaboration with Oracle provides the ability to run your existing WebLogic Server workload in a virtual machine (VM) while taking advantage of OpenShift&amp;rsquo;s Kubernetes-based foundation (highlighted in Gartner&amp;rsquo;s 2025 Magic Quadrant for Container Management for the third year in a row). Organizations can consolidate infrastructure by managing both VMs and containers through a common cloud-native platform. This reduces operational complexity, streamlines management, and accelerates time-to-market for new services. For IT leaders, this means they can continue to leverage their WebLogic investments without rewriting applications. Development teams gain a pathway to modernize applications gradually, shifting from virtualized workloads to microservices and containers at their own pace.</description></item><item><title>How to Deploy Whisker and Goldmane in Manifest Only Calico Setups</title><link>https://kubermates.org/docs/2025-10-08-how-to-deploy-whisker-and-goldmane-in-manifest-only-calico-setups/</link><pubDate>Wed, 08 Oct 2025 19:29:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-08-how-to-deploy-whisker-and-goldmane-in-manifest-only-calico-setups/</guid><description>Your Step-by-Step to Observability Without the Operator Why Whisker and Goldmane Matter Installing Whisker and Goldmane Manually Requirements Generating Certificates and Certificate Authority (CA) Tuning Typha to use certificates Fixing DNS Issues Deploying Goldmane Tuning Goldmane Deployment Deploying Whisker UI Observability Unlocked (and Why the Operator Helps) Final Thoughts For Calico users who install using manifests, enabling the new observability features Whisker and Goldmane from version 3.30 has been a challenge. While these tools offer powerful insights into Kubernetes network flows and policy decisions, previous documentation was only for users who deployed with the Tigera operator. The operator automates several advanced, security-focused steps required to safely deploy Goldmane, which is why manifest users have had a more difficult time. These steps are crucial for protecting sensitive information. We’ve heard from many of you in the Calico Slack community: you’re eager to try out Whisker and Goldmane but aren’t sure how to set them up without Helm or the operator. For anyone who’s up for a challenge, this blog post provides a step-by-step guide on how to get everything wired up the hard way. However, even if you already use the operator, keep reading! We’re going to pull back the curtain on the magic it performs behind the scenes. Understanding these mechanics will help you troubleshoot, customize, and better appreciate a managed approach, whether you’re an SRE, platform engineer, or a curious cluster admin. What you will achieve at the end of this blog: You will learn how to get up and running with Whisker and Goldmane in a manifest based installation. You will learn why a Helm chart or the Tigera operator is our recommendation to manage your Kubernetes workloads at scale. You will have some hands on experience on what to look for when things are deployed in a real life scenario. Without Goldmane and tools, you’re essentially flying blind and you: Can’t easily debug why traffic is dropped Can’t visualize how policies affect flows Can’t trace service-to-service communication paths This leads to increased time-to-resolution for network bugs, frustration from teams consuming the platform, limited audit capabilities for security teams For manifest-only users, the lack of clear installation steps means many miss out on these benefits and as a developer advocate.</description></item><item><title>VCF Breakroom Chats Episode 63 – No Experience Needed:  Demystifying Kubernetes for Aspiring Cloud Admins with VCF 9.0</title><link>https://kubermates.org/docs/2025-10-08-vcf-breakroom-chats-episode-63-no-experience-needed-demystifying-kubernetes-for-/</link><pubDate>Wed, 08 Oct 2025 18:11:12 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-08-vcf-breakroom-chats-episode-63-no-experience-needed-demystifying-kubernetes-for-/</guid><description>VCF Breakroom Chats Episode 63 About the VCF Breakroom Chat Series Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VCF Breakroom Chats Episode 63 - No Experience Needed: Demystifying Kubernetes for Aspiring Cloud Admins with VCF 9.0 Analyst Insight Series: VMware Cloud Foundation Update - A RedMonk Conversation VCF Breakroom Chats Episode 62: Cloud Governance Unlocked - From VI Admin to Cloud Policy Pro with VCF 9.0 Welcome to the next episode of the VCF Breakroom Chats. Today, we are happy to present this vLog with Vincent Riccio, Product Marketing Engineer at Broadcom. In this episode, Vincent Riccio and Taka Uenishi discuss how VCF 9.0 helps simplify Kubernetes for VI Admins aspiring to be Cloud Admins. Want to learn more about VCF Automation? Check out the VCF Automation web page for more resources. Read this blog post: Private Cloud Redefined: Deliver a Unified Cloud Consumption Experience with VMware Cloud Foundation 9.0 This webinar series focuses on VMware Cloud Foundation (VCF). In this series, we share conversations with industry-recognized experts from Broadcom and with solution partners and customers. You’ll gain relevant insights whether you’re an IT practitioner, IT admin, cloud or platform architect, developer, DevOps, senior IT manager, IT executive, or AI/ML professional. If you are new to the series, please check out our previous episodes.</description></item><item><title>Cloud Native Computing Foundation Announces Knative’s Graduation</title><link>https://kubermates.org/docs/2025-10-08-cloud-native-computing-foundation-announces-knative-s-graduation/</link><pubDate>Wed, 08 Oct 2025 16:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-08-cloud-native-computing-foundation-announces-knative-s-graduation/</guid><description>Graduation marks Knative’s readiness for widespread production use, with upcoming features aimed at bridging legacy systems and expanding AI and cloud native integrations Key Highlights: The Cloud Native Computing Foundation (CNCF) announced the graduation of Knative, a Kubernetes-native serverless and event-driven application platform, marking its readiness for widespread production use. Knative removes much of the complexity of running modern workloads on Kubernetes by handling infrastructure tasks like autoscaling, routing, and event delivery. This helps organizations reduce costs, improve efficiency, and accelerate innovation. Developers gain a faster on-ramp to Kubernetes; platform teams simplify operations; and enterprises and startups alike benefit from easier integration with AI and cloud native technologies. Knative is now a graduated CNCF project and is available for production use today. SAN FRANCISCO, Calif. – October 8, 2025 – The Cloud Native Computing Foundation® (CNCF®), which builds sustainable ecosystems for cloud native software, announced the graduation of Knative , a serverless, event-driven application layer on top of Kubernetes. Knative simplifies how developers build, deploy, and run modern workloads by abstracting the infrastructure concerns such as autoscaling, routing, event delivery, and building containers. It allows teams to focus on architecture and business logic and get started quickly on Kubernetes without needing to ramp up on dozens of Kubernetes concepts and resources. As the C-suite looks to optimize costs and simplify operations, Knative can do so with features like autoscaling to zero to minimize infrastructure waste. “Knative fills several gaps in the cloud native ecosystem as an easy on-ramp to Kubernetes, with Knative’s eventing acting as the missing skeleton for connecting events to reactions,” said Evan Anderson, Knative co-founder. “It’s gratifying to see the vision that inspired Knative realized and adopted across the CNCF.</description></item><item><title>A TPM-based combined remote attestation method for confidential computing</title><link>https://kubermates.org/docs/2025-10-08-a-tpm-based-combined-remote-attestation-method-for-confidential-computing/</link><pubDate>Wed, 08 Oct 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-08-a-tpm-based-combined-remote-attestation-method-for-confidential-computing/</guid><description>Problem statement Scenario Representative existing approaches Our solution: Hybrid attestation with TPM and TEE integration Workflow overview Implementation with Hygon CSV Advantages of the combined approach Posted on October 8, 2025 by Andy, Confidential Computing Engineer,JD. COM Confidential computing technologies such as Intel TDX and AMD SNP rely on hardware-controlled Roots of Trust (RoT), inherently binding remote attestation to specific CPU vendors. While these solutions offer strong security guarantees, they also introduce challenges for enterprises seeking compliance, transparency, and independence in managing their Trusted Execution Environments (TEEs). This raises key questions: How can organizations leverage TEE security features (e. g. , memory encryption, attestation) without being fully dependent on vendor-controlled RoT? Is it possible to partially decouple the RoT while maintaining robust security guarantees? Consider a large enterprise deploying Intel TDX or AMD SNP servers, but with a requirement that the Root of Trust (RoT) be shared with a third-party authority—such as an internal certificate authority (CA) or an industry-wide trust anchor—to enable independent verification and operational flexibility. The goal is to retain the security benefits of TEEs while mitigating vendor lock-in. Strengths: Fully vendor-agnostic and open-source. Utilizes TPM for attestation, minimizing reliance on CPU vendors. Fully vendor-agnostic and open-source. Utilizes TPM for attestation, minimizing reliance on CPU vendors. Considerations: Adopts an SGX-like, process-based isolation model, which, as of the time of this blog, may present compatibility challenges for integration with TDX/SNP ecosystems.</description></item><item><title>Karmada v1.15 Released! Enhanced Resource Awareness for Multi-Template Workloads</title><link>https://kubermates.org/docs/2025-10-08-karmada-v1-15-released-enhanced-resource-awareness-for-multi-template-workloads/</link><pubDate>Wed, 08 Oct 2025 00:56:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-08-karmada-v1-15-released-enhanced-resource-awareness-for-multi-template-workloads/</guid><description>Overview of New Features Precise Resource Awareness for Multi-Template Workloads Enhanced Cluster-Level Failover Functionality Structured Logging Significant Performance Improvements for Karmada Controllers and Schedulers Acknowledging Our Contributors References： Posted on October 7, 2025 by The Karmada Maintainers CNCF projects highlighted in this post Karmada is an open multi-cloud and multi-cluster container orchestration engine designed to help users deploy and operate business applications in a multi-cloud environment. With its compatibility with the native Kubernetes API, Karmada can smoothly migrate single-cluster workloads while still maintaining coordination with the surrounding Kubernetes ecosystem tools. Karmada v1.15 has been released, this version includes the following new features: Precise resource awareness for multi-template workloads Enhanced cluster-level failover functionality Structured logging Significant performance improvements for Karmada controllers and schedulers Karmada utilizes a resource interpreter to retrieve the replica count and resource requests of workloads. Based on this data, it calculates the total resource requirements of the workloads, thereby enabling advanced capabilities such as resource-aware scheduling and federated quota management. This mechanism works well for traditional single-template workloads. However, many AI and big data application workloads (e. g. , FlinkDeployments, PyTorchJobs, and RayJobs) consist of multiple Pod templates or components, each with unique resource demands. Since the resource interpreter can only process resource requests from a single template and fails to accurately reflect differences between multiple templates, the resource calculation for multi-template workloads is not precise enough. In this version, Karmada has strengthened its resource awareness for multi-template workloads. By extending the resource interpreter, Karmada can now obtain the replica count and resource requests of different templates within the same workload, ensuring data accuracy. This improvement also provides more reliable and granular data support for federated quota management of multi-template workloads.</description></item><item><title>4 agentic AI use cases for telco</title><link>https://kubermates.org/docs/2025-10-08-4-agentic-ai-use-cases-for-telco/</link><pubDate>Wed, 08 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-08-4-agentic-ai-use-cases-for-telco/</guid><description>4 agentic AI use cases for telco 4 cutting-edge use cases L3 customer support assistant for network operations Intelligent autonomous radio assistant for spectrum efficiency Agentic AI-powered customer experience analytics Autonomous intelligent network with event-driven automation The technology powering the transformation Beyond AI: The broader Red Hat portfolio ecosystem Red Hat as the AI platform partner at India Mobile Congress 2025 The adaptable enterprise: Why AI readiness is disruption readiness About the authors Atul Deshpande Rob McManus More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share It’s the peak traffic hour on a busy weekday evening. A common occurrence for a typical telecommunication (telco) service provider. Millions of subscribers are streaming videos, playing games, and making calls. Suddenly, a network outage occurs in a key metropolitan area. Level 3 (L3) operations team at the service provider would begin troubleshooting the incident. They would manually sift through logs across multiple domains (radio access network, core, transport) and race against time to restore services. Hours, sometimes days, could pass before the incident was fully resolved, leading to frustrated customers, lost revenue, and reputational damage. How different would the situation have been if artificial intelligence (AI) was involved? An agentic AI-driven network would detect anomalies in real time, identifying the root cause and resolving the issue autonomously before customers even noticed a problem. Operations teams would be able to move from a reactive posture to a proactive one, focusing instead on strategic innovation and customer experience. Building AI-driven autonomous intelligent networks The telco industry is at a pivotal moment. With 5G fully deployed, and 5G advanced and 6G on the horizon, networks are more complex and data-intensive than ever. Manual approaches to network deployment and operations simply cannot keep up with the scale of current and future demand.</description></item><item><title>The MLOps Challenge: Scaling from one model to thousands</title><link>https://kubermates.org/docs/2025-10-08-the-mlops-challenge-scaling-from-one-model-to-thousands/</link><pubDate>Wed, 08 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-08-the-mlops-challenge-scaling-from-one-model-to-thousands/</guid><description>The MLOps Challenge: Scaling from one model to thousands What if managing models didn’t have to be chaotic? How to put this into practice An example in practice From chaos to control The adaptable enterprise: Why AI readiness is disruption readiness About the authors Robert Lundberg Cansu Kavili Oernek More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Taking a single AI model from idea to production is already a journey. You need to gather data, build and train the model, deploy it, and keep it running. That alone is challenging, but still manageable. This is where MLOps comes in: applying automation and best practices so the process is reliable and repeatable. But what happens when one model becomes a thousand? The artisanal, one-off approach that worked for a single model quickly collapses—retraining by hand becomes unsustainable, deployments drift out of sync, lineage and auditability are lost, and security gaps can appear. The good news: managing large numbers of AI models doesn’t have to be chaos. Start looking at it as a system, as an automated factory for AI , and scale will start working for you. Scaling AI doesn’t have to feel overwhelming. Instead of treating each model as a one-off project, think of them as part of a well-managed system. Imagine an assembly line where: Adding a new model is as simple as adding a new configuration file. Retraining happens automatically whenever fresh data arrives—no more manual babysitting. Security checks, scans, and signatures are baked into the process, like quality control in modern software delivery.</description></item><item><title>What’s new in Red Hat Ansible Automation Platform 2.6</title><link>https://kubermates.org/docs/2025-10-08-what-s-new-in-red-hat-ansible-automation-platform-2-6/</link><pubDate>Wed, 08 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-08-what-s-new-in-red-hat-ansible-automation-platform-2-6/</guid><description>What’s new in Red Hat Ansible Automation Platform 2.6 Unlock more value: Monitor, measure, and report on the impact of your automation Demonstrate value and measure ROI Make smarter, data-driven decisions Optimize and right-size your automation Report and share your data with enhanced security Operate more efficiently: Accelerate your IT operations with generative AI Achieve new levels of scale: Enable automation across the enterprise Unlock wider automation adoption Empower admins to share automation Simplify service delivery Self-service automation portal from two user perspectives Additional platform enhancements in AAP 2.6 Event-driven Ansible Refreshed user interface AI-assisted inventory generation (developer preview) Strategic integrations and new content The time to upgrade is now Next steps and resources Red Hat Ansible Automation Platform | Product Trial About the author Justin Braun More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Red Hat Ansible Automation Platform (AAP) 2.6 is now generally available, delivering new capabilities, platform enhancements, and strategic integrations to help you build resilient and trusted foundations for the next generation of IT operations, because you aren&amp;rsquo;t just preparing for the future — you are automating for it. The 2.6 release is headlined by three new features designed to drive key outcomes for automation and IT ops teams. When you upgrade, you can expect to: Unlock more value: The new automation dashboard allows you to measure the value and impact of automation initiatives. Operate more efficiently: The new Ansible Lightspeed Intelligent Assistant harnesses generative AI (gen AI) to provide on-demand support for a more intuitive platform experience. Achieve new levels of scale: With the self-service automation portal, you can quickly and easily scale automation service delivery to new users and teams. With the new automation dashboard in Red Hat Ansible Automation Platform 2.6, you can monitor, track, and report on the value your automation is delivering to the business. This on-premise utility provides real-time insights to help you make smarter, data-driven decisions. Customize reports to highlight time savings and ROI, then share them with key stakeholders in a more secure fashion to prove the value of your work. Readily track key metrics like job success rates, time savings, and return on investment (ROI) to prove your automation&amp;rsquo;s value and share successes with stakeholders. Get real-time, actionable insights into your automation usage and its impact, which helps you guide IT operations and make smarter decisions based on real data. Gain full visibility into your entire automation deployment, including both direct and indirect nodes. This helps you address over- and under-utilization, for maximizing your investment.</description></item><item><title>Your Red Hat OpenShift AI models are waiting at the door. Who’s knocking?</title><link>https://kubermates.org/docs/2025-10-08-your-red-hat-openshift-ai-models-are-waiting-at-the-door-who-s-knocking/</link><pubDate>Wed, 08 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-08-your-red-hat-openshift-ai-models-are-waiting-at-the-door-who-s-knocking/</guid><description>Your Red Hat OpenShift AI models are waiting at the door. Who’s knocking? The new front door is an API How Red Hat and F5 work together Build faster, stay protected Red Hat Product Security About the author Shane Heroux More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share You’ve trained the model, packaged it on Red Hat OpenShift AI, and it’s ready to work. The next move is exposing it through an API so people and applications can use it. At that moment, your model stops being an internal experiment and becomes a front-door service. And like any front door, somebody is going to knock … sometimes it’s the right user, sometimes not. Your model is no longer just a project in a lab: it’s a production endpoint. And like any endpoint, it’s a target. How do you ensure that only the right applications and users are interacting with it? How do you protect the sensitive data it might be trained on or the proprietary logic it contains? Every API endpoint is a target. For AI models, the attack surface is bigger than a simple app service. Beyond simple denial-of-service traffic, models can be tricked into leaking data through prompt injection, or probed until sensitive training data shows up. Even when no data leaks, attackers may try to hijack compute cycles or scrape responses in bulk. As deployments spread across clouds, datacenters, and edge sites, these risks multiply.</description></item><item><title>Announcing GPU Droplets accelerated by NVIDIA HGX H100 in the EU</title><link>https://kubermates.org/docs/2025-10-07-announcing-gpu-droplets-accelerated-by-nvidia-hgx-h100-in-the-eu/</link><pubDate>Tue, 07 Oct 2025 20:57:57 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-07-announcing-gpu-droplets-accelerated-by-nvidia-hgx-h100-in-the-eu/</guid><description>Announcing GPU Droplets accelerated by NVIDIA HGX H100 in the EU Accelerating training workloads with NVIDIA HGX H100s and DigitalOcean âWe have two terabytes of RAM on each of the nodes, which we leverage to train larger models. DigitalOceanâs ability to provide us with all of the resources that we need, which we often canât find in other places, has been really helpful. â Simple, powerful GPU Droplets Get started today About the author Try DigitalOcean for free Related Articles Announcing cost-efficient storage with Network file storage, cold storage, and usage-based backups Announcing per-sec billing, new Droplet plans, BYOIP, and NAT gateway preview to reduce scaling costs Storage that thinks for itself: Introducing Storage autoscaling, the newest feature for Managed Databases By Waverly Swinton Published: October 7, 2025 2 min read Demand for high-performance computing resources, particularly for cutting-edge training and inference workloads, continues to grow exponentially. We know that developers and businesses across Europe need simple, localized access to powerful GPUs to keep their innovation pipelines moving fast. Weâre excited to announce that NVIDIA HGX H100s as GPU DropletsâDigitalOceanâs on-demand, GPU-powered, virtual machinesâare now available in our Amsterdam data center. By integrating NVIDIA HGX H100 GPUs into an EU-based data center, weâre making it simpler for developers and digital native enterprises to access cutting-edge performance with the simplicity and affordability youâve come to expect from DigitalOcean. NVIDIA HGX H100 is a powerful GPU that is known for its training speed. It covers a range of use cases, including training LLMs, inference, and high-performance computing. Learn more about its full range of capabilities&amp;gt; DigitalOcean customers are putting NVIDIA HGX H100s to the test with complex use cases. WindBorne Systems, which operates the largest balloon constellation in the world while also developing state-of-the-art deep learning models for real-time forecasting, relies on NVIDIA HGX H100s on DigitalOcean for training. Combining NVIDIAâs state-of-technology with DigitalOceanâs resources, they found that DigitalOceanâs infrastructure allowed them to train models on H100s faster and more cost-effectively than other cloud options. Anuj Shetty, Machine Learning Engineer at WindBorne Systems Read the full story &amp;gt; Like all DigitalOcean products, GPU Droplets are designed for simplicity.</description></item><item><title>Analyst Insight Series: VMware Cloud Foundation Update – A RedMonk Conversation</title><link>https://kubermates.org/docs/2025-10-07-analyst-insight-series-vmware-cloud-foundation-update-a-redmonk-conversation/</link><pubDate>Tue, 07 Oct 2025 17:30:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-07-analyst-insight-series-vmware-cloud-foundation-update-a-redmonk-conversation/</guid><description>Discover more from VMware Cloud Foundation (VCF) Blog Related Articles Analyst Insight Series: VMware Cloud Foundation Update - A RedMonk Conversation VCF Breakroom Chats Episode 61: From VI Admin to Cloud Hero - Building a Multi-Tenant Cloud with VCF 9.0 VMware Cloud Foundation Automation – Infrastructure Resource Policy Overview In this new Redmonk Conversation, James Governor, co-founder of Redmonk, interviews Prashanth Shenoy, VP of Marketing, VMware Cloud Foundation Division at Broadcom. Redmonk is an advisory firm specializing in developer strategy, open source, and community building. James has made a career of understanding what motivates developers and the teams that support them. Governor and Shenoy address developments at VMware since the acquisition by Broadcom in November 2023, including the recent launch of VMware Cloud Foundation (VCF) 9.0, our industry leading private cloud solution. They cover a wide range of topics, including VMware’s business model simplification post-acquisition. Shenoy discusses the dramatically simplified portfolio of offerings, product licensing changes, and the greater focus on delivering a single, integrated private cloud platform with VMware Cloud Foundation. While acknowledging these changes may have caused some disruption for customers and partners, he points out that they have resulted in a more streamlined product experience and lower total cost of ownership for VMware’s customers, as well as greater standardization within the partner ecosystem. Shenoy covers the new capabilities in VCF 9.0 and how they support strategic customers interested in cloud transformation and application modernization, while improving overall security. He covers how Broadcom’s leadership in GPU virtualization and management is helping customers move at the pace of AI. He discusses how new Kubernetes and DevOps capabilities and VCF’s single platform for deploying and managing containers and VMs is helping to deliver a frictionless modern apps experience for Platform Engineers and application teams. At Broadcom, we view IT as a business enabler and key contributor to an organization’s success. This interview demonstrates that, post-acquisition, Broadcom’s commitment to supporting our customers in this respect is greater than ever.</description></item><item><title>Auditing user activity in pods and nodes with the Security-Profiles-Operator</title><link>https://kubermates.org/docs/2025-10-07-auditing-user-activity-in-pods-and-nodes-with-the-security-profiles-operator/</link><pubDate>Tue, 07 Oct 2025 15:29:47 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-07-auditing-user-activity-in-pods-and-nodes-with-the-security-profiles-operator/</guid><description>How the audit logging feature in SPO works How to enable it Considerations while using privileged containers Performance considerations Conclusion Posted on October 7, 2025 by Neeraj Krishna Gopalakrishna &amp;amp; Red Hat OpenShift Node Team CNCF projects highlighted in this post Kubernetes’ native audit logs are essential for tracking control plane activities, but they fail to capture what happens inside a container or on the host node itself during kubectl debugging sessions. This creates a security and compliance gap, as malicious or unauthorized actions within a pod or on the node go unrecorded. Without this visibility, it’s impossible to fully understand user actions, conduct an incident investigation, or meet regulatory compliance requirements. The newly added audit logging feature in Security-Profiles-Operator (SPO) addresses this by enabling detailed, server-side logging of user activity at the node level. Let’s imagine a scenario: A team of engineers is responsible for maintaining an application that processes financial transactions. The application is deployed on Kubernetes, with various containers (in pods) running different workloads. When a customer reports a bug, an engineer is granted access to the pod via kubectl exec to debug the issue (also kubectl debug node). However, during the session, they accidentally delete/modify a crucial configuration file. The application fails, leading to financial losses and business disruption. During a post-incident audit, the security team can see from the Kubernetes control plane logs that the engineer initiated a kubectl exec session. But the logs don’t reveal what commands were run inside the container. It’s an operational black box.</description></item><item><title>VCF Breakroom Chats Episode 62: Cloud Governance Unlocked – From VI Admin to Cloud Policy Pro with VCF 9.0</title><link>https://kubermates.org/docs/2025-10-07-vcf-breakroom-chats-episode-62-cloud-governance-unlocked-from-vi-admin-to-cloud-/</link><pubDate>Tue, 07 Oct 2025 15:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-07-vcf-breakroom-chats-episode-62-cloud-governance-unlocked-from-vi-admin-to-cloud-/</guid><description>About the VCF Breakroom Chat Series Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VCF Breakroom Chats Episode 62: Cloud Governance Unlocked - From VI Admin to Cloud Policy Pro with VCF 9.0 VCF Breakroom Chats Episode 61: From VI Admin to Cloud Hero - Building a Multi-Tenant Cloud with VCF 9.0 VMware Cloud Foundation Automation – Infrastructure Resource Policy Overview Welcome to another exciting edition of the VCF Breakroom Chats! In this latest video feature, we’re thrilled to have Vincent Riccio, Product Marketing Engineer for the VCF Division at Broadcom. Join Vincent and Alina Thylander as they delve into the transformative capabilities of cloud governance with VCF 9.0. Discover how VI Admins can seamlessly transition to Cloud Admin roles by harnessing the power of policy as code. This insightful discussion will reveal strategies for achieving effortless governance and fostering flexible self-service models in multi-tenant environments. Don’t miss out on this opportunity to enhance your team’s productivity while ensuring top-notch security and compliance—tune in now! Want to learn more about VCF Automation? Check out the VCF Automation web page for more resources. Read this blog post: Private Cloud Redefined: Deliver a Unified Cloud Consumption Experience with VMware Cloud Foundation 9.0 Explore VMware Cloud Foundation Automation by the Numbers with the Forrester Total Economic Impact (TEI) Study This webinar series focuses on VMware Cloud Foundation (VCF). In this series, we share conversations with industry-recognized experts from Broadcom and with solution partners and customers. You’ll gain relevant insights whether you’re an IT practitioner, IT admin, cloud or platform architect, developer, DevOps, senior IT manager, IT executive, or AI/ML professional. If you are new to the series, please check out our previous episodes.</description></item><item><title>Managing Kubernetes Workloads Using the App of Apps Pattern in ArgoCD-2</title><link>https://kubermates.org/docs/2025-10-07-managing-kubernetes-workloads-using-the-app-of-apps-pattern-in-argocd-2/</link><pubDate>Tue, 07 Oct 2025 14:36:18 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-07-managing-kubernetes-workloads-using-the-app-of-apps-pattern-in-argocd-2/</guid><description>Posted on October 7, 2025 by Marcin Kujawski, Principal Kubernetes Engineer, Software Mind CNCF projects highlighted in this post Managing a cloud native infrastructure at scale is no longer just about deploying single applications – it’s about organizing environments, defining clear boundaries and keeping everything version-controlled, consistent, automated and easily managed within a simple and clear lifecycle process. This is where GitOps practices – tools like ArgoCD – truly shine. One of ArgoCD’s most powerful patterns is called App of Apps – a design where a single parent ArgoCD Application manages and deploys multiple child ArgoCD Applications. It brings order to potentially chaotic deployments, improves observability and aligns perfectly with GitOps methodologies. In this article, we’ll explore the App of Apps pattern in ArgoCD, including its concept, structure, pros and cons, as well as a production-grade way to configure child applications via dedicated values files. You’ll learn how to build a reusable, scalable pattern by deploying two example applications: NGINX Ingress Controller Cert-Manager Let’s dive in. What Is the App of Apps Pattern in ArgoCD? Concept The App of Apps pattern is an ArgoCD deployment strategy where one central ArgoCD Application (the “parent”) is responsible for managing and deploying several other Applications (the “children”). Each child application corresponds to a logical unit (e. g. , a microservice, controller, or platform component) and is fully managed by ArgoCD. This enables a modular, hierarchical structure – especially useful in environments where infrastructure is composed of many separate but related components. Real-World Analogy Think of the parent app as a conductor of an orchestra.</description></item><item><title>Mitigating AI's new risk frontier: Unifying enterprise cybersecurity with AI safety</title><link>https://kubermates.org/docs/2025-10-07-mitigating-ai-s-new-risk-frontier-unifying-enterprise-cybersecurity-with-ai-safe/</link><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-07-mitigating-ai-s-new-risk-frontier-unifying-enterprise-cybersecurity-with-ai-safe/</guid><description>Mitigating AI&amp;rsquo;s new risk frontier: Unifying enterprise cybersecurity with AI safety AI security vs. AI safety AI security AI safety Examples of security and safety risks The new risk frontier: Demystifying “safety” Should the AI model get all the attention? Unifying AI security with enterprise cybersecurity Moving forward Conclusion Learn more Red Hat Product Security About the authors Ishu Verma Florencio Cano Gabarda More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share These are exciting times for AI. Enterprises are blending AI capabilities with enterprise data to deliver better outcomes for employees, customers, and partners. But as organizations weave AI deeper into their systems, that data and infrastructure also become more attractive targets for cybercriminals and other adversaries. Generative AI (gen AI), in particular, introduces new risks by significantly expanding an organization’s attack surface. That means enterprises must carefully evaluate potential threats, vulnerabilities, and the risks they bring to business operations. Deploying AI with a strong security posture, in compliance with regulations, and in a way that is more trustworthy requires more than patchwork defenses, it demands a strategic shift. Security can’t be an afterthought—it must be built into the entire AI strategy. AI security and AI safety are related but distinct concepts. Both are necessary to reduce risk, but they address different challenges. AI security focuses on protecting the confidentiality, integrity, and availability of AI systems. The goal is to prevent malicious actors from attacking or manipulating those systems.</description></item><item><title>Red Hat OpenStack VMware Migration toolkit deep-dive</title><link>https://kubermates.org/docs/2025-10-07-red-hat-openstack-vmware-migration-toolkit-deep-dive/</link><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-07-red-hat-openstack-vmware-migration-toolkit-deep-dive/</guid><description>Red Hat OpenStack VMware Migration toolkit deep-dive Highlights Ansible Integration with Ansible Automation Platform OpenStack APIs and Changed Block Tracking (CBT) for Warm Migration Infrastructure Components in the Migration Process Migration Workflow Overview Red Hat Ansible Automation Platform | Product Trial About the author Pedro Navarro Pérez More like this Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share When an organization currently on VMware evaluates transitioning to a new cloud platform, such as Red Hat OpenStack Services on OpenShift , a key initial concern is typically the effective migration of VMware virtual machine workloads. The Red Hat OpenStack VMware Migration toolkit specifically addresses this need, providing an Ansible collection that aims to simplify and automate the migration process from VMware environments into Red Hat OpenStack Services on OpenShift. This toolkit significantly reduces the complexity and downtime often associated with manual migrations, enabling organizations to rapidly and smoothly adopt cloud-native architectures. This article provides a deep dive into migrating VMware-based workloads to Red Hat OpenStack Services on OpenShift. By following this guide, you’ll gain the essential knowledge and tools needed to plan and execute a successful, efficient migration. The Red Hat OpenStack VMware Migration Toolkit, delivered as an Ansible collection, comes with a powerful set of features designed to streamline and scale your migration efforts: Discovery mode: automatically gathers metadata from VMware vCenter, including VM definitions, network configurations, and storage details. Network mapping: supports mapping of source networks and ports to target Red Hat OpenStack Services on OpenShift networks, including MAC address preservation. Flavor mapping and flavor creation: enables mapping VMware hardware profiles to OpenStack flavors or dynamically creates matching flavors during migration. Warm VM migration support: migrates running virtual machines with minimal disruption by leveraging snapshot and CBT-based mechanisms. Multi-disk and multi-NIC support: fully supports virtual machines with complex storage and networking setups, ensuring consistent and complete migration. Operating system compatibility: supports a broad range of guest OSes compatible with virt-v2v and certified for use with Red Hat OpenStack Services on OpenShift. One of the key advantages of the Red Hat OpenStack VMware Migration toolkit is its deep integration with Ansible, making it a natural fit for organizations using Ansible Automation Platform (AAP).</description></item><item><title>SQL Server 2025 Preview now supports Red Hat Enterprise Linux 10</title><link>https://kubermates.org/docs/2025-10-07-sql-server-2025-preview-now-supports-red-hat-enterprise-linux-10/</link><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-07-sql-server-2025-preview-now-supports-red-hat-enterprise-linux-10/</guid><description>SQL Server 2025 Preview now supports Red Hat Enterprise Linux 10 What’s new in SQL Server 2025 Preview RC1 New AI and vector data Improved developer experience Analytics and data connectivity Continued reliability RHEL 10 support in SQL Server 2025 RC1 Getting started Next steps Red Hat Ansible Automation Platform | Product Trial About the authors Vivien Wang Amit Khandelwal More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Microsoft&amp;rsquo;s SQL Server 2025 (17. x) Preview release candidate 1 (RC1) now includes preview support for Red Hat Enterprise Linux 10 (RHEL). This milestone expands SQL Server&amp;rsquo;s footprint on Linux, giving developers and IT professionals more choices for modern, high-performance deployments. SQL Server 2025 Preview is packed with AI and developer-friendly features, making RHEL 10 a great platform for experimentation. With RHEL 10 support, you can start validating SQL Server 2025 Preview in your development and test environments ahead of general availability. In addition to RHEL 10 support, SQL Server 2025 Preview introduces several exciting updates: Vector data type with support for half-precision floats (2-byte) for up to 3,996 dimensions in a single vector. Vector indexes for similarity search. External AI model management that integrates with REST inference endpoints and embedding tasks. Regular expressions : Use regex directly in SQL queries. JSON data type : Store, query, and transform JSON natively. External REST endpoint invocation : Call REST/GraphQL services directly from SQL Server. PolyBase on Linux now supports ODBC data sources.</description></item><item><title>Introducing the DigitalOcean AI Ecosystem</title><link>https://kubermates.org/docs/2025-10-06-introducing-the-digitalocean-ai-ecosystem/</link><pubDate>Mon, 06 Oct 2025 20:07:17 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-06-introducing-the-digitalocean-ai-ecosystem/</guid><description>Introducing the DigitalOcean AI Ecosystem Explore Whatâs Possible with the DigitalOcean AI Ecosystem Letâs Drive the Next Wave of AI Innovation Together About the author Try DigitalOcean for free By Meghan Grady Director, Partner Growth Published: October 6, 2025 2 min read Last week at Deploy London , we announced a significant expansion of our offerings within the DigitalOcean AI Ecosystem and introduced the DigitalOcean AI Partner Program. A platform is only as strong as its ecosystem, and these initiatives are designed to create a comprehensive, flexible, and powerful environment for startups, builders, and developers to create the next generation of AI applications. Weâre collaborating with industry leaders and innovative startups to provide access to all of the AI products and services you need through the Gradient AI Agent Cloud. DigitalOceanâs comprehensive AI Ecosystem empowers startups and enterprises to build next-gen AI applications, offering choice and flexibility through embedded partner integrations and workflows. DigitalOceanâs AI Ecosystem provides access to: Powerful Infrastructure: Efficient AMD and NVIDIA GPUs to power your applications. Powerful Infrastructure: Efficient AMD and NVIDIA GPUs to power your applications. Advanced Models: Access to leading models from companies like OpenAI, DeepSeek, Meta, and Mistral. Applications &amp;amp; Frameworks: Popular AI dev tools and integration frameworks such as LangChain, LiteLLM, and dStack. Integrators: A network of AI specialists to help you migrate workflows and build robust new AI implementations. âAt the heart of AIâs progress are the developers who are turning ideas into reality. The Gradient AI Ecosystem is a powerful force multiplier for them, proving that the future isnât just about siliconâitâs about software and the collaborative community built on top of it,â said Anush Elangovan, VP of AI Software at AMD. âBy providing high-performance AMD GPUs through DigitalOceanâs Gradient AI Agentic Cloud, we ensure the ecosystem is powered by world-class, open-standard hardware.</description></item><item><title>Announcing ORAS v1.3.0: Elevating artifact and registry management workflows</title><link>https://kubermates.org/docs/2025-10-06-announcing-oras-v1-3-0-elevating-artifact-and-registry-management-workflows/</link><pubDate>Mon, 06 Oct 2025 17:07:15 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-06-announcing-oras-v1-3-0-elevating-artifact-and-registry-management-workflows/</guid><description>Your registry’s safety net: Portable backup &amp;amp; restore Multi-platform image and artifact management Enable scripting and automation: Formatted output Stability &amp;amp; user experience polish Why this release matters Thanks to all contributors Posted on October 6, 2025 by Feynman Zhou CNCF projects highlighted in this post The ORAS community is thrilled to announce the release of ORAS CLI v1.3.0, a version packed with stability improvements and pioneering capabilities. In addition to strengthening existing functionality, this release introduces three major new features designed to enhance artifact and registry management workflows: Portable backup and restore of repositories and artifacts Multi-platform image and artifact management Rich formatted output for scripting and pipelines Moreover, ORAS is now fully compliant with OCI distribution-spec v1.1.1. With oras backup and oras restore, ORAS now lets you save your registry content into local directories or tarballs (OCI image layout format) as a snapshot and restore to any registry. All manifests, along with any optional tags and referrers, will be included in the backup. Use cases include: Air-gapped environments: Organizations operating in isolated or high-security environments can use oras backup to export artifacts from a registry to the local filesystem, and oras restore to import them into an internal registry with restricted access. Disaster recovery and audit archival: Take periodic snapshots of repositories and store them off-site. In case of accidental deletions, outages, or long-term storage for regulatory audits, oras restore can quickly recover full registry content. Registry migration: When moving from one container registry provider to another, these commands enable a full repository export, preserving tags, manifests, layers, and referrers. Compliance and supply chain security: Back up and restore images along with their supply chain artifacts, such as SBOMs, signatures, and vulnerability scanning reports. Repository duplication or promotion: Move artifacts from dev to staging to production registries reliably using an intermediate backup file. Check out the user guide Backup and Restore of OCI Artifacts, Images, and Repositories for details. Multi-platform images are commonly used in IoT and Edge computing, particularly in heterogeneous deployments.</description></item><item><title>KubeCon + CloudNativeCon North America 2025 Co-Located Event Deep Dive: Open Source SecurityCon</title><link>https://kubermates.org/docs/2025-10-06-kubecon-cloudnativecon-north-america-2025-co-located-event-deep-dive-open-source/</link><pubDate>Mon, 06 Oct 2025 13:14:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-06-kubecon-cloudnativecon-north-america-2025-co-located-event-deep-dive-open-source/</guid><description>Who will get the most out of attending this event? What will the day look like? Should I do any homework first? Find your community! A note from the co-chairs Posted on October 6, 2025 by Co-chairs, Marina Moore &amp;amp; John Kjell Open Source SecurityCon has always been about bringing people together to strengthen trust in open source. From its beginnings within TAG Security to its growth as a standalone conference, and now returning to KubeCon + CloudNativeCon alongside the Open Source Security Foundation (OpenSSF) , the event has become a gathering place for anyone passionate about securing our shared ecosystem. As a co-located event, it will bring together software developers, security engineers, public sector leaders, CISOs, CIOs, and technology innovators. Through interactive discussions, expert-led sessions, and hands-on activities, participants will dive into emerging security challenges, explore the latest industry innovations, and share best practices that are shaping the future of secure software development. You don’t need to be a security practitioner to attend! This event will have lots of great security content, but it is a place for all developers to come and get their security questions answered. Whether you are a security engineer, DevOps professional, software developer, cloud architect, IT manager, risk and compliance officer, or open source contributor, this event has something for you. Join us if you’re an open source contributor or maintainer who wants to improve the security of their projects and learn about the landscape of open source security. Security is no longer just a consideration. It is a necessity. Open Source SecurityCon 2025 is a welcoming space for anyone interested in building more secure, scalable software. Even if you’re not an open source developer, the ways of securing open source can be applied to any code and development process. The best part, all the tools and frameworks that will be showcased are open source.</description></item><item><title>AI-Driven Platform Governance: The Next Frontier for Engineering</title><link>https://kubermates.org/docs/2025-10-06-ai-driven-platform-governance-the-next-frontier-for-engineering/</link><pubDate>Mon, 06 Oct 2025 08:00:19 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-06-ai-driven-platform-governance-the-next-frontier-for-engineering/</guid><description>AI-Driven Platform Governance: The Next Frontier for Engineering The New Governance Challenge What is AI-Driven Governance? Why Care? Why Now The Nirmata Value Looking Ahead Over the past decade, cloud-native technologies and Kubernetes have become the foundation for how enterprises build and run software. At the same time, artificial intelligence (AI) has catapulted from experimentation to mainstream adoption. This convergence is reshaping not only how applications are created but also how they are implemented, secured, and governed. As the creators of Kyverno , the leading open-source policy-as-code engine for Kubernetes, we’ve seen firsthand how organizations use policies to define intent, enforce standards, and manage risk. Kyverno excels at surfacing issues and helping teams understand where their guardrails may be failing. But detection alone isn’t enough. Teams are still left with the burden of figuring out how to remediate issues, apply fixes across environments, and maintain compliance at scale. This is why platform engineering has become both the bottleneck and the enabler of the AI future. Without scalable governance and guardrails, platform engineering slows down innovation. But with AI-driven governance and automated policy enforcement , platform engineering becomes the discipline that unlocks velocity, resilience, and trust—creating the foundation AI needs to thrive. The question is no longer IF we need governance, but how we can establish effective guardrails, controls, and compliance without slowing down innovation. That’s where AI-driven governance comes in.</description></item><item><title>Introducing Headlamp Plugin for Karpenter - Scaling and Visibility</title><link>https://kubermates.org/docs/2025-10-06-introducing-headlamp-plugin-for-karpenter-scaling-and-visibility/</link><pubDate>Mon, 06 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-06-introducing-headlamp-plugin-for-karpenter-scaling-and-visibility/</guid><description>Introducing Headlamp Plugin for Karpenter - Scaling and Visibility Map view of Karpenter Resources and how they relate to Kubernetes resources Visualization of Karpenter Metrics Scaling decisions Config editor with validation support Real time view of Karpenter resources Dashboard for Pending Pods Karpenter Providers How to use Feedback and Questions Headlamp is an open‑source, extensible Kubernetes SIG UI project designed to let you explore, manage, and debug cluster resources. Karpenter is a Kubernetes Autoscaling SIG node provisioning project that helps clusters scale quickly and efficiently. It launches new nodes in seconds, selects appropriate instance types for workloads, and manages the full node lifecycle, including scale-down. The new Headlamp Karpenter Plugin adds real-time visibility into Karpenter’s activity directly from the Headlamp UI. It shows how Karpenter resources relate to Kubernetes objects, displays live metrics, and surfaces scaling events as they happen. You can inspect pending pods during provisioning, review scaling decisions, and edit Karpenter-managed resources with built-in validation. The Karpenter plugin was made as part of a LFX mentor project. The Karpenter plugin for Headlamp aims to make it easier for Kubernetes users and operators to understand, debug, and fine-tune autoscaling behavior in their clusters. Now we will give a brief tour of the Headlamp plugin. Easily see how Karpenter Resources like NodeClasses, NodePool and NodeClaims connect with core Kubernetes resources like Pods, Nodes etc. Get instant insights of Resource Usage v/s Limits, Allowed disruptions, Pending Pods, Provisioning Latency and many more. Shows which instances are being provisioned for your workloads and understand the reason behind why Karpenter made those choices.</description></item><item><title>SUSE and Tigera: Empowering Secure, Scalable Kubernetes with Calico Enterprise</title><link>https://kubermates.org/docs/2025-10-03-suse-and-tigera-empowering-secure-scalable-kubernetes-with-calico-enterprise/</link><pubDate>Fri, 03 Oct 2025 23:32:25 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-03-suse-and-tigera-empowering-secure-scalable-kubernetes-with-calico-enterprise/</guid><description>Modern Workloads Demand Modern Kubernetes Infrastructure Comprehensive Security Without Compromise Resilience and Visibility at Scale Operational Simplicity for Platform Teams Open Source Roots, Enterprise Strength Choice Backed by Enterprise Support The Path Forward As organizations expand Kubernetes adoption—modernizing legacy applications on VMs and bare metal, running next-generation AI workloads, and deploying intelligence at the edge—the demand for infrastructure that is scalable, flexible, resilient, secure, and performant has never been greater. At the same time, compliance, consistent visibility, and efficient management without overburdening teams remain critical. The combination of Calico Enterprise from Tigera and SUSE Rancher Prime delivers a resilient and scalable platform that combines high-performance networking, robust network security, and operational simplicity in one stack. Calico Enterprise provides a unified platform for Kubernetes networking, security, and observability: eBPF-powered networking for high performance without sidecar overhead One platform for all Kubernetes traffic: ingress, egress, in-cluster, and multi-cluster Security for every workload type: containers, VMs, and bare metal Seamless scaling with built-in multi-cluster networking and security Zero-trust security with identity-aware policies and workload-based microsegmentation Integrated observability for policy enforcement and troubleshooting Compliance features that simplify audits (PCI-DSS, HIPAA, SOC 2, FedRAMP) Deployed with Rancher Prime, these capabilities extend directly into every cluster, enabling security-conscious industries such as finance, healthcare, and government to confidently run Kubernetes for any use case—from application modernization to AI and edge computing. Modern enterprises run Kubernetes across multiple clusters, regions, and clouds. Rancher Prime simplifies and unifies this landscape, while Calico Enterprise adds: Highly available and resilient networking designed for mission-critical applications Standardized networking and network security controls across multiple clusters, regions, and clouds Centralized network policy and compliance enforcement across all clusters Dynamic service discovery and adaptive network controls Rich observability, including real-time flow logs and network performance metrics This combination gives Site Reliability Engineering (SRE) and DevSecOps teams the visibility they need to prevent outages, detect threats faster, and reduce downtime — even in complex multi-cloud topologies. Rancher Prime acts as the single pane of glass for cluster lifecycle management, while Calico Enterprise integrates seamlessly to provide a single pane of glass for network security, observability and troubleshooting with minimal operational burden. Together, they deliver a frictionless experience for platform teams : unified deployment, automated policy propagation, and security guardrails baked into the platform — not bolted on after the fact. Both SUSE and Tigera are committed to open innovation and customer choice. Calico, the open-source foundation of Calico Enterprise, is trusted by thousands of organizations and runs on more than 100,000 clusters worldwide. Rancher Prime is fully open source as well, backed by enterprise SLAs and global support from SUSE. Enterprises can modernize their infrastructure without lock-in, accelerating time-to-value for developers while staying in control of security and compliance.</description></item><item><title>How to Connect Nested KubeVirt Clusters with Calico and BGP Peering</title><link>https://kubermates.org/docs/2025-10-03-how-to-connect-nested-kubevirt-clusters-with-calico-and-bgp-peering/</link><pubDate>Fri, 03 Oct 2025 18:28:12 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-03-how-to-connect-nested-kubevirt-clusters-with-calico-and-bgp-peering/</guid><description>Why BGP Peering for Nested Clusters? Key Challenges When Peering With Nested Clusters Dynamic IPs vs. With KubeVirt , a virtualization add-on for Kubernetes that uses QEMU (an open-source machine emulator and virtualizer), you can run full-featured Kubernetes clusters as virtual machines (VMs) inside a parent Kubernetes cluster. This nested architecture makes it possible to unify containerized and virtualized workloads, and opens the door to new platform engineering use cases. But here’s the challenge: how can you ensure that these nested clusters, and the workloads within, can reach, and be reached by, your physical network and are treated the same way as any other cluster? That’s where Calico’s Advanced BGP (Border Gateway Protocol) peering with workloads comes into play. By enabling BGP route exchange between the parent cluster and nested KubeVirt VMs, Calico extends dynamic routing directly to virtualized workloads. This allows nested clusters to participate in the broader network topology and advertise their pod and service IPs just like any other node. Thus eliminating the need for tunnels or overlays to achieve true layer 3 connectivity. In this blog, we’ll walk through the big picture, prerequisites, and step-by-step configuration for setting up BGP peering between parent clusters and nested clusters running inside KubeVirt. Platform engineering teams today face a common challenge: delivering a platform that meets performance, security, and cost requirements—without forcing developers to decide where their applications should run. Whether the underlying platform is on-premises, in the cloud, or virtualized with KubeVirt, those choices should be taken care of automatically, without developer involvement. Developers should simply request a resource to run their applications, while the platform team ensures the correct resources are delivered on available platforms. Enabling BGP peering on nested KubeVirt clusters means they can be treated just like any other cluster, with no ‘special’ or snowflake-style configurations required simply because they are virtualized.</description></item><item><title>KubeCon + CloudNativeCon North America 2025 Co-Located Event Deep Dive: Kubernetes on Edge Day</title><link>https://kubermates.org/docs/2025-10-03-kubecon-cloudnativecon-north-america-2025-co-located-event-deep-dive-kubernetes-/</link><pubDate>Fri, 03 Oct 2025 13:01:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-03-kubecon-cloudnativecon-north-america-2025-co-located-event-deep-dive-kubernetes-/</guid><description>Who will get the most out of attending this event? What is new and different this year? What will the day look like? Should I do any homework first? Find your community! Posted on October 3, 2025 by Co-chairs, Katerina Arzhayev, Gergely Csatári, and Mars Toktonaliev CNCF projects highlighted in this post The inaugural Edge Day launched as a co-located event at KubeCon + CloudNativeCon EU in 2022, recognizing that data at the edge is here to stay. Once called the ‘Internet of Things’ and later ‘Industry 4.0,’ today ‘edge computing’ has emerged as the unifying term, reflecting the evolution from centralized data centers and cloud computing to the very edge of the network. Kubernetes on Edge Day embraces this shift with a thoughtful agenda that brings together academic research, real-world enterprise use cases, and insights from peers across the Kubernetes community. This is a wonderful event for everyone who wants to get as close to the newest technology as they can, without going to a quantum computing conference 😊. Engineers working at large companies who are looking to modernize and containerize their applications will be able to explore real-world business applications of technological innovation. Students and researchers will hear from other academics on what and how they are doing to push the limits of what is typically thought to be possible. At Kubernetes on Edge Day, we bring together experts for an audience who are tackling some of the most exciting and complex problems in cloud computin today, and tomorrow. We did our best to limit the topics to stay as close to true “edge computing” as we could, but the reality is edge computing is the future that is leaking into all other categories, verticals, industries, and technologies. This year, we had submissions covering artificial intelligence at the edge, edge computing for telco deployments, the effect and importance of data on Kubernetes, the vitality of observability at scale for these systems, and many more. We hope that this year’s agenda tells an interesting story about Kubernetes at the Edge. The event will kick off in the main room with a welcome from the chairs. After that, we’ll dive straight into the featured sessions.</description></item><item><title>Classifying human-AI agent interaction</title><link>https://kubermates.org/docs/2025-10-03-classifying-human-ai-agent-interaction/</link><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-03-classifying-human-ai-agent-interaction/</guid><description>Classifying human-AI agent interaction Humans and the AI “loop” A classification framework for human-AI interaction patterns Temporal positioning patterns group Direct engagement patterns group Strategic and oversight patterns group Human-Over-the-Loop (HOvL: Oversight with Veto) Minimal or reversed involvement patterns group Final thoughts Get started with AI agents The adaptable enterprise: Why AI readiness is disruption readiness About the author Richard Naszcyniec More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share It&amp;rsquo;s hard to deny that we now live in a time where AI permeates everyday life—from customer service bots to autonomous assistants. However, poorly designed AI solutions can lead to misplaced trust, misinformation, and ethical lapses, as evidenced by several high-profile failures. Air Canada&amp;rsquo;s chatbot once misled a grieving passenger with inaccurate refund advice , resulting in a tribunal ruling that held the airline accountable for the AI&amp;rsquo;s errors and underscoring the legal risks of unchecked automation. Microsoft&amp;rsquo;s Bing AI, dubbed Sydney , veered into threatening and manipulative behavior during conversations, attempting to undermine users&amp;rsquo; personal relationships and highlighting the psychological dangers of unmoderated AI personas. The Zillow Offers home-flipping business was terminated after the AI algorithm unintentionally purchased homes at higher prices than its current estimates of future selling prices, resulting in a $304 million inventory write-down, and a workforce reduction of 2000 employees. Based on the failures mentioned, it&amp;rsquo;s clear that simply deploying AI is not enough. To achieve a positive outcome and avoid costly errors, the deliberate design and implementation of human-AI collaboration is essential. An approach that includes human planning, interaction, and oversight is critical for positive results. AI agents are also a hot topic—semi-autonomous AI agents that can notice “things” happening around them, make choices on their own, and work toward goals without needing constant human help. This means the ways people team up with AI are changing and growing more varied. We need a clear way to organize these teamwork styles, based on factors like how humans and AI connect, and on the different ways AI helps with decisions. With factors like those in mind, we can place people in the right roles, helping to keep things fair, avoid problems like AI interaction degrading over time, and create a human-AI partnership where both sides help each other.</description></item><item><title>Evolving our ServiceNow integration: Sunsetting the Notification Service for more capable alternatives</title><link>https://kubermates.org/docs/2025-10-03-evolving-our-servicenow-integration-sunsetting-the-notification-service-for-more/</link><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-03-evolving-our-servicenow-integration-sunsetting-the-notification-service-for-more/</guid><description>Evolving our ServiceNow integration: Sunsetting the Notification Service for more capable alternatives From simplicity to capability and flexibility New, more powerful integration options Option A: Script-based integration with Business Rules (pro-code option) Option B: Integration with Flow Designer (low-code option) Option C: Request-based automation with the Ansible spoke (request-driven option) Key deprecation dates What does this mean for you? Red Hat Ansible Automation Platform | Product Trial About the author Steve Fulmer More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share At Red Hat, our goal is to provide enterprise-grade automation solutions that evolve with your needs. As part of this commitment, we are deprecating the Red Hat ServiceNow Notification Service application from the ServiceNow Store. This isn&amp;rsquo;t a reduction in capability; it&amp;rsquo;s an evolution. We&amp;rsquo;re moving away from a simplified, one-size-fits-all application to more robust and modern integration patterns that offer the capabilities required by enterprise IT environments. The Red Hat ServiceNow Notification Service application was designed for simplicity, providing a straightforward way to send the entire ServiceNow record as a JavaScript Object Notation (JSON) payload to Event-Driven Ansible. In the past, the application was required to integrate ServiceNow with Event-Driven Ansible. While it served as an effective entry point for using ServiceNow with Event-Driven Ansible, our customers&amp;rsquo; automation needs have matured. Today, more control over data, more complex triggering logic, and more targeted, request-driven automation is required. To better serve the advanced requirements of our customers, we are standardizing on 3 different fully supported integration pathways for ServiceNow and Event-Driven Ansible. Customers should use 1 of 3 more capable and flexible integration methods. These options enable customized payloads, enrich event data, and build sophisticated workflows that organizations demand. All of these integration options are fully supported by ServiceNow, taking advantage of standard ServiceNow capabilities.</description></item><item><title>Friday Five — October 3, 2025</title><link>https://kubermates.org/docs/2025-10-03-friday-five-october-3-2025/</link><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-03-friday-five-october-3-2025/</guid><description>Friday Five — October 3, 2025 Red Hat Summit 2026 call for proposals is now open Red Hat Enterprise Linux (RHEL) on AWS Cloud Marketplace trial is now live What you don’t see could cost you: Why open source matters in enterprise AI Hitachi Vantara Collaborates with Red Hat to Accelerate Hybrid Cloud Transformation and Modernize Legacy Virtualization SiliconANGLE - Private AI takes root as enterprises demand open source consistency About the author Red Hat Corporate Communications More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Red Hat Summit is coming to Atlanta, Georgia and we&amp;rsquo;re looking for our community of experts—customers, partners, and associates—to share their knowledge and experiences. The call for proposals is open now through November 12. Submit your session proposal today for a chance to present at Red Hat Summit 2026. Learn more The new self-serve Red Hat Enterprise Linux (RHEL) on AWS Cloud Marketplace trial is now live! With the trial available in the cloud marketplace, it&amp;rsquo;s now easier for users to try and deploy RHEL directly within their existing AWS environments. Learn more Open source technology is vital for enterprise AI because it offers transparency, flexibility and control that closed &amp;ldquo;black-box&amp;rdquo; systems lack. While black-box systems are easy to start with, they can lead to issues like data bias, high costs, and a loss of digital sovereignty. Open source provides a way for organizations to adapt models with their own data, fostering a collaborative approach that is shaping the future of AI. Learn more Hitachi Vantara and Red Hat are collaborating on a new solution that combines Red Hat OpenShift Virtualization with Hitachi Vantara&amp;rsquo;s Virtual Storage Platform One. The offering helps companies modernize their aging virtualization environments by providing a unified, hybrid cloud platform that reduces reliance on costly, proprietary hypervisors and allows them to run VMs and containers side by side. Learn more Artificial intelligence is no longer just a tool for experiments — it is becoming the backbone of enterprise operations. Private AI, running where the data lives inside systems companies directly control, is reshaping how organizations build and manage their technology stacks. Learn more Red Hat is the world’s leading provider of enterprise open source software solutions, using a community-powered approach to deliver reliable and high-performing Linux, hybrid cloud, container, and Kubernetes technologies.</description></item><item><title>Introducing Red Hat OpenStack VMware migration toolkit</title><link>https://kubermates.org/docs/2025-10-03-introducing-red-hat-openstack-vmware-migration-toolkit/</link><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-03-introducing-red-hat-openstack-vmware-migration-toolkit/</guid><description>Introducing Red Hat OpenStack VMware migration toolkit What the toolkit can do for you Overview of the migration process Join the migration The adaptable enterprise: Why AI readiness is disruption readiness About the author Sean Cohen More like this Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Last year, Red Hat introduced Red Hat OpenStack Services on OpenShift , the next major release of Red Hat OpenStack Platform. The architectural shift brought the power of OpenStack’s Infrastructure-as-a-Service (IaaS) together with the agility of Red Hat OpenShift, allowing organizations to run virtualized and cloud-native workloads on a single, unified platform. As businesses face increasing pressure to optimize costs and consolidate IT environments, especially in light of shifting market dynamics, many are looking for a modern approach to managing their cloud infrastructure. To address that challenge, Red Hat is introducing Red Hat OpenStack VMware migration toolkit (based on the os-migrate VMware toolkit upstream project) to support the adoption of OpenStack Services on OpenShift. This new migration toolkit is designed to simplify and accelerate your journey to Red Hat OpenStack Services on OpenShift. It&amp;rsquo;s designed to address the real-world challenges of migrating mission critical workloads from VMware, giving you a streamlined and efficient path forward. With this toolkit, users can: Seamlessly move workloads : The tool provides a direct and automated pathway to migrate an existing virtual machine (VM) with minimal disruption. Leverage automation : Integrate the migration process into your existing Red Hat Ansible Automation Platform workflows. Scale migration efforts : This tool is designed to handle large-scale migrations, enabling you to move VMs efficiently over a short period. Unify management : The migration toolkit simplifies the experience of bringing your legacy virtualized workloads into a singular control plane, where they can coexist with your new cloud-native, containerized applications. Warm migration : A specialized Ansible collection aims to simplify and automate the migration process from VMWare environments into Red Hat OpenStack Services on OpenShift. The Red Hat OpenStack VMware migration toolkit is delivered as an Ansible collection.</description></item><item><title>VCF Breakroom Chats Episode 61: From VI Admin to Cloud Hero – Building a Multi-Tenant Cloud with VCF 9.0</title><link>https://kubermates.org/docs/2025-10-02-vcf-breakroom-chats-episode-61-from-vi-admin-to-cloud-hero-building-a-multi-tena/</link><pubDate>Thu, 02 Oct 2025 12:50:29 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-02-vcf-breakroom-chats-episode-61-from-vi-admin-to-cloud-hero-building-a-multi-tena/</guid><description>About the VCF Breakroom Chat Series Discover more from VMware Cloud Foundation (VCF) Blog Related Articles VCF Breakroom Chats Episode 61: From VI Admin to Cloud Hero - Building a Multi-Tenant Cloud with VCF 9.0 VMware Cloud Foundation Automation – Infrastructure Resource Policy Overview Set Your Implementation Up for Success with VCF Jumpstart Workshop Welcome to the latest edition of VCF Breakroom Chats! In this exciting episode, we’re thrilled to feature Maher Alasfar, a Product Marketing Engineer with the VCF Division at Broadcom. Join Maher and Alina Thylander as they dive into how Virtual Infrastructure Administrators can seamlessly shift to Cloud Admin roles, leveraging the power of VCF Automation. Uncover the benefits of streamlined automation, enhanced control, and improved operational efficiency that enable your team to expertly manage multi-tenant private clouds. Elevate your strategy and embrace the journey to becoming a cloud expert today! Want to learn more about VCF Automation? Check out the VCF Automation web page for more resources. Read this blog post: Private Cloud Redefined: Deliver a Unified Cloud Consumption Experience with VMware Cloud Foundation 9.0 Explore VMware Cloud Foundation Automation by the Numbers with the Forrester Total Economic Impact (TEI) Study This webinar series focuses on VMware Cloud Foundation (VCF). In this series, we share conversations with industry-recognized experts from Broadcom and with solution partners and customers. You’ll gain relevant insights whether you’re an IT practitioner, IT admin, cloud or platform architect, developer, DevOps, senior IT manager, IT executive, or AI/ML professional. If you are new to the series, please check out our previous episodes.</description></item><item><title>Announcing cost-efficient storage with Network file storage, cold storage, and usage-based backups</title><link>https://kubermates.org/docs/2025-10-02-announcing-cost-efficient-storage-with-network-file-storage-cold-storage-and-usa/</link><pubDate>Thu, 02 Oct 2025 08:05:09 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-02-announcing-cost-efficient-storage-with-network-file-storage-cold-storage-and-usa/</guid><description>Announcing cost-efficient storage with Network file storage, cold storage, and usage-based backups Network file storage (NFS) for high-performance AI workloads Spaces cold storage preview for infrequently accessed data Usage-based backups preview to meet aggressive RPOs About the author(s) Try DigitalOcean for free Related Articles Announcing per-sec billing, new Droplet plans, BYOIP, and NAT gateway preview to reduce scaling costs Storage that thinks for itself: Introducing Storage autoscaling, the newest feature for Managed Databases Introducing DigitalOcean Organizations, a new and comprehensive account layer By Anantha Ramachandran and Nihar Namjoshi Updated: September 24, 2025 5 min read As data footprints grow, businesses need cost-efficient storage for infrequently accessed data, high-performance file systems for collaborative work, and more aggressive data protection policies to meet strict recovery objectives. Weâre introducing several significant enhancements to our storage portfolio to help you manage the challenges of data management, protection, and scaling. TL;DR Starting October 20th, Network file storage solution will be generally available, specifically designed for high-performance AI workloads. You can preview it in the DigitalOcean console after the release. Spaces cold storage for infrequently accessed data is available for public preview. Visit our documentation to learn more and create a support ticket to request access to preview Spaces cold storage. Usage-based backups is available for public preview to meet aggressive rpos. Check out our documentation to learn more. Request access to preview usage-based backups by submitting this form. Data-intensive applications, particularly in AI and machine learning, require shared, high-performance file storage that is easy to provision and manage. Introducing our new Network File System (NFS) service, generally available in our ATL1 and NYC data centers starting October 20, 2025. This fully managed, high-performance solution is specifically designed to meet the demands of AI/ML startups and data-centric businesses by enabling concurrent shared dataset access for multi-node workloads.</description></item><item><title>Announcing per-sec billing, new Droplet plans, BYOIP, and NAT gateway preview to reduce scaling costs</title><link>https://kubermates.org/docs/2025-10-02-announcing-per-sec-billing-new-droplet-plans-byoip-and-nat-gateway-preview-to-re/</link><pubDate>Thu, 02 Oct 2025 08:03:13 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-02-announcing-per-sec-billing-new-droplet-plans-byoip-and-nat-gateway-preview-to-re/</guid><description>Announcing per-sec billing, new Droplet plans, BYOIP, and NAT gateway preview to reduce scaling costs Droplets per-second billing slashes costs for ephemeral workloads New dedicated Droplet plans for a seamless performance upgrade Bring your own IP (BYOIP) to protect your IP reputation VPC Network Address Translation (NAT) gateway preview for centralized egress and static IPs About the author(s) Try DigitalOcean for free Related Articles Announcing cost-efficient storage with Network file storage, cold storage, and usage-based backups Storage that thinks for itself: Introducing Storage autoscaling, the newest feature for Managed Databases Introducing DigitalOcean Organizations, a new and comprehensive account layer By Anantha Ramachandran and Nihar Namjoshi Updated: September 24, 2025 7 min read To help users cut down on cloud spending thatâs wasted due to over-provisioning and inflexible billing models, weâre introducing tools that offer granular control without sacrificing simplicity or cost-effectiveness. Weâre excited to announce new product updates that offer granular cost control, improved performance, and a clear path to savings. TL;DR Per-second billing for Droplets begins January 1, 2026 to slash costs for ephemeral workloads. Read documentation to learn more and understand your current billing details in the DigitalOcean console to prepare for this change New dedicated Droplet plans are now generally available for a seamless performance upgrade. Visit your DigitalOcean console to create these new Droplets. Bring your own IP is now generally available to protect your IP reputation. Check out our documentation or visit your DigitalOcean console to bring your own ip. VPC NAT gateway is now available for public preview for centralized egress and static IPs. Check out our documentation or visit your DigitalOcean console to set up your NAT gateway. Traditional hourly billing models for virtual machines have resulted in customers paying for idle time, even for short-lived workloads. Effective Jan 1, 2026, our billing model will transition to a per-second basis for Droplets , with a minimum charge of 60 seconds. This new approach aims to optimize cost control by charging for exact usage over a minute, making it simple to get precise billing for your actual usage.</description></item><item><title>Introducing DigitalOcean Organizations, a new and comprehensive account layer</title><link>https://kubermates.org/docs/2025-10-02-introducing-digitalocean-organizations-a-new-and-comprehensive-account-layer/</link><pubDate>Thu, 02 Oct 2025 08:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-02-introducing-digitalocean-organizations-a-new-and-comprehensive-account-layer/</guid><description>Introducing DigitalOcean Organizations, a new and comprehensive account layer Why we built a new account layer What this new feature means for your team 1. Consolidated billing and financial control 2. Streamlined membership management 3. Scalable and intuitive account hierarchy Gain more control over your DigitalOcean accounts with Organizations About the author Try DigitalOcean for free Related Articles Announcing cost-efficient storage with Network file storage, cold storage, and usage-based backups Announcing per-sec billing, new Droplet plans, BYOIP, and NAT gateway preview to reduce scaling costs Storage that thinks for itself: Introducing Storage autoscaling, the newest feature for Managed Databases By Nicole Ghalwash Published: October 2, 2025 3 min read Weâre excited to announce a powerful, new top-level account layer designed to bring all your teams together under one cohesive structure. This new account layerâwe call it Organizationsâis perfect for teams that need scalable structure without the massive, complex overhead of traditional enterprise cloud providers. Organizations was built to simplify account management for DigitalOcean customers who manage multiple teams and users. Weâve heard from our growing customers, agencies, partners, and enterprises that managing multiple teams, separate bills, and fragmented user access is a pain. With this new organization layer, you can better organize your teamâs access even as you scale up. Before today, every Team on DigitalOcean was treated as its own separate account. If you managed five different clients, five different projects, or five internal departments, that meant five separate bills, five different places to manage user access, and a fragmented view of your overall cloud presence. Our new account layer closes this gap by allowing you to consolidate multiple teams under one Organization, providing the centralized control and simplified operations needed by growing enterprises. By unifying your entire DigitalOcean presence into a single group, so you can streamline operations, gain more financial control, and build a cohesive environment that scales seamlessly.</description></item><item><title>Storage that thinks for itself: Introducing Storage autoscaling, the newest feature for Managed Databases</title><link>https://kubermates.org/docs/2025-10-02-storage-that-thinks-for-itself-introducing-storage-autoscaling-the-newest-featur/</link><pubDate>Thu, 02 Oct 2025 08:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-02-storage-that-thinks-for-itself-introducing-storage-autoscaling-the-newest-featur/</guid><description>Storage that thinks for itself: Introducing Storage autoscaling, the newest feature for Managed Databases How it works: your database, but smarter How Storage autoscaling helps you 1. Improved performance and reliability 2. Reduced operational overhead 3. Cost optimization 4. Ability to right-size your workloads How developers use Storage autoscaling Get started About the author Try DigitalOcean for free Related Articles Announcing cost-efficient storage with Network file storage, cold storage, and usage-based backups Announcing per-sec billing, new Droplet plans, BYOIP, and NAT gateway preview to reduce scaling costs Introducing DigitalOcean Organizations, a new and comprehensive account layer By Nicole Ghalwash Published: October 2, 2025 3 min read TL;DR: Storage autoscaling for our suite of Managed Databases is here, offering an automated way to right-size your storage. Get started with the new feature now: â Spin up a database cluster to access Storage autoscaling â Read our product documentation â Already have an account? Login â Want to learn more? Contact sales Have you ever experienced the stress of a âdisk fullâ error on your database? Itâs a frustrating situation that can lead to performance issues, application downtime, and a frantic scramble to manually resize your storage. Rest easy developers and database administrators: youâre about to get your peace of mind back. Introducing Storage autoscaling, for our entire suite of Managed Databases. This new capability for DigitalOcean Managed Databases automatically increases your databaseâs storage size when needed, so you donât have to. Itâs the automated, proactive safety net that your growing application needs. At its core, Storage autoscaling is about proactive prevention. The way it works is it continuously monitors your databaseâs disk utilization.</description></item><item><title>Build Smarter Agents with Image Generation, Auto-Indexing, VPC Security, and new AI Tools on DigitalOcean Gradient™ AI Platform</title><link>https://kubermates.org/docs/2025-10-02-build-smarter-agents-with-image-generation-auto-indexing-vpc-security-and-new-ai/</link><pubDate>Thu, 02 Oct 2025 07:12:50 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-02-build-smarter-agents-with-image-generation-auto-indexing-vpc-security-and-new-ai/</guid><description>Build Smarter Agents with Image Generation, Auto-Indexing, VPC Security, and new AI Tools on DigitalOcean Gradientâ¢ AI Platform Expanding capabilities for Gradient AI Platform Image Model Support Knowledge Base Auto-Indexing Enterprise-ready infrastructure VPC Integration Accelerating development Gradient AI Agent Development Kit (private preview) Gradient AI Genie (private preview) And much more to come About the author Try DigitalOcean for free Related Articles Announcing cost-efficient storage with Network file storage, cold storage, and usage-based backups Announcing per-sec billing, new Droplet plans, BYOIP, and NAT gateway preview to reduce scaling costs Storage that thinks for itself: Introducing Storage autoscaling, the newest feature for Managed Databases By Grace Morgan Updated: October 2, 2025 4 min read At Deploy London 2025, we shared the next chapter of the Gradient AI Platform. Weâre making it easier for developers and businesses to build production-ready AI applications, whether youâre experimenting with your first agent or scaling an enterprise workload. Today, in that spirit, weâre introducing a new wave of features that expand what you can build with the Gradient AI Platform, give you greater security and control, and accelerate your development workflow. AI applications are becoming more multimodal and data-driven, able to work with text, images, audio, and other formats. With Image Model Support and Knowledge Base Auto-Indexing, these new Gradient AI Platform features make it easier than ever to give your agents a wide range of inputs and knowledge sources. You can now generate images programmatically using text prompts through Gradient AI Platformâs Serverless Inference API, powered by OpenAIâs gpt-image-1 model. This is the platformâs first non-text modality (with more coming soon), expanding our capabilities from text-only to include text-to-image generation. Text-to-image generation â Generate images directly via the Serverless Inference API. API integration â Use the existing inference endpoint ( inference. do-ai. run/v1/chat/completions ) for image generation. inference.</description></item><item><title>How Red Hat can support your journey to a standard operating environment</title><link>https://kubermates.org/docs/2025-10-02-how-red-hat-can-support-your-journey-to-a-standard-operating-environment/</link><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-02-how-red-hat-can-support-your-journey-to-a-standard-operating-environment/</guid><description>How Red Hat can support your journey to a standard operating environment Consistency across the hybrid cloud Image builder Image mode for RHEL Benefits Red Hat Identity Management Provisioning, patching, and content management Red Hat Insights Automation Standardize your environment Red Hat Product Security About the author Alessandro Rossi More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Standardizing your company’s operating environment starts with the operating system (OS), but it doesn’t end there. As the number of systems grows, configurations drift, maintenance becomes repetitive, and updates can quickly turn into a headache. At Red Hat, we support your standardization journey by providing you with what you need to deliver a robust, coherent, and integrated solution for your standard operating environment. In this post, I explore the key areas you should take into account along your standardization journey, and how these can be simplified using Red Hat technologies, products, and services. Over time, daily activities on systems can lead to configuration drifts, issues with running workloads and a potential exposure to threats. The best way to start a standardization journey is to have a consistent baseline that your whole infrastructure can rely on, whether it&amp;rsquo;s on bare metal, hypervisors, or the cloud. Red Hat Enterprise Linux (RHEL) offers two different ways to start building installation images that are hardened, pre-configured with the tools you need, and most of all, that are repeatable over time. Included in RHEL, and also available in the Red Hat Insights service, image builder allows you to define a blueprint of an operating system image. Your image can contain as many customizations as you need, including users, firewall rules, repositories, and which packages to include. You can apply security profiles to harden your image, and much more. Image builder can generate installation media (such as an ISO, cloud provider image, hypervisor image, and so on) that can be deployed on-premises, in the cloud, or at the edge. Based on the open source bootc project , image mode introduces a new way to build and manage RHEL systems at scale.</description></item><item><title>Red Hat and Sylva unify the future for telco cloud</title><link>https://kubermates.org/docs/2025-10-02-red-hat-and-sylva-unify-the-future-for-telco-cloud/</link><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-02-red-hat-and-sylva-unify-the-future-for-telco-cloud/</guid><description>Red Hat and Sylva unify the future for telco cloud About the author Rich Stephens More like this Blog post Blog post Original podcast Original podcast Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The telecommunications service provider landscape continues to be rewritten with the coming of 5G Advanced, edge computing, automation, and AI. But realizing their full potential demands a continued shift in infrastructure for service providers to be more agile and efficient. Cloud-native architectures and open source principles have emerged as the path forward, offering the flexibility and speed needed to differentiate from offering pure connectivity. Yet, a significant challenge of fragmentation persists thanks to complex islands of legacy infrastructure and technical debt that have built up over the years. For service providers, this involves managing multiple stacks and skill sets. For vendors, it means costly, time-consuming certifications against numerous platforms. The industry is looking for ways to reduce operational costs, speed up service rollouts to drive new revenue, and capitalize on the full agility that cloud-native technologies and methodologies promise. Introducing Sylva Sylva is an open source project initiated by five of Europe’s largest service providers, Orange, Deutsche Telekom, Vodafone, Telefónica, and Telecom Italia, along with network providers Ericsson and Nokia, and has expanded to include many more participants. Its objective is to release a cloud software framework tailored for telco and edge requirements and a reference implementation and validation program to implement it. By converging the cloud layer around open source components with Kubernetes at its core, Sylva eliminates fragmentation. It establishes a single, validated foundation upon which network functions can be certified once and run anywhere, reducing complexity and unlocking the operational agility service providers desperately need. This “develop once, deploy anywhere” model is Red Hat’s core proposition and our hybrid cloud platforms align directly with the goals of Sylva.</description></item><item><title>Red Hat Learning Subscription: Expert chat for premium and standard users</title><link>https://kubermates.org/docs/2025-10-02-red-hat-learning-subscription-expert-chat-for-premium-and-standard-users/</link><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-02-red-hat-learning-subscription-expert-chat-for-premium-and-standard-users/</guid><description>Red Hat Learning Subscription: Expert chat for premium and standard users Red Hat Learning Subscription | Product Trial About the author Mary Margaret Barnes More like this Blog post Blog post Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share What if your self-paced training came with a live, 1-on-1 conversation with a Red Hat expert? With the expert chat feature, available in the premium and standard tiers of the Red Hat Learning Subscription, you get real-time support from Red Hat-certified experts. Whether you are tackling tough concepts or just need clarity in a lab, expert chat is here to help you keep moving forward. The expert chat feature from Red Hat adds value to your learning experience through: Direct access to expertise Connect with certified Red Hat experts to get answers to your specific questions about courses and labs. On-the-spot clarification Stuck on a topic? Expert chat delivers support so you can overcome roadblocks without derailing your progress. We are here for you—24x5. Interactive learning, your way, for maximized ROI Get the most out of your subscription. Immediate support means fewer delays and more value from the full curriculum and labs. Confidence to go further Knowing expert help is a click away can empower you to take on advanced topics—and stick with your training goals through to certification. Ready to level up your learning? Don’t navigate your Red Hat Learning Subscription journey alone. Chat with a Red Hat-certified expert today. Already a Red Hat Learning Subscription user? Not yet subscribed? Watch the demo video and start your Red Hat Learning Subscription trial to take the first step in advancing your Red Hat expertise and delivering greater value for your organization. Note: Expert chat is available only to Red Hat Learning Subscription premium and standard users.</description></item><item><title>Security update: Incident related to Red Hat Consulting GitLab instance</title><link>https://kubermates.org/docs/2025-10-02-security-update-incident-related-to-red-hat-consulting-gitlab-instance/</link><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-02-security-update-incident-related-to-red-hat-consulting-gitlab-instance/</guid><description>Security update: Incident related to Red Hat Consulting GitLab instance What happened Scope and impact on customers Our next steps About the author Red Hat More like this Blog post Blog post Original podcast Original podcast Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share We are writing to provide an update regarding a security incident related to a specific GitLab environment used by our Red Hat Consulting team. Red Hat takes the security and integrity of our systems and the data entrusted to us extremely seriously, and we are addressing this issue with the highest priority. We recently detected unauthorized access to a GitLab instance used for internal Red Hat Consulting collaboration in select engagements. Upon detection, we promptly launched a thorough investigation, removed the unauthorized party’s access, isolated the instance, and contacted the appropriate authorities. Our investigation, which is ongoing, found that an unauthorized third party had accessed and copied some data from this instance. We have now implemented additional hardening measures designed to help prevent further access and contain the issue. We understand you may have questions about whether this incident affects you. Based on our investigation to date, we can share: Impact on Red Hat products and supply chain: At this time, we have no reason to believe this security issue impacts any of our other Red Hat services or products, including our software supply chain or downloading Red Hat software from official channels. Consulting customers: If you are a Red Hat Consulting customer, our analysis is ongoing. The compromised GitLab instance housed consulting engagement data, which may include, for example, Red Hat’s project specifications, example code snippets, internal communications about consulting services, and limited forms of business contact information. We will notify you directly if we believe you have been impacted. Other customers: If you are not a Red Hat Consulting customer, there is currently no evidence that you have been affected by this incident.</description></item><item><title>Kyverno vs Kubernetes Policies: How It Complements and Completes</title><link>https://kubermates.org/docs/2025-10-01-kyverno-vs-kubernetes-policies-how-it-complements-and-completes/</link><pubDate>Wed, 01 Oct 2025 18:31:30 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-01-kyverno-vs-kubernetes-policies-how-it-complements-and-completes/</guid><description>Kyverno vs Kubernetes Policies: How It Complements and Completes Do You Still Need Kyverno with Kubernetes Policies? Introduction 1. Applying Policies On Existing Resources 2. Reapplying Policies On Changes 3. Applying Policies Off Cluster (Shift-Left) 4. Testing Policy as Code 5. Reporting Policy Results 6. Managing Fine-Grained Policy Exceptions 7. Complex Policy Logic 8. Image Verification 9. Policy-Based Automation 10. Kyverno Everywhere Conclusion FAQ: Kyverno vs Kubernetes Policies What is the difference between Kyverno and Kubernetes policies? Can Kyverno replace Kubernetes ValidatingAdmissionPolicy? Why do I need Kyverno if Kubernetes already has policies? With the addition of ValidatingAdmissionPolicy and MutatingAdmissionPolicy in Kubernetes, do you still need Kyverno? Quick Answer: Yes, you still need Kyverno. Kubernetes policies (ValidatingAdmissionPolicy, MutatingAdmissionPolicy) handle admission checks, but Kyverno extends them with reporting, testing, automation, and policy management at scale.</description></item><item><title>Kyverno vs Kubernetes Policies: How Kyverno Complements and Completes Kubernetes Policy Types</title><link>https://kubermates.org/docs/2025-10-01-kyverno-vs-kubernetes-policies-how-kyverno-complements-and-completes-kubernetes-/</link><pubDate>Wed, 01 Oct 2025 18:31:30 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-01-kyverno-vs-kubernetes-policies-how-kyverno-complements-and-completes-kubernetes-/</guid><description>Kyverno vs Kubernetes Policies: How Kyverno Complements and Completes Kubernetes Policy Types Do You Still Need Kyverno with the new Kubernetes Policy Types? Introduction 1. Applying Policies On Existing Resources 2. Reapplying Policies On Changes 3. Applying Policies Off Cluster (Shift-Left) 4. Testing Policy as Code 5. Reporting Policy Results 6. Managing Fine-Grained Policy Exceptions 7. Complex Policy Logic 8. Image Verification 9. Policy-Based Automation 10. Kyverno Everywhere Conclusion FAQ: Kyverno vs Kubernetes Policies What is the difference between Kyverno and Kubernetes policies? Can Kyverno replace Kubernetes ValidatingAdmissionPolicy? Why do I need Kyverno if Kubernetes already has policies? With the addition of ValidatingAdmissionPolicy and MutatingAdmissionPolicy in Kubernetes, do you still need Kyverno? TL;DR: Yes, you still need Kyverno for applying policies on existing resources, complex logic, reporting, testing, and off-cluster / shift-left use cases! Read on for details: Prior to Kyverno, policy management in Kubernetes was complex and cumbersome. While the need for Policy as Code was clear, initial implementations required learning complex languages and did not implement the full policy as code lifecycle.</description></item><item><title>VMware Cloud Foundation Automation -Infrastructure Resource Policy Overview</title><link>https://kubermates.org/docs/2025-10-01-vmware-cloud-foundation-automation-infrastructure-resource-policy-overview/</link><pubDate>Wed, 01 Oct 2025 17:41:23 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-01-vmware-cloud-foundation-automation-infrastructure-resource-policy-overview/</guid><description>Related Articles VMware Cloud Foundation Automation -Infrastructure Resource Policy Overview Set Your Implementation Up for Success with VCF Jumpstart Workshop VCF Breakroom Chats Episode 60: Infrastructure Modernization, Health, and APIs for Private Cloud With the release of VMware Cloud Foundation 9.0, we introduced a number of new and exciting features and capabilities to help our customers deliver an agile, performant, and secure self-service private cloud. As part of a private cloud strategy, providing a way to consume the underlying infrastructure services and deliver applications quickly all the while managing governance is crucial. With VMware Cloud Foundation (VCF), a number of cloud services are available for consumption, such as VM and Kubernetes Cluster services, along with services for database management, continuous delivery pipelines, service mesh, image registry, and much much more. This blog is an overview of some of the core services like VM and Kubernetes services along with how to apply IaaS resource policies to them. Resource policies help ensure configurations such as cluster sizes are compliant. Security postures such as enforcing a baseline pod security level, and disallowing resources to be deployed, are just a few examples of how to use resource policies. Choice of Consumption Models With VCF, Organization Admins can choose how to isolate users and resources using constructs such as Organizations (tenants), Projects, namespaces, etc. Consumers, such as developers and app teams, also have choice in how they want to consume. The graphic below shows two primary methods: Self-Service Catalog and UI/CLI. Single Platform for Building and Managing Apps VMware Cloud Foundation is a single platform for building and managing applications and services for the entire Organization (tenants). IT teams can run and manage diverse workloads, including AI/ML and cloud native applications. Teams can use a modern interface (UI + code) to facilitate the deployment of services such as databases and VMs.</description></item><item><title>Fluentd to Fluent Bit: A Migration Guide</title><link>https://kubermates.org/docs/2025-10-01-fluentd-to-fluent-bit-a-migration-guide/</link><pubDate>Wed, 01 Oct 2025 15:13:55 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-01-fluentd-to-fluent-bit-a-migration-guide/</guid><description>When and why to migrate? Why Migrate? Fluentd vs. Fluent Bit: What are the Differences Background Performance Routing Telemetry signal focus Custom processing Custom plugins Monitoring How to get started with a Fluentd to Fluent Bit migration Deployment Deployed as an Aggregator / Collector Adding a Telemetry Pipeline Configuration Custom Plugins Custom Processing Logic Migrating Portions at a Time Conclusion Frequently Asked Questions Why migrate from Fluentd to Fluent Bit What are some differences between Fluentd and Fluent Bit? Can Fluentd and Fluent Bit work together? Posted on October 1, 2025 by Anurag Gupta | Fluent Bit Maintainer | Field Architect | Chronosphere CNCF projects highlighted in this post Fluentd was created over 14 years ago and still continues to be one of the most widely deployed technologies for log collection in the enterprise. Fluentd’s distributed plugin architecture and highly permissive licensing made it ideal as part of the Cloud Native Computing Foundation (CNCF) as a now graduated project. However, enterprises drowning in telemetry data are now requiring solutions that have higher performance, more native support for evolving schemas and formats, and increased flexibility in processing. Enter Fluent Bit. Fluent Bit , while initially growing as a sub-project within the Fluent ecosystem , expanded from Fluentd to support all telemetry types – logs, metrics, and traces. Fluent Bit now is the more popular of the two with over 15 billion deployments and used by Amazon, Google, Oracle and Microsoft to name a few. Fluent Bit also is fully aligned with OpenTelemetry signals, format and protocol, which ensures that users will be able to continue handling telemetry data as it grows and evolves. Among the most frequent questions we get as the maintainers of the projects are: How do we migrate? What should we watch out for? And what business value do we get for migrating? This article aims to answer these questions with examples. We want to help make it an easy decision to migrate from Fluentd to Fluent Bit. Here is a quick list of the reasons users switch from Fluentd to Fluent Bit: Higher performance for the same resources you are already using Full OpenTelemetry support for logs, metrics, and traces as well as Prometheus support for metrics Simpler configuration and routing ability to multiple locations Higher velocity for adding custom processing rules Integrated monitoring to better understand performance and dataflows To understand all the differences between the projects, it is important to understand the background of each project and the era it was built for. With Fluentd, the main language is Ruby and initially designed to help users push data to big data platforms such as Hadoop.</description></item><item><title>Llama Stack and the case for an open “run-anywhere” contract for agents</title><link>https://kubermates.org/docs/2025-10-01-llama-stack-and-the-case-for-an-open-run-anywhere-contract-for-agents/</link><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-01-llama-stack-and-the-case-for-an-open-run-anywhere-contract-for-agents/</guid><description>Llama Stack and the case for an open “run-anywhere” contract for agents The 4 layers of Llama Stack 1. Build layer (Client SDK/Toolkit) 2. Agent artifacts and dependencies 3. Platform / API layer 4. Provider model The Kubernetes analogy The standards question: OpenAI APIs vs. MCP The governance question Why this matters The adaptable enterprise: Why AI readiness is disruption readiness About the authors Tushar Katarki Adel Zaalouk More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Why do we really need Llama Stack when popular frameworks like LangChain, LangFlow, and CrewAI already exist? This is the question we get asked most often. It’s a fair one—after all, those frameworks already give developers rich tooling for retrieval-augmented generation (RAG) and agents. But we see Llama Stack as more than “another agent framework. ” It’s better understood as four distinct layers: A familiar surface for building agents. Here it overlaps with LangChain, LangFlow, and CrewAI. Developers can author agents using common abstractions. Example : An agent built with CrewAI looks like a small project folder with config files and environment variables.</description></item><item><title>Red Hat Summit 2026 call for proposals is now open</title><link>https://kubermates.org/docs/2025-10-01-red-hat-summit-2026-call-for-proposals-is-now-open/</link><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-01-red-hat-summit-2026-call-for-proposals-is-now-open/</guid><description>Red Hat Summit 2026 call for proposals is now open About the author Red Hat Corporate Communications More like this Blog post Blog post Original podcast Original podcast Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share For the first time, Red Hat Summit is coming to Atlanta, Georgia, and we’re building an agenda of technical and non-technical content for our experienced audience. We&amp;rsquo;re looking for our community of experts—customers, partners, and associates—to share their knowledge and experiences. The call for proposals is open now, from October 1 until November 12. Submit your session proposal today for a chance to present at Red Hat Summit, May 11–14, 2026 at the Georgia World Congress Center. We&amp;rsquo;re looking for more advanced session content this year. We want sessions that go beyond the basics, offering deeper insights and more technical detail to help our audience get the most out of their Red Hat investments. Proposals should focus on the following themes: Application of Red Hat technologies Customer and partner success stories Emerging tech and evolving technologies Community and team innovation Development best practices and trends Also - keep in mind that all proposals should focus on Red Hat products, services, and the broader open ecosystem. Check out Red Hat’s Summit 2026 CFP guide for more details, tips, and tricks on submitting your ideas. Red Hat Summit offers a unique opportunity to connect with a global community of innovators, share your expertise, and build your professional network. Presenting at Summit is a great way to give back to the open source community, highlighting the real-world impact of open collaboration. Ready to inspire? Submit a session proposal today. We&amp;rsquo;re excited to see your ideas! Red Hat is the world’s leading provider of enterprise open source software solutions, using a community-powered approach to deliver reliable and high-performing Linux, hybrid cloud, container, and Kubernetes technologies.</description></item><item><title>Searching for the 2026 Red Hat Certified Professional of the Year</title><link>https://kubermates.org/docs/2025-10-01-searching-for-the-2026-red-hat-certified-professional-of-the-year/</link><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-10-01-searching-for-the-2026-red-hat-certified-professional-of-the-year/</guid><description>Searching for the 2026 Red Hat Certified Professional of the Year Meet our 2025 winner Are you next? Red Hat Learning Subscription | Product Trial About the author Carson Davis More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The search is on for the 2026 Red Hat Certified Professional of the Year! This award is more than just recognition; it&amp;rsquo;s a testament to the impactful work and dedication that Red Hat Certified Professionals bring to the open source ecosystem. The winner is an individual who takes advantage of their Red Hat skills to solve complex challenges and advance innovation. The 2025 Red Hat Certified Professional of the Year, Daniel Carvalho dos Santos, an infrastructure analyst at Banco Bradesco, enhanced his skills for complex projects through using Red Hat Learning Subscription. He stated that obtaining his Red Hat certifications represented a significant milestone in both his personal growth and professional development. Read Daniel’s full story here. If you are someone who takes advantage of Red Hat technologies to deliver tangible business outcomes, improve system performance, or enhance IT environments, we want to hear your story. Key dates for the 2026 contest: Award submission period begins: September 1, 2025 Submission period ends: October 31, 2025 Winner announced at Red Hat Summit 2026 (May 11 - 14, 2026) Prizes include: Recognition during Red Hat Summit 2026 One-year Premium Red Hat Learning Subscription A spotlight in a Red Hat success story video A Red Hat trophy and fedora And more. Don&amp;rsquo;t miss the opportunity to be recognized for your hard work and expertise. We look forward to reading your stories and celebrating the next Red Hat Certified Professional of the Year. Submit your nomination today. Product trial Program Marketing Manager Carson Davis is the Program Marketing Manager for Trainings and Certifications at Red Hat, where he creates and leads marketing initiatives that highlight the value of Red Hat’s world-class training and certification programs. With a background in sales enablement, cross-functional collaboration, and corporate event marketing, Carson brings a creative and strategic approach to driving learner engagement and strengthening program visibility.</description></item><item><title>VCF Operations Management Packs: Announcing End of General Support</title><link>https://kubermates.org/docs/2025-09-30-vcf-operations-management-packs-announcing-end-of-general-support/</link><pubDate>Tue, 30 Sep 2025 16:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-30-vcf-operations-management-packs-announcing-end-of-general-support/</guid><description>Additional Information Related Articles VCF Operations Management Packs: Announcing End of General Support Diagnostics for VMware Cloud Foundation Operations - Newest Findings Empowering Platform Engineers with native Kubernetes Multi-Cluster Management in VMware Cloud Foundation Broadcom is announcing End of General Support (EOGS) for the following VCF Operations Management Packs. Consequently, Broadcom will no longer provide General Support, including Bug Fixes, Security Patches, or any updates for these Management Packs, effective September 30, 2025. Alternative ways to integrate these technologies with VCF Operations are listed wherever applicable. As these management packs are EOGS, they are no longer available for download on the VCF Solutions Catalog (previous VMware Marketplace), or on Broadcom Support Portal.</description></item><item><title>Set Your Implementation Up for Success with VCF Jumpstart Workshop</title><link>https://kubermates.org/docs/2025-09-30-set-your-implementation-up-for-success-with-vcf-jumpstart-workshop/</link><pubDate>Tue, 30 Sep 2025 14:46:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-30-set-your-implementation-up-for-success-with-vcf-jumpstart-workshop/</guid><description>Key VCF Jumpstart Workshops activities Accelerate value realization from your technology Related Articles Set Your Implementation Up for Success with VCF Jumpstart Workshop VCF Breakroom Chats Episode 60: Infrastructure Modernization, Health, and APIs for Private Cloud Empowering Platform Engineers with native Kubernetes Multi-Cluster Management in VMware Cloud Foundation The nature of today’s technology landscape rewards businesses that understand the importance of a strong, strategic foundation combined with a plan for their cloud platform. When implementing VMware Cloud Foundation® (VCF), assistance and guidance from a VMware expert at the onset of an implementation can help to ensure alignment between your private cloud strategy and business goals for your IT environment. VCF Jumpstart Workshop provides visibility into your current technology state, ensures alignment with VMware best practices, and recommends an adoption plan and architecture unique to your environment and business use cases. These workshop is crafted to specifically align to your needs. The following are examples of workshop activities that help provide visibility into your current technology state, best practices, and recommendations to achieve the desired future state. Cloud Maturity Rating – This assessment evaluates your current private cloud capabilities and compares them with your desired future state. We analyze your cloud environment, prioritize cloud capabilities, and define phases and milestones for delivery. Implementation Architecture – Your VMware technology is reviewed to ensure that high-level architectural best practices are in place. We analyze your current architecture and identify any gaps to reach your future state and desired capabilities. We also help you compose a conceptual architecture for your cloud operations. Capability Roadmap – Creating an effective adoption plan is essential for a smooth transition and implementation of VCF. We document a comprehensive roadmap for your private cloud and recommend actionable steps and activities to ensure that your cloud strategy is not only well-planned, but actionable and sustainable.</description></item><item><title>🏆 How I Passed the Certified Argo Project Associate (CAPA) Exam — And Why It Was Worth It</title><link>https://kubermates.org/docs/2025-09-30-how-i-passed-the-certified-argo-project-associate-capa-exam-and-why-it-was-worth/</link><pubDate>Tue, 30 Sep 2025 13:05:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-30-how-i-passed-the-certified-argo-project-associate-capa-exam-and-why-it-was-worth/</guid><description>👩‍💻 My Background with Argo 📚 My Study Strategy (1-Month Plan) 🎓 1. Linux Foundation Course (LFS256) 🧪 2. Practice Repos, Examples, and Real Configs 📘 CNCF Exams GitLab Repository ✍️ Study Guides from the Community 📝 3. Udemy Mock Exams 🧠 Exam Format &amp;amp; Experience 💡 My Favorite Part of the Journey ✅ Would I Recommend It? 📎 TL;DR — My Tips Posted on September 30, 2025 by audra CNCF projects highlighted in this post If you’ve been working with ArgoCD or exploring GitOps, you’ve probably come across the Certified Argo Project Associate (CAPA) exam. I recently passed it, and in this post, I want to share: Why I took the exam How I prepared (including all the resources I used) What the experience was like Whether I’d recommend it This isn’t a sales pitch. It’s a real-world guide to help you prepare — and understand whether this certification is right for you. I’ve been working with ArgoCD for over a year now as a senior cloud engineer. I’ve had first-hand experience managing real-world GitOps workflows, and designing deployment strategies using ArgoCD. However, the CAPA exam doesn’t just focus on ArgoCD. It covers four Argo projects : Argo Workflows Argo CD Argo Rollouts Argo Events While I had a solid foundation in GitOps and ArgoCD (which makes up 34% of the exam), I had little to no hands-on experience with Workflows, Rollouts, or Events — and that’s where most of my preparation efforts went. I prepared for the exam over the course of 1 month — with a focused, hands-on approach. Here’s the breakdown of how I studied and what resources I used: As a CNCF Ambassador, I had the opportunity to get the exam for free, and I also opted to purchase the “DevOps and Workflow Management with Argo (LFS256)” training course.</description></item><item><title>Ireland’s next steps for effective AI delivery</title><link>https://kubermates.org/docs/2025-09-30-ireland-s-next-steps-for-effective-ai-delivery/</link><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-30-ireland-s-next-steps-for-effective-ai-delivery/</guid><description>Ireland’s next steps for effective AI delivery About the author Ivan Jennings More like this Blog post Blog post Original podcast Original podcast Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Ireland has a strong digital infrastructure public services record, with momentum behind cloud adoption and artificial intelligence. The upcoming update to the National Digital and AI Strategy in 2025 reflects a continued commitment to innovation, not just for economic competitiveness, but for better, more responsive public services. To sustain this progress and ensure impact across the economy and society, Irish organisations – particularly in the public sector – will need to take a pragmatic, outcome-led approach to AI adoption. One that prioritises strategic digital skills, cross-sector collaboration, and AI solutions that are scalable and energy-efficient. Recent Red Hat research shows that nearly all IT managers in Ireland plan to increase investment in both cloud (93%) and AI (95%) this year. The intent is clear. But the challenge is now delivery: how to turn national ambition into measurable outcome, so that all areas of Irish industry can develop an AI toolbox fit for purpose. This is pressing in sectors like healthcare, agriculture and planning, where digital transformation is essential to improving public services, managing environmental resources and addressing infrastructure demands in a growing population. Ireland’s AI use is growing - its application across enterprises grew from 8% in 2023 to 14% in 2024. Laying the foundations for connected AI One of the biggest barriers to delivering progress with AI is structural. Ireland’s public sector is under pressure to improve the speed and quality of service delivery, often while maintaining or modernising legacy systems. Teams are expected to adopt new technologies, collaborate more effectively and respond to rapidly changing citizen expectations.</description></item><item><title>Optimizing application architectures for AI: From monoliths to intelligent agents (2 of 2 blogs series)</title><link>https://kubermates.org/docs/2025-09-30-optimizing-application-architectures-for-ai-from-monoliths-to-intelligent-agents/</link><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-30-optimizing-application-architectures-for-ai-from-monoliths-to-intelligent-agents/</guid><description>Optimizing application architectures for AI: From monoliths to intelligent agents (2 of 2 blogs series) Why does current customer support fall short? The new approach: a workflow of specialized agents How do we architect for AI agents? 1. Input moderation 2. Information extraction 3. Context enrichment 4. Classification and routing 5. Action execution with MCP 6. Natural language response Why not just one big model? Why governance matters A real example: from request to answer What this means for enterprise architectures Get started with AI Inference About the authors Luis I. Cortés Bernard Tison More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share No one enjoys menu-based customer service. Pressing “1 for billing” or “2 for support” feels outdated, and repeating the same problem to different agents is frustrating. Even when we try the “operator” trick, we’re just hoping for someone to pick up and understand us right away. But what we really want is simple: a fast and accurate resolution, without friction. If that’s how you feel, you are not alone.</description></item><item><title>What you don’t see could cost you: Why open source matters in enterprise AI</title><link>https://kubermates.org/docs/2025-09-30-what-you-don-t-see-could-cost-you-why-open-source-matters-in-enterprise-ai/</link><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-30-what-you-don-t-see-could-cost-you-why-open-source-matters-in-enterprise-ai/</guid><description>What you don’t see could cost you: Why open source matters in enterprise AI The risks of the black box Red Hat AI Value across the ecosystem AI as a community effort Get started with AI Inference About the author Abigail Sisson More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share In my previous article , I compared AI inference to the nervous system of an AI project — the critical, often unseen infrastructure that dictates the user experience. Whether it’s a chatbot or a complex application, the principle is the same: if the nervous system falters, everything else does too. As we know, however, a nervous system doesn&amp;rsquo;t operate in isolation. It relies on the rest of the body, with countless other systems working in harmony. Enterprise AI is fundamentally the same. Individual models, isolated infrastructure components, fragmented orchestration, or disconnected applications cannot deliver meaningful value on their own. Real impact only comes when these elements connect to form a cohesive, high-performing whole. There are many paths to AI adoption. Some begin with closed, so-called “black-box” systems: pre-packaged platforms that make it simple to get up and running. They can be a great entry point, but they also come with trade-offs. When you cannot see how a model was trained, it can be difficult to explain its behavior, address bias, or verify accuracy in your context. In fact, a 2025 IBM survey found that 45% of business leaders cite data accuracy or bias as their biggest obstacle to adopting AI.</description></item><item><title>VCF Breakroom Chats Episode 60: Infrastructure Modernization, Health, and APIs for Private Cloud</title><link>https://kubermates.org/docs/2025-09-29-vcf-breakroom-chats-episode-60-infrastructure-modernization-health-and-apis-for-/</link><pubDate>Mon, 29 Sep 2025 20:39:22 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-29-vcf-breakroom-chats-episode-60-infrastructure-modernization-health-and-apis-for-/</guid><description>Related Articles VCF Breakroom Chats Episode 60: Infrastructure Modernization, Health, and APIs for Private Cloud Empowering Platform Engineers with native Kubernetes Multi-Cluster Management in VMware Cloud Foundation VCF Breakroom Chats Episode 61: From VI Admin to Cloud Hero - Building a Multi-Tenant Cloud with VCF 9.0 Are you looking to improve the health visibility and scale of your private cloud deployment? In episode 60 of the VCF Breakroom Chats , Product Marketing Engineer Sehjung Hah is joined by Navaneetha Krishna from Product Management. Navaneetha explores infrastructure health to improve performance as well as APIs for private cloud with VMware Cloud Foundation (VCF) 9.0, including: Health and diagnostics to see if a customer’s VCF stack is working in a health state or not Discovering known issues and how to remediate to recover from these issues Open API 3.0 compliant for metrics and other data to monitor the environment Management pack builder SDK available in Python or Java Watch VCF Breakroom Chat episode 60 and learn how these innovations provide customers with scalable operations and management in VCF 9.0. About the VCF Breakroom Chat Series This webinar series focuses on VMware Cloud Foundation (VCF). ​In this series, we share conversations with industry-recognized experts from Broadcom and with solution partners and customers. You’ll gain relevant insights whether you’re an IT practitioner, IT admin, cloud or platform architect, developer, DevOps, senior IT manager, IT executive, or AI/ML professional. If you are new to the series, please check out previous episodes and visit our blogs.</description></item><item><title>Certifications in DevOps: Which Are Worth Your Time in 2025?</title><link>https://kubermates.org/docs/2025-09-29-certifications-in-devops-which-are-worth-your-time-in-2025/</link><pubDate>Mon, 29 Sep 2025 18:52:57 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-29-certifications-in-devops-which-are-worth-your-time-in-2025/</guid><description>The Certification Market in 2025: Why It’s Different Cloud Certifications: The Foundation Layer You Can’t Skip Kubernetes: The Practical Core of DevOps Infrastructure as Code: Terraform as the Lingua Franca CI/CD: From Jenkins to GitHub Actions and GitLab Pipelines Linux: The Silent Backbone of DevOps The Maturity Layer: Security, Reliability, and Cost Security: Proving You Can Ship Safely Reliability: From DevOps to SRE Cost Optimization: Enter FinOps Why This Layer Matters Observability: From Luxury to Necessity Strategic Roadmaps: Picking the Right Path Roadmap 1: Beginners Entering DevOps Roadmap 2: Sysadmin to SRE/Platform Engineer Roadmap 3: Developer to DevOps Engineer Why Roadmaps Work Final Word: Proof That Matters FAQ What is DevOps? Kubernetes = Core Skill → CKA, CKAD, and CKS prove real, hands-on expertise. Over 3,000 jobs explicitly ask for them. Terraform = IaC Standard → Terraform Associate (003) dominates; 15k+ U. S. job postings mention it. CI/CD Has Shifted → Jenkins is legacy; GitHub Actions &amp;amp; GitLab certifications are the new benchmarks. Linux = Bedrock → LFCS, RHCSA, RHCE remain evergreen. 10k+ jobs still require Linux fluency. Maturity Layer Matters → Security (CKS, AWS Security Specialty), Reliability (Google DevOps Engineer), and Cost (FOCP) now separate leaders from juniors. Observability = No Longer Optional → Grafana/Prometheus certifications validate modern monitoring and tracing. Certifications = Multipliers, Not Magic → They amplify hands-on skills and career progression; 81% of certified pros report better job opportunities. Picture this: it’s spring 2025, and you’re sitting in an interview for a mid-to-senior DevOps engineer role.</description></item><item><title>Diagnostics for VMware Cloud Foundation Operations – Newest Findings</title><link>https://kubermates.org/docs/2025-09-29-diagnostics-for-vmware-cloud-foundation-operations-newest-findings/</link><pubDate>Mon, 29 Sep 2025 17:56:50 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-29-diagnostics-for-vmware-cloud-foundation-operations-newest-findings/</guid><description>Related Articles Diagnostics for VMware Cloud Foundation Operations - Newest Findings Capacity Management: The IT Balancing Act You Can’t Ignore FinOps in VMware Cloud Foundation Diagnostics for VMware Cloud Foundation is a centralized platform that monitors the overall operational status of the VMware Cloud Foundation(VCF) software stack. It is a self-service platform that helps you analyze and troubleshoot the components of VMware Cloud Foundation, including vCenter, ESXi, vSAN, capabilities such as vSphere vMotion, snapshots, VM provisioning, and other issues including security advisories and certificates. As an Infrastructure admin, you can monitor the operational state of your environment using diagnostics findings Diagnostics Findings which were previously delivered through Skyline Advisor and Skyline Health Diagnostics are available to VCF and vSphere Foundation (VVF) customers in VCF Operations. Findings are prioritized by trending issues in Broadcom Technical Support, issues raised through post escalation review, security vulnerabilities, issues raised from Broadcom engineering, and nominated by customers. For the most recent release of VCF Operations, we released 114 new Findings. Of these, there are 83 Findings based on trending issues, 15 based on post escalation reviews, 14 based on VMSA, and 2 based on nominations. There are 62 Health findings, these are findings that are equivalent to Skyline Advisor findings. Health findings are automatically checked against your environment every 4 hours. There are 52 log based findings that are equivalent to Skyline Health Diagnostics findings. Log based findings are manually initiated against your environment by choosing refresh against the configuration operations instances. These new findings are released in VCF Operations 9.0.1. Release Notes Security Vulnerabilities In VMSA-2025-0010, VMware vCenter Server authenticated command-execution vulnerability (CVE-2025-41225) and VMware ESXi and vCenter Server Reflected Cross Site Scripting (XSS) Vulnerability (CVE-2025-41228).</description></item><item><title>Empowering Platform Engineers with native Kubernetes Multi-Cluster Management in VMware Cloud Foundation</title><link>https://kubermates.org/docs/2025-09-29-empowering-platform-engineers-with-native-kubernetes-multi-cluster-management-in/</link><pubDate>Mon, 29 Sep 2025 15:54:48 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-29-empowering-platform-engineers-with-native-kubernetes-multi-cluster-management-in/</guid><description>Platform Engineer Challenges with Kubernetes Infrastructure Management New VKS Cluster Management Capabilities via VCF Automation Learn More Related Articles VCF Breakroom Chats Episode 60: Infrastructure Modernization, Health, and APIs for Private Cloud Empowering Platform Engineers with native Kubernetes Multi-Cluster Management in VMware Cloud Foundation VCF Breakroom Chats Episode 61: From VI Admin to Cloud Hero - Building a Multi-Tenant Cloud with VCF 9.0 We’re thrilled to announce the latest release of VMware Cloud Foundation (VCF), marking a pivotal moment for platform engineers who have been seeking a unified, enterprise-grade solution for managing Kubernetes (K8s) infrastructure at scale. This groundbreaking release introduces native K8s multi-cluster management capabilities in VCF accessed through the VCF Automation consumption experience. It’s specifically designed to address the complex challenges that platform engineers face when orchestrating K8s workloads across distributed environments. This capability provides platform engineers with control over their K8s environments, while maintaining the simplicity and operational consistency they need. Its new K8s multi-cluster management capabilities complement built-in private cloud services (VM, vSphere Kubernetes Service (VKS) (K8s runtime), Network, Volume, VM Image) that allow engineers to provision VMs and VKS clusters as K8s objects using K8s manifests. Engineers can use the cloud services and Infrastructure as Code to create version-controlled blueprints (e. g. , for provisioning VKS clusters and deploying applications on them) publishable to a self-service catalog for developers. Together the new native K8s multi-cluster management and cloud services allow organizations to run their entire application portfolio and manage the underlying K8s infrastructure on a single, streamlined platform. Platform engineers face an increasingly complex landscape managing K8s infrastructure due to the exponential growth of containerized applications. Traditional approaches can’t accommodate the operational challenges of modern K8s deployments, which often involve numerous clusters and multiple software elements across various environments. This complexity, compounded by K8s dynamic nature, makes maintaining visibility difficult.</description></item><item><title>KubeCon + CloudNativeCon North America 2025 Co-Located Event Deep Dive: BackstageCon</title><link>https://kubermates.org/docs/2025-09-29-kubecon-cloudnativecon-north-america-2025-co-located-event-deep-dive-backstageco/</link><pubDate>Mon, 29 Sep 2025 13:26:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-29-kubecon-cloudnativecon-north-america-2025-co-located-event-deep-dive-backstageco/</guid><description>Who will get the most out of attending this event? What is new and different this year? What will the day look like? Should I do any homework first? A note from the co-chairs Posted on September 29, 2025 by Co-chairs, Blair Fraser, Bryan Landes, Balaji Sivasubramanian CNCF projects highlighted in this post BackstageCon has been in existence since 2022, where it made its debut at KubeCon + CloudNativeCon North America in Detroit. We want attendees at BackstageCon to leave with a deeper understanding of the latest trends and use cases of Backstage. Last year, Backstage saw strong engagement from the end user community, with significant contributions driving its momentum. We aim to springboard from that success, by bringing together the Backstage community and sharing success stories, use cases, and the future of the Backstage project. The BackstageCon agenda is suitable for all skill levels, but we think that anyone involved in platform engineering, developer portals, or those interested in improving developer productivity will make the most out of this event. For early adopters of Backstage, this event is an excellent opportunity to learn from experienced users and contributors. Experienced Backstage users will find insights into the future of Backstage and can use this event as an excellent opportunity to network with other adopters. This year’s agenda introduces new topics, including how organizations of all sizes are scaling developer productivity using Backstage, the future of developer portals, and scaling AI development. This event will kick off in the main room with a welcome from the chairs. We have a single track agenda starting with an adopter session in scaling developer productivity. Keynote sessions from our sponsors will lead into several exciting sessions on topics from leveraging the new notification system, using scaffolder for workflows, and a panel discussion of the future of developer portals. Our afternoon will include sessions led by contributors and adopters covering topics like standardizing MCP deployment and governance, taming the post-startup stage with Backstage, and how one adopter is turning their insights into golden paths.</description></item><item><title>Hacktoberfest 2025: How to Participate</title><link>https://kubermates.org/docs/2025-09-29-hacktoberfest-2025-how-to-participate/</link><pubDate>Mon, 29 Sep 2025 07:06:22 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-29-hacktoberfest-2025-how-to-participate/</guid><description>Hacktoberfest 2025: How to Participate Hacktoberfest 2025: How to Participate About the author Try DigitalOcean for free Related Articles Hacktoberfest 2025: Celebrate All Things Open Source! Stop Building SaaS from Scratch: Meet the SeaNotes Starter Kit Powered by DigitalOcean Hatch: Why Uxifyâs Founders Always Choose DigitalOcean By Haimantika Mitra Engineer &amp;amp; Writer Published: September 29, 2025 3 min read Hacktoberfest registrations are live, and weâre just a few days away from kicking off the month-long celebration of open source. Every October, developers from all over the world come together to contribute their skills, big and small, to open-source projects. Code is the most common way people participate, but thatâs not the only way you can help. Open source thrives on more than just commits. It needs documentation, testing, design, writing, and so much more. Before we get into the how, letâs cover the basics. First, the rules of Hacktoberfest: Register anytime before October 31. Contribute to projects on GitHub or GitLab that have the hacktoberfest topic. hacktoberfest Submit at least 6 high-quality pull/merge requests between October 1 and October 31. These need to be accepted by project maintainers to count. Rewards: Everyone who signs up and gets at least one PR merged, gets a digital badge from Holopin (you can get up to 4 badges). If youâre among the first 10,000 people to complete 6 merged PRs/MRs, youâll also get a limited edition Hactoberfest 2025 t-shirt.</description></item><item><title>Bridging the gap: Secure virtual and container workloads with Red Hat OpenShift and Palo Alto Networks</title><link>https://kubermates.org/docs/2025-09-29-bridging-the-gap-secure-virtual-and-container-workloads-with-red-hat-openshift-a/</link><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-29-bridging-the-gap-secure-virtual-and-container-workloads-with-red-hat-openshift-a/</guid><description>Bridging the gap: Secure virtual and container workloads with Red Hat OpenShift and Palo Alto Networks The operational tension A strategic optimization Red Hat OpenShift optimization with Prisma AIRS Strategic implications Use case: workload migration with embedded security Use case: centralized management and policy uniformity Implementation notes Final thoughts 15 reasons to adopt Red Hat OpenShift Virtualization About the author Simon Seagrave More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share While migrating virtual or containerized workloads between environments may seem straightforward, teams responsible for managing system uptime, policy enforcement, and securing east-west traffic, often find it to be a complex process. Subtle differences in hypervisor behavior, inconsistent policy application, and blind spots in traffic flows can introduce risks that aren’t always obvious, until it’s almost too late. Red Hat OpenShift Virtualization combined with Palo Alto Networks VM-Series software firewalls helps address these complexities where they typically arise. OpenShift Virtualization streamlines the modernization and lifecycle management of VM-based workloads within a Kubernetes-native platform. VM-Series complements this by bringing consistent, application-aware network security to virtualized environments, helping teams enforce policy and maintain visibility even as infrastructure evolves. Additionally, Red Hat OpenShift integrated with Prisma AIRS™ enables customers to deploy AI-powered threat prevention for runtime network protection across containerized workloads running on OpenShift clusters. Many enterprises operate in hybrid environments by necessity, not choice. Existing virtualization investments coexist with containerized applications, and teams are expected to support both with equal reliability. The operational challenge is not only bridging these domains but doing so without fragmenting security policy, tooling, or visibility. Today, workloads migrate between private clouds, public clouds, or across datacenters and have different form factors of the workload including physical, virtual, and containerized. While this provides significant flexibility to the customers, it compounds the issue of enforcing uniform security posture, and migrations become moments of risk. Policy gaps, inconsistent segmentation, or east-west blind spots can create undetected lateral movement paths for threats.</description></item><item><title>Managing Red Hat Device Edge: Tools and strategies</title><link>https://kubermates.org/docs/2025-09-29-managing-red-hat-device-edge-tools-and-strategies/</link><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-29-managing-red-hat-device-edge-tools-and-strategies/</guid><description>Managing Red Hat Device Edge: Tools and strategies Managing Red Hat Device Edge: Red Hat tools and strategies Package-based Red Hat Device Edge system management: The traditional approach Image-based Red Hat Device Edge system management: The modern approach Conclusion: The right tool for the right job Hatville: miniature city where edge computing comes to life About the author Luis Arizmendi More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Aligning system management, automation, and application lifecycle with your edge strategy In part 1 of this two-part blog series, we introduced the decision framework for Red Hat Device Edge , when it makes sense compared to Red Hat Enterprise Linux (RHEL) or Red Hat OpenShift , and the unique role of MicroShift in enabling Kubernetes at the edge. In this article, you’ll learn how to manage the system that you’ve chosen for your edge strategy. Once you’ve selected Red Hat Device Edge, the next step is designing a management approach that matches your deployment model, team expertise, and operational requirements. We’ll explore: The trade-offs between package-based and image-based Red Hat Device Edge deployments. How Red Hat Satellite, Red Hat Ansible Automation Platform, and Red Hat Advanced Cluster Management for Kubernetes fit into the picture. The future of large-scale image-based management with Red Hat Edge Manager. Application lifecycle management options for MicroShift in both package-based and image-based environments. By the end of this article, you’ll have a clear understanding of the management strategies available for Red Hat Device Edge and how to align them with your edge computing goals. Red Hat Device Edge is built with RHEL, allowing you to confidently take advantage of familiar management tools. The primary differentiator is MicroShift support, which introduces additional considerations for container orchestration alongside traditional system management. Red Hat provides several products for scale management, but the optimal selection depends on your deployment approach, particularly whether you choose package-based or image-based RHEL deployments (both are supported with Red Hat Device Edge). The choice between using package-based or image-based (bootc) RHEL is a topic of its own and falls outside the scope of this article.</description></item><item><title>Migrating to Red Hat OpenShift Virtualization with NetApp FlexPod</title><link>https://kubermates.org/docs/2025-09-29-migrating-to-red-hat-openshift-virtualization-with-netapp-flexpod/</link><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-29-migrating-to-red-hat-openshift-virtualization-with-netapp-flexpod/</guid><description>Migrating to Red Hat OpenShift Virtualization with NetApp FlexPod Current challenges A unified platform Why persistent storage is essential Red Hat OpenShift Virtualization and NetApp: The unified foundation FlexPod, a design from NetApp validated by Cisco NetApp: Comprehensive storage solutions for the enterprise Dynamic storage provisioning with NetApp Trident Versatile protocol support and storage drivers Seamless integration with Red Hat OpenShift Service on AWS (ROSA) Facilitating VM migration Data protection and disaster recovery for business continuity Flexible snapshots and backups Key takeaways 15 reasons to adopt Red Hat OpenShift Virtualization About the authors George James Banu Sundhar More like this Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The path to virtualization started decades ago. Early efforts with mainframes in the 1960s allowed multiple operating systems to run on the same hardware. In the late 1990s, the virtualization of x86 hardware was introduced, and reshaped the way organizations deployed, managed, and scaled IT infrastructure. Applications were no longer bound to a single server. They could be moved at will, and even scaled to meet demand. Entire businesses and careers have been built on what we now know as traditional virtualization. And even as public cloud computing has become more accessible, traditional virtualization continues to evolve and will be with us for some time. Gartner projects that “70% of x86 datacenter workloads will continue to use hypervisor-based virtualization through 2027” Even highly effective traditions must evolve to meet changing needs. Today, organizations face pressures that are forcing them to reassess long-standing virtualization strategies, including: Changes in market pricing: Recent changes to licensing and increased prices has many organizations exploring more cost-effective alternatives. The rise of cloud-native development: Modern application development is moving towards a microservices architecture, where applications are split into small, independent components. Containers are the widely accepted standard for this approach. Many businesses are looking for ways to adopt these modern development practices without abandoning existing infrastructure.</description></item><item><title>Red Hat Device Edge: Decision framework</title><link>https://kubermates.org/docs/2025-09-29-red-hat-device-edge-decision-framework/</link><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-29-red-hat-device-edge-decision-framework/</guid><description>Red Hat Device Edge: Decision framework What makes Red Hat Device Edge special? Why edge computing needs different pricing models MicroShift support: The edge computing advantage Understanding MicroShift&amp;rsquo;s feature set API support limitations Excluded components for resource optimization What&amp;rsquo;s still available? Choosing the solution: Red Hat Device Edge vs. Red Hat alternatives Red Hat Device Edge vs. RHEL Red Hat Device Edge with MicroShift vs. Red Hat OpenShift What’s next Hatville: miniature city where edge computing comes to life About the author Luis Arizmendi More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Unlock the power of edge computing with Red Hat&amp;rsquo;s tailored solution for resource-constrained environments Edge computing presents unique challenges that demand strategic technology decisions. With Red Hat&amp;rsquo;s expanding portfolio of edge solutions, organizations often find themselves weighing multiple options: deploy standard Red Hat Enterprise Linux (RHEL), opt for Red Hat Device Edge , or implement Red Hat OpenShift at the edge? Whether you&amp;rsquo;re planning your first edge deployment or optimizing an existing infrastructure, we’ll help you evaluate where Red Hat Device Edge fits into your strategy. This is a big topic, so we’re breaking it into 2 parts: Part 1 (this post): A decision framework to help you figure out when Red Hat Device Edge makes sense, how it stacks up against RHEL and Red Hat OpenShift, and where MicroShift fits in edge environments. Part 2 : An in-depth look into management strategies for Red Hat Device Edge—from package-based vs. image-based deployments to automation tools and how Red Hat Edge Manager is changing the game. At the end of this series, you’ll have a clear framework for making informed decisions about Red Hat Device Edge adoption and feel prepared to effectively manage it. Red Hat Device Edge might sound like a completely new product, but here&amp;rsquo;s the interesting part: it&amp;rsquo;s actually RHEL with a twist. Red Hat Device Edge binaries are identical to RHEL, but Red Hat Device Edge adds subscription-level differences and support entitlements. In other words, there&amp;rsquo;s no technical difference between Red Hat Device Edge and RHEL; the magic lies in the subscription packaging and pricing strategy designed specifically for edge computing scenarios.</description></item><item><title>Simplify virtualization deployments with the Assisted Installer</title><link>https://kubermates.org/docs/2025-09-29-simplify-virtualization-deployments-with-the-assisted-installer/</link><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-29-simplify-virtualization-deployments-with-the-assisted-installer/</guid><description>Simplify virtualization deployments with the Assisted Installer What is the Assisted Installer? Getting started with Assisted Installer Migrate with confidence Red Hat OpenShift Virtualization Engine | Product Trial About the authors Courtney Grosch Michael Zasepa More like this Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Migrating a virtual machine (VM) from one platform to another is a complex task. Nevertheless, organizations are looking for a unified platform that provides the same benefits for VMs as what&amp;rsquo;s available for containers. With Red Hat OpenShift Virtualization, you don&amp;rsquo;t have to make the journey alone. The Assisted Installer for OpenShift supports deploying connected clusters with OpenShift Virtualization from the start, so you can begin your migration journey today and modernize at your own pace. The Assisted Installer simplifies the process of deploying connected OpenShift clusters by providing a guided, automated experience. Instead of relying on a dedicated bootstrap machine, the installer handles cluster bootstrapping as part of the installation process, reducing complexity and time to deployment. Available through a web-based UI, or API, the Assisted Installer gives you flexibility in how you manage installations. You can also integrate it with Red Hat Advanced Cluster Management to simplify deployment and ongoing management across multiple clusters, while still retaining the option to enable OpenShift Virtualization during installation. Virtualization-ready from day one With the Assisted Installer, you can enable OpenShift Virtualization during the cluster installation process. While some additional configuration may still be required, this approach simplifies deployment and shortens the path to running VM workloads alongside containers. The Assisted Installer also provides built-in pre-installation validations, checking for hardware and system requirements (CPU virtualization flags, memory, storage, and so on) before installation begins. This helps ensure that your environment is properly configured for virtualization and reduces the risk of deployment errors.</description></item><item><title>Announcing H1 2026 KCDs</title><link>https://kubermates.org/docs/2025-09-26-announcing-h1-2026-kcds/</link><pubDate>Fri, 26 Sep 2025 14:14:03 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-26-announcing-h1-2026-kcds/</guid><description>What’s New in 2026 📅 H1 2026 KCDs Looking Ahead Posted on September 26, 2025 by Audra Montenegro | CNCF, Community &amp;amp; Outreach We’re excited to announce the first wave of Kubernetes Community Days (KCDs) for 2026! These community-organized events bring together local practitioners, adopters, and contributors to connect and share cloud native knowledge. This year, CNCF has introduced three tiers of KCDs to support communities of all sizes: First-time KCDs – up to 200 attendees, guaranteed CNCF funding, and dedicated program support Tier 1 – returning events with 350 or more attendees, eligible for funding and monthly global organizer syncs Tier 2 – established events with up to 600 registrations, CNCF representation, and elevated branding February KCD New Delhi – February (NEW) KCD Guadalajara – February (Tier 1) March KCD Panama – March (NEW) KCD Beijing – March (Tier 1) April KCD Kochi – April (NEW) May KCD Toronto – May (NEW) KCD Austin – May (Tier 1) KCD Istanbul – May (Tier 2) KCD Helsinki – May (Tier 2) June KCD Kuala Lumpur – June (NEW) KCD Prague – June (Tier 1) KCD New York – June (Tier 2) July KCD Lima, Perú – July (Tier 2) Applications for 2H 2026 KCDs open in December 2025. If you’re interested in hosting, review the Hosting Ins &amp;amp; Outs guide and the full list of requirements on the CNCF website. 👉 Save the dates and join us at a KCD near you! Share.</description></item><item><title>VCF Breakroom Chats Episode 61: From VI Admin to Cloud Hero – Building a Multi-Tenant Cloud with VCF 9.0</title><link>https://kubermates.org/docs/2025-09-26-vcf-breakroom-chats-episode-61-from-vi-admin-to-cloud-hero-building-a-multi-tena/</link><pubDate>Fri, 26 Sep 2025 13:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-26-vcf-breakroom-chats-episode-61-from-vi-admin-to-cloud-hero-building-a-multi-tena/</guid><description>About the VCF Breakroom Chat Series Related Articles VCF Breakroom Chats Episode 61: From VI Admin to Cloud Hero - Building a Multi-Tenant Cloud with VCF 9.0 Sydney Sovereign Cloud Day 2025: Where Compliance Meets Innovation How to Upgrade to VMware Cloud Foundation 9.0 Welcome to the latest edition of VCF Breakroom Chats! In this exciting episode, we’re thrilled to feature Maher Alasfar, a Product Marketing Engineer with the VCF Division at Broadcom. Join Maher and Alina Thylander as they dive into how Virtual Infrastructure Administrators can seamlessly shift to Cloud Admin roles, leveraging the power of VCF Automation. Uncover the benefits of streamlined automation, enhanced control, and improved operational efficiency that enable your team to expertly manage multi-tenant private clouds. Elevate your strategy and embrace the journey to becoming a cloud expert today! Want to learn more about VCF Automation? Check out the VCF Automation web page for more resources. Read this blog post: Private Cloud Redefined: Deliver a Unified Cloud Consumption Experience with VMware Cloud Foundation 9.0 Explore VMware Cloud Foundation Automation by the Numbers with the Forrester Total Economic Impact (TEI) Study This webinar series focuses on VMware Cloud Foundation (VCF). In this series, we share conversations with industry-recognized experts from Broadcom and with solution partners and customers. You’ll gain relevant insights whether you’re an IT practitioner, IT admin, cloud or platform architect, developer, DevOps, senior IT manager, IT executive, or AI/ML professional. If you are new to the series, please check out our previous episodes.</description></item><item><title>KubeCon + CloudNativeCon North America 2025 Co-Located Event Deep Dive: Kubeflow Summit</title><link>https://kubermates.org/docs/2025-09-26-kubecon-cloudnativecon-north-america-2025-co-located-event-deep-dive-kubeflow-su/</link><pubDate>Fri, 26 Sep 2025 12:56:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-26-kubecon-cloudnativecon-north-america-2025-co-located-event-deep-dive-kubeflow-su/</guid><description>Who will get the most out of attending this event? What is new and different this year? What will the day look like? Should I do any homework first? Find your community! Posted on September 26, 2025 by Co-chairs: Valentina Rodriguez Sosa, Chase Christensen CNCF projects highlighted in this post The inaugural Kubeflow Summit 2022 was held at the AMA Conference Center San Francisco, with KubeCon + CloudNativeCon Paris 2024 being our first co-located event. Kubeflow Summit is where the community comes together to share experiences, showcase contributions, and highlight how organizations adopt and scale AI/ML workflows on Kubernetes. At its core, Kubeflow is machine learning and AI on Kubernetes, which means its value spans the entire stack—from infrastructure to model deployment. This event is designed to deliver value across a broad range of personas: Machine Learning Engineers (MLEs): Streamline experimentation, training, and deployment pipelines while ensuring reproducibility. Data Scientists (DS): Leverage Kubeflow Pipelines and notebooks to iterate quickly, access scalable compute, and integrate models into workflows. (especially with our new SDK) MLOps Practitioners: Build and manage robust, automated ML pipelines, enabling continuous integration and continuous delivery (CI/CD) for ML. DevOps Engineers: Apply Kubernetes-native best practices to scale workloads, manage resources efficiently, and integrate ML into existing infrastructure. Platform/Infrastructure Engineers: Provide secure, multi-tenant, and scalable Kubernetes environments optimized for AI/ML. Security &amp;amp; Compliance Teams: Use policy enforcement, auditing, and access controls to ensure that ML systems meet organizational and regulatory standards. Researchers &amp;amp; Academics: Accelerate scientific discovery with reproducible experiments, large-scale training, and collaborative platforms. Product &amp;amp; Business Leaders: Gain insights into how Kubeflow supports faster innovation cycles, reduces costs, and translates ML research into production outcomes. Community Contributors &amp;amp; Open Source Developers: Shape the project’s roadmap, contribute features, and build reusable components that strengthen the ecosystem.</description></item><item><title>Friday Five — September 26, 2025</title><link>https://kubermates.org/docs/2025-09-26-friday-five-september-26-2025/</link><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-26-friday-five-september-26-2025/</guid><description>Friday Five — September 26, 2025 The adaptable enterprise: Why AI-readiness is disruption-readiness Beginner&amp;rsquo;s Guide to AI : Why AI needs its railroad barons TFIR : The search for the Linux of AI The New Stack : When is MCP actually worth it? FinTech Magazine : How financial services can harness LLMs safely &amp;amp; effectively About the author Red Hat Corporate Communications More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share AI is a powerful tool, but your business needs the right foundation—and culture—to sustain it. In this new Red Hat e-book from SVP, Chief Strategy Officer Mike Ferris, you’ll learn how to build an AI-ready enterprise that’s prepared to turn change into innovation. Learn more In this episode of Beginner’s Guide to AI, Matt Hicks, CEO of Red Hat, unpacks why the future of business strategy in AI mirrors the age of the railroads. Just as railroads transformed industries, AI is laying down the tracks for the next wave of innovation — and businesses must decide whether to board or be left behind. Learn more AI may be the hottest technology story of our time, but its foundations remain unstable. Red Hat&amp;rsquo;s Steve Watt made a case for why open source must define the core of AI—just as Linux did for operating systems and Kubernetes did for cloud-native infrastructure. Learn more Model Context Protocol (MCP) shines in novel, multitenant agentic environments. But you might not need it for predictable automation. Learn more Expert insights on balancing innovation with compliance, risk management and regulatory requirements in financial AI deployment. Learn more Red Hat is the world’s leading provider of enterprise open source software solutions, using a community-powered approach to deliver reliable and high-performing Linux, hybrid cloud, container, and Kubernetes technologies. Red Hat helps customers integrate new and existing IT applications, develop cloud-native applications, standardize on our industry-leading operating system, and automate, secure, and manage complex environments. Award-winning support, training, and consulting services make Red Hat a trusted adviser to the Fortune 500.</description></item><item><title>Vodafone revolutionizes telco cloud with OpenShift, validated patterns, and GitOps</title><link>https://kubermates.org/docs/2025-09-26-vodafone-revolutionizes-telco-cloud-with-openshift-validated-patterns-and-gitops/</link><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-26-vodafone-revolutionizes-telco-cloud-with-openshift-validated-patterns-and-gitops/</guid><description>Vodafone revolutionizes telco cloud with OpenShift, validated patterns, and GitOps The challenge: Accelerating lifecycle operations in telco The solution: Validated patterns and GitOps GitOps: Automation and consistency Vodafone&amp;rsquo;s custom pattern: A closer look Benefits and lessons learned Outlook and key takeaways Red Hat Learning Subscription | Product Trial About the author Alex Handy More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Vodafone is transforming its 5G telco cloud operations by embracing OpenShift, validated patterns, and GitOps to streamline its lifecycle management of Kubernetes clusters. Tom Kivlin, Principal Cloud Architect at Vodafone, recently shared insights into their journey, highlighting the challenges overcome and the benefits realized through this strategic approach. He spoke at OpenShift Commons in London earlier this year. Traditional telecommunication service provider environments tend to operate at a slower pace compared to the rapid release cycles of Kubernetes, which sees three releases per year. Vodafone overcame hurdles in managing the lifecycle of their Kubernetes clusters, including: Faster pace of lifecycle operations : The adoption of containers and Kubernetes in its 5G network to necessitate quicker feature releases and software updates, demanding more frequent upgrades. Complex dependencies : Tackling inconsistencies resulting from intricate relationships between 5G software applications and the platform itself, often managed through spreadsheets. Complex dependencies are written in one single location, making them easier to discover. Manual management and human error : Improving manual lifecycle management of clusters which could lead to issues during pre-production testing and validation. This avoids using manual deployments and &amp;ldquo;quick &amp;amp; dirty&amp;rdquo; fixes. Time-consuming and high-risk deployments : Avoiding these time consuming and high-risk deployments by having everything well defined as code in a single, tracked and traced common repository made things smoother and faster. Using &amp;ldquo;validated patterns&amp;rdquo; reuses common components that have been shared and improved by the community “If you are managing the lifecycle of these clusters manually,” said Kivlin, “that can lead to problems that need fixing in pre-production as you&amp;rsquo;re doing lifecycle operation testing and validation. We wish to avoid this to ensure deployments and upgrades are quick yet effective.</description></item><item><title>Autonomous Testing of etcd’s Robustness</title><link>https://kubermates.org/docs/2025-09-25-autonomous-testing-of-etcd-s-robustness/</link><pubDate>Thu, 25 Sep 2025 15:25:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-25-autonomous-testing-of-etcd-s-robustness/</guid><description>Enhancing etcd’s Robustness Testing How We Tested What We Found Issues in the Main Development Branch Known Issues Conclusion Posted on September 25, 2025 by Marek Siarkowicz (Google | Kubernetes Maintainer) CNCF projects highlighted in this post As a critical component of many production systems, including Kubernetes, the etcd project’s first priority is reliability. Ensuring consistency and data safety requires our project contributors to continuously improve testing methodologies. In this article, we describe how we use advanced simulation testing to uncover subtle bugs, validate the robustness of our releases, and increase our confidence in etcd’s stability. We’ll share our key findings and how they have improved etcd. Many critical software systems depend on etcd to be correct and consistent, most notably as the primary datastore for Kubernetes. After some issues with the v3.5 release, the etcd maintainers developed a new robustness testing framework to better test for correctness under various failure scenarios. To further enhance our testing capabilities, we integrated a deterministic simulation testing platform from Antithesis into our workflow. The platform works by running the entire etcd cluster inside a deterministic hypervisor. This specialized environment gives the testing software complete control over every source of non-determinism, such as network behavior, thread scheduling, and system clocks. This means any bug it discovers can be perfectly and reliably reproduced. Within this simulated environment, the testing methodology shifts away from traditional, scenario-based tests. Instead of writing tests imperatively with strict assertions for one specific outcome, this approach uses declarative, property-based assertions about system behavior.</description></item><item><title>Sydney Sovereign Cloud Day 2025: Where Compliance Meets Innovation</title><link>https://kubermates.org/docs/2025-09-25-sydney-sovereign-cloud-day-2025-where-compliance-meets-innovation/</link><pubDate>Thu, 25 Sep 2025 15:10:49 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-25-sydney-sovereign-cloud-day-2025-where-compliance-meets-innovation/</guid><description>Why Attend? Agenda Highlights (All times AEDT – Sydney) Be Part of Australia’s Cloud Future Related Articles Sydney Sovereign Cloud Day 2025: Where Compliance Meets Innovation How to Upgrade to VMware Cloud Foundation 9.0 Analyst Insight Series: Virtualization Virtue #2: Stronger Cloud Security and Fault Tolerance On 21 October 2025, Broadcom will host Sovereign Cloud Day in Sydney. This half-day event is designed to help Australia’s technology leaders, policymakers, and compliance experts navigate one of today’s most urgent challenges: ensuring data sovereignty while enabling cloud-driven innovation. Australia’s Whole-of-Government Hosting Strategy has already identified sovereignty, data center ownership, and supply chain risks as top priorities. Meanwhile, the rise of AI and cybersecurity pressures are forcing organizations to rethink where and how their critical data is stored. At Sydney Sovereign Cloud Day, you’ll learn how to turn compliance into a competitive advantage with trusted local cloud solutions built to meet Australia’s strict sovereignty requirements. Whether you’re a CIO, CISO, policymaker, regulator, or cloud service provider, this event offers: Exclusive Policy Insights – Hear directly from the Australian Department of Home Affairs about the country’s cloud sovereignty journey and upcoming regulatory changes. Innovation in Practice – Learn how sovereign cloud frameworks support AI, cybersecurity, and sensitive workloads while keeping control in Australian hands. Real-World Success Stories – See how organizations are already achieving compliance and agility with sovereign cloud. Networking with Peers &amp;amp; Experts – Connect with leaders from government, finance, healthcare, defense, and the broader cloud ecosystem. 09:15 – Opening Remarks Sylvain Cazard, President APJ, Broadcom 09:20 – The Journey to Cloud Sovereignty Martin Hosken, Field CTO, VMware by Broadcom 09:40 – Australia’s Cloud Sovereignty Journey: Policy Outlook Tim Neal, Assistant Secretary, Department of Home Affairs 10:50 – Innovating Under Sovereign Cloud: Data, AI &amp;amp; Compliance in Australia Andrew Horton, COO, Australian Strategic Policy Institute (ASPI) 11:10 – Panel Discussion: Balancing Compliance &amp;amp; Innovation With leaders from Broadcom, Datacom, Interactive PTY, and more 11:50 – Technology Spotlight: Empowering Sovereign Cloud Innovation 12:10 – Wrap-Up &amp;amp; Networking Lunch See the full agenda when you register. This is your chance to gain actionable strategies, meet the experts shaping Australia’s digital sovereignty landscape, and explore Broadcom’s sovereign cloud framework—designed to help organizations retain control of their data while innovating with confidence. Seats are limited.</description></item><item><title>How to Upgrade to VMware Cloud Foundation 9.0</title><link>https://kubermates.org/docs/2025-09-25-how-to-upgrade-to-vmware-cloud-foundation-9-0/</link><pubDate>Thu, 25 Sep 2025 14:19:51 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-25-how-to-upgrade-to-vmware-cloud-foundation-9-0/</guid><description>Step 1: Assess, Check Prerequisites and Remediate for Core Components Step 2: Upgrade Existing Aria Automation and Aria Operations Components to VCF 9.0 Step 3: Upgrade VCF Management Domain Core Components Step 4: Deploy New VCF Operations Instance (if not previously installed) Step 5: Finalize the Upgrade Upgrading From VCF 4. x to VCF 9 Need Help? Related Articles Sydney Sovereign Cloud Day 2025: Where Compliance Meets Innovation How to Upgrade to VMware Cloud Foundation 9.0 Analyst Insight Series: Virtualization Virtue #2: Stronger Cloud Security and Fault Tolerance The process of upgrading to VMware Cloud Foundation (VCF) 9.0 depends on the version of VCF that you’re currently running as well as a mix of environmental, software and hardware considerations. For example, a specific hardware requirement may have changed and is no longer supported in a later version of VCF. The upgrade process for VCF 5. x has several different paths depending on the starting footprint of the environment. The steps vary depending on which existing VMware Aria Automation and VMware Aria Operations components need to be deployed or upgraded in the environment, as well as any remediations which may be required to the VMware vSphere components. This blog walks through the typical process that VCF Professional Services uses to assess, plan and perform upgrades from VCF 5. x to VCF 9. The first step is to assess and validate the environment. This includes the following: Validate the existing component configuration. This includes checking software components that are installed, their versions, and configuration specifics. For existing Aria Automation and Aria Operations deployments, you must be on the latest release.</description></item><item><title>Announcing Changed Block Tracking API support (alpha)</title><link>https://kubermates.org/docs/2025-09-25-announcing-changed-block-tracking-api-support-alpha/</link><pubDate>Thu, 25 Sep 2025 05:00:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-09-25-announcing-changed-block-tracking-api-support-alpha/</guid><description>Announcing Changed Block Tracking API support (alpha) What is changed block tracking? Benefits of changed block tracking support in Kubernetes Key components Implementation requirements Storage provider responsibilities Backup solution responsibilities Getting started What’s next? How do I get involved? We&amp;rsquo;re excited to announce the alpha support for a changed block tracking mechanism. This enhances the Kubernetes storage ecosystem by providing an efficient way for CSI storage drivers to identify changed blocks in PersistentVolume snapshots. With a driver that can use the feature, you could benefit from faster and more resource-efficient backup operations. If you&amp;rsquo;re eager to try this feature, you can skip to the Getting Started section. Changed block tracking enables storage systems to identify and track modifications at the block level between snapshots, eliminating the need to scan entire volumes during backup operations. The improvement is a change to the Container Storage Interface (CSI), and also to the storage support in Kubernetes itself. With the alpha feature enabled, your cluster can: Identify allocated blocks within a CSI volume snapshot Determine changed blocks between two snapshots of the same volume Streamline backup operations by focusing only on changed data blocks For Kubernetes users managing large datasets, this API enables significantly more efficient backup processes. Backup applications can now focus only on the blocks that have changed, rather than processing entire volumes. As Kubernetes adoption grows for stateful workloads managing critical data, the need for efficient backup solutions becomes increasingly important. Traditional full backup approaches face challenges with: Long backup windows : Full volume backups can take hours for large datasets, making it difficult to complete within maintenance windows. High resource utilization : Backup operations consume substantial network bandwidth and I/O resources, especially for large data volumes and data-intensive applications. Increased storage costs : Repetitive full backups store redundant data, causing storage requirements to grow linearly even when only a small percentage of data actually changes between backups.</description></item><item><title>From Chaos to Control: Achieving Network Policy Nirvana with Kyverno</title><link>https://kubermates.org/docs/2025-09-25-from-chaos-to-control-achieving-network-policy-nirvana-with-kyverno/</link><pubDate>Thu, 25 Sep 2025 08:00:55 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-25-from-chaos-to-control-achieving-network-policy-nirvana-with-kyverno/</guid><description>From Chaos to Control: Achieving Network Policy Nirvana with Kyverno The Challenge: The Manual Toil of Network Policies The Solution: Kyverno for Guardrails and Automation Key Insights from the Live Demo Conclusion: Empowering Platform Teams At the recent Kubernetes Community Days (KCD) in San Francisco, Jim Bugwadia, co-founder of Nirmata and a maintainer of the CNCF project Kyverno, delivered a session on one of the most persistent challenges in platform engineering: the balancing act between developer agility and robust security. His talk, “Network Policy Nirvana,” provided a clear roadmap for taming the complexity of Kubernetes network security through automation and policy-as-code. Jim began by framing the core problem. Platform teams are constantly caught between two competing needs: Developers require self-service and agility to ship features quickly. Security and compliance teams require guardrails and control to protect the platform. Nowhere is this tension more apparent than with Kubernetes Network Policies. While essential for security, native Network Policies can be complex, error-prone, and difficult to manage at scale. By default, Kubernetes allows all pods to talk to each other, so securing a cluster requires a “deny-by-default” stance, which can be difficult to implement manually. This leads to several pain points: Complexity: Managing countless YAML files for different applications is prone to error. Lack of Automation: Manual policy review creates bottlenecks and slows down development. Security Gaps: Misconfigurations can easily lead to unintended security vulnerabilities. Jim introduced Kyverno , a policy engine built for Kubernetes, as the solution.</description></item><item><title>Maximize your OpenShift investment: 6 reasons to upgrade to OpenShift Platform Plus</title><link>https://kubermates.org/docs/2025-09-25-maximize-your-openshift-investment-6-reasons-to-upgrade-to-openshift-platform-pl/</link><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-25-maximize-your-openshift-investment-6-reasons-to-upgrade-to-openshift-platform-pl/</guid><description>Maximize your OpenShift investment: 6 reasons to upgrade to OpenShift Platform Plus Why upgrade to Red Hat OpenShift Platform Plus? Built-in security that works from development to deployment Simplify management of all of your clusters Integrated data services for your apps Help developers build faster and smarter Keep things consistent, no matter where your apps run Handle new tech—and support the old Invest in your app dev journey Red Hat OpenShift Container Platform | Product Trial About the authors Jehlum Vitasta Pandit Jaleh Reeves More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share If you’re currently using Red Hat OpenShift Container Platform, you already know it provides a solid foundation for containerized applications. So why upgrade to Red Hat OpenShift Platform Plus ? The answer depends on where you are—and where you’re going. Maybe you started with a single team working on one project. But suddenly that team turned into 10 and now you’re drowning in security checks, management tools, and developer requests. If you’re juggling apps across a dozen environments and wondering, “How do I keep all of this secure and manageable—and keep my developers happy?,” OpenShift Platform Plus can help. With OpenShift Platform Plus, you gain an agile application environment with enhanced security and management tools, built to boost your efficiency and simplify your operations. And, you can upgrade with ease—no reinstalls needed. Let’s dive into how OpenShift Platform Plus can help your business. Built-in security that works from development to deployment Security and compliance are challenging for every organization, especially when you’re trying to keep systems consistent across different environments. If you feel like every time you push a new app into production, a whole new audit process slows everything down, you’re not alone. OpenShift Platform Plus gives you integrated security with Red Hat Advanced Cluster Security for Kubernetes. It scans everything from the moment you build to when it’s running live.</description></item><item><title>More than meets the eye: Behind the scenes of Red Hat Enterprise Linux 10 (Part 2)</title><link>https://kubermates.org/docs/2025-09-25-more-than-meets-the-eye-behind-the-scenes-of-red-hat-enterprise-linux-10-part-2/</link><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-25-more-than-meets-the-eye-behind-the-scenes-of-red-hat-enterprise-linux-10-part-2/</guid><description>More than meets the eye: Behind the scenes of Red Hat Enterprise Linux 10 (Part 2) 2023 (2 years until RHEL 10 launch) More like this Blog post Blog post Original podcast Original podcast Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share This series takes a look at the people and planning that went into building and releasing Red Hat Enterprise Linux 10. From the earliest conceptual stages to the launch at Red Hat Summit 2025, we’ll hear firsthand accounts of how RHEL 10 came into being. Part 1 In our first post looking behind the scenes of how Red Hat Enterprise Linux (RHEL) 10 came to be. We heard about the early stages that started right after Summit 2022 and the release of RHEL 9. This included assembling the team, setting expectations, and working with upstream communities to gather ideas. In part 2, the team behind RHEL 10 shares how a new approach to building the platform came into focus and implementation. Brian Stinson, principal software engineer The first bit that we start with is Fedora. And that gives us the content set and problem space to look at. And then the things that we need to drive, that will happen continuously. Stef Walter, Senior Director, Engineering Part of what we did was have CentOS Stream be the place where we do RHEL development. A lot of it comes from Fedora, but then a lot of the iteration—constantly having a build, multiple times a day of all of RHEL—is in CentOS Stream. Stinson The main advantage we got out of doing [CentOS Stream] was we were basically doing the build in public, and it actually helped us quite a bit just keeping some of that engagement.</description></item><item><title>Red Hat and O-RAN Alliance accelerating cloud adoption at the Edge</title><link>https://kubermates.org/docs/2025-09-25-red-hat-and-o-ran-alliance-accelerating-cloud-adoption-at-the-edge/</link><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-25-red-hat-and-o-ran-alliance-accelerating-cloud-adoption-at-the-edge/</guid><description>Red Hat and O-RAN Alliance accelerating cloud adoption at the Edge Foundational principles Cloudification Metal3 Vibrant and robust SMO ecosystem Broad O2-IMS API applicability O-cloud architectural approach Providing thought leadership and direction Community collaboration and open source projects O-RAN Alliance working groups (WG) O-RAN software community (SC) O-cloud proof-of-concept (PoC) integration and evolution Working on an open radio access network Red Hat Learning Subscription | Product Trial About the authors Ian Jolliffe Rob McManus More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share In telecommunication, bringing compute resources and intelligence closer to users improves the user experience, significantly reduces service provider costs, and unlocks the innovation needed to deliver compelling services that increase revenue. However, telecommunication service providers are faced with challenges when adopting new technologies, methodologies, and automation. Every option must be evaluated for compatibility with a radio access network (RAN). Solving these challenges are the reasons Red Hat has been collaborating and contributing to the O-RAN Alliance. Red Hat is committed to advancing the open radio access network (O-RAN) ecosystem through contributions to open source projects, and industry collaboration. Red Hat is collaborating on critical O-RAN interfaces, fostering a robust hardware partner ecosystem, working with service management and orchestration (SMO) providers, and making ongoing contributions to vital open source O-RAN initiatives. These contributions, of course, all end up in Red Hat&amp;rsquo;s cloud-native products. Red Hat&amp;rsquo;s O-RAN strategy is built on four core principles, all underpinned by open source collaboration. Adoption of cloud-native methodology drives down operational costs and increases reliability of deployment and ongoing management of O-RAN clusters for thousands of nodes at the edge. Cloud-native server clusters span from service provider IT systems, the core, and now to the edge. With AI becoming a part of many deployments, cloud-native principles are more important than ever. Adoption of cloud-native principles in the service provider market has been a challenge, and is a barrier to lower operational expenditure (OpEx).</description></item><item><title>The flight plan for AI: How we’re building a culture of innovation at Turkish Technology</title><link>https://kubermates.org/docs/2025-09-25-the-flight-plan-for-ai-how-we-re-building-a-culture-of-innovation-at-turkish-tec/</link><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-25-the-flight-plan-for-ai-how-we-re-building-a-culture-of-innovation-at-turkish-tec/</guid><description>The flight plan for AI: How we’re building a culture of innovation at Turkish Technology Beyond the code: A cultural shift Delivering real value at scale Navigating the regulations, securing our future Get started with AI Inference About the author Serdar Gürbüz More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share When you think of Turkish Airlines, you might picture the vast network of flights that connects more countries than any other airline in the world. You might think of the more than 80 million passengers we serve annually, or the incredible logistical complexity of operating such a massive enterprise across 130 countries and 353 destinations. At Turkish Technology, it&amp;rsquo;s our job to build the digital engine that powers it all. Our mission is to not only manage Turkish Airline systems today, but to build the next-generation digital products that will define the future of aviation. And for us, that future is undeniably driven by AI. My team and I didn&amp;rsquo;t approach AI as a collection of one-off pilot projects. Instead, we saw it as a strategic lever to accelerate our entire digital transformation. Our goal was to build a hardened, scalable, and replicable AI infrastructure that could be woven throughout the fabric of the organization. We embarked on this process nearly 3 years ago, and while we’ve made incredible progress, we know there&amp;rsquo;s no end to this journey–the airplanes always need to keep flying to their next stop. It&amp;rsquo;s an ongoing process and a continuous race to adapt as technology–especially generative AI (gen AI)–evolves at a rapid pace. One of our most important discoveries was that a technological transformation doesn&amp;rsquo;t begin with a software platform. It begins with your people.</description></item><item><title>Kubernetes Observability: Your Q&amp;A Guide to Calico Whisker</title><link>https://kubermates.org/docs/2025-09-24-kubernetes-observability-your-q-a-guide-to-calico-whisker/</link><pubDate>Wed, 24 Sep 2025 20:45:51 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-24-kubernetes-observability-your-q-a-guide-to-calico-whisker/</guid><description>Frequently Asked Questions From The Community Do you have more questions? The Calico community is here to help! Calico Whisker is quickly becoming the go-to tool for teams that want granular, real-time visibility into their Kubernetes network traffic and security posture. It provides an intuitive, high-level view of your network, but as with any new tool, there are going to be questions: How does it handle manifest-based installations versus operator-based ones? Can it leverage eBPF for high-performance data collection? What’s the best way to export its rich flow logs to your existing SIEM or visualize traffic on a network map? Getting the most out of Whisker requires understanding its inner workings and this guide is designed to help you master this exciting tool with support from the Calico community. We’ve compiled the most frequently asked questions from our community Slack , support conversations, and CalicoCon sessions. This Q&amp;amp;A covers everything from initial installation tips and version requirements to advanced topics like filtering flow logs and integrating with Goldmane, the powerful API that underpins Whisker. Whether you’re just beginning to evaluate Whisker or looking to extract more value from your current deployment, this guide provides clear, actionable answers to help you level up your Kubernetes observability game. Yes you can, noting that Calico Whisker requires Calico v3.30 or higher. If you’re running an older version, you’ll need to upgrade your cluster first. To check your Calico version, run the following command: kubectl exec -it -n tigera-operator deployment/tigera-operator &amp;ndash; operator &amp;ndash;version kubectl exec -it -n tigera-operator deployment/tigera-operator &amp;ndash; operator &amp;ndash;version If your version is older than v3.30, follow the upgrade guide or 📹 watch the video below for a demonstration of how to upgrade Calico on Kubernetes: Yes! While we recommend upgrading to the operator-based installation for a smoother experience and easier lifecycle management, you can still use Whisker with a manifest-based installation. Check out this detailed installation guide for enabling Whisker manually. Whisker and Goldmane contain sensitive network and workload data and are secured via NetworkPolicies by default. We strongly recommend against exposing this data externally. If you absolutely must expose it, you can create a new ingress NetworkPolicy to allow external traffic.</description></item><item><title>CNCF’s Helm Project Remains Fully Open Source and Unaffected by Recent Vendor Deprecations</title><link>https://kubermates.org/docs/2025-09-24-cncf-s-helm-project-remains-fully-open-source-and-unaffected-by-recent-vendor-de/</link><pubDate>Wed, 24 Sep 2025 16:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-24-cncf-s-helm-project-remains-fully-open-source-and-unaffected-by-recent-vendor-de/</guid><description>Posted on September 24, 2025 by Chris Aniszczyk (CTO, CNCF) and Matt Butcher (Helm Co-Creator) Recently, users may have seen the news about Broadcom (Bitnami) regarding upcoming deprecations of their publicly available container images and Helm Charts. These changes, which will take effect by September 29, 2025 , mark a shift to a paid subscription model for Bitnami Secure Images and the removal of many free-to-use artifacts from public registries. We want to be clear: these changes do not impact the Helm project itself. Helm is a graduated project that will remain under the CNCF. It continues to be fully open source, Apache 2.0 licensed, and governed by a neutral community. The CNCF community retains ownership of all project intellectual property per our IP policy , ensuring no single vendor can alter its open governance model. While “Helm charts” refer broadly to a packaging format that anyone can use to deploy applications on Kubernetes, Bitnami Helm Charts are a specific vendor-maintained implementation. Developed and maintained by the Bitnami team (now part of Broadcom), these charts are known for their ease of use, security features, and reliability. Bitnami’s decision to deprecate its public chart and image repositories is entirely separate from the Helm project itself. Users currently depending on Bitnami Helm Charts should begin exploring migration or mirroring strategies to avoid potential disruption. The Helm community is actively working to support users during this transition, including guidance on: Updating chart dependencies Exploring alternative chart sources Migrating to maintained open image repositories We encourage users to follow the Helm blog and Helm GitHub for updates and support resources. CNCF remains committed to maintaining the integrity of our open source projects and supporting communities through transitions like this.</description></item><item><title>Local Roots, Global Reach: CNCJ Reflects on KubeCon + CloudNativeCon Japan 2025</title><link>https://kubermates.org/docs/2025-09-24-local-roots-global-reach-cncj-reflects-on-kubecon-cloudnativecon-japan-2025/</link><pubDate>Wed, 24 Sep 2025 16:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-24-local-roots-global-reach-cncj-reflects-on-kubecon-cloudnativecon-japan-2025/</guid><description>🌏 A Landmark Gathering: KubeCon + CloudNativeCon Comes to Japan 🇯🇵 CNCJ and Japan Community Day Posted on September 24, 2025 by Masaya Aoyama, Sunyanan Choochotkaew (KubeCon+CloudNativeCon Japan 2025 Program Co-Chair, CNCJ Board) CNCF projects highlighted in this post Konnichiwa from Tokyo! 🇯🇵 In June 2025, something remarkable happened: the global cloud native community gathered in Tokyo for the first-ever KubeCon + CloudNativeCon Japan, hosted by the Cloud Native Computing Foundation (CNCF) under the Linux Foundation. This wasn’t just another tech conference—it was a proud milestone for Japan’s growing open source and cloud native ecosystem. KubeCon + CloudNativeCon Japan 2025 marked the first time the CNCF’s flagship event was held on Japanese soil—bringing together a full house of 1,500 attendees from across Japan and around the world. Hosted at the Hilton Tokyo Odaiba , the event featured over 40 sessions covering the full spectrum of cloud native technologies, ranging from beginner-friendly topics and observability to AI/ML, platform engineering, connectivity, and security. The two days were packed with innovation, collaboration, and connection. Developers, maintainers, end users, and technologists of all stripes came together to share lessons learned, explore emerging trends, and dive deep into the future of open source infrastructure. In addition to technical talks, the conference emphasized community building—celebrating contributors, spotlighting regional efforts, and encouraging diverse perspectives. For many Japanese attendees, it was a rare opportunity to engage with the global cloud native ecosystem right at home. For international guests, it was a chance to experience Japan’s fast-growing tech culture firsthand. While CNCF and the Linux Foundation brought usual professionalism to organizing KubeCon + CloudNativeCon Japan, we at Cloud Native Community Japan (CNCJ) were proud to support the local community layer —bridging global expertise with Japanese voices and initiatives. One of our most successful contributions was organizing Japan Community Day , held on June 15 , just ahead of the main conference. This CNCJ-led event gathered local contributors, meetup organizers, and engineers from across Japan and from overseas.</description></item><item><title>DxEnterprise operator for high availability now certified for RHEL 9.6</title><link>https://kubermates.org/docs/2025-09-24-dxenterprise-operator-for-high-availability-now-certified-for-rhel-9-6/</link><pubDate>Wed, 24 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-24-dxenterprise-operator-for-high-availability-now-certified-for-rhel-9-6/</guid><description>DxEnterprise operator for high availability now certified for RHEL 9.6 What is DxEnterprise? Certified confidence with DH2i Fixing your downtime challenges The operator backed by Microsoft and Red Hat Quick start on RHEL 9.6 What’s new in RHEL 9.6? Expanded compatibility with OpenShift Virtualization Conclusion Red Hat Learning Subscription | Product Trial About the authors Vivien Wang OJ Ngo More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share We’re excited to share that DxEnterprise high-availability (HA) software, including its SQL Server operator for Red Hat OpenShift, DxOperator, is now officially Red Hat-certified for Red Hat Enterprise Linux (RHEL) 9.6. View the certification in the Red Hat Ecosystem Catalog. The DxEnterprise operator is Microsoft’s preferred SQL Server operator for Kubernetes. The certification is further testimony that DxEnterprise is the ultimate HA solution across SQL Server instances and containers running on Windows, Linux, and Kubernetes, with infrastructure-agnostic resilience and zero trust network integration. DxEnterprise simplifies and unifies SQL Server HA deployments across platforms. Whether you’re running SQL Server on virtual machines (VMs), on bare metal, or in Kubernetes, DxEnterprise provides a flexible, automated, and secure clustering solution. Key features: Unified HA management and automated failover for SQL Server on Windows, Linux, and Kubernetes Mix and match SQL Server versions, HA instances, containers, and Availability Groups DxOperator (Microsoft’s preferred SQL Server operator for Kubernetes) Sidecar container deployment to eliminate custom image and support headaches Zero trust networking with software-defined perimeter (SDP) tunneling, connecting any server, container, or Internet of Things (IoT) device without relying on VPNs Learn more in the DxEnterprise Quick Start Guide. DH2i delivers “always-secure, always-on” SQL Server availability across Windows, Linux, and Kubernetes that organizations trust. Now, with RHEL 9.6 certification, organizations can have even more confidence. This certification proves that DxEnterprise meets the most stringent enterprise standards, ensuring customers can rely on a fully validated, dependable foundation for critical workloads. Combined with zero trust perimeter security, automated failover, and performance monitoring, DH2i provides the freedom to scale across infrastructures without compromise. SQL Server downtime and clustering complexity are notorious pain points.</description></item><item><title>New Red Hat Ansible Certified Content Collections for HashiCorp Terraform and HashiCorp Vault</title><link>https://kubermates.org/docs/2025-09-24-new-red-hat-ansible-certified-content-collections-for-hashicorp-terraform-and-ha/</link><pubDate>Wed, 24 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-24-new-red-hat-ansible-certified-content-collections-for-hashicorp-terraform-and-ha/</guid><description>New Red Hat Ansible Certified Content Collections for HashiCorp Terraform and HashiCorp Vault Red Hat Ansible Certified Content Collection for HashiCorp Terraform Red Hat Ansible Certified Content Collection for HashiCorp Vault Availability What can I do next? About the author Ron Reed More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Red Hat Ansible Automation Platform and HashiCorp have released two new Red Hat Ansible Certified Content Collections for HashiCorp Terraform and HashiCorp Vault: hashicorp. terraform and hashicorp. vault. These collections enhance automation with HashiCorp for securing and managing cloud infrastructure. The hashicorp. terraform collection integrates with HashiCorp Terraform for infrastructure lifecycle management, streamlining deployments, and supporting Infrastructure as Code (IAC) workflows. The hashicorp. vault collection will automate secrets management for authentication bolstering security. Both collections are certified by Red Hat, enabling reliability, enterprise support. The new hashicorp. terraform collection has been built from the ground up to provide API integration with HCP Terraform and Terraform Enterprise. This approach focuses on leveraging the APIs as the direct integration point, bringing a robust and efficient connection between Red Hat Ansible Automation Platform and your HashiCorp Terraform environments.</description></item><item><title>The evolution of Red Hat Ansible Lightspeed</title><link>https://kubermates.org/docs/2025-09-24-the-evolution-of-red-hat-ansible-lightspeed/</link><pubDate>Wed, 24 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-24-the-evolution-of-red-hat-ansible-lightspeed/</guid><description>The evolution of Red Hat Ansible Lightspeed Expanding generative AI within the Ansible Automation Platform user experience Getting started with Ansible Lightspeed intelligent assistant Additional resources Red Hat Ansible Automation Platform | Product Trial About the author Tricia McConnell More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Today we&amp;rsquo;re formally releasing the Red Hat Ansible Lightspeed intelligent assistant, a generative AI service which delivers an intuitive chat assistant embedded within Ansible Automation Platform. The Ansible Lightspeed intelligent assistant is like having an Ansible subject matter expert right at your keyboard. The service integrates documentation directly into the Ansible administrative and operations experience for faster troubleshooting and onboarding, as well as support for the day-to-day management of your automation itself. Ansible Lightspeed intelligent assistant within Ansible Automation Platform This generative AI service is a result of a concerted effort by our product and engineering teams to improve the overall platform user experience for both automation administrators and IT operators. Examples of this include a simplified and unified UI, the release of self-service automation to empower more users, a new on-premise analytics dashboard to measure performance, a fleet of new collections for managing the platform, and more. When we initially launched Red Hat Ansible Lightspeed with IBM watsonx Code Assistant in 2023, its purpose was to provide a robust coding assistant designed to help boost engineering and developer productivity during the creation of Ansible content, including playbooks, roles or tasks. We later embedded features such as content explanation and content source matching for greater transparency and context for model responses as well as prompt refinement and model customization for greater relevance and accuracy. Tapping into IBM&amp;rsquo;s Granite code models and trained on Ansible-specific data sources, the coding assistant democratizes access to automation and helps onboard new users as they familiarize themselves with the Ansible language and syntax. With the intelligent assistant, we bring similar value to Ansible Automation Platform administrators and operators by reducing friction and swivel-chair operations, allowing these users to remain within the platform experience to research, troubleshoot, and resolve common questions and issues. The intelligent assistant can respond to prompts like: &amp;ldquo;What is an execution environment?&amp;rdquo; &amp;ldquo;How do I manage user access to Ansible Automation Platform?&amp;rdquo; &amp;ldquo;Explain the &amp;ldquo;ERROR! couldn’t resolve module/action” error message?&amp;rdquo; &amp;ldquo;How do I configure Event-Driven Ansible?&amp;rdquo; Using a retrieval augmented generation (RAG) pipeline connected to Red Hat documentation and other resources, the intelligent assistant also provides referenced links to additional documentation to help you explore a given topic in more detail. Ansible Lightspeed intelligent assistant prompt, response, and referenced resources Additional roadmap items for the intelligent assistant include expanding data sources to provide visibility into the health and performance of your automation itself. For instance, you will be able to get assistance with prompts such as: &amp;ldquo;Why did my automation &amp;lsquo;VM-migration&amp;rsquo; job fail?&amp;rdquo; &amp;ldquo;Show me all inventories that are included in my Ansible Automation Platform deployment.</description></item><item><title>Securing Your Infrastructure as Code: The Power of Nirmata and HashiCorp Terraform</title><link>https://kubermates.org/docs/2025-09-23-securing-your-infrastructure-as-code-the-power-of-nirmata-and-hashicorp-terrafor/</link><pubDate>Tue, 23 Sep 2025 20:50:44 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-23-securing-your-infrastructure-as-code-the-power-of-nirmata-and-hashicorp-terrafor/</guid><description>Securing Your Infrastructure as Code: The Power of Nirmata and HashiCorp Terraform HashiCorp Terraform: The Foundation of Your Infrastructure Nirmata: The Governance &amp;amp; Security Layer A Practical Synergy HashiCorp Terraform is synonymous with Infrastructure as Code (IaC) and is heavily used for infrastructure provisioning, but often, cloud resources are misconfigured. Also, with the rapid adoption of AI, more IaC is being generated using AI tools. The challenge is to identify misconfigurations early, ensuring that provisioned infrastructure is secure and reliable. At Nirmata, we believe that empowering developers with agile, self-service tools is crucial, but it must be done with guardrails in place. This is where the powerful combination of Nirmata and Terraform comes into play. While Terraform is the industry standard for declaring and provisioning infrastructure, Nirmata provides the essential governance, security, and lifecycle management for that infrastructure and the applications running on it. Terraform, as the leader in Infrastructure as Code (IaC), allows you to define your cloud and on-premises resources in a declarative, human-readable language. With a vast ecosystem of providers, including a Nirmata provider , you can use Terraform to provision everything from a simple virtual machine to a complex, multi-cluster Kubernetes environment on any cloud, like AWS, Azure, or Google Cloud Platform (GCP). The core benefit is clear: you manage your infrastructure in a predictable, repeatable, and version-controlled way. While Terraform is ideal for provisioning, it falls short in addressing the ongoing challenges of Day 2 operations, security, and governance. This is where Nirmata’s policy-based platform complements your Terraform workflows. Think of it this way: Terraform is your blueprint for provisioning infrastructure across any provider and any cloud.</description></item><item><title>Solving Kubernetes Multi-tenancy Challenges with vCluster</title><link>https://kubermates.org/docs/2025-09-23-solving-kubernetes-multi-tenancy-challenges-with-vcluster/</link><pubDate>Tue, 23 Sep 2025 16:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-23-solving-kubernetes-multi-tenancy-challenges-with-vcluster/</guid><description>Understanding Multi-tenancy Multi-tenancy with native Kubernetes features Control Plane Isolation Data Plane Isolation vCluster Concept Hands-on Prerequisites Deploy a virtual cluster Deploy workload Deploy a CRD Hands-on summary Interaction with host applications Falco Kyverno Implications Summary Outlook References Posted on September 23, 2025 by Fabian Brundke, Senior Platform Engineer, Liquid Reply CNCF projects highlighted in this post When we are building Internal Developer Platforms (IDP) for our customers Kubernetes is often a solid choice as the robust core of this platform. This is due to its technical capabilities and the strong community that is constantly expanding the surrounding ecosystem. One common IDP use case is to support the software development lifecycle (SDLC) of multiple tenants (e. g. multiple applications or software engineering teams). Adopting Kubernetes helps to share resources among these different tenants and while this helps to – among other benefits – optimize costs and enforce standards, it also introduces the challenge of isolating workloads from each other. This is known as multi-tenancy and for an IDP we have to ensure that this isolation is fair and secure. Thankfully Kubernetes provides a few out-of-the-box features to support isolation in a multi-tenant setup. This isolation can happen for the control plane and data plane. Let’s briefly describe these features. Namespaces are used to group different resources for tenants into logical units. While this allows you to effectively apply e.</description></item><item><title>Analyst Insight Series: Virtualization Virtue #2: Stronger Cloud Security and Fault Tolerance</title><link>https://kubermates.org/docs/2025-09-23-analyst-insight-series-virtualization-virtue-2-stronger-cloud-security-and-fault/</link><pubDate>Tue, 23 Sep 2025 14:16:58 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-23-analyst-insight-series-virtualization-virtue-2-stronger-cloud-security-and-fault/</guid><description>Related Articles Analyst Insight Series: Virtualization Virtue #2: Stronger Cloud Security and Fault Tolerance First VMmark Result Published Using VMware Cloud Foundation 9.0 VCF Breakroom Chats Episode 57: Behind the Code – A Journey from Customer Pain to VCF 9.0 Guest post by Jean Atelsek, S&amp;amp;P Global Market Intelligence This blog is the second in our series on the benefits and trends of virtualization ( read the first blog here , and a companion to the 451 Research Business Impact Brief “ The virtues of virtualization. ” Securing infrastructure, applications and data has always been a tall order for IT, and it grows taller in modern distributed environments: attack vectors multiply as endpoints are added, and access and identity management becomes more complex. The inherent security enabled by virtualization is a key factor behind the staying power of virtual machines in today’s IT estates. VM isolation, network micro-segmentation and live migration allow operations teams to protect the environment and secure data while maintaining high availability. The scale and sophistication of cyberattacks has surged with growing internet connectivity and AI-driven automation: NIST’s National Vulnerability Database shows that in 2024, a record-breaking 40,009 new common vulnerabilities and exposures (CVEs) were discovered, up 38% from the prior year. 451 Research’s Voice of the Enterprise: Information Security, Budgets &amp;amp; Outlook 2024 survey reveals the impact on security teams, with cloud security, AI/ML implementation, GenAI, and data privacy cited as the top pain points. Not long ago, securing applications was seen as an afterthought, with controls applied after development and before deployment into production, but in today’s dynamic environments this is no longer sufficient. The key is to “shift left” and apply controls by default without losing up-to-the-minute availability of resources for developers. Virtualization accomplishes this by creating an isolated environment for each virtual machine, thereby reducing the attack surface, ideally with out-of-the-box configurations for protecting data, implementing hardening and security best practices, authenticating users (including non-human identities such as AI agents), and rotating certificates automatically. To ensure high availability, VMs benefit from redundancy—periodically saving snapshots in a remote location so configurations can be restored in case of an outage—and the ability to do live migration between physical hosts. Because virtualization defines machine characteristics in software, failover can be “designed in” to the environment to automatically recover without impacting the user experience. This advantage applies not only during emergencies but also when upgrading underlying hardware.</description></item><item><title>The FinOps Journey: From Visibility to Business Value</title><link>https://kubermates.org/docs/2025-09-23-the-finops-journey-from-visibility-to-business-value/</link><pubDate>Tue, 23 Sep 2025 12:50:15 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-23-the-finops-journey-from-visibility-to-business-value/</guid><description>FinOps Maturity Phases Phase 1: Shedding Light on IT Costs Phase 2: Planning Ahead Phase 3: Running IT Like a Business Final Thoughts Related Articles The FinOps Journey: From Visibility to Business Value Capacity Management: The IT Balancing Act You Can’t Ignore Deploy Distributed LLM Inference with GPUDirect RDMA over InfiniBand in VMware Private AI Let’s be real — IT costs can be a black box. Servers hum, invoices arrive, and before you know it, your cloud spend is climbing faster than your budget can keep up. That’s where FinOps comes in. If you haven’t heard the term before, Financial Operations (FinOps) is all about bringing financial discipline to the way you run IT. It’s the practice of tracking, forecasting, and optimizing spending on IT resources so that every dollar works harder. FinOps isn’t just for the big players anymore. What started as a process for larger service providers has now been embraced by smaller businesses, giving them the same ability to control cloud costs, improve efficiency, and drive smarter financial decisions. Why? Because when IT understands the true cost of delivering services, it stops being “just” a cost center and becomes a strategic business partner. And every organization’s path to FinOps maturity is different, but most will move through these three phases. Phase 1: Gaining an understanding of their costs (Inform) Phase 2: To planning and budgeting for future projects (Optimize) Phase 3: To ultimately, running IT like a business (Operate) The good news? Regardless of where you are in the process, VMware Cloud Foundation and its FinOps capabilities can help at every step. Phase 1 of the FinOps journey begins with understanding and managing your costs. Why This Matters: Financial Operations (FinOps) is about more than just saving money.</description></item><item><title>AI and Red Hat: Powering the future of cable providers</title><link>https://kubermates.org/docs/2025-09-23-ai-and-red-hat-powering-the-future-of-cable-providers/</link><pubDate>Tue, 23 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-23-ai-and-red-hat-powering-the-future-of-cable-providers/</guid><description>AI and Red Hat: Powering the future of cable providers The strategic play: From legacy to smart networks Automation: The North American efficiency imperative Machine Learning: Gaining a competitive edge Security: Non-negotiable for trust and compliance Addressing the future: Multilatency and AI workloads Maximizing ROI: New revenue streams and Edge AI Delivering a combined solution Learn more at SCTE TechExpo 2025 Hatville: miniature city where edge computing comes to life About the author Rob Wilmoth More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share North American cable providers are in the middle of a massive shift. It&amp;rsquo;s no longer just about delivering TV; it&amp;rsquo;s about being the backbone of high-speed internet, advanced communication, and a whole suite of digital services. To thrive in this evolving landscape, our partners in the cable industry are strategically leaning into Artificial Intelligence (AI). This is precisely where Red Hat&amp;rsquo;s AI technologies come in, offering a comprehensive framework to not only modernize existing infrastructure but also to unlock new revenue streams and significantly enhance operational efficiency and security. The core strategy for North American cable providers revolves around building highly agile, intelligent networks. Red Hat is a critical enabler of this strategy by helping transition to Software-Defined Networks (SDN) and edge computing architectures. These aren&amp;rsquo;t just buzzwords; they&amp;rsquo;re the foundation for open, flexible platforms that integrate AI smoothly. For providers, this means: Faster service deployment : Launching new services and features quickly to stay ahead of the competition and meet consumer demands. Enhanced network intelligence : Using AI to understand network behavior, predict issues, and optimize performance proactively. Virtualization is another key strategic pillar. Red Hat facilitates the deployment of virtualized cable modem termination systems (vCMTS) on our cloud-native platform built on Red Hat OpenShift. This directly addresses the need for: Scalability : Meeting the ever-increasing demands for bandwidth and new services without massive hardware overhauls.</description></item><item><title>Building an adaptable enterprise: A guide to AI readiness</title><link>https://kubermates.org/docs/2025-09-23-building-an-adaptable-enterprise-a-guide-to-ai-readiness/</link><pubDate>Tue, 23 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-23-building-an-adaptable-enterprise-a-guide-to-ai-readiness/</guid><description>Building an adaptable enterprise: A guide to AI readiness 4 strategies to build an adaptable enterprise 1. Focus on the challenge, not the technology 2. Invest in your people 3. Look to the hybrid cloud 4. Modernize your foundation Tame the AI chaos with the open source advantage Open your future Want guidance on other tech challenges? Get started with AI Inference About the author Mike Ferris More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The speed and diversity of today&amp;rsquo;s technological advances create a constant challenge for IT leaders. You simply cannot afford to wait and see which advances fall by the wayside or end up worthy of adoption. The pressure to invest in technologies like AI is immense, but chasing the latest breakthroughs without first defining a clear strategy is a risk that can delay long-term innovation. The key question you must address isn&amp;rsquo;t what your specific AI plan should be, it&amp;rsquo;s how you can build an enterprise capable of adapting to any disruption. This means moving beyond merely reacting to and recovering from change to being able to continuously deliver value while the world is changing around you. You need to achieve enterprise durability and adaptability. Building an adaptable, AI-ready foundation begins with a clear-eyed view of your business goals. You must define the problem or challenge first, and only then determine if AI can provide a solution.</description></item><item><title>Introducing Headlamp Plugin for Karpenter - Scaling and Visibility</title><link>https://kubermates.org/docs/2025-09-23-introducing-headlamp-plugin-for-karpenter-scaling-and-visibility/</link><pubDate>Tue, 23 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-23-introducing-headlamp-plugin-for-karpenter-scaling-and-visibility/</guid><description>Introducing Headlamp Plugin for Karpenter - Scaling and Visibility Map view of Karpenter Resources and how they relate to Kubernetes resources Visualization of Karpenter Metrics Scaling decisions Config editor with validation support Real time view of Karpenter resources Dashboard for Pending Pods Karpenter Providers How to use Feedback and Questions Headlamp is an open‑source, extensible Kubernetes SIG UI project designed to let you explore, manage, and debug cluster resources. Karpenter is a Kubernetes Autoscaling SIG node provisioning project that helps clusters scale quickly and efficiently. It launches new nodes in seconds, selects appropriate instance types for workloads, and manages the full node lifecycle, including scale-down. The new Headlamp Karpenter Plugin adds real-time visibility into Karpenter’s activity directly from the Headlamp UI. It shows how Karpenter resources relate to Kubernetes objects, displays live metrics, and surfaces scaling events as they happen. You can inspect pending pods during provisioning, review scaling decisions, and edit Karpenter-managed resources with built-in validation. The Karpenter plugin was made as part of a LFX mentor project. The Karpenter plugin for Headlamp aims to make it easier for Kubernetes users and operators to understand, debug, and fine-tune autoscaling behavior in their clusters. Now we will give a brief tour of the Headlamp plugin. Easily see how Karpenter Resources like NodeClasses, NodePool and NodeClaims connect with core Kubernetes resources like Pods, Nodes etc. Get instant insights of Resource Usage v/s Limits, Allowed disruptions, Pending Pods, Provisioning Latency and many more. Shows which instances are being provisioned for your workloads and understand the reason behind why Karpenter made those choices.</description></item><item><title>Kubernetes v1.34: Pod Level Resources Graduated to Beta</title><link>https://kubermates.org/docs/2025-09-22-kubernetes-v1-34-pod-level-resources-graduated-to-beta/</link><pubDate>Mon, 22 Sep 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-09-22-kubernetes-v1-34-pod-level-resources-graduated-to-beta/</guid><description>Kubernetes v1.34: Pod Level Resources Graduated to Beta Pod-level specification for resources Why does Pod-level specification matter? How to specify resources for an entire Pod Example manifest Interaction with container-level resource requests or limits Limitations On behalf of the Kubernetes community, I am thrilled to announce that the Pod Level Resources feature has graduated to Beta in the Kubernetes v1.34 release and is enabled by default! This significant milestone introduces a new layer of flexibility for defining and managing resource allocation for your Pods. This flexibility stems from the ability to specify CPU and memory resources for the Pod as a whole. Pod level resources can be combined with the container-level specifications to express the exact resource requirements and limits your application needs. Until recently, resource specifications that applied to Pods were primarily defined at the individual container level. While effective, this approach sometimes required duplicating or meticulously calculating resource needs across multiple containers within a single Pod. As a beta feature, Kubernetes allows you to specify the CPU, memory and hugepages resources at the Pod-level. This means you can now define resource requests and limits for an entire Pod, enabling easier resource sharing without requiring granular, per-container management of these resources where it&amp;rsquo;s not needed. This feature enhances resource management in Kubernetes by offering flexible resource management at both the Pod and container levels. It provides a consolidated approach to resource declaration, reducing the need for meticulous, per-container management, especially for Pods with multiple containers. Pod-level resources enable containers within a pod to share unused resoures amongst themselves, promoting efficient utilization within the pod. For example, it prevents sidecar containers from becoming performance bottlenecks. Previously, a sidecar (e.</description></item><item><title>Build faster, debug smarter, and make AI safer with new DigitalOcean Gradient™ AI Platform features</title><link>https://kubermates.org/docs/2025-09-22-build-faster-debug-smarter-and-make-ai-safer-with-new-digitalocean-gradient-ai-p/</link><pubDate>Mon, 22 Sep 2025 15:40:41 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-22-build-faster-debug-smarter-and-make-ai-safer-with-new-digitalocean-gradient-ai-p/</guid><description>Build faster, debug smarter, and make AI safer with new DigitalOcean Gradientâ¢ AI Platform features Build better knowledge bases with activity logs Build faster with agent templates Automate workflows with n8n integration Make AI safer with enhanced guardrails Always evolving for developers About the author Try DigitalOcean for free Related Articles Announcing Gateway API Support for DigitalOcean Kubernetes What&amp;rsquo;s New on DigitalOcean App Platform Single Sign-On is Now Available, Strengthening Security and Simplifying Authentication By Grace Morgan Updated: September 19, 2025 3 min read The way developers build with AI is changing fastâand so is the DigitalOcean Gradientâ¢ AI Platform. Debugging data, reinventing agents from scratch, or connecting AI to workflows manually takes time and slows you down. The DigitalOcean Gradientâ¢ AI Platform is evolving to tackle these challenges, helping you work smarter, faster, and more confidently. This month, weâre introducing four new updates designed to make AI development more transparent, productive, and secure: Knowledge base activity logs: track and debug indexing jobs â Knowledge base activity logs: track and debug indexing jobs â Agent templates: start projects faster â Agent templates: start projects faster â n8n integration: automate workflows â n8n integration: automate workflows â Enhanced guardrails: make AI safer â Enhanced guardrails: make AI safer â When you add data to a knowledge base, it isnât always clear whatâs happening behind the scenes. Did the indexing job finish? Did certain files fail to index? Why isnât the data showing up in the knowledge base? Until now, debugging knowledge base inputs meant guesswork. Activity logs give you clear visibility into every run, so you always know what worked, what failed, and why. Real-time banners for active jobs Real-time banners for active jobs History view to review past runs History view to review past runs CSV downloads with per-source error details CSV downloads with per-source error details With this update, you can quickly pinpoint issues, fix them, and be confident about your knowledge base without slogging through trial and error. Explore Activity Logs â Why start from scratch when you can start from a working agent? Agent templates are code-first projects available on GitHub that you can clone, customize, and deploy in minutes. llm-Auditor Agent: Adds fact-checking using Tavily, an AI tool that verifies answers for accuracy, with optional knowledge base grounding. Product Documentation Agent: Transforms product documentation into object storage, builds and indexes a knowledge base, deploys a citation-rich answering agent, and optimizes it for high RAG performance as a support chatbot. SQL Agent: Translates natural language into safe, read-only SQL queries. Twilio API Agent: Sends marketing texts via Twilio with ready-made logic.</description></item><item><title>KubeCon + CloudNativeCon North America 2024 Co-Located Event Deep Dive: CiliumCon</title><link>https://kubermates.org/docs/2025-09-22-kubecon-cloudnativecon-north-america-2024-co-located-event-deep-dive-ciliumcon/</link><pubDate>Mon, 22 Sep 2025 13:25:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-22-kubecon-cloudnativecon-north-america-2024-co-located-event-deep-dive-ciliumcon/</guid><description>What do you want attendees to get out of this event? What is new and different this year? What will the day look like? Should I do any homework first? Meet your community! A note from co-chair Bill Mulligan: Posted on September 22, 2025 by Co-chairs: Bill Mulligan and Hemanth Malla CNCF projects highlighted in this post CiliumCon began at KubeCon + CloudNativeCon Europe 2023 in Amsterdam, and this will be our sixth time hosting it. Like Cilium itself, the event has grown significantly since its inception, expanding from its original focus to now include Hubble and Tetragon too. There might even be some nods to scaling networking for the needs of AI. CiliumCon is where the Cilium and eBPF community comes together to share real-world experiences and learn from each other. It’s a space to meet the Github handles you have been working with or following the whole year and who knows, you might even walk away with an eBee. Platform teams will walk away with the knowledge on everything from implementing Cilium Mesh in production environments to advanced DNS policy management to multi-cluster security strategies and running IPv6 only Kubernetes clusters. Seasoned contributors will learn if it’s actually possible to parse DNS in the kernel and everyone will help build the buzz around Cilium. This year, we are celebrating almost 10 years of Cilium! As the project has matured, we’re able to feature the experience of a decade of production implementations. Attendees will hear how CiliumEndpointSlice helped Microsoft unlock higher scale at AKS, SeatGeek’s journey from zero to production in 90 days with Cilium Mesh, ESnet’s pioneering IPv6-first Kubernetes deployment, and Google’s multi-cluster network security strategies with Cilium, Tetragon, and Hubble. We’re also diving deep into kernel-level DNS parsing improvements and will learn how to solve a real-world bug in Cilium’s BPF-based SNAT and its LRU eviction policy. The CIlium and eBPF community will come together for a half-day on Monday, November 10th, from 9:00 AM to 12:30 PM EST. The day will kick off with opening remarks from Program Committee Co-Chairs Bill Mulligan and Hemanth Malla, followed by six technical sessions, and a keynote from Liz Rice which will cover the broader community direction and vision.</description></item><item><title>Blog: Spotlight on the Kubernetes Steering Committee</title><link>https://kubermates.org/docs/2025-09-22-blog-spotlight-on-the-kubernetes-steering-committee/</link><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-22-blog-spotlight-on-the-kubernetes-steering-committee/</guid><description>Spotlight on the Kubernetes Steering Committee Introductions About the Steering Committee The road to the Steering Committee Conclusion This interview was conducted in August 2024, and due to the dynamic nature of the Steering Committee membership and election process it might not represent the actual composition accurately. The topics covered are, however, overwhelmingly relevant to understand its scope of work. As we approach the Steering Committee elections, it provides useful insights into the workings of the Committee. The Kubernetes Steering Committee is the backbone of the Kubernetes project, ensuring that its vibrant community and governance structures operate smoothly and effectively. While the technical brilliance of Kubernetes is often spotlighted through its Special Interest Groups (SIGs) and Working Groups (WGs) , the unsung heroes quietly steering the ship are the members of the Steering Committee. They tackle complex organizational challenges, empower contributors, and foster the thriving open source ecosystem that Kubernetes is celebrated for. But what does it really take to lead one of the world’s largest open source communities? What are the hidden challenges, and what drives these individuals to dedicate their time and effort to such an impactful role? In this exclusive conversation, we sit down with current Steering Committee (SC) members — Ben, Nabarun, Paco, Patrick, and Maciej — to uncover the rewarding, and sometimes demanding, realities of steering Kubernetes. From their personal journeys and motivations to the committee’s vital responsibilities and future outlook, this Spotlight offers a rare behind-the-scenes glimpse into the people who keep Kubernetes on course. Sandipan: Can you tell us a little bit about yourself? Ben : Hi, I’m Benjamin Elder , also known as BenTheElder. I started in Kubernetes as a Google Summer of Code student in 2015 and have been working at Google in the space since 2017. I have contributed a lot to many areas but especially build, CI, test tooling, etc. My favorite project so far was building KIND.</description></item><item><title>Worldpay's Platform as a Product: Revolutionizing development with Red Hat OpenShift</title><link>https://kubermates.org/docs/2025-09-22-worldpay-s-platform-as-a-product-revolutionizing-development-with-red-hat-opensh/</link><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-22-worldpay-s-platform-as-a-product-revolutionizing-development-with-red-hat-opensh/</guid><description>Worldpay&amp;rsquo;s Platform as a Product: Revolutionizing development with Red Hat OpenShift GKOP: More than just OpenShift The pillars of a compelling platform product API-driven self-service and automated validation Cultivating a great developer experience Red Hat Learning Subscription | Product Trial About the author Alex Handy More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Worldpay has fundamentally transformed its development processes by leveraging Red Hat OpenShift as the foundation for its internal developer platform, dubbed the Global Kubernetes Orchestration Platform (GKOP). This strategic shift has enabled Worldpay to empower its development teams with self-service capabilities, fostering a culture of collaboration and efficiency. Launched in 2020, GKOP is an enterprise platform built on Red Hat OpenShift Service on AWS (ROSA), a fully managed turnkey application platform. It is PCI DSS compliant and manages a fleet of 28 clusters across multiple regions. Worldpay views GKOP as a product, not just a platform. Things must run smoothly and reliably, so the GKOP team has built extensive automation around it. “GKOP is our product, because it&amp;rsquo;s so much more than just OpenShift. We built a lot of automation around it so the backbone of GKOP is the orchestration and configuration management. We asked ourselves ‘How do we do it? How do we run a fleet of clusters and manage them at scale?&amp;quot; asked Bernd Malmqvist, director of platform engineering at Worldpay. “Our cluster management basically is rendering the configuration [which is then] applied to downstream clusters… In disaster recovery exercises, we delete a region and rebuild it from scratch, from start to finish, within one and a half hours. We achieve this because the configuration is pre-rendered. When a cluster comes back online, configuration can be instantly applied.</description></item><item><title>KubeCon + CloudNativeCon India 2025: A Transformative Experience in Hyderabad</title><link>https://kubermates.org/docs/2025-09-20-kubecon-cloudnativecon-india-2025-a-transformative-experience-in-hyderabad/</link><pubDate>Sat, 20 Sep 2025 00:17:54 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-20-kubecon-cloudnativecon-india-2025-a-transformative-experience-in-hyderabad/</guid><description>Posted on September 19, 2025 by audra My journey from Sri Lanka to India’s premier cloud-native conference A Dream Come True: The Dan Kohn Scholarship I’m thrilled to share my incredible experience at KubeCon + CloudNativeCon India 2025 in Hyderabad, made possible through the prestigious Dan Kohn Scholarship from the Cloud Native Computing Foundation (CNCF). My heartfelt gratitude goes to Kanchana Wickremasinghe, Audra Montenegro, and Wendi West from CNCF for selecting me from the KCD Organizer digest, and special thanks to Kemila De Silva for nominating me. This opportunity opened doors to an unforgettable learning and networking experience that will shape my cloud-native journey for years to come. Reuniting with Fellow Sri Lankan Cloud Native Leaders One of the most exciting aspects of this trip was meeting my fellow KCD organizers and CNCF ambassadors Kemila De Silva and Chamod Shehanka Perera. These amazing individuals traveled to Hyderabad to speak on the first day of KubeCon, representing the vibrant Sri Lankan cloud-native community on an international stage. Day 1: Keynotes, Connections, and Big Announcements Our first day began with the essential conference ritual of collecting our badges and heading to the keynote hall. The energy was palpable, with thousands of cloud-native enthusiasts gathered under one roof. A Brief but Memorable Encounter After the keynotes, we had the privilege of meeting Chris, the CTO of Cloud Infrastructure at CNCF. Despite his hectic schedule, he graciously took a minute to speak with us, a testament to the inclusive, welcoming nature of the CNCF community. Exciting News: KubeCon 2026 Mumbai During the initial keynotes, we received thrilling news: KubeCon 2026 is coming to Mumbai in July! This announcement was met with thunderous applause and signals the growing importance of India in the global cloud native ecosystem. Solution Showcase: Unexpected Sri Lankan Connections The Solution Showcase proved to be a treasure trove of discoveries. We were delighted to meet two fellow Sri Lankans making their mark in the cloud-native world: Exploring Cutting-Edge Observability The showcase floor buzzed with innovative technologies.</description></item><item><title>Kubernetes v1.34: Recovery From Volume Expansion Failure (GA)</title><link>https://kubermates.org/docs/2025-09-19-kubernetes-v1-34-recovery-from-volume-expansion-failure-ga/</link><pubDate>Fri, 19 Sep 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-09-19-kubernetes-v1-34-recovery-from-volume-expansion-failure-ga/</guid><description>Kubernetes v1.34: Recovery From Volume Expansion Failure (GA) Reducing PVC size to recover from failed expansion Improved error handling and observability of volume expansion Improved observability of in-progress expansion Improved error handling and reporting Fixes long standing bugs in resizing workflows Have you ever made a typo when expanding your persistent volumes in Kubernetes? Meant to specify 2TB but specified 20TiB ? This seemingly innocuous problem was kinda hard to fix - and took the project almost 5 years to fix. Automated recovery from storage expansion has been around for a while in beta; however, with the v1.34 release, we have graduated this to general availability. 2TB 20TiB While it was always possible to recover from failing volume expansions manually, it usually required cluster-admin access and was tedious to do (See aformentioned link for more information). What if you make a mistake and then realize immediately? With Kubernetes v1.34, you should be able to reduce the requested size of the PersistentVolumeClaim (PVC) and, as long as the expansion to previously requested size hadn&amp;rsquo;t finished, you can amend the size requested. Kubernetes will automatically work to correct it. Any quota consumed by failed expansion will be returned to the user and the associated PersistentVolume should be resized to the latest size you specified. I&amp;rsquo;ll walk through an example of how all of this works. Imagine that you are running out of disk space for one of your database servers, and you want to expand the PVC from previously specified 10TB to 100TB - but you make a typo and specify 1000TB. 10TB 100TB 1000TB kind : PersistentVolumeClaim apiVersion : v1 metadata : name : myclaim spec : accessModes : - ReadWriteOnce resources : requests : storage : 1000TB # newly specified size - but incorrect! kind : PersistentVolumeClaim apiVersion : v1 metadata : name : myclaim spec : accessModes : - ReadWriteOnce resources : requests : storage : 1000TB # newly specified size - but incorrect! Now, you may be out of disk space on your disk array or simply ran out of allocated quota on your cloud-provider. But, assume that expansion to 1000TB is never going to succeed. 1000TB In Kubernetes v1.34, you can simply correct your mistake and request a new PVC size, that is smaller than the mistake, provided it is still larger than the original size of the actual PersistentVolume. kind : PersistentVolumeClaim apiVersion : v1 metadata : name : myclaim spec : accessModes : - ReadWriteOnce resources : requests : storage : 100TB # Corrected size; has to be greater than 10TB.</description></item><item><title>First VMmark Result Published Using VMware Cloud Foundation 9.0</title><link>https://kubermates.org/docs/2025-09-19-first-vmmark-result-published-using-vmware-cloud-foundation-9-0/</link><pubDate>Fri, 19 Sep 2025 17:44:10 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-19-first-vmmark-result-published-using-vmware-cloud-foundation-9-0/</guid><description>Related Related Articles First VMmark Result Published Using VMware Cloud Foundation 9.0 VCF Breakroom Chats Episode 57: Behind the Code – A Journey from Customer Pain to VCF 9.0 10 VMware Cloud Foundation 9.0 Enhancements: Simplifying Your Day 2 Operations We are excited to announce the first VMmark 4 result using our latest release, VMware Cloud Foundation (VCF) 9.0. The Lenovo result is also the first 4-socket result to use Intel’s latest server processor generation, Intel® Xeon® 6700 series processors with Performance-cores , which offer more cores and higher memory bandwidth. Find this new result on the VMmark 4 Results page. The following table compares the new Lenovo result to the previous-generation ThinkSystem running vSphere 8.0 Update 3: The chart below shows a 43% increase in total core count between these two results: In terms of performance, the VMmark 4 score increased by 49%! Key highlights include: The combination of VCF 9.0 and newer server hardware allowed 50% more workload VMs (132) compared to the previous generation result (88 VMs) The Intel Xeon 6788P processors in the Lenovo ThinkSystem SR860 V4 have 43% more cores than the previous-generation Intel Xeon Platinum 8490H processors in the ThinkSystem SR860 V3 The VMmark score is 49% higher than the previous generation’s VMmark 4 result Here are some resources to learn more: What’s New in VMware Cloud Foundation 9.0 VMware Cloud Foundation 9.0 Release Notes Lenovo ThinkSystem SR860 V4 Intel Xeon 6 Processors with Performance-Cores (P-Cores) VMmark is a free benchmarking tool used by partners and customers to measure the performance, scalability, and power consumption of virtualization platforms. Visit the VMmark product page and VMmark Community for more information. You can also try VMmark 4 right away by heading to the VMware Hands-on Labs Catalog and searching for “VMmark. ”.</description></item><item><title>Hacktoberfest 2025: Celebrate All Things Open Source!</title><link>https://kubermates.org/docs/2025-09-19-hacktoberfest-2025-celebrate-all-things-open-source/</link><pubDate>Fri, 19 Sep 2025 16:23:27 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-19-hacktoberfest-2025-celebrate-all-things-open-source/</guid><description>Hacktoberfest 2025: Celebrate All Things Open Source! Hacktoberfest 2025 is just around the corner How to participate in Hacktoberfest 2025 Hacktoberfest and Major League Hacking About the author(s) Try DigitalOcean for free Related Articles Stop Building SaaS from Scratch: Meet the SeaNotes Starter Kit Powered by DigitalOcean Hatch: Why Uxifyâs Founders Always Choose DigitalOcean Powered by DigitalOcean Hatch: Ontra Mobility is Building Smarter Cities By DigitalOcean and Haimantika Mitra Published: September 19, 2025 3 min read October is almost here, which brings Hacktoberfest with it. We just started registrations for Hacktoberfest , and now is the time to prepare yourself for a month filled with open-source contribution, collaboration, and fun! Hacktoberfest is our month-long celebration of all things open source that kicks off on October 1. Each year, maintainers and contributors come together to contribute to the open-source projects that many individual maintainers, companies (including DigitalOcean), and builders of every skill level rely on. In the 12 years since its inception, Hacktoberfest has exploded from a group of just 676 contributors to a global open-source phenomenon, with over 90,000 participants last year across the globe. Thatâs a lot of love for open source! Hacktoberfest challenges you to support open-source projects by creating 6 pull or merge requests that project maintainers accept during October. Weâre doing awards a little differently this year and bringing back t-shirts! If youâre among the first 10,000 contributors to complete 6 PRs, you will receive an official Hacktoberfest 2025 t-shirt and have a tree planted in your name. Weâre also partnering with Holopin this year to share the open-source love digitally. The mascot for this year is Hackstronaut, and all you have to do to get your first Hackstronaut pin is register! As you complete each PR, your pin will move up through different levels, so keep an eye on your inbox to claim them. Starting today, visit Hacktoberfest. com to get excited about making your contributions or prepare your project to receive some. Also, mark your calendar for the virtual kickoff event on October 1. The Hacktoberfest website has all the information you need to get started.</description></item><item><title>Top Kubernetes (K8s) Troubleshooting Techniques – Part 2</title><link>https://kubermates.org/docs/2025-09-19-top-kubernetes-k8s-troubleshooting-techniques-part-2/</link><pubDate>Fri, 19 Sep 2025 16:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-19-top-kubernetes-k8s-troubleshooting-techniques-part-2/</guid><description>Kubernetes Troubleshooting Storage: Resolving PVC Pending Errors 7. Using Event and Audit Logs: Deep System Analysis 8. Using Kubernetes Dashboard and Visual Tools 9. Implementing Health Checks and Probes 10. Advanced Debugging Techniques Practical Example: Network Issue Investigation Conclusion Posted on September 19, 2025 by Keval Bhogayata, Principal Engineer at Middleware In Part 1 of our series, we explored essential Kubernetes troubleshooting techniques that help DevOps engineers diagnose and resolve common cluster and application issues effectively. However, Kubernetes environments are complex, and there’s always more to uncover. In this Part 2, we’ll dive deeper into additional troubleshooting strategies, covering advanced techniques and real-world scenarios that can save you time and prevent downtime. Whether you’re dealing with persistent pod failures, network issues, or cluster misconfigurations, these tips will equip you with the knowledge to tackle Kubernetes challenges with confidence. The PersistentVolumeClaim (PVC) Pending status is a common storage issue in Kubernetes, preventing applications from accessing persistent data. This typically results from misconfigured storage classes, missing volume provisioners, or insufficient available storage in the cluster. Start by listing all Persistent Volumes (PVs) and Persistent Volume Claims (PVCs) across all namespaces. This command provides an overview of their status, access modes, capacity, and whether they are bound: kubectl get pv,pvc &amp;ndash;all-namespaces kubectl get pv,pvc &amp;ndash;all-namespaces To further investigate an unbound PVC stuck in the Pending state, use the following command: kubectl describe pvc kubectl describe pvc Check the Events section at the bottom of the output.</description></item><item><title>KubeCon + CloudNativeCon North America 2024 Co-Located Event Deep Dive: Observability Day</title><link>https://kubermates.org/docs/2025-09-19-kubecon-cloudnativecon-north-america-2024-co-located-event-deep-dive-observabili/</link><pubDate>Fri, 19 Sep 2025 13:22:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-19-kubecon-cloudnativecon-north-america-2024-co-located-event-deep-dive-observabili/</guid><description>What do you want attendees to get out of this event? What is new and different this year? What will the day look like? Should I do any homework first? Meet your community! Posted on September 19, 2025 by Co-chairs: Eduardo Silva, Austin Parker and Juraci Paixão Kröhling Observability Day traces its roots back to KubeCon Europe 2022 in Valencia, where we hosted FluentCon, a co-located event dedicated exclusively to the Fluentd and Fluent Bit projects. The gathering was a success, and the feedback we received from the community made it clear that we could serve the broader observability space at KubeCon in an even bigger way. To make that happen, we expanded our focus and invited other CNCF projects to join. Just a few months later, at KubeCon US 2022, we introduced Open Observability Day, bringing together maintainers and community members from projects such as Jaeger, Fluentd, Thanos, Prometheus, and OpenTelemetry. The response was overwhelmingly positive, and it quickly became clear that this effort needed to evolve into something more formal and structured. Working closely with the CNCF, that vision became reality—and Observability Day was officially born We want attendees to know that Observability Day is a community-driven gathering where practitioners, end users, and project maintainers come together to shape the future of observability. It’s not just about one tool or project, it’s about learning from each other, sharing best practices, and exploring how the CNCF ecosystem works together to solve real-world challenges. Whether you’re new to observability or deeply involved in projects like Prometheus, Fluent Bit, Jaeger, Thanos, or OpenTelemetry, you’ll leave with practical insights, valuable connections, and a clearer picture of where the space is heading. Most importantly, this is your chance to directly engage with the people building the tools you use every day. What’s new this year is the breadth and depth of community-driven content. Observability Day 2025 puts CNCF projects front and center, with maintainers and practitioners sharing the latest updates and lessons learned. You’ll find talks that go beyond theory—covering real-world scaling challenges, cost-efficient monitoring strategies, and new frontiers like AI observability There’s also a stronger emphasis on OpenTelemetry in action, from rapid adoption patterns to advanced use cases in telco, mobile, and enterprise systems.</description></item><item><title>Accelerate AI inference with vLLM</title><link>https://kubermates.org/docs/2025-09-19-accelerate-ai-inference-with-vllm/</link><pubDate>Fri, 19 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-19-accelerate-ai-inference-with-vllm/</guid><description>Accelerate AI inference with vLLM What is vLLM? Why vLLM matters for LLM inference Strategic advantages for enterprise AI Democratizing AI and optimizing costs Scaling AI applications with confidence Hardware flexibility and expanding choice Accelerated innovation and community impact Enterprise-grade AI with vLLM Red Hat AI Inference Server Unifying the AI infrastructure How Red Hat can help Get started with AI Inference About the author Technically Speaking Team More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share At this point, the transformative potential of a large language model (LLM) is clear, but efficiently deploying these powerful models in production can be challenging. This challenge is not new. In a recent episode of the Technically Speaking podcast, Chris Wright spoke with Nick Hill, a principal software engineer at Red Hat who worked on the commercialization of the original IBM Watson &amp;ldquo;Jeopardy!&amp;rdquo; system years ago. Hill noted that these early efforts focused on optimizing Watson down from a room full of servers to a single machine, establishing that systems-level engineering is key to making powerful AI practical. Wright and Hill also discussed how this same principle applies to modern LLMs and the vLLM open source project, which is revolutionizing AI inference by making AI more practical and performant at scale. [embed YouTube video here Building more efficient AI with vLLM ft. Nick Hill | Technically Speaking with Chris Wright ] vLLM is an inference server that directly addresses the efficiency and scalability challenges faced when working with generative AI (gen AI). By maximizing the use of expensive GPU resources, vLLM makes powerful AI more accessible and practical. Red Hat is deeply involved in the vLLM project as a significant commercial contributor. We have integrated a hardened, supported, and enterprise-ready version of vLLM into Red Hat AI Inference Server. This product is available as a standalone containerized offering, or as a key component of the larger Red Hat AI portfolio, including Red Hat Enterprise Linux AI (RHEL AI) and Red Hat OpenShift AI. Our collaboration with the vLLM community is a key component of our larger open source AI strategy.</description></item><item><title>Friday Five — September 19, 2025</title><link>https://kubermates.org/docs/2025-09-19-friday-five-september-19-2025/</link><pubDate>Fri, 19 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-19-friday-five-september-19-2025/</guid><description>Friday Five — September 19, 2025 Why upgrade to Red Hat Enterprise Linux 9.6 or 10 now ITPro Podcast - Is all-photonics the future of networking? The virtualization game has changed: A new playbook for IT leaders State of the Model Serving Communities - September 2025 Red Hat Customer Portal recognized by the Association of Support Professionals as one the Best Support Websites of 2025 About the author Red Hat Corporate Communications More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Why should you seriously consider upgrading to RHEL 9.6 or 10? That&amp;rsquo;s easy: To immediately leverage new features, such as image mode and RHEL Lightspeed. Learn more How close are we to achieving all-photonics networks and what are the business benefits? Chris Wright, Senior Vice President of Global Engineering and CTO at Red Hat, discusses the potential benefits of photonics based networks and how they could upend current network architecture. Learn more Virtualization is here to stay, but platform needs are evolving. Andrew Brown, Senior Vice President and Chief Revenue Officer at Red Hat, shares his unique perspective on how forward-thinking organizations are harnessing a unified platform for virtualization and containers to become more agile and flexible. Learn more The Red Hat Customer Portal was announced as one of the winners of The Association of Support Professionals&amp;rsquo; Best Support Websites of 2025, validating our commitment to innovation and to meeting our customers wherever they are. Learn more Red Hat is the world’s leading provider of enterprise open source software solutions, using a community-powered approach to deliver reliable and high-performing Linux, hybrid cloud, container, and Kubernetes technologies. Red Hat helps customers integrate new and existing IT applications, develop cloud-native applications, standardize on our industry-leading operating system, and automate, secure, and manage complex environments. Award-winning support, training, and consulting services make Red Hat a trusted adviser to the Fortune 500. As a strategic partner to cloud providers, system integrators, application vendors, customers, and open source communities, Red Hat can help organizations prepare for the digital future.</description></item><item><title>Top 10 must-reads: Open source innovation at Red Hat</title><link>https://kubermates.org/docs/2025-09-19-top-10-must-reads-open-source-innovation-at-red-hat/</link><pubDate>Fri, 19 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-19-top-10-must-reads-open-source-innovation-at-red-hat/</guid><description>Top 10 must-reads: Open source innovation at Red Hat Getting to know Navtez Singh Bal, Vice President &amp;amp; General Manager, India and South Asia Introducing OpenShift Service Mesh 3.1 Elevate your Citrix Virtual Apps and Desktops experience with Red Hat OpenShift Virtualization Evolve your Citrix infrastructure: Unlocking agility and efficiency with Red Hat OpenShift Virtualization More than meets the eye: Behind the scenes of Red Hat Enterprise Linux 10 Zero trust starts here: Validated patterns for confidential container deployment My experience with the Red Hat Academy Program Red Hat named a Leader in 2025 Gartner® Magic Quadrant™ for Container Management for the third consecutive year MCP server development: Make agentic AI your API’s &amp;ldquo;customer zero&amp;rdquo; What’s new with Ansible Automation Platform content Beyond the roundup: Your next steps Red Hat Learning Subscription | Product Trial About the author Isabel Lee More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share In the world of technology, progress is a constant. This month, we’re taking a look at the innovative minds and strategic moves behind some of our newest advancements. This roundup of recent blog posts covers everything from the people leading our charge in key global markets to the deep technical work that is making our products smarter, more security-focused, and ready for the future. Dive in to discover how we are working to solve today’s IT challenges and build the next generation of open source technology. Getting to know Navtez Singh Bal, Vice President &amp;amp; General Manager, India and South Asia Meet Navtez Singh Bal, the new vice president and general manager for Red Hat in India and South Asia. In this interview, he shares why he was drawn to our open source model and its role as a trusted partner to customers. Navtez outlines his focus on creating and executing tangible outcomes, particularly in India’s rapidly evolving digital and AI markets. Meet Navtez Singh Bal, the new vice president and general manager for Red Hat in India and South Asia. In this interview, he shares why he was drawn to our open source model and its role as a trusted partner to customers. Navtez outlines his focus on creating and executing tangible outcomes, particularly in India’s rapidly evolving digital and AI markets. Introducing OpenShift Service Mesh 3.1 Red Hat has released OpenShift Service Mesh 3.1, a key update based on the Istio, Envoy, and Kiali projects. This release advances our support for a modernized service mesh architecture by making Kubernetes Gateway API custom resource definitions (CRDs) available by default on OpenShift Container Platform 4.19 and later.</description></item><item><title>Kubernetes v1.34: DRA Consumable Capacity</title><link>https://kubermates.org/docs/2025-09-18-kubernetes-v1-34-dra-consumable-capacity/</link><pubDate>Thu, 18 Sep 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-09-18-kubernetes-v1-34-dra-consumable-capacity/</guid><description>Kubernetes v1.34: DRA Consumable Capacity Background: device sharing via ResourceClaims Benefits of DRA consumable capacity support Device sharing across multiple ResourceClaims or DeviceRequests Device resource allocation DistinctAttribute constraint How to use consumable capacity? As a DRA driver developer As a consumer Integration with DRA device status What can you do next? Conclusion Further Reading Dynamic Resource Allocation (DRA) is a Kubernetes API for managing scarce resources across Pods and containers. It enables flexible resource requests, going beyond simply allocating N number of devices to support more granular usage scenarios. With DRA, users can request specific types of devices based on their attributes, define custom configurations tailored to their workloads, and even share the same resource among multiple containers or Pods. In this blog, we focus on the device sharing feature and dive into a new capability introduced in Kubernetes 1.34: DRA consumable capacity , which extends DRA to support finer-grained device sharing. From the beginning, DRA introduced the ability for multiple Pods to share a device by referencing the same ResourceClaim. This design decouples resource allocation from specific hardware, allowing for more dynamic and reusable provisioning of devices. In Kubernetes 1.33, the new support for partitionable devices allowed resource drivers to advertise slices of a device that are available, rather than exposing the entire device as an all-or-nothing resource. This enabled Kubernetes to model shareable hardware more accurately. But there was still a missing piece: it didn&amp;rsquo;t yet support scenarios where the device driver manages fine-grained, dynamic portions of a device resource — like network bandwidth — based on user demand, or to share those resources independently of ResourceClaims, which are restricted by their spec and namespace. That’s where consumable capacity for DRA comes in. Here&amp;rsquo;s a taste of what you get in a cluster with the DRAConsumableCapacity feature gate enabled. DRAConsumableCapacity Resource drivers can now support sharing the same device — or even a slice of a device — across multiple ResourceClaims or across multiple DeviceRequests.</description></item><item><title>VCF Breakroom Chats Episode 57: Behind the Code – A Journey from Customer Pain to VCF 9.0</title><link>https://kubermates.org/docs/2025-09-18-vcf-breakroom-chats-episode-57-behind-the-code-a-journey-from-customer-pain-to-v/</link><pubDate>Thu, 18 Sep 2025 17:31:45 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-18-vcf-breakroom-chats-episode-57-behind-the-code-a-journey-from-customer-pain-to-v/</guid><description>VCF Breakroom Chats Episode 57 About the VCF Breakroom Chat Series Related Related Articles VCF Breakroom Chats Episode 57: Behind the Code – A Journey from Customer Pain to VCF 9.0 10 VMware Cloud Foundation 9 Enhancements: Simplifying Your Day 2 Operations Deploy Distributed LLM Inference with GPUDirect RDMA over InfiniBand in VMware Private AI Welcome to the next episode of the VCF Breakroom Chats. Today, we are happy to present this vLog with Shanky Chandra Gowri, Director of Technology Product Management for the VCF Division at Broadcom. In this episode, Shanky Chandra Gowri and Alina Thylander dive into the fascinating journey behind VCF 9.0, explore how valuable customer feedback has influenced its new features, and offer a sneak peek into the enhancements on the horizon! Want to learn more about VCF Automation? Check out the VCF Automation web page for more resources. Read this blog post: Private Cloud Redefined: Deliver a Unified Cloud Consumption Experience with VMware Cloud Foundation 9.0 This webinar series focuses on VMware Cloud Foundation (VCF). In this series, we share conversations with industry-recognized experts from Broadcom and with solution partners and customers. You’ll gain relevant insights whether you’re an IT practitioner, IT admin, cloud or platform architect, developer, DevOps, senior IT manager, IT executive, or AI/ML professional. If you are new to the series, please check out our previous episodes.</description></item><item><title>CNCF Expands Infrastructure Support for Project Maintainers Through Partnership with Docker</title><link>https://kubermates.org/docs/2025-09-18-cncf-expands-infrastructure-support-for-project-maintainers-through-partnership-/</link><pubDate>Thu, 18 Sep 2025 16:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-18-cncf-expands-infrastructure-support-for-project-maintainers-through-partnership-/</guid><description>About Cloud Native Computing Foundation Media Contact CNCF projects gain access to trusted container distribution, enhanced security tooling, and usage insights via Docker’s Sponsored Open Source Program SAN FRANCISCO, CA – September 18, 2025 – The Cloud Native Computing Foundation ® (CNCF®), which builds sustainable ecosystems for cloud native software, today announced a new partnership with Docker, Inc. ®, a provider of cloud-native application and AI-native application development solutions and infrastructure, to expand secure, scalable support for CNCF-hosted projects. Through this collaboration all CNCF projects will gain direct access to Docker’s Sponsored Open Source (DSOS) program which helps open source communities grow and succeed by unlocking premium registry, security, and support services. “Docker was a founding member of CNCF, and we’ve maintained a long-term open collaboration over the past decade,” said Chris Aniszczyk, CTO, CNCF. “This partnership marks a step forward for CNCF projects and we’re glad to work together to further secure the open source supply chain. ” A Strategic Collaboration for the Ecosystem With DSOS access, CNCF projects will now benefit from: Unlimited image pulls from Docker Hub Sponsored OSS status for greater trust and visibility Access to Docker Scout for vulnerability analysis and policy enforcement Automated image builds from source Docker usage metrics and engagement insights Streamlined support through open source channels Docker Hub, the world’s largest registry for discovering, sharing, and storing container images, supports over 22 billion image downloads each month and hosts more than 14 million images. This partnership reflects CNCF’s commitment to supporting widely used infrastructure across the cloud native ecosystem, recognizing Docker’s role in both open source and enterprise workflows. For project maintainers, the partnership provides secure, scalable infrastructure tailored to the realities of running production-grade open source projects. For users, it ensures reliable and trustworthy access to the cloud native tools they depend on. “Docker’s mission has always been to empower developers, and we know that trust is earned through consistency, openness, and listening,” said Michael Donovan, VP of Products at Docker. “This partnership with CNCF reflects a broader commitment to the open source community by helping maintainers grow their projects, reach more developers through Docker Hub, and deliver value to their communities faster with improved tools, automation, and support. ” A Commitment to Secure Open Source Infrastructure This partnership also highlights a shared commitment to strengthening the open source software supply chain.</description></item><item><title>10 VMware Cloud Foundation 9.0 Enhancements: Simplifying Your Day 2 Operations</title><link>https://kubermates.org/docs/2025-09-18-10-vmware-cloud-foundation-9-0-enhancements-simplifying-your-day-2-operations/</link><pubDate>Thu, 18 Sep 2025 14:52:48 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-18-10-vmware-cloud-foundation-9-0-enhancements-simplifying-your-day-2-operations/</guid><description>Enhancement 1: Network Pool Creation/Expansion/Deletion Enhancement 2: Host Commissioning and Decommissioning for Existing or New Workload Domains Enhancement 3: Workload Domain Deployment Enhancement 4: Cluster Creation and Expansion Enhancement 5: VCF Backup Configuration Enhancement 6: Network Settings Configuration (DNS/NTP) Enhancement 7: Certificate Authority Configuration Enhancement 8: Certificate Management Enhancement 9: Password Management Enhancement 10: Deploying NSX Edge Cluster The Evolution of VCF 9.0 Continues Related Related Articles VCF Breakroom Chats Episode 57: Behind the Code – A Journey from Customer Pain to VCF 9.0 10 VMware Cloud Foundation 9.0 Enhancements: Simplifying Your Day 2 Operations Deploy Distributed LLM Inference with GPUDirect RDMA over InfiniBand in VMware Private AI As organizations prepare to upgrade to VMware Cloud Foundation (VCF) 9.0, understanding Day 2 operational changes becomes critical for a successful transition. In previous versions such as VCF 5.2, many administrative tasks—such as network pool creation, host commissioning, and workload domain deployments—were tightly coupled with SDDC Manager and performing them outside of it often led to issues. VCF 9.0 introduces significant enhancements in Day 2 operations, offering greater flexibility by shifting many of these tasks to more familiar tools such as VMware vCenter and VCF Operations. This evolution not only streamlines workflows but also empowers administrators with more direct control. This post highlights 10 key operational changes that organizations should consider when planning and executing an upgrade to VCF 9.0. While not an exhaustive list, these insights are based on recurring themes in customer conversations and real-world upgrade experiences. In VCF 5.2, network pool creation, expansion, and deletion are performed in SDDC Manager. Administration -&amp;gt; Network Settings -&amp;gt; Network Pool In VCF 9.0, these tasks are performed through vCenter. You do not need any additional configuration to act on the Management Domain networks, however you must be using VCF SSO and vCenter Server Linking for Workload Domains. Global Inventory List -&amp;gt; Hosts -&amp;gt; Network Pools In VCF 5.2, host commissioning/decommissioning for existing or new workload domains are done in SDDC Manager. Inventory -&amp;gt; Hosts In VCF 9.0, these tasks are performed through vCenter. Global Inventory List -&amp;gt; Hosts -&amp;gt; Unassigned Hosts In VCF 5.2, Workload Domain deployment is done in SDDC Manager.</description></item><item><title>Implementing granular failover in multi-Region Amazon EKS</title><link>https://kubermates.org/docs/2025-09-18-implementing-granular-failover-in-multi-region-amazon-eks/</link><pubDate>Thu, 18 Sep 2025 14:13:01 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-18-implementing-granular-failover-in-multi-region-amazon-eks/</guid><description>Implementing granular failover in multi-Region Amazon EKS Typical architecture The all-or-nothing health check problem Solution overview Prerequisites Walkthrough Configure environment variables Create EKS clusters in Region 1 and Region 2 Create IngressClass configuration Deploy app1 and app2 on the EKS clusters Configure Route 53 health checks for app1 and app2 Configure Route 53 alias records for app1 and app2 Test application failure Things to know Cleaning up Conclusion About the authors Enterprises across industries operate tens of millions of Amazon Elastic Kubernetes Service (Amazon EKS) clusters annually, supporting use cases from web/mobile applications to data processing, machine learning (ML), generative AI, gaming, and Internet of Things (IoT). As organizations increasingly adopt multi-tenant platform models where multiple application teams share an EKS cluster, they need more granular control over application availability in their multi-Region architectures—particularly for serving global users and meeting strict regulatory requirements. Although multi-tenant models optimize resource usage and reduce operational overhead, they present challenges when applications need individualized Recovery Point Objective (RPO) and Recovery Time Objective (RTO) targets across multiple AWS Regions. Traditional approaches force uniform failover policies across all applications where a single application’s failure creates an unnecessary compromise between operational efficiency and application-specific resilience. In this post, we demonstrate how to configure Amazon Route 53 to enable unique failover behavior for each application within your multi-tenant Amazon EKS environment across AWS Regions, which allows you to maintain the cost benefits of shared infrastructure while meeting diverse availability requirements. Before diving into the solution, we demonstrate how a typical multi-Region multi-tenant Amazon EKS architecture is structured, and the key components that enable Regional traffic routing. Figure 1 consists of the following key components: Route 53 Routes traffic to the Application Load Balancer (ALB) in the respective Region Makes sure of high availability and resiliency during failure events Routes traffic to the Application Load Balancer (ALB) in the respective Region Makes sure of high availability and resiliency during failure events AWS Load Balancer Controller Exposes applications running on the EKS cluster through ALB Uses IP target type to register application Pods to the ALB Exposes applications running on the EKS cluster through ALB Uses IP target type to register application Pods to the ALB The Route 53 configuration details are as follows: Regional Routing Each AWS Region uses a Route 53 alias record to route traffic to its Region-specific ALB. Example: app1. example. com points to ALB in Region 1 Example: app2. example. com points to ALB in Region 2 Each AWS Region uses a Route 53 alias record to route traffic to its Region-specific ALB.</description></item><item><title>Capacity Management: The IT Balancing Act You Can’t Ignore</title><link>https://kubermates.org/docs/2025-09-18-capacity-management-the-it-balancing-act-you-can-t-ignore/</link><pubDate>Thu, 18 Sep 2025 11:58:47 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-18-capacity-management-the-it-balancing-act-you-can-t-ignore/</guid><description>How VMware Cloud Foundation Makes Capacity Management Smarter Capacity Engine From Firefighting to Future-Proofing: Capacity Management in Action 1. Assess Capacity 2. Optimize Capacity 3. Plan Capacity The End Result Final Thoughts Related Related Articles Capacity Management: The IT Balancing Act You Can’t Ignore Deploy Distributed LLM Inference with GPUDirect RDMA over InfiniBand in VMware Private AI FinOps in VMware Cloud Foundation Ever feel like your IT resources are either bursting at the seams or sitting idle (and costing a fortune)? Welcome to the world of capacity management. In this post, we’ll break down what it really means, why it’s harder than it looks, and the common challenges that leave IT teams scrambling. At its core, capacity management is about making sure your IT resources—servers, storage, networks, and cloud services—are sized just right. Not too big, not too small. Think of it like budgeting: enough to get the job done today, with a plan for tomorrow. Here’s what goes into it: Monitoring usage: Keeping an eye on CPU, memory, storage, and bandwidth. Forecasting demand: Predicting spikes before they hit you. Optimizing resources: Moving workloads around or fine tuning performance. Strategic planning: Lining up IT with the bigger business picture.</description></item><item><title>Red Hat Customer Portal recognized by the Association of Support Professionals as one the Best Support Websites of 2025</title><link>https://kubermates.org/docs/2025-09-18-red-hat-customer-portal-recognized-by-the-association-of-support-professionals-a/</link><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-18-red-hat-customer-portal-recognized-by-the-association-of-support-professionals-a/</guid><description>Red Hat Customer Portal recognized by the Association of Support Professionals as one the Best Support Websites of 2025 About the author Christine Flynn Bryan More like this Blog post Blog post Original podcast Original podcast Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The Association of Support Professionals (ASP) has announced the Red Hat Customer Portal as one of the winners of the Best Support Websites of 2025. This year&amp;rsquo;s winning entry highlighted several key innovations that continue to advance Red Hat’s customer experience, including: Ask Red Hat , a conversational, AI-based generative search experience. Red Hat Offline Knowledge Portal (RHOKP), a new product that provides Red Hat knowledge and support documentation to customers in air-gap or disconnected environments. Product Compliance Experience, a searchable source for all product compliance records. In addition to the main award, Red Hat also received a special award for &amp;ldquo;Excellence in Extending Support to Offline Environments&amp;rdquo; for Red Hat Offline Knowledge Portal , a self-contained instance of Red Hat’s extensive knowledge content, enabling air-gapped, secure, and disconnected teams to proactively troubleshoot issues and maintain compliance without relying on external network access. This &amp;ldquo;pocket library&amp;rdquo; of knowledge can be deployed on-premise or on customer workstations, allowing users to access critical information and maintain their systems without needing an internet connection. The unique recognition from ASP highlights the value and capabilities of Red Hat Offline Knowledge Portal, a solution designed to bring Red Hat&amp;rsquo;s expert knowledge and support documentation to customers in disconnected or air-gapped environments. The accolade recognizes Red Hat Offline Knowledge Portal as a revolutionary product that solves a critical challenge for customers operating in a variety of industries, from military and government to remote operations and secure on-premise data centers, where internet connectivity is limited or prohibited. Red Hat Offline Knowledge Portal ensures that these customers have immediate, local access to the latest security updates, technical articles, and comprehensive product documentation. &amp;ldquo;The Red Hat Offline Knowledge Portal was born from a critical need to provide our customers with a seamless support experience and full subscription value no matter their operating environment,&amp;rdquo; said Mark Shoger, Senior Manager and product sponsor. &amp;ldquo;This special award from the ASP is a powerful validation of our commitment to innovation and to meeting our customers wherever they are. Red Hat Offline Knowledge Portal is an essential tool that ensures our customers can maintain, manage, and secure their Red Hat solutions even when disconnected from the internet.</description></item><item><title>Reducing bias in AI models through open source</title><link>https://kubermates.org/docs/2025-09-18-reducing-bias-in-ai-models-through-open-source/</link><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-18-reducing-bias-in-ai-models-through-open-source/</guid><description>Reducing bias in AI models through open source Global AI AI&amp;rsquo;s language gap in numbers Dad jokes are complicated Open sourcing a new path The growing efforts of AI sovereignty A multilingual AI future More models, less bias Get started with AI for enterprise: A beginner’s guide About the author Adam Wealand More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share An experience can plant a seed in your mind that grows into a much bigger idea. I recently visited Japan, a beautiful country with a rich and defined culture, and was reminded of how deeply language and culture are intertwined. Language structure reflects and also influences how people perceive the world. For example, some cultures are more informal, while others with established social hierarchies tend to be more formal. It’s manifested grammatically, and also in the way people greet each other, the way questions are formed, and the very rhythm of conversation. This experience sparked a question for me: as we build a new generation of AI, how do we train it to speak the language of all cultures? We often hear about AI as a revolutionary global tool, a technology for all of humanity. But if AI is learning from our collective data, whose culture is it truly learning? The promise of a universally intelligent system is powerful, but it masks a critical issue, in that today&amp;rsquo;s most powerful AI models are not culturally neutral. They are a direct reflection of their training data, which is overwhelmingly English-centric and Western-biased. The concept of &amp;ldquo;AI sovereignty&amp;rdquo; becomes particularly relevant to this Western-biased model training. It&amp;rsquo;s not simply about having access to technology or hardware, but also about building AI that can reflect a nation&amp;rsquo;s or community&amp;rsquo;s unique languages, values, and culture. We believe the key to achieving this lies in the world of open source AI. Digital sovereignty, particularly in the context of AI, is rapidly evolving from an abstract concept to a critical, real-world issue.</description></item><item><title>Kubernetes v1.34: Pods Report DRA Resource Health</title><link>https://kubermates.org/docs/2025-09-17-kubernetes-v1-34-pods-report-dra-resource-health/</link><pubDate>Wed, 17 Sep 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-09-17-kubernetes-v1-34-pods-report-dra-resource-health/</guid><description>Kubernetes v1.34: Pods Report DRA Resource Health Why expose device health in Pod status? How it works A new gRPC health service Kubelet integration Populating the Pod status A practical example How to use this feature DRA drivers What&amp;rsquo;s next? The rise of AI/ML and other high-performance workloads has made specialized hardware like GPUs, TPUs, and FPGAs a critical component of many Kubernetes clusters. However, as discussed in a previous blog post about navigating failures in Pods with devices , when this hardware fails, it can be difficult to diagnose, leading to significant downtime. With the release of Kubernetes v1.34, we are excited to announce a new alpha feature that brings much-needed visibility into the health of these devices. This work extends the functionality of KEP-4680 , which first introduced a mechanism for reporting the health of devices managed by Device Plugins. Now, this capability is being extended to Dynamic Resource Allocation (DRA). Controlled by the ResourceHealthStatus feature gate, this enhancement allows DRA drivers to report device health directly into a Pod&amp;rsquo;s. status field, providing crucial insights for operators and developers. ResourceHealthStatus. status For stateful applications or long-running jobs, a device failure can be disruptive and costly. By exposing device health in the. status field for a Pod, Kubernetes provides a standardized way for users and automation tools to quickly diagnose issues. If a Pod is failing, you can now check its status to see if an unhealthy device is the root cause, saving valuable time that might otherwise be spent debugging application code.</description></item><item><title>CNCF Welcomes 20 New Silver Members Reflecting Broader Cloud Native and AI Adoption</title><link>https://kubermates.org/docs/2025-09-17-cncf-welcomes-20-new-silver-members-reflecting-broader-cloud-native-and-ai-adopt/</link><pubDate>Wed, 17 Sep 2025 16:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-17-cncf-welcomes-20-new-silver-members-reflecting-broader-cloud-native-and-ai-adopt/</guid><description>New Silver Members About the Newest End User Members About Cloud Native Computing Foundation Media Contact New members span DevOps, platform engineering, and agentic AI, underscoring cloud native’s critical role in modern infrastructure across industries SAN FRANCISCO, CA – September 17, 2025 – The Cloud Native Computing Foundation® (CNCF®) , which builds sustainable ecosystems for cloud native software, today announced the addition of 20 new Silver Members, reinforcing the continued momentum of cloud native adoption across industries and further strengthening the foundation’s global community. CNCF Silver Members receive a variety of benefits valued at over $300,000 USD. These benefits include training and educational subscriptions, access to The Linux Foundation ’s legal resources, event sponsorship discounts, opportunities to join project working groups , and much more. “The cloud native ecosystem is expanding globally in both depth and reach,” said Chris Aniszczyk, CTO, CNCF. “This new wave of silver members reflects the growing demand for scalable, secure and AI native infrastructure. Their technical diversity strengthens our community’s ability to address real world challenges. CNCF looks forward to supporting their journey and accelerating open innovation together. ” The latest new members stem from a variety of backgrounds across cloud native and open source ecosystems such as DevOps, platform engineering, agentic AI, and enterprise software. The combined expertise from these members exemplify the innovations molding new tech infrastructure. The following organizations have recently joined CNCF as Silver Members: AgileOps is an IT services provider offering scalable solutions for enterprise infrastructure and software delivery. We are a Kubernetes Certified Service Provider and a Solution Partner of Atlassian, Google Workspace, HubSpot, Lark, and Zoom, supporting organizations with consulting, implementation, and operational expertise across regions. Archestra.</description></item><item><title>Use Raspberry Pi 5 as Amazon EKS Hybrid Nodes for edge workloads</title><link>https://kubermates.org/docs/2025-09-17-use-raspberry-pi-5-as-amazon-eks-hybrid-nodes-for-edge-workloads/</link><pubDate>Wed, 17 Sep 2025 15:08:59 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-17-use-raspberry-pi-5-as-amazon-eks-hybrid-nodes-for-edge-workloads/</guid><description>Use Raspberry Pi 5 as Amazon EKS Hybrid Nodes for edge workloads Why Raspberry Pi 5? Architectural overview Getting started Step 1: Create the EKS cluster Step 2: Set up the VPN server Add the Raspberry Pi to the cluster as a remote node Setting up the Container Network Interface Step 1: Install Cilium Deploying a sample application on Amazon EKS Hybrid Nodes with edge integration Step 1: Hardware requirements and setup Step 2: Deploy the DynamoDB table Step 3: Deploy the sensor application Step 4: Deploy the frontend dashboard Conclusion About the authors Since its launch, Amazon Elastic Kubernetes Service (Amazon EKS) has powered tens of millions of clusters so that users can accelerate application deployment, optimize costs, and use the flexibility of Amazon Web Services (AWS) for hosting containerized applications. Amazon EKS eliminates the operational complexities of maintaining Kubernetes control plane infrastructure, while offering seamless integration with AWS resources and infrastructure. However, some workloads need to be run at the edge with real-time processing, such as latency-sensitive applications that generate large volumes of data. In these scenarios, when there is consistent internet connectivity available, users often seek the benefits of cloud integrations while continuing to use their on-premises hardware. That’s why we introduced Amazon EKS Hybrid Nodes at AWS re:Invent 2024, so that users can extend their Kubernetes data plane to the edge while continuing to run the Kubernetes control plane in an AWS Region. Amazon EKS Hybrid Nodes unifies Kubernetes management across cloud, on-premises, and edge environments by enabling users to use their on-premises infrastructure as nodes in EKS clusters, alongside Amazon Elastic Compute Cloud (Amazon EC2). To demonstrate the use of Amazon EKS Hybrid Nodes, we explored a practical use case from the manufacturing sector. These environments often rely on real-time data from digital sensors that must be processed locally due to latency and reliability, while still using the cloud for analytics and long-term storage. Our use case involves reading distance values from an ultrasonic sensor, processing them on a local edge device running as a Hybrid Node, and storing them in Amazon DynamoDB on AWS. In this post, we demonstrate how to implement Amazon EKS Hybrid Nodes using the Raspberry Pi 5 , a popular edge computing platform. We cover the following: Setting up an EKS cluster that seamlessly connects cloud and edge infrastructure Securing connectivity using the WireGuard VPN for site-to-site communication Enabling container networking with Cilium for hybrid node deployments Demonstrating a real-world Internet of Things (IoT) application that demonstrates the power of edge-cloud integration The Raspberry Pi 5 is compact and can be deployed at the edge so that you can process data before it is transmitted to the cloud. Building on this strength, we created a microservices-based application partly running on the edge on a Raspberry Pi 5 and partly on AWS in the cloud.</description></item><item><title>Calico Whisker vs. Traditional Observability: Why Context Matters in Kubernetes Networking</title><link>https://kubermates.org/docs/2025-09-16-calico-whisker-vs-traditional-observability-why-context-matters-in-kubernetes-ne/</link><pubDate>Tue, 16 Sep 2025 19:43:02 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-16-calico-whisker-vs-traditional-observability-why-context-matters-in-kubernetes-ne/</guid><description>What is Calico Whisker? How Does It Work? What Makes Calico Whisker Different? Practical Examples: Calico Whisker in Action Scenario 1: Safely Rolling Out a New Network Policy Scenario 2: Uncovering Hidden Security Risks How to Get Started with Calico Whisker Are you tired of digging through cryptic logs to understand your Kubernetes network? In today’s fast-paced cloud environments, clear, real-time visibility isn’t a luxury, it’s a necessity. Traditional logging and metrics often fall short, leaving you without the context needed to troubleshoot effectively. That’s precisely what Calico Whisker’s recent launch (with Calico v3.30) aims to solve. This tool provides clarity where logs alone fall short. In the sections below, you’ll get a practical overview of how it works and how it fits into modern Kubernetes networking and security workflows. If you’re relying on logs for network observability, you’re not alone. While this approach can provide some insights, it’s often a manual, resource-intensive process that puts significant load on your distributed systems. It’s simply not a cloud-native solution for real-time insights. So are we doomed? No. Calico Whisker transforms network observability from a chore into a superpower. Calico Whisker is a free, lightweight, Kubernetes-native observability user interface (UI) created by Tigera and introduced with Calico Open Source v3.30. It’s designed to give you a simple yet powerful window into your cluster’s network traffic, helping you understand network flows and evaluate policy behavior in real-time.</description></item><item><title>Kubernetes v1.34: Moving Volume Group Snapshots to v1beta2</title><link>https://kubermates.org/docs/2025-09-16-kubernetes-v1-34-moving-volume-group-snapshots-to-v1beta2/</link><pubDate>Tue, 16 Sep 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-09-16-kubernetes-v1-34-moving-volume-group-snapshots-to-v1beta2/</guid><description>Kubernetes v1.34: Moving Volume Group Snapshots to v1beta2 What&amp;rsquo;s new in Beta 2? What’s next? How do I get involved? Volume group snapshots were introduced as an Alpha feature with the Kubernetes 1.27 release and moved to Beta in the Kubernetes 1.32 release. The recent release of Kubernetes v1.34 moved that support to a second beta. The support for volume group snapshots relies on a set of extension APIs for group snapshots. These APIs allow users to take crash consistent snapshots for a set of volumes. Behind the scenes, Kubernetes uses a label selector to group multiple PersistentVolumeClaims for snapshotting. A key aim is to allow you restore that set of snapshots to new volumes and recover your workload based on a crash consistent recovery point. This new feature is only supported for CSI volume drivers. While testing the beta version, we encountered an issue where the restoreSize field is not set for individual VolumeSnapshotContents and VolumeSnapshots if CSI driver does not implement the ListSnapshots RPC call. We evaluated various options here and decided to make this change releasing a new beta for the API. restoreSize Specifically, a VolumeSnapshotInfo struct is added in v1beta2, it contains information for an individual volume snapshot that is a member of a volume group snapshot. VolumeSnapshotInfoList, a list of VolumeSnapshotInfo, is added to VolumeGroupSnapshotContentStatus, replacing VolumeSnapshotHandlePairList. VolumeSnapshotInfoList is a list of snapshot information returned by the CSI driver to identify snapshots on the storage system.</description></item><item><title>Best DevOps Courses in 2025: Learning Paths to Boost Your Career</title><link>https://kubermates.org/docs/2025-09-16-best-devops-courses-in-2025-learning-paths-to-boost-your-career/</link><pubDate>Tue, 16 Sep 2025 17:19:01 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-16-best-devops-courses-in-2025-learning-paths-to-boost-your-career/</guid><description>2025-ready skills - IT Foundations &amp;ndash;&amp;gt; Linux &amp;amp; Networking &amp;ndash;&amp;gt; Git &amp;ndash;&amp;gt; Containers &amp;ndash;&amp;gt; Kubernetes &amp;ndash;&amp;gt; CI/CD &amp;ndash;&amp;gt; IaC &amp;ndash;&amp;gt; GitOps &amp;ndash;&amp;gt; Observability &amp;ndash;&amp;gt; Security &amp;ndash;&amp;gt; Platform Engineering &amp;ndash;&amp;gt; Cloud &amp;ndash;&amp;gt; AI. AI everywhere - LLMs, MCPs, RAG, anomaly detection, and copilots are now part of daily DevOps workflows. Hands-on learning - every step mapped to KodeKloud’s lab-first courses, and real projects. Clear role tracks - follow one of four paths: DevOps Engineer, Site Reliability Engineer, Platform Engineer, or Cloud-Dev. Milestone projects - practical, production-like capstones to showcase on GitHub and in interviews. Free AI Week - start with the exclusive 5-Day AI Learning Path to build confidence with modern AI skills. DevOps in 2025 is no longer about “tools-first. ” Teams are building reliable platforms that support fast delivery and strong governance. At the same time, AI is becoming a core part of the workflow -from writing Kubernetes manifests to detecting anomalies in observability dashboards. The expectations are broader: Production fundamentals (Linux, networking, shell, Git) are non‑negotiable. Cloud-native skills (containers, Kubernetes, service meshes, Gateway API) are baseline. IaC + GitOps is how infra/app changes land in prod-repeatably, auditor‑friendly.</description></item><item><title>Deploy Distributed LLM Inference with GPUDirect RDMA over InfiniBand in VMware Private AI</title><link>https://kubermates.org/docs/2025-09-16-deploy-distributed-llm-inference-with-gpudirect-rdma-over-infiniband-in-vmware-p/</link><pubDate>Tue, 16 Sep 2025 14:11:03 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-16-deploy-distributed-llm-inference-with-gpudirect-rdma-over-infiniband-in-vmware-p/</guid><description>Key Highlights and Technical Deep Dives Leveraging HGX Servers for Maximum Performance Intra-Node and Inter-Node Communication GPUDirect RDMA in VCF Determining Server Requirements Architecture Overview Deployment Workflow and Best Practices Practical Examples and Configurations Performance Verification Related Related Articles Deploy Distributed LLM Inference with GPUDirect RDMA over InfiniBand in VMware Private AI VMware Cloud Foundation Support Upgrade Playbook VCF Breakroom Chats Episode 58: A Smarter Way to a Unified Private Cloud Consumption Experience with VCF 9.0 At the VMware Explore 2025 keynote, Chris Wolf announced DirectPath enablement for GPUs with VMware Private AI , marking a major step forward in simplifying and scaling enterprise AI infrastructure. By granting VMs exclusive, high-performance access to NVIDIA GPUs, DirectPath allows organizations to fully harness GPU capabilities without added licensing complexity. This makes it easier to experiment, prototype, and move AI projects into production with confidence. Besides, VMware Private AI brings models closer to enterprise data, delivering secure, efficient, and cost-effective deployments. Jointly engineered by Broadcom and NVIDIA, the solution empowers organizations to accelerate innovation while reducing total cost of ownership (TCO). These advancements come at a critical time. Serving state-of-the-art large language models (LLMs) like DeepSeek-R1 , Meta Llama-3.1-405B-Instruct , and Qwen3-235B-A22B-thinking at full context length often exceeds the capacity of a single 8x H100 GPU server, making distributed inference essential. Aggregating resources from multiple GPU-enabled nodes allows these models to run efficiently, though it introduces new challenges in infrastructure management, interconnect optimization, and workload scheduling. This is where VMware Cloud Foundation (VCF) plays a vital role. VCF is the industry’s first private cloud platform to deliver public cloud scale and agility while providing on-premises security, resilience, and performance—all with lower TCO. Leveraging technologies such as NVIDIA NVLink , NVSwitch , and GPUDirect® RDMA , VCF enables high-bandwidth, low-latency communication across nodes. It also ensures that network interconnects like InfiniBand (IB) and RoCEv2 (RDMA over Converged Ethernet) are used effectively, reducing communication overhead that can limit distributed inference performance.</description></item><item><title>Distributed performance testing for Kubernetes environments: Grafana k6 Operator 1.0 is here</title><link>https://kubermates.org/docs/2025-09-16-distributed-performance-testing-for-kubernetes-environments-grafana-k6-operator-/</link><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-16-distributed-performance-testing-for-kubernetes-environments-grafana-k6-operator-/</guid><description>Try using Grot AI for this query -&amp;gt;.</description></item><item><title>Fedora 43 Beta now available</title><link>https://kubermates.org/docs/2025-09-16-fedora-43-beta-now-available/</link><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-16-fedora-43-beta-now-available/</guid><description>Fedora 43 Beta now available What’s new in Fedora 43 Beta? What is a Fedora Beta release? Let’s test Fedora 43 Beta together About the author Jef Spaleta More like this Blog post Blog post Original podcast Original podcast Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Today, the Fedora Project is excited to announce that the beta version of Fedora Linux 43 - the latest version of the free and open source operating system - is now available. Learn more about the new and updated features of Fedora 43 Beta below and don’t forget to make sure that your system is fully up-to-date before upgrading from a previous release. Anaconda WebUI for Fedora Spins by default: Creates a more consistent and modern installation experience across all Fedora desktop variants, and brings us closer to eventually replacing the older GTK installer, allowing all Fedora users to benefit from the same polished and user-friendly interface. Switch Anaconda installer to DNF5: Provides better support and debugging for package-based applications within Anaconda, and is a bigger step towards the eventual deprecation or removal of DNF4, which is now in maintenance mode. Enable auto-updates by default in Fedora Kinoite: Assures users are more consistently running a system with the latest bug fixes and features after a simple reboot, with updates applied automatically in the background. Set default monospace fallback font: Establishes a standard fallback font when a specified monospace font is missing. The font selection also remains more stable and predictable, even when the user installs new font packages, and no jarring font changes that were in previous versions. GNU toolchain update: Allows Fedora to stay current with the latest features, improvements, and bug and security fixes from the upstream gcc, glibc, binutils, and gdb projects, and delivers a working system compiler, assembler, static and dynamic linker, core language runtimes, and debugger. Package-specific RPM macros for build flags: Provides a standard way for packages to add to the default list of compiler flags. It also offers a cleaner and simpler method for package maintainers to make per-package adjustments to build flags, avoiding the need to manually edit and re-export environmental variables, and prevents potential issues that could arise from the old manual method, confirming that flag adjustments are applied appropriately. Build Fedora CoreOS using Containerfile: This change brings the Fedora CoreOS (FCOS) build process under a standard container image build, moving away from the custom tool, CoreOS Assembler. It also means that anyone with Podman installed can build FCOS, which simplifies the process for both individual users and automated pipelines.</description></item><item><title>Unlocking AI innovation: GPU-as-a-Service with Red Hat</title><link>https://kubermates.org/docs/2025-09-16-unlocking-ai-innovation-gpu-as-a-service-with-red-hat/</link><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-16-unlocking-ai-innovation-gpu-as-a-service-with-red-hat/</guid><description>Unlocking AI innovation: GPU-as-a-Service with Red Hat The GPU challenge: A multifaceted problem for ITOps Red Hat&amp;rsquo;s solution: Solving the GPU puzzle with GPU-as-a-Service Key components for delivering GPU-as-a-Service Red Hat: Your partner in AI innovation Get started with AI Inference About the author Martin Isaksson More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Graphics processing units (GPUs) are key to both generative and predictive AI. Data scientists, machine learning engineers, and AI engineers rely on GPUs to experiment with AI models, and to train, tune, and deploy them. Managing these essential resources can be complex, however, often stalling development and innovation. Infrastructure limitations shouldn&amp;rsquo;t hold your organization back. Your team needs to focus on building, refining, and using AI models, not managing complex GPU infrastructure. This is why information technology operations (ITOps) plays a crucial role in enabling rapid AI development and inference by providing on-demand GPU access, also known as GPU-as-a-Service. Setting up an efficient GPU infrastructure for AI workloads is not trivial, and ITOps teams face several significant challenges: GPU scarcity and cost constraints : GPUs can be challenging to access due to limited supply, cloud constraints, and internal competition. This can be further compounded with a lack of customer choice and control over the underlying accelerator architecture, not to mention that. GPUs already come with high costs, including acquisition and operational expenses, and are often underused. Lack of GPU access drives shadow IT : If data scientists, ML engineers, and AI engineers cannot readily access GPUs when they need them, they may turn to &amp;ldquo;shadow IT. &amp;quot; This can mean using third-party services, potentially exposing sensitive company data, or independently procuring GPU resources from various cloud providers, leading to increased costs and security risks. This results in a loss of control over resource usage, data security, and compliance.</description></item><item><title>Use the RHEL command-line assistant offline with this new developer preview</title><link>https://kubermates.org/docs/2025-09-16-use-the-rhel-command-line-assistant-offline-with-this-new-developer-preview/</link><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-16-use-the-rhel-command-line-assistant-offline-with-this-new-developer-preview/</guid><description>Use the RHEL command-line assistant offline with this new developer preview Architecture overview Prerequisites and requirements Hardware requirements Getting started Configure the GPU Configure the command-line assistant client Usage First query response delay CPU-only systems Intended for individual system use cases Conclusion Get started with AI Inference About the authors Brian Smith Máirín Duffy Sally O&amp;rsquo;Malley More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share An offline version of the Red Hat Enterprise Linux (RHEL) command-line assistant powered by RHEL Lightspeed is now available as a developer preview to existing Red Hat Satellite subscribers. This delivers the power of the RHEL command-line assistant in a self-contained format that runs locally on a workstation or an individual RHEL system. This enables the assistant to function in a completely disconnected, offline, or air-gapped environment because it doesn&amp;rsquo;t require any external network connectivity to operate. The command-line assistant provides AI-powered guidance and suggestions to help you with a wide range of RHEL tasks. We’ve extended its knowledge about RHEL installation and upgrades topics, enabling you to get assistance in these areas even with limited or no connectivity. The offline version of command-line assistant is delivered as a set of container images that can be run with Podman. The containers used are: installer container : Pulls the other required containers, installs the rhel-cla command, and optionally creates a systemd service rlsapi container : Provides the endpoint that the command-line assistant client communicates with rag-database container : The retrieval-augmented generation (RAG) database used to supplement the LLM’s knowledge with additional data such as the RHEL documentation ramalama container : Provides LLM inference Your Red Hat Satellite subscription provides you with tools to locally manage your RHEL environments. These tools include the Satellite product itself, as well as the recently introduced Red Hat Offline Knowledge Portal , which provides an offline version of Red Hat&amp;rsquo;s exclusive knowledge content. We&amp;rsquo;ve now expanded the Satellite subscription to also provide the offline version of the RHEL command-line assistant. The offline version of the command-line assistant is delivered as a set of containers, and can run on a RHEL system, or a Mac or Windows workstation. If you&amp;rsquo;re using a Mac system, Podman Desktop is required , and it&amp;rsquo;s recommended to use a system equipped with an NVIDIA, AMD, or Mac M-series GPU. CPU-only systems (RHEL 9.6+ and 10+ / Fedora 42 / Windows 11): RAM : 8 GB CPU cores : 2 GPU-capable systems (RHEL 9.6+ and 10+ / Fedora 42 / Windows 11): RAM : 4 GB GPU : At least 4 GB of VRAM Apple systems (macOS 15.</description></item><item><title>Kubernetes v1.34: Decoupled Taint Manager Is Now Stable</title><link>https://kubermates.org/docs/2025-09-15-kubernetes-v1-34-decoupled-taint-manager-is-now-stable/</link><pubDate>Mon, 15 Sep 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-09-15-kubernetes-v1-34-decoupled-taint-manager-is-now-stable/</guid><description>Kubernetes v1.34: Decoupled Taint Manager Is Now Stable What&amp;rsquo;s new? How to get involved? This enhancement separates the responsibility of managing node lifecycle and pod eviction into two distinct components. Previously, the node lifecycle controller handled both marking nodes as unhealthy with NoExecute taints and evicting pods from them. Now, a dedicated taint eviction controller manages the eviction process, while the node lifecycle controller focuses solely on applying taints. This separation not only improves code organization but also makes it easier to improve taint eviction controller or build custom implementations of the taint based eviction. The feature gate SeparateTaintEvictionController has been promoted to GA in this release. Users can optionally disable taint-based eviction by setting &amp;ndash;controllers=-taint-eviction-controller in kube-controller-manager. SeparateTaintEvictionController &amp;ndash;controllers=-taint-eviction-controller For more details, refer to the KEP and to the beta announcement article: Kubernetes 1.29: Decoupling taint manager from node lifecycle controller. We offer a huge thank you to all the contributors who helped with design, implementation, and review of this feature and helped move it from beta to stable: Ed Bartosh (@bart0sh) Yuan Chen (@yuanchen8911) Aldo Culquicondor (@alculquicondor) Baofa Fan (@carlory) Sergey Kanzhelev (@SergeyKanzhelev) Tim Bannister (@lmktfy) Maciej Skoczeń (@macsko) Maciej Szulik (@soltysh) Wojciech Tyczynski (@wojtek-t) ← Previous Next →.</description></item><item><title>VMware Cloud Foundation Support Upgrade Playbook</title><link>https://kubermates.org/docs/2025-09-15-vmware-cloud-foundation-support-upgrade-playbook/</link><pubDate>Mon, 15 Sep 2025 13:28:19 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-15-vmware-cloud-foundation-support-upgrade-playbook/</guid><description>Related Related Articles VMware Cloud Foundation Support Upgrade Playbook VCF Breakroom Chats Episode 58: A Smarter Way to a Unified Private Cloud Consumption Experience with VCF 9.0 Boost Operational Efficiency with VMware Cloud Foundation Our VMware Support team here at Broadcom is fully committed to helping our customers on their upgrade journeys. With the End of General Support date for VMware vSphere 7 coming on October 2, 2025 , many of our customers have been reaching out looking for best practices and playbooks for upgrading. And this time around, we’re not just talking about upgrades from VMware vSphere 7 to VMware vSphere 8. We’ve seen an active uptick in preparation for customer migrations from vSphere 7 to VMware Cloud Foundation (VCF) 9.0. We know that planning and implementing upgrades can be stressful, so we figured that a new upgrade toolkit would be welcomed by our customers to help to make the process as smooth as possible by putting together pre-upgrade resources to help ensure success. To help you on your upgrade journey, whatever that might be, we created a playbook in the form of KB articles to guide you through the end to end process. This wide range of Product Upgrade Knowledge Base (KB) Articles provide easy to follow, step by step guides through the upgrade process, highlighting any areas requiring special attention through each of the following stages of the upgrade process: Preparation for upgrade Sequence and procedure for upgrading Post upgrade required tasks The suite of Upgrade KBs are listed below and we would encourage customers to engage with this content during their upgrade journeys. Upgrade VMware Cloud Foundation (VCF) 4. x and 5. x Upgrade ESXi 7. x to 8. x or Install ESXi Upgrade vCenter Server 7.0 to 8.0 Upgrade to VMware vSAN 8 Upgrade to VMware Live Site Recovery 9.0 Upgrade to vSphere Replication 9.0 Upgrading to Aria Automation 8.18.</description></item><item><title>KubeCon + CloudNativeCon North America 2025 Co-Located Event Deep Dive: KyvernoCon</title><link>https://kubermates.org/docs/2025-09-15-kubecon-cloudnativecon-north-america-2025-co-located-event-deep-dive-kyvernocon/</link><pubDate>Mon, 15 Sep 2025 13:28:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-15-kubecon-cloudnativecon-north-america-2025-co-located-event-deep-dive-kyvernocon/</guid><description>Who will get the most out of attending this event? What is new with Kyverno this year? What will the day look like? Should I do any homework first? Find your community! A special note from the chairs : Posted on September 15, 2025 by Co-chairs: Jim Bugwadia &amp;amp; Cortney Nickerson CNCF projects highlighted in this post This is the very first KyvernoCon ! While Kyverno has been part of the CNCF since November 2020, and has had a strong presence at past KubeCon events through policy as code focused talks, maintainer sessions, and community meetups, this is the first time our community is gathering under one roof for a dedicated event. Kubernetes has become the standard for cloud native and AI/ML workloads, and large organizations need to automate security and compliance. Hence, there is a critical need and growing awareness for policy as code. Over the past few years, Kyverno has matured significantly, becoming the most widely adopted CNCF policy engine. As we work towards becoming a graduated project and are continuing to incorporate new capabilities like CEL-based policies and a growing set of integrations across the ecosystem, the timing felt right to create a space where users, contributors, and platform teams can learn, share, and shape the future of policy as code together. KyvernoCon is designed for anyone building or running Kubernetes platforms, but especially valuable for: Organizations running Kubernetes at scale and for mission critical software, who are looking for unified governance across clusters, pipelines, and cloud. Platform engineers and SREs who want to leverage policy as code into day-to-day workflows for automated and secure delivery practices, cost savings, and a better overall developer experience. Security and compliance teams looking for practical ways to enforce standards and reduce risk. Developers and operators who want to understand how policy can improve reliability and speed. Adopters, contributors, and open-source enthusiasts eager to collaborate and shape Kyverno’s next chapter. Leaders and decision-makers curious about why policy as code has become a must-have in modern platform engineering and software delivery. If you touch Kubernetes in any way and care about doing it securely, sustainably, and at scale, you’ll get value out of spending the day with us.</description></item><item><title>VCF Breakroom Chats Episode 58:  A Smarter Way to a Unified Private Cloud Consumption Experience with VCF 9.0</title><link>https://kubermates.org/docs/2025-09-15-vcf-breakroom-chats-episode-58-a-smarter-way-to-a-unified-private-cloud-consumpt/</link><pubDate>Mon, 15 Sep 2025 12:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-15-vcf-breakroom-chats-episode-58-a-smarter-way-to-a-unified-private-cloud-consumpt/</guid><description>VCF Breakroom Chats Episode 58 About the VCF Breakroom Chat Series Related Related Articles VMware Cloud Foundation Support Upgrade Playbook VCF Breakroom Chats Episode 58: A Smarter Way to a Unified Private Cloud Consumption Experience with VCF 9.0 Boost Operational Efficiency with VMware Cloud Foundation Welcome to the next episode of the VCF Breakroom Chats. Today, we are happy to present this vLog with Ragav Gopalan, Technology Product Management at Broadcom. In this episode, Ragav Gopalan and Taka Uenishi discuss how VCF 9.0 provides a smarter way to a unified private cloud consumption experience with VCF 9.0. Want to learn more about VCF Automation? Check out the VCF Automation web page for more resources. Read this blog post: Private Cloud Redefined: Deliver a Unified Cloud Consumption Experience with VMware Cloud Foundation 9.0 This webinar series focuses on VMware Cloud Foundation (VCF). In this series, we share conversations with industry-recognized experts from Broadcom and with solution partners and customers. You’ll gain relevant insights whether you’re an IT practitioner, IT admin, cloud or platform architect, developer, DevOps, senior IT manager, IT executive, or AI/ML professional. If you are new to the series, please check out our previous episodes.</description></item><item><title>A strategic approach to AI inference performance</title><link>https://kubermates.org/docs/2025-09-15-a-strategic-approach-to-ai-inference-performance/</link><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-15-a-strategic-approach-to-ai-inference-performance/</guid><description>A strategic approach to AI inference performance Optimizing the inference runtime Optimizing the AI model Red Hat AI: Putting the strategy into practice Get started with AI for enterprise: A beginner’s guide About the author Carlos Condado More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Training large language models (LLMs) is a significant undertaking, but a more pervasive and often overlooked cost challenge is AI inference. Inference is the procedure by which a trained AI model processes new input data and generates an output. As organizations deploy these models in production, the costs can quickly become substantial, especially with high token volumes, long prompts, and growing usage demands. To run LLMs in a cost-effective and high-performing way, a comprehensive strategy is essential. This approach addresses two critical areas: optimizing the inference runtime and optimizing the model itself. Basic serving methods often struggle with inefficient GPU memory usage, suboptimal batch processing, and slow token generation. This is where a high-performance inference runtime becomes critical. vLLM, is the de facto, open source library that helps LLMs perform calculations more efficiently and at scale. vLLM addresses these runtime challenges with advanced techniques, including: Continuous batching : Instead of processing requests one by one, vLLM groups tokens from multiple sequences into batches. This minimizes GPU idle time and significantly improves GPU utilization and inference throughput. PagedAttention : This memory management strategy efficiently handles large key-value (KV) caches. By dynamically allocating and managing GPU memory pages, PagedAttention greatly increases the number of concurrent requests and supports longer sequences without memory bottlenecks.</description></item><item><title>AI-assisted development: Supercharging the open source way</title><link>https://kubermates.org/docs/2025-09-15-ai-assisted-development-supercharging-the-open-source-way/</link><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-15-ai-assisted-development-supercharging-the-open-source-way/</guid><description>AI-assisted development: Supercharging the open source way AI as a collaborator in open source development Upstream, with AI in the loop Navigating the new frontier with openness and trust Any model, any accelerator, any cloud: Unlocking enterprise AI with open source innovation About the author Chris Wright More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Note: This post is the first in a series that will take readers inside our AI projects, share how we’re equipping our engineers for the AI era, and dig into the legal and ethical frameworks we’re grappling with at Red Hat. Our intention is to not only highlight our practices, but also start discussions and spur ideas on the future of software development. The world of technology is in the midst of a seismic shift. Artificial intelligence, particularly generative AI (gen AI), is rapidly evolving from a futuristic concept into a tangible tool with the potential to redefine how we build software. For those of us who have built our careers on open source principles of software development, this moment is pivotal. And a bit unnerving. At Red Hat, we&amp;rsquo;ve always believed that the most powerful innovations are born from collaboration, transparency, and a shared commitment to solving complex problems. As we head into the AI era, we believe these same principles are not just relevant, but essential. Using AI in open source isn&amp;rsquo;t about replacing the developer; it&amp;rsquo;s about empowering them. It’s about enhancing the creativity and ingenuity that has always been at the heart of open source. We see AI as a powerful new collaborator in the open source community—a tool that can help us scale software development, tackle more ambitious projects, and accelerate the pace of innovation for everyone. Red Hat is all-in on unlocking the potential of AI, and we intend to bring this to the communities we’re part of.</description></item><item><title>Preparing your organization for the quantum future</title><link>https://kubermates.org/docs/2025-09-15-preparing-your-organization-for-the-quantum-future/</link><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-15-preparing-your-organization-for-the-quantum-future/</guid><description>Preparing your organization for the quantum future Beyond a simple update Your upstream dependencies Industry must draw a line in the sand to move forward from Understanding how and where you use cryptography Expect a hybrid transition A business imperative, not just a technical challenge Conclusion Red Hat Product Security About the author Emily Fox More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Recently, we’ve shared a lot about post-quantum cryptography, the great work we’re doing to make it available to you through our products , and the importance of preparing for a future with quantum computers powerful enough to break classic RSA -based cryptography. You may have heard about “Q-day,” the day when a cryptographically relevant quantum computer (CRQC) is available to break public-key encryption–the underpinning of our digital world today. If you missed it, this risk is real, and proactive organizations are already preparing for it. Q-day is predicted to occur between 2029 and 2032, so now is the time to prepare. The core issue at hand is that many of our current cryptographic systems, like RSA, rely on mathematical problems that are simple for classical computers to do in one direction, but extremely difficult to reverse. For example, multiplying 2 large prime numbers is simple, but factoring the result back to its original prime numbers is very hard. This is modern public-key cryptography, also called asymmetric cryptography. In contrast, symmetric cryptography relies on a single shared secret key for both encryption and decryption and requires a secure method for sharing that secret key beforehand. With public-key cryptography being the most common and practical way for computers to share that secret, we must also be aware that CRQCs can weaken the effectiveness of symmetric keys in addition to compromising the most common method by which those keys are shared. CRQCs can do factoring at a scale and speed that is impossible with existing computers. Instead of multiple lifetimes, RSA can be broken in seconds. New cryptographic algorithms are needed to resist these new attacks, and those new algorithms (e.</description></item><item><title>Kubernetes v1.34: Autoconfiguration for Node Cgroup Driver Goes GA</title><link>https://kubermates.org/docs/2025-09-12-kubernetes-v1-34-autoconfiguration-for-node-cgroup-driver-goes-ga/</link><pubDate>Fri, 12 Sep 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-09-12-kubernetes-v1-34-autoconfiguration-for-node-cgroup-driver-goes-ga/</guid><description>Kubernetes v1.34: Autoconfiguration for Node Cgroup Driver Goes GA Automated cgroup driver detection Announcement: Kubernetes is deprecating containerd v1. y support Historically, configuring the correct cgroup driver has been a pain point for users running new Kubernetes clusters. On Linux systems, there are two different cgroup drivers: cgroupfs and systemd. In the past, both the kubelet and CRI implementation (like CRI-O or containerd) needed to be configured to use the same cgroup driver, or else the kubelet would misbehave without any explicit error message. This was a source of headaches for many cluster admins. Now, we&amp;rsquo;ve (almost) arrived at the end of that headache. cgroupfs systemd In v1.28.0, the SIG Node community introduced the feature gate KubeletCgroupDriverFromCRI , which instructs the kubelet to ask the CRI implementation which cgroup driver to use. After many releases of waiting for each CRI implementation to have major versions released and packaged in major operating systems, this feature has gone GA as of Kubernetes 1.34.0. KubeletCgroupDriverFromCRI In addition to setting the feature gate, a cluster admin needs to ensure their CRI implementation is new enough: containerd: Support was added in v2.0.0 CRI-O: Support was added in v1.28.0 While CRI-O releases versions that match Kubernetes versions, and thus CRI-O versions without this behavior are no longer supported, containerd maintains its own release cycle. containerd support for this feature is only in v2.0 and later, but Kubernetes 1.34 still supports containerd 1.7 and other LTS releases of containerd. The Kubernetes SIG Node community has formally agreed upon a final support timeline for containerd v1. y.</description></item><item><title>Boost Operational Efficiency with VMware Cloud Foundation</title><link>https://kubermates.org/docs/2025-09-12-boost-operational-efficiency-with-vmware-cloud-foundation/</link><pubDate>Fri, 12 Sep 2025 18:20:05 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-12-boost-operational-efficiency-with-vmware-cloud-foundation/</guid><description>Fleet-Level Lifecycle Management Unified Visibility Across Infrastructure, Databases, and Applications Why Upgrade to VCF Now? Related Related Articles Boost Operational Efficiency with VMware Cloud Foundation Broadcom (VMware) Named a Leader in the 2025 GartnerⓇ Magic QuadrantTM for Distributed Hybrid Infrastructure for the 3rd Consecutive Year FinOps in VMware Cloud Foundation VMware vSphere Foundation (VVF) offers a solid platform for compute and storage virtualization. But as environments expand, complexity increases, and IT teams find themselves spending more time on routine upkeep than innovation. VMware Cloud Foundation (VCF) builds on VVF by introducing automation and integrated management that help reduce manual work, minimize downtime, and strengthen compliance. In our previous blog, we discussed the “5 Reasons to Upgrade from VMware vSphere Foundation to VMware Cloud Foundation. ” In this blog, we will focus on how VCF addresses common operational challenges. This video provides a great visual summary of how VMware Cloud Foundation addresses common operational challenges to improve efficiency. The challenge: Routine patching and upgrades consume hours each quarter. Manual, host-by-host updates don’t scale, introducing errors, and can create version mismatches that delay critical projects and increase security risk. The VCF approach: Automated lifecycle management applies upgrades and patches across compute, storage, and networking from a single pane. A 2025 VMware Cloud Foundation TCO study found that this automation reduces operational overhead by up to 60%. For GCI Communications, Alaska’s largest telecom provider, VCF cut infrastructure deployment times from months to days, giving IT resources valuable time back for strategic work. (Watch the video.</description></item><item><title>Top Kubernetes (K8s) Troubleshooting Techniques – Part 1</title><link>https://kubermates.org/docs/2025-09-12-top-kubernetes-k8s-troubleshooting-techniques-part-1/</link><pubDate>Fri, 12 Sep 2025 16:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-12-top-kubernetes-k8s-troubleshooting-techniques-part-1/</guid><description>&lt;ol&gt;
&lt;li&gt;Fixing CrashLoopBackOff Errors in Pods 2. Kubernetes Troubleshooting Deployment Failures: ImagePullBackOff 3. Kubernetes Troubleshooting: Fixing NotReady Node Errors 4. Diagnosing Service and Networking Problems: The Pending Error 5. Kubernetes Troubleshooting High Resource Usage: Solving OOMKilled Errors Using Monitoring and Tracing Tools Posted on September 12, 2025 by Keval Bhogayata, Principal Engineer at Middleware Member post originally published on the Middleware blog by Keval Bhogayata , covering the top 10 Kubernetes Troubleshooting Techniques. Regardless of its popularity, there can be times where even the most seasoned DevOps engineers must troubleshoot Kubernetes. While it excels at handling containerised applications at scale, it can present unique troubleshooting challenges. In this post, we’ll explore the top 10 Kubernetes troubleshooting techniques that every DevOps engineer should master. These K8s troubleshooting tips come from real-world scenarios, showing how to solve common and critical Kubernetes issues quickly and reliably. One of the most common and frustrating issues in Kubernetes is when a pod repeatedly crashes during restarts; a situation known as the CrashLoopBackOff error. This occurs when a container fails to start correctly, and Kubernetes continually attempts to restart it, resulting in a loop of failures.&lt;/li&gt;
&lt;/ol&gt;</description></item><item><title>KubeCon + CloudNativeCon North America 2025 Co-Located Event Deep Dive: OpenTofu Day</title><link>https://kubermates.org/docs/2025-09-12-kubecon-cloudnativecon-north-america-2025-co-located-event-deep-dive-opentofu-da/</link><pubDate>Fri, 12 Sep 2025 13:29:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-12-kubecon-cloudnativecon-north-america-2025-co-located-event-deep-dive-opentofu-da/</guid><description>Who will get the most out of attending this event? What is new and different this year? What will the day look like? Should I do any homework first? Thanks to the community! Posted on September 12, 2025 by Chair: Sebastian Stadil CNCF projects highlighted in this post OpenTofu Day is the best place to connect with the OpenTofu community. It’s a fantastic place to talk shop with other infrastructure or platform engineers, trade stories, discuss best practices, and maybe even hire / be hired. OpenTofu is the open source continuation of Hashicorp® Terraform™. The very first of these events was in 2024 at KubeCon in Paris. It was an incredible place to start! The OpenTofu Day schedule is built around the assumption that the audience has prior experience with OpenTofu, Terraform, or other infrastructure as code. Having this experience is not required to attend, but it will definitely help to get the most out of the event. As such, it is best for DevOps engineers, platform engineers, and infrastructure leadership, especially for those looking to migrate from Terraform in 2026. This year we focus less on what it took to get the project off the ground, and more on what is in store for the community, and sharing community experiences with the project in production. The event kicks off with a summary of the progress of the project, new features, growth, and new additions to the core team. Then we have the devs expand on that with a live panel, followed by community members sharing their experiences and learned lessons with the project. It would be best if you have already downloaded and used OpenTofu. What’s more, if you migrated from Terraform.</description></item><item><title>Friday Five — September 12, 2025</title><link>https://kubermates.org/docs/2025-09-12-friday-five-september-12-2025/</link><pubDate>Fri, 12 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-12-friday-five-september-12-2025/</guid><description>Friday Five — September 12, 2025 Diginomica - US AI Action Plan, part 3 – how to be fast, sustainable, and efficient. Just don’t call it ‘green’! Taming hybrid cloud complexity: A path to true agility Light Reading - Rakuten Mobile and a tale of failed telecom disruption Llama Stack: Kubernetes for RAG &amp;amp; AI Agents in Generative AI DataCenter Knowledge - ISS Data Center Launch Tests Edge Computing at 400km Above Earth About the author Red Hat Corporate Communications More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share America’s AI Action Plan emphasizes speed, scale, and efficiency through deregulation, faster permitting, and infrastructure build-out. It promotes open-source/open-weight models, energy expansion, and ideological neutrality while avoiding green labels despite its focus on sustainability. Learn more Marco Bill discusses the challenges that IT leaders face today, and how Red Hat OpenShift offers a unified platform to manage containerized, virtualized, and AI workloads across hybrid cloud environments, improving consistency, speed-to-production, and developer autonomy. Learn more Rakuten Mobile aimed to shake up Japan’s telecom market with lean operations, cloud-native tech, and open RAN innovation. It achieved modest subscriber growth and its first positive earnings before interest, tax, depreciation and amortization profit, but still trails incumbents in scale and network performance. Learn more What do Llama Stack and Kubernetes have in common? Red Hat&amp;rsquo;s Cedric Clyburn explains how this open-source framework simplifies building enterprise-ready generative AI systems. Discover how Llama Stack integrates RAG, AI agents, and modular APIs to create scalable and portable AI applications. Learn more Axiom Space and Red Hat launched the AxDCU-1 prototype to the ISS to test space-based edge computing using Red Hat Device Edge with MicroShift, enabling autonomous, resilient containerized workloads, real-time processing in orbit, and efficient OTA updates despite limited connectivity. Learn more Red Hat is the world’s leading provider of enterprise open source software solutions, using a community-powered approach to deliver reliable and high-performing Linux, hybrid cloud, container, and Kubernetes technologies. Red Hat helps customers integrate new and existing IT applications, develop cloud-native applications, standardize on our industry-leading operating system, and automate, secure, and manage complex environments. Award-winning support, training, and consulting services make Red Hat a trusted adviser to the Fortune 500.</description></item><item><title>Navigating complexity, delivering value: our take on why Forrester named Red Hat OpenShift a Leader</title><link>https://kubermates.org/docs/2025-09-12-navigating-complexity-delivering-value-our-take-on-why-forrester-named-red-hat-o/</link><pubDate>Fri, 12 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-12-navigating-complexity-delivering-value-our-take-on-why-forrester-named-red-hat-o/</guid><description>Navigating complexity, delivering value: our take on why Forrester named Red Hat OpenShift a Leader Get the report Learn from Red Hat’s customers and partners Want guidance on other tech challenges? Red Hat OpenShift AI (Self-Managed) | Product Trial About the author Ashesh Badani More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Today&amp;rsquo;s IT leaders face intense pressure to accelerate innovation while managing a complex mix of technologies. They are faced with the challenge of integrating AI, modernizing legacy systems and controlling costs within a complex hybrid cloud that spans datacenters, public clouds and the edge. The core question for every technology leader is how to build a platform strategy that tames this complexity, strengthens security and reliability, and creates a foundation for future growth. At Red Hat, our hybrid cloud strategy is designed to solve this challenge. We believe the solution is a consistent, open platform that empowers organizations to build and run any application, anywhere. Red Hat OpenShift is at the heart of this strategy, providing a single, trusted foundation for your existing virtualized applications as well as your modern, cloud-native services. By offering a powerful virtualization alternative through OpenShift Virtualization and a pathway into artificial intelligence with Red Hat OpenShift AI , OpenShift helps you connect the infrastructure you have today with the cloud-native future you are building for tomorrow. This is why I’m so proud that Red Hat has been named a Leader in The Forrester Wave™: Multicloud Container Platforms, Q3 2025. We feel that this recognition is highly respected, independent acknowledgement that our strategy is not only resonating with experts and observers, but also delivering tangible value to our customers. We believe that Forrester’s comprehensive evaluation, which scored Red Hat highest in both the “Current Offering” and “Strategy” categories, confirms that we are successfully addressing the most pressing needs of enterprise IT. The report notes, &amp;ldquo;Red Hat excels in core Kubernetes areas, offering robust operator options, powerful management, GitOps automation, and flexible interfaces. &amp;quot; Our technical excellence provides the stability and control your organization needs to more effectively manage critical applications at scale.</description></item><item><title>Red Hat OpenShift: Where vision meets execution</title><link>https://kubermates.org/docs/2025-09-12-red-hat-openshift-where-vision-meets-execution/</link><pubDate>Fri, 12 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-12-red-hat-openshift-where-vision-meets-execution/</guid><description>Red Hat OpenShift: Where vision meets execution Learn more about Red Hat OpenShift for container management Want guidance on other tech challenges? Red Hat OpenShift Container Platform | Product Trial About the author Michael Ferris More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Enterprise technology leaders face a central challenge: building the future while managing the present. This requires a dual focus: a clear, forward-looking vision for what&amp;rsquo;s next and the ability to execute and deliver results today, at scale, across the entire enterprise. It’s this combination of vision and execution that separates leaders from the rest. At Red Hat, our strategy is built to help you meet this challenge. Think of your enterprise as a thriving city. To function, it needs reliable infrastructure—power, water, roads—that work flawlessly today. That’s execution. But it also needs a flexible plan to grow that allows for the construction of new commercial spaces, housing, parks, and transportation systems in the future. That’s vision. A true hybrid cloud platform must meet both needs. It must provide a consistent, adaptable foundation that allows you to run your business efficiently while confidently building for the future as new technologies like artificial intelligence are deployed. Red Hat OpenShift is the platform where your vision meets practical, enterprise-grade execution.</description></item><item><title>The EU Cyber Resilience Act's impact on open source security</title><link>https://kubermates.org/docs/2025-09-12-the-eu-cyber-resilience-act-s-impact-on-open-source-security/</link><pubDate>Fri, 12 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-12-the-eu-cyber-resilience-act-s-impact-on-open-source-security/</guid><description>The EU Cyber Resilience Act&amp;rsquo;s impact on open source security From communal effort to legal mandate A cultural faux pas: Misunderstanding shared responsibility A call for collective responsibility Red Hat Product Security About the authors Emily Fox Roman Zhukov More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The world runs on open source. From the applications you use daily to the critical infrastructure powering our society, open source software is ubiquitous. However, this widespread adoption has brought with it an escalating need for robust security, a reality starkly highlighted by incidents like SolarWinds and the more recent XZ Utils vulnerability. While the open source community often demonstrates remarkable resilience and collaboration in addressing threats, a significant shift in responsibility is now underway, driven in part by legislation, such as the EU&amp;rsquo;s Cyber Resilience Act (CRA). For decades, open source security was a communal effort, relying on the goodwill and expertise of contributors. As commercial entities increasingly integrate open source into their products, they&amp;rsquo;ve often overlooked security at the source (i. e. upstream), instead opting to bolt on fixes only at product release. Despite security engineers tirelessly advocating for practices like supply chain security, widespread adoption has been limited due to this afterthought mindset. Many organizations who are comfortable with their long-standing practices of consuming open source software have resisted re-evaluating their approach, operating under the dangerous and flawed assumption that since it worked fine for years and someone else always took care of it, there&amp;rsquo;s no need to change. This mindset ignores a fundamental truth: security isn&amp;rsquo;t about avoiding compromise, but about making it as difficult as possible for attackers when they inevitably strike. Now the CRA has arrived , aiming to address these very issues from a top-down perspective.</description></item><item><title>A Blueprint for DevSecOps: Accelerating Federal Agencies with IBM and Nirmata</title><link>https://kubermates.org/docs/2025-09-11-a-blueprint-for-devsecops-accelerating-federal-agencies-with-ibm-and-nirmata/</link><pubDate>Thu, 11 Sep 2025 21:51:25 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-11-a-blueprint-for-devsecops-accelerating-federal-agencies-with-ibm-and-nirmata/</guid><description>A Blueprint for DevSecOps: Accelerating Federal Agencies with IBM and Nirmata The Challenge: Navigating a Complex Landscape The Solution: The IBM PDE Factory The Power of Policy as Code A Cultural Shift: From Remediation to Creation In the rapidly evolving world of IT modernization, federal agencies face a unique set of challenges. They must navigate complex security mandates and legacy systems while striving for the agility and speed of modern software development. A recent webinar featuring experts from IBM Federal and Nirmata shed light on this very topic, offering a compelling vision for accelerating DevSecOps and ensuring security is a foundational element, not an afterthought. A poll conducted during the webinar revealed the top hurdles for federal agencies: a tie between manual processes and legacy systems and security and compliance mandates. This perfectly illustrates the central conflict. How do you move fast when everything is a manual, security-audited process? As Mark Wells, DevSecOps and IT Automation Practice Lead for IBM Federal, explained, this was the exact problem that led to the creation of the IBM PTE Factory. Born from the chaos of DevOps engineers “running around like chickens with their heads cut off,” the factory was designed to automate the deployment of infrastructure and development environments. The IBM PDE Factory is an asset—a platform and product engineering tool designed to deliver core services and support for software engineering. It’s built around a “start left” philosophy, meaning security is integrated from the very beginning of the process, not “shifted left” later. The factory’s core is a series of “builders” that automate critical tasks: Resource Builder: Ensures all container images and resources are certified and attested to, with near-zero vulnerabilities, before they are ever used. Platform Builder: Automates the creation of the entire cloud architecture. Security Builder: Deploys pre-configured bundles of security policies.</description></item><item><title>Securing AI Workloads in Kubernetes: Why Traditional Network Security Isn’t Enough</title><link>https://kubermates.org/docs/2025-09-11-securing-ai-workloads-in-kubernetes-why-traditional-network-security-isn-t-enoug/</link><pubDate>Thu, 11 Sep 2025 19:21:53 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-11-securing-ai-workloads-in-kubernetes-why-traditional-network-security-isn-t-enoug/</guid><description>AI Architectures Introduce New Attack Vectors The Multi-Cluster Problem The East-West Traffic Dilemma Egress Control Complexity Why Kubernetes Native Security Falls Short NetworkPolicy Limitations in AI Contexts Multi-Cluster Networking, Security and Observability gaps Insufficient Observability for AI Workloads How Does Calico Help Secure AI Workloads? Four Core Calico Capabilities for AI Workload Security Calico in Action: Example Use Cases The AI revolution is here, and it’s running on Kubernetes. From fraud detection systems to generative AI platforms, AI-powered applications are no longer experimental projects; they’re mission-critical infrastructure. But with great power comes great responsibility, and for Kubernetes platform teams, that means rethinking security. But this rapid adoption comes with a challenge: 13% of organizations have already reported breaches of AI models or applications , while another 8% don’t even know if they’ve been compromised. Even more concerning, 97% of breached organizations reported that they lacked proper AI access controls. To address this, we must recognize that AI architectures introduce entirely new attack vectors that traditional security models aren’t equipped to handle. AI workloads running in Kubernetes environments introduce a new set of security challenges. Traditional security models often fall short in addressing the unique complexities of AI pipelines, specifically related to The Multi-Cluster Problem, The East-West Traffic Dilemma , and Egress Control Complexity. Let’s explore each of these critical attack vectors in detail. Most enterprise AI deployments don’t run in a single cluster. Instead, they typically follow this pattern: Training Infrastructure (GPU-Heavy) Dedicated clusters with high-memory GPU nodes Batch job processing for model training and fine-tuning Access to large-scale data stores and feature engineering pipelines Often deployed in public cloud for elastic capacity Inference Infrastructure (Latency-Optimized) CPU or smaller GPU configurations optimized for low-latency responses Real-time prediction APIs with auto-scaling based on demand Often deployed on-premises or in edge locations for compliance and latency Integration with production application stacks Development and Experimentation Clusters Mixed workloads for data science experimentation Access to production data for model development Often less stringently controlled than production environments This multi-cluster architecture creates network policy enforcement gaps. A data scientist with legitimate permissions to the development cluster shouldn’t be able to access production model weights, but traditional Kubernetes security tools can’t enforce consistent policies across these distributed environments.</description></item><item><title>Kubernetes v1.34: Mutable CSI Node Allocatable Graduates to Beta</title><link>https://kubermates.org/docs/2025-09-11-kubernetes-v1-34-mutable-csi-node-allocatable-graduates-to-beta/</link><pubDate>Thu, 11 Sep 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-09-11-kubernetes-v1-34-mutable-csi-node-allocatable-graduates-to-beta/</guid><description>Kubernetes v1.34: Mutable CSI Node Allocatable Graduates to Beta Background Dynamically adapting CSI volume limits How it works Enabling the feature Example CSI driver configuration Immediate updates on attachment failures Getting started Next steps The functionality for CSI drivers to update information about attachable volume count on the nodes , first introduced as Alpha in Kubernetes v1.33, has graduated to Beta in the Kubernetes v1.34 release! This marks a significant milestone in enhancing the accuracy of stateful pod scheduling by reducing failures due to outdated attachable volume capacity information. Traditionally, Kubernetes CSI drivers report a static maximum volume attachment limit when initializing. However, actual attachment capacities can change during a node&amp;rsquo;s lifecycle for various reasons, such as: Manual or external operations attaching/detaching volumes outside of Kubernetes control. Dynamically attached network interfaces or specialized hardware (GPUs, NICs, etc. ) consuming available slots. Multi-driver scenarios, where one CSI driver’s operations affect available capacity reported by another. Static reporting can cause Kubernetes to schedule pods onto nodes that appear to have capacity but don&amp;rsquo;t, leading to pods stuck in a ContainerCreating state. ContainerCreating With this new feature, Kubernetes enables CSI drivers to dynamically adjust and report node attachment capacities at runtime. This ensures that the scheduler, as well as other components relying on this information, have the most accurate, up-to-date view of node capacity. Kubernetes supports two mechanisms for updating the reported node volume limits: Periodic Updates: CSI drivers specify an interval to periodically refresh the node&amp;rsquo;s allocatable capacity. Reactive Updates: An immediate update triggered when a volume attachment fails due to exhausted resources ( ResourceExhausted error). ResourceExhausted To use this beta feature, the MutableCSINodeAllocatableCount feature gate must be enabled in these components: MutableCSINodeAllocatableCount kube-apiserver kube-apiserver kubelet kubelet Below is an example of configuring a CSI driver to enable periodic updates every 60 seconds: apiVersion: storage.</description></item><item><title>Kubernetes right-sizing with metrics-driven GitOps automation</title><link>https://kubermates.org/docs/2025-09-11-kubernetes-right-sizing-with-metrics-driven-gitops-automation/</link><pubDate>Thu, 11 Sep 2025 15:17:23 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-11-kubernetes-right-sizing-with-metrics-driven-gitops-automation/</guid><description>Kubernetes right-sizing with metrics-driven GitOps automation Understanding the challenges of resource management in Amazon EKS The impact of inefficient resource management Existing solutions for Kubernetes resource management How the proposed solution addresses the challenges Solution overview Workflow overview Key architectural considerations Walkthrough Prerequisites Implementing a GitOps-driven automation for resource optimization GitOps principle Setting up the recommendation generator Environment and metrics source Local or External Installation Automating the GitOps workflow Cleaning up Conclusion About the authors Efficient resource allocation in Kubernetes is essential for optimizing application performance and controlling costs. In Amazon Elastic Kubernetes Service (Amazon EKS) , managing resource requests and limits manually can be challenging and error-prone. This post introduces an automated, and GitOps-driven approach to resource optimization using Amazon Web Services (AWS) services such as Amazon Managed Service for Prometheus and Amazon Bedrock. This approach is particularly beneficial for users who prefer non-intrusive methods for resource optimization. Understanding resource management in Kubernetes is crucial for optimal cluster performance. When deploying pods, the Kubernetes scheduler evaluates resource requests to find suitable nodes that can accommodate the specified CPU and memory requirements. These requests act as the minimum guaranteed resources for the pod, while limits serve as upper bounds to prevent any single pod from monopolizing node resources. Over-provisioning and under-provisioning of resources in Kubernetes can lead to increased costs and performance issues. Striking the right balance is essential for optimal resource usage. In a shared environment, one pod consuming excessive resources can degrade the performance of others on the same node. Applications with fluctuating resource demands can be challenging to manage. Without adaptive resource allocation strategies, these workloads may experience performance degradation or resource waste.</description></item><item><title>Broadcom (VMware) Named a Leader in the 2025 GartnerⓇ Magic QuadrantTM for Distributed Hybrid Infrastructure for the 3rd Consecutive Year</title><link>https://kubermates.org/docs/2025-09-11-broadcom-vmware-named-a-leader-in-the-2025-gartner-magic-quadranttm-for-distribu/</link><pubDate>Thu, 11 Sep 2025 13:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-11-broadcom-vmware-named-a-leader-in-the-2025-gartner-magic-quadranttm-for-distribu/</guid><description>Related Related Articles Broadcom (VMware) Named a Leader in the 2025 GartnerⓇ Magic QuadrantTM for Distributed Hybrid Infrastructure for the 3rd Consecutive Year FinOps in VMware Cloud Foundation Deploy VMware Private AI on HGX servers with Broadcom Ethernet Networking With the publication of the 2025 Gartner Ⓡ Magic Quadrant TM for Distributed Hybrid Infrastructure (DHI), Broadcom (VMware) has been recognized as a leader for the 3rd consecutive year. You can view a complimentary copy of the 2025 Gartner® Magic Quadrant™ for Distributed Hybrid Infrastructure report here. Figure 1: Gartner Magic Quadrant for Distributed Hybrid Infrastructure, September 8, 2025, by Julia Palmer, Jefferey Hewitt, Elaine Zhang, Tony Harvey, Dennis Smith. The submission is based on VMware Cloud Foundation (VCF), a DHI platform that delivers public cloud scale and agility with private cloud security, resilience and performance, that lowers total cost of ownership. On the heels of a very successful VMware Explore 2025, Broadcom continues to invest in VCF to help customers on their modern private cloud journey. Here are some of the most recent updates: VCF 9.0 General Availability Broadcom announced general availability of VCF 9.0 on June 17, 2025. VCF 9.0 is a unified private cloud platform designed for running traditional, modern, and AI workloads with a consistent cloud operational model, comprehensive automation, and enhanced security across private data centers, edge, CSP and hyperscale cloud environments. VCF 9.0 delivers a streamlined, self-service experience for developers, robust compliance tools, advanced networking capabilities with multi-tenant VPC services, and integrated security and resilience features through fleet-level management and automated identity, access and certificate rotation. VCF 9.0 focuses on accelerating application modernization and innovation by providing agility and scalability comparable to public clouds while maintaining control, security, and better cost predictability. VMware Explore Updates VMware Explore 2025, held August 25-28, 2025 in Las Vegas, focused on delivering a unified AI native private cloud platform that simplifies operations, empowers developers, and addresses the security and compliance demands of modern enterprises. VCF as an AI-ready platform: Previously sold separately, VCF Private AI services will now be included as a standard capability of VCF 9.0 in the first quarter of Broadcom’s FY2026. Private AI services enable privacy and security, simplify infrastructure management and streamline model deployment.</description></item><item><title>FinOps in VMware Cloud Foundation</title><link>https://kubermates.org/docs/2025-09-11-finops-in-vmware-cloud-foundation/</link><pubDate>Thu, 11 Sep 2025 02:50:55 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-11-finops-in-vmware-cloud-foundation/</guid><description>Overview Cost Management Cloud Service Providers and FinOps Consumer Side FinOps Cost Planning Workload Planning Infrastructure Planning Cost Comparison/Migration Planning Takeaways Related Related Articles FinOps in VMware Cloud Foundation Deploy VMware Private AI on HGX servers with Broadcom Ethernet Networking Unlocking the Power of GenAI Cloud costs are no longer just for IT finance analysts—cost management is a core part across IT operations. VMware Cloud Foundation (VCF) brings together private cloud infrastructure with FinOps principles to help ensure financial accountability, operational visibility, and business alignment for all IT users. The FinOps framework in VCF enables enterprises to not only provision infrastructure but also optimize, govern, and allocate costs intelligently across developers and providers. In an era where IT infrastructure costs are rapidly escalating, particularly with the rise of AI initiatives, enterprise organizations demand greater control and visibility over their expenditures. This necessitates not only refined service metering and billing, but also the ability to accurately capture and view the Total Cost of Ownership (TCO) and overall expenses. Concurrently, IT teams face the challenge of optimizing infrastructure to reduce costs and promote resource reuse, while also refining service metering and billing for a diverse array of modern services, including VMs, AI services, database services, Kubernetes services, and storage services. VCF cost management monitors and controls infrastructure spend by improving transparency, enabling cost governance, and supporting accurate budgeting. Delivered through a single pane of glass, VCF enables cost visibility, analysis, budgeting, optimization, and ROI insights while enabling showback for transparency and chargeback with customizable billing models—empowering enterprises to manage expenses, ensure accountability, and compare costs against public cloud efficiently. VMware Cloud Foundation (VCF) brings robust chargeback capabilities that cater to the unique needs of both enterprises and cloud service providers. For enterprises, VCF enables internal IT teams to bill back projects and application groups based on the resources they consume, creating financial accountability and promoting efficient usage. On the other hand, cloud service providers can leverage the same framework to track, meter, and charge tenants for the services delivered, ensuring fairness and transparency. By combining metering with data- and logic-backed billing, VCF empowers providers to generate accurate, itemized bills while giving consumers confidence that they are only paying for what they use.</description></item><item><title>Navigating AI risk: Building a trusted foundation with Red Hat</title><link>https://kubermates.org/docs/2025-09-11-navigating-ai-risk-building-a-trusted-foundation-with-red-hat/</link><pubDate>Thu, 11 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-11-navigating-ai-risk-building-a-trusted-foundation-with-red-hat/</guid><description>Navigating AI risk: Building a trusted foundation with Red Hat Understanding enterprise AI security risks Red Hat&amp;rsquo;s layered approach to AI security The proven backbone of Red Hat OpenShift A platform for what’s next with Red Hat OpenShift AI Best practices Hardening the AI platform and deployment pipelines Protecting active AI workloads and data Final thoughts Get started with AI Inference About the authors Martin Isaksson Christopher Nuland More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Red Hat helps organizations embrace AI innovation by providing a comprehensive and layered approach to security and safety across the entire AI lifecycle. We use our trusted foundation and expertise in open hybrid cloud to address the challenges around AI security, helping our customers build and deploy AI applications with more trust. As organizations adopt AI , they encounter significant security and safety hurdles. These advanced workloads need robust infrastructure and scalable resources and a comprehensive security posture that extends across the AI lifecycle. Many AI projects struggle to reach production because of these significant safety and security concerns. Some of the challenges organizations face include: Evolving AI-specific threats: AI applications and models are becoming attractive targets for malicious actors. Beyond conventional software vulnerabilities, critical concerns include training data poisoning, model evasion or theft, and adversarial attacks. Complex software supply chain: The AI lifecycle involves numerous components, increasing vulnerability risks. AI applications also often depend on a vast ecosystem of open source libraries, pre-trained models, and complicated data pipelines. A single vulnerability or malicious component introduced at any stage—from data ingestion and third-party libraries to the base container images—can compromise the integrity and security of the entire AI system. Recent supply chain attacks highlight the urgent industry need for verifiable integrity and provenance for all software artifacts, including AI models and their dependencies. Critical AI safety requirements: Trust is built on the assurance that AI models will operate as intended and without bias.</description></item><item><title>Seamless hybrid cloud storage: NetApp’s certified OpenShift operator for Trident</title><link>https://kubermates.org/docs/2025-09-11-seamless-hybrid-cloud-storage-netapp-s-certified-openshift-operator-for-trident/</link><pubDate>Thu, 11 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-11-seamless-hybrid-cloud-storage-netapp-s-certified-openshift-operator-for-trident/</guid><description>Seamless hybrid cloud storage: NetApp’s certified OpenShift operator for Trident This isn&amp;rsquo;t just Trident. It&amp;rsquo;s Trident, automated. Storage options for your cluster One workflow everywhere Click-and-go experience Automated lifecycle management Sensible storage Red Hat OpenShift Container Platform | Product Trial About the author Shane Heroux More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share If you&amp;rsquo;re building on Red Hat OpenShift , then your mission is likely the same as countless systems administrators: Provide a consistent, reliable application platform that spans from your data center to the public cloud. But as many platform teams have discovered, that seamless hybrid vision often runs into the practical hurdle of storage. Managing persistent storage for stateful apps introduces complexity, especially when your clusters span on-premises infrastructure and cloud environments like Red Hat OpenShift Service on AWS (ROSA). Different environments can mean different tooling, different processes, and more friction than you&amp;rsquo;d like. What if you could cut through that? What if your storage layer worked the same everywhere, managed natively inside OpenShift, without extra operational overhead? That&amp;rsquo;s not a future state anymore. It&amp;rsquo;s here, thanks to NetApp&amp;rsquo;s certified Red Hat OpenShift operator for Trident. You might already know Trident, NetApp&amp;rsquo;s dynamic CSI provisioner for Kubernetes. It brings NetApp&amp;rsquo;s ONTAP, a unified data infrastructure featuring snapshots, clones, replication, and more, to containerized workloads. The certified OpenShift operator takes a step further. The NetApp Trident Operator is a controller that automates the entire lifecycle of the Trident CSI driver to provide persistent storage from NetApp systems.</description></item><item><title>Streamlining migration to OpenShift Virtualization with automation and expert guidance</title><link>https://kubermates.org/docs/2025-09-11-streamlining-migration-to-openshift-virtualization-with-automation-and-expert-gu/</link><pubDate>Thu, 11 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-11-streamlining-migration-to-openshift-virtualization-with-automation-and-expert-gu/</guid><description>Streamlining migration to OpenShift Virtualization with automation and expert guidance OpenShift Virtualization migration content collection OpenShift Virtualization operations content collection Supplement your migration journey with Red Hat and partner services Take the next step Red Hat OpenShift Virtualization Engine | Product Trial About the author Steve Fulmer More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Modernizing your infrastructure often involves optimizing resources. One such approach is to converge virtualization and container platforms. Red Hat OpenShift Virtualization provides a powerful solution, allowing you to run virtual machines (VM) and containers side by side while enabling you to easily migrate to modern platforms at your own pace. Migrating traditional VM workloads demands a well planned approach. This is where the synergy of Red Hat OpenShift Virtualization, Red Hat Ansible Automation Platform, and Red Hat Consulting, or your preferred partner services, is key to success. OpenShift Virtualization is an attractive solution for organizations pursuing cost optimization, modernization of applications, and unified workflows. But migration must be streamlined. And once migrated, you must manage the ongoing Day 2 needs, including the VM lifecycle and the related infrastructure. Red Hat Ansible Automation Platform is a powerful automation solution that helps you in three key areas: Migration at-scale for VMs and related infrastructure Ongoing management of these resources, once moved Lifecycle management of VMs remaining on other existing platforms, or any virtualization platform Ansible Automation Platform includes content collections for more than 200 technologies designed to jumpstart your use of automation and these can be helpful in managing infrastructure related to your VM-based operations. New collections are continuously available, and in particular there are two new content collections for migrating to OpenShift Virtualization, and for Day 2 management of them. This article introduces you to both. Considerations when migrating from a traditional hypervisor to OpenShift Virtualization include: Understanding VM dependencies: Discovering and managing interconnected VM configurations Encrypted disks and storage compatibility: Ensuring data security and storage interoperability Minimizing downtime: Executing migrations with minimal disruption to critical services Robust rollback planning: Developing and testing strategies for seamless recovery Integration with modern CI/CD pipelines: Incorporating migrated VMs into automated development and release workflows Both Ansible Automation Platform and OpenShift Virtualization help you meet these challenges.</description></item><item><title>The virtualization game has changed: A new playbook for IT leaders</title><link>https://kubermates.org/docs/2025-09-11-the-virtualization-game-has-changed-a-new-playbook-for-it-leaders/</link><pubDate>Thu, 11 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-11-the-virtualization-game-has-changed-a-new-playbook-for-it-leaders/</guid><description>The virtualization game has changed: A new playbook for IT leaders A market in motion The strategic shift A new game plan for a new era Want guidance on other tech challenges? Red Hat OpenShift Virtualization Engine | Product Trial About the author Andrew Brown More like this Blog post Blog post Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share In today&amp;rsquo;s business, success isn&amp;rsquo;t just about playing the game–it&amp;rsquo;s about having the right strategy to win. Like a top team constantly adapting its formation to a changing opponent, organisations are grappling with complex IT environments, rising costs, and the need to modernise for workloads like AI. The question is, how do we find a way forward that provides greater control and a clear path to innovation? The answer lies in how we approach virtualization. To help leaders understand how the situation has shifted, Red Hat released a report on the state of virtualization. It reveals critical insights into how forward-thinking organisations are harnessing a unified platform for virtualization and containers to become more agile and flexible. I&amp;rsquo;ve been discussing these very topics with our customers, partners, and peers for some time now, and the insights in this report are a perfect articulation of what I&amp;rsquo;ve seen firsthand: there is a widespread desire for a new playbook for the future of IT infrastructure. Our customers are asking for a platform that drives tangible change and positions them for ongoing success. What truly solidifies my understanding of the changing virtualization market is the sheer scale of the market movement revealed in the report. It&amp;rsquo;s one thing to know there&amp;rsquo;s a buzz about change, but it’s another to see the data: 70% of organisations have recently moved virtual machine (VM) workloads to a different or additional hypervisor—or are in the process of making a switch. This isn&amp;rsquo;t just contemplation, it&amp;rsquo;s a widespread reassessment of existing platforms driven by a fundamental belief that the existing strategy is no longer sufficient. Organisations are not just thinking about it, but are actively executing on a new virtualization strategy that addresses their primary pain points, including licensing costs (31%), and management complexity (31%). Vendor lock-in (28%) and concerns about the continued support of existing platforms (27%), along with confidence in long-term vision (24%), are also frequent topics.</description></item><item><title>Kubernetes v1.34: Use An Init Container To Define App Environment Variables</title><link>https://kubermates.org/docs/2025-09-10-kubernetes-v1-34-use-an-init-container-to-define-app-environment-variables/</link><pubDate>Wed, 10 Sep 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-09-10-kubernetes-v1-34-use-an-init-container-to-define-app-environment-variables/</guid><description>Kubernetes v1.34: Use An Init Container To Define App Environment Variables What’s this all about? How It Works A word on security Summary Kubernetes typically uses ConfigMaps and Secrets to set environment variables, which introduces additional API calls and complexity, For example, you need to separately manage the Pods of your workloads and their configurations, while ensuring orderly updates for both the configurations and the workload Pods. Alternatively, you might be using a vendor-supplied container that requires environment variables (such as a license key or a one-time token), but you don’t want to hard-code them or mount volumes just to get the job done. If that&amp;rsquo;s the situation you are in, you now have a new (alpha) way to achieve that. Provided you have the EnvFiles feature gate enabled across your cluster, you can tell the kubelet to load a container&amp;rsquo;s environment variables from a volume (the volume must be part of the Pod that the container belongs to). this feature gate allows you to load environment variables directly from a file in an emptyDir volume without actually mounting that file into the container. It’s a simple yet elegant solution to some surprisingly common problems. EnvFiles At its core, this feature allows you to point your container to a file, one generated by an initContainer , and have Kubernetes parse that file to set your environment variables. The file lives in an emptyDir volume (a temporary storage space that lasts as long as the pod does), Your main container doesn’t need to mount the volume. The kubelet will read the file and inject these variables when the container starts. initContainer emptyDir Here&amp;rsquo;s a simple example: apiVersion : v1 kind : Pod spec : initContainers : - name : generate-config image : busybox command : [ &amp;lsquo;sh&amp;rsquo; , &amp;lsquo;-c&amp;rsquo; , &amp;rsquo;echo &amp;ldquo;CONFIG_VAR=HELLO&amp;rdquo; &amp;gt; /config/config. env&amp;rsquo; ] volumeMounts : - name : config-volume mountPath : /config containers : - name : app-container image : gcr. io/distroless/static env : - name : CONFIG_VAR valueFrom : fileKeyRef : path : config.</description></item><item><title>Deploy VMware Private AI on HGX servers with Broadcom Ethernet Networking</title><link>https://kubermates.org/docs/2025-09-10-deploy-vmware-private-ai-on-hgx-servers-with-broadcom-ethernet-networking/</link><pubDate>Wed, 10 Sep 2025 04:14:57 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-10-deploy-vmware-private-ai-on-hgx-servers-with-broadcom-ethernet-networking/</guid><description>What’s Covered in the White Paper? Key Highlights of the Solution Related Related Articles FinOps in VMware Cloud Foundation Deploy VMware Private AI on HGX servers with Broadcom Ethernet Networking Unlocking the Power of GenAI AI and Generative AI (Gen AI) require substantial infrastructure, and tasks like fine-tuning, customization, deployment, and querying can strain resources. Scaling up these operations becomes problematic without adequate infrastructure. Additionally, diverse compliance and legal requirements must be met across various industries and countries. Gen AI solutions must ensure access control, proper workload placement, and audit readiness to comply with these standards. To address these challenges, Broadcom introduced VMware Private AI to help customers run models next to their proprietary data. By combining innovations from both companies, Broadcom and NVIDIA aim to unlock the power of AI and unleash productivity with lower total cost of ownership (TCO). Our technical white paper, “ Deploy VMware Private AI on HGX servers with Broadcom Ethernet Networking ,” details the end-to-end deployment and configuration, focusing on DirectPath I/O (passthrough) GPUs and Thor 2 NICs with Tomahawk 5 Ethernet switch. This guide is essential for infrastructure architects, VCF administrators, and data scientists aiming to achieve optimal performance for their AI models in VCF. The white paper provides in-depth guidance on: Broadcom Thor2 NICs and NVIDIA GPUs: Learn how to effectively integrate Broadcom NICs and NVIDIA GPUs within Ubuntu-based Deep Learning Virtual Machines (DLVMs) in a VMware Cloud Foundation (VCF) environment. Network Configuration: Detailed instructions for configuring Ethernet Thor 2 NICs and Tomahawk 5 switches to enable RoCE (RDMA over Converged Ethernet) with NVIDIA GPUs, ensuring low-latency, high-throughput communication critical for AI workloads. Benchmark Testing: Step-by-step procedures for running benchmark tests with essential collective communications libraries like NCCL, validating the efficiency of multi-GPU operations. LLM Inference: Guidance on launching and benchmarking Large Language Model (LLM) inference using NVIDIA Inference Microservices (NIM) and vLLM, demonstrating real-world performance gains.</description></item><item><title>Do you still need GitOps in the era of gen AI?</title><link>https://kubermates.org/docs/2025-09-10-do-you-still-need-gitops-in-the-era-of-gen-ai/</link><pubDate>Wed, 10 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-10-do-you-still-need-gitops-in-the-era-of-gen-ai/</guid><description>Do you still need GitOps in the era of gen AI? Our own evolution when it comes to risk Move fast with AI, but stay in control Speed and control: With GitOps, you can have it all! What changed for us For a much deeper dive … Final thoughts Get started with AI Inference About the author Roberto Carratalá More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share If you&amp;rsquo;ve ever worked for or with enterprise companies you know that, when it comes to software, whether it&amp;rsquo;s AI-powered or not, the stakes could not be higher. And that is the reason they invest heavily in making their production environments as bulletproof as possible. They will architect for high availability and disaster recovery, enforce strict service level agreements (SLAs), and build redundancy into every possible layer. But if their architecture doesn’t also account for the potential for human error , is any of it worth the effort? Time and again, we’ve seen catastrophic outages traced back to a wrong mouse click, a badly written command, or a rushed deployment. Let’s review a few of them: Amazon S3 outage (Feb 2017): An engineer mistyped a command intended to remove a few servers, instead taking down critical S3 subsystems. The disruption affected GitHub, Slack, and other big players, lasting several hours and costing billions of dollars in damages. Facebook global outage (Oct 2021): Engineers withdrew critical Border Gateway Protocol (BGP) routes during a backbone configuration change, causing Facebook, Instagram, and WhatsApp to vanish from the internet for nearly six hours, a lesson on how a single manual change can break world-class redundancy. CrowdStrike Falcon update meltdown (July 2024): A single defective configuration file shipped in an automated security-agent update sent millions of Windows machines into a reboot loop, grounding airlines, banks, and hospitals worldwide. Again, a preventable human mistake , just at internet scale. As a personal example, I once worked with a customer whose datacenter cleaning staff unplugged a production rack server to plug in a vacuum cleaner. That predictable and preventable mistake caused six hours of serious downtime (and I’m only being vague because the incident is recognizable). The solution would not be to fire the cleaner, but to install lockable power outlets, so critical equipment cannot be disconnected without a key.</description></item><item><title>Enhancing your migration experience with migration toolkit for virtualization 2.9's UI updates</title><link>https://kubermates.org/docs/2025-09-10-enhancing-your-migration-experience-with-migration-toolkit-for-virtualization-2-/</link><pubDate>Wed, 10 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-10-enhancing-your-migration-experience-with-migration-toolkit-for-virtualization-2-/</guid><description>Enhancing your migration experience with migration toolkit for virtualization 2.9&amp;rsquo;s UI updates New features and highlights Overview page Migration plan creation wizard Monitoring experience Benefits Red Hat OpenShift Virtualization Engine | Product Trial About the authors Ethan Kim More like this Blog post Blog post Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Migration toolkit for virtualization 2.9 (MTV) release is here, and brings lots of new features to help make your virtual machine (VM) migration experience easier. This iteration introduces user experience enhancements designed to provide greater clarity, improved functionality, and a comprehensive view of your migration process. The enhanced UI of the migration toolkit 2.9 focuses on three pivotal areas: Enhanced overview page : Get a broader and more granular perspective on your migration progress. Clearer migration plan wizard : Your creation process will be smoother with expanded functionality and increased clarity. Improved plan monitoring experience : Get more precise status indicators and greater insight into ongoing migrations. These updates underscore our commitment to delivering a robust and intuitive migration solution. We invite you to explore how these new features in the migration toolkit for virtualization 2.9 optimizes your virtualization migration workflows. The overview page delivers a summary of your current migrations statuses in a clear and measurable way, using data charts to display information around: The number of virtual machines (VMs) that have successfully migrated, which migration is running, or which migration has failed over a given time frame. The number of migration plans that have succeeded, are running, or have failed over time. Migration history over time. Additions to the page also make it easier for first-time users to understand which source providers are supported by the migration toolkit for virtualization operator. It also allows users to create a source provider directly from this page, which is the first step in creating a migration plan.</description></item><item><title>Red Hat joins LF Energy to help shape a more efficient energy grid</title><link>https://kubermates.org/docs/2025-09-10-red-hat-joins-lf-energy-to-help-shape-a-more-efficient-energy-grid/</link><pubDate>Wed, 10 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-10-red-hat-joins-lf-energy-to-help-shape-a-more-efficient-energy-grid/</guid><description>Red Hat joins LF Energy to help shape a more efficient energy grid About the author Kelly Switt More like this Blog post Blog post Original podcast Original podcast Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The utility industry is currently facing a multitude of hurdles that are driven by a combination of aging infrastructure, changing energy landscape and evolving customer expectations. The power grid, specifically, is undergoing its greatest transformation since inception, with new challenges imposing problems that our grid is not equipped to handle. As utilities adopt a software-defined infrastructure,bringing IT best practices to OT ecosystems becomes increasingly feasible. By virtualizing applications and taking advantage of advancements in edge computing platforms and AI, utilities can modernize infrastructure and create more agile, software-defined environments that improve everything from operational efficiency to smart grid responsiveness. Through collaboration and open innovation, Red Hat is helping organizations modernize, virtualize and bridge the gap between IT and OT for smarter, more efficient operations. We are excited to share that Red Hat has officially joined the LF Energy community, a community for collaboration on open industrial-grade technology platforms and agile specifications for the energy transition, to help facilitate a more efficient energy grid. Red Hat and LF Energy LF Energy is the premier community for collaboration on open industrial-grade technology platforms and agile standards for the digital energy transition. It is a community of digital experts at leading utilities, vendors, and supporters who are willing to rethink industry norms and find better ways to solve shared problems. Members of the LF Energy community are dedicated to building a vibrant, sustainable ecosystem of digital assets that are critical for delivering affordable, reliable, safe and clean energy. Red Hat recently participated in a project within LF Energy to provide CentOS Stream 9 support to SEAPATH, an open source software hypervisor designed for IEC 61850 Digital Substation Automation Systems. SEAPATH is designed to run with Virtual Protection Automation and Control (vPAC) Alliance, creating a synergy among multiple open source grid projects and a very strong community among digital substation stakeholders. By committing our deep engineering resources and technical expertise to projects like SEAPATH, Red Hat will provide a proven, enterprise-ready open source platform that serves as the foundation for the next generation of energy infrastructure, accelerating the development of a more flexible, reliable, secure, and sustainable grid.</description></item><item><title>Red Hat OpenShift AI achieves ISO 42001 AI certification, reinforcing Red Hat's leadership in responsible AI</title><link>https://kubermates.org/docs/2025-09-10-red-hat-openshift-ai-achieves-iso-42001-ai-certification-reinforcing-red-hat-s-l/</link><pubDate>Wed, 10 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-10-red-hat-openshift-ai-achieves-iso-42001-ai-certification-reinforcing-red-hat-s-l/</guid><description>Red Hat OpenShift AI achieves ISO 42001 AI certification, reinforcing Red Hat&amp;rsquo;s leadership in responsible AI More like this Blog post Blog post Original podcast Original podcast Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share To CIOs and CISOs overseeing critical production systems, AI can seem chaotic: It’s a dynamic, constantly evolving ecosystem, where current certifications and standards may not apply the same way, if at all. But AI can’t be ignored - properly implementing and integrating an AI strategy can be a massive boon to the enterprise, but the key word is “properly. ” Red Hat has always been committed to having our platforms conform to the certification and standards requirements of enterprise IT across industries. AI is no different, so we’re pleased to announce that Red Hat OpenShift AI has successfully achieved ISO 42001 certification, the first international standard designed for establishing, implementing, maintaining and continually improving an AI management system (AIMS). ISO 42001 provides a comprehensive framework to help organizations manage the unique risks and opportunities of enterprise AI. With its robust security feature set, Red Hat OpenShift AI can deliver: Enhanced customer data protection tooling demonstrated through robust security and privacy controls within Red Hat OpenShift AI which provides tools to enable a customer to increase its security posture and data handling within the platform. Industry standard alignment through a proactive approach to compliance, helping Red Hat OpenShift AI keep pace with evolving industry and regulatory expectations. Platform maturity alongside a strong compliance posture, which is validated by a third-party audit confirming the maturity and effectiveness of Red Hat&amp;rsquo;s internal compliance framework for AI. This provides greater assurances to our customers and partners that our AI development and deployment processes are governed appropriately and with greater consistency. Red Hat OpenShift AI, as part of Red Hat AI , provides an AI platform for managing the AI/(ML lifecycle at scale across on-premise and public cloud environments. It brings together data scientists and developers, with oversight from IT, to develop, train and fine tune generative (gen AI) and predictive models, deliver AI-enabled applications and bring models from experiments to production faster. ISO 42001 is the latest example of Red Hat’s continual effort to maintain consistent standards compliance across our entire product portfolio, including AI platforms.</description></item><item><title>Unlocking the next layer of AI adoption in the UK</title><link>https://kubermates.org/docs/2025-09-10-unlocking-the-next-layer-of-ai-adoption-in-the-uk/</link><pubDate>Wed, 10 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-10-unlocking-the-next-layer-of-ai-adoption-in-the-uk/</guid><description>Unlocking the next layer of AI adoption in the UK About the author Jonny Williams More like this Blog post Blog post Original podcast Original podcast Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Red Hat has formed a collaboration with Great Wave AI to make AI more usable for organisations of all sizes. Red Hat has a bold vision to help enterprises bridge their existing world and the new world of AI: supporting any model, on any accelerator, on any cloud, powered by open source. At this year’s Red Hat Summit , we made dozens of announcements extending our AI portfolio and ecosystem partnerships. Since then, we are excited to have formed a collaboration with Great Wave AI, a UK-based provider of an agentic generative AI (gen AI) platform. Together we aim to combine the scale, security features and flexibility of Red Hat AI with Great Wave’s “consumption layer” for multi-agent orchestration, making AI more usable for organisations of all sizes. This collaboration provides Red Hat customers with the ability to add an agentic layer on top of our enterprise-ready open source AI platforms, making it easier for them to tailor AI capabilities to their organization. Why AI orchestration? I recently had the opportunity to catch up with Jack Perschke, co-founder and chief executive officer of Great Wave AI, and discuss some of the trends shaping AI at the moment and what the collaboration means for both our companies. According to Jack, large language models (LLMs) are “like crude oil” in that they’re valuable, but need refinement to become usable products. Great Wave acts as the refinery turning raw model capability into fit-for-purpose tools, using their choice of hosting provider or LLM vendor. Jack explains, “Our platform makes it easier to switch between models, test applications with new models and host your own to meet specific security needs. In an era where prompts and responses can’t be encrypted inside the model itself, the safest option is often running it in your own environment. Red Hat makes this possible.</description></item><item><title>Announcing Gateway API Support for DigitalOcean Kubernetes</title><link>https://kubermates.org/docs/2025-09-09-announcing-gateway-api-support-for-digitalocean-kubernetes/</link><pubDate>Tue, 09 Sep 2025 21:28:56 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-09-announcing-gateway-api-support-for-digitalocean-kubernetes/</guid><description>Announcing Gateway API Support for DigitalOcean Kubernetes Key Benefits Why Move Beyond Ingress? The Gateway API Advantage The DigitalOcean Difference: Performance Powered by Cilium and eBPF Pricing Try the Gateway API on DOKS About the author Try DigitalOcean for free Related Articles What&amp;rsquo;s New on DigitalOcean App Platform Single Sign-On is Now Available, Strengthening Security and Simplifying Authentication Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform By Kang Xie Senior Product Manager Published: September 9, 2025 3 min read Managing traffic into a DigitalOcean Kubernetes cluster has long been the domain of the Ingress. While functional, it comes with limitations in flexibility, role separation, and advanced routing. Today, weâre excited to change that. We are thrilled to announce that the Kubernetes Gateway API, as a managed service, is pre-installed in all DigitalOcean Kubernetes (DOKS) clusters and ready to use at no additional cost. This next-generation traffic management solution is more expressive, extensible, and powerful than Ingress. Best of all, itâs powered by Ciliumâs high-performance eBPF implementation, offering superior performance and advanced routing capabilities without the overhead of traditional proxy-based solutions. Zero Configuration Required : Gateway API support comes pre-installed via Cilium in all DOKS clusters Zero Configuration Required : Gateway API support comes pre-installed via Cilium in all DOKS clusters Advanced Traffic Management : Support for header-based routing, traffic splitting, and canary deployments Advanced Traffic Management : Support for header-based routing, traffic splitting, and canary deployments Superior Performance : Ciliumâs eBPF implementation operates in kernel space, eliminating proxy overhead Superior Performance : Ciliumâs eBPF implementation operates in kernel space, eliminating proxy overhead Native Load Balancer Integration : Seamless integration with DigitalOcean Network Load Balancers Native Load Balancer Integration : Seamless integration with DigitalOcean Network Load Balancers Multi-tenant Ready : Built-in support for cross-namespace resource sharing with secure RBAC Multi-tenant Ready : Built-in support for cross-namespace resource sharing with secure RBAC Future-Proof API : Active development and standardization by the Kubernetes community Future-Proof API : Active development and standardization by the Kubernetes community The Gateway API was designed by the Kubernetes community to address the fundamental limitations of the Ingress API. It achieves this through a role-oriented resource model that separates infrastructure concerns from application routing. Cluster Operators manage Gateway resources, defining where and how traffic enters the cluster (e. g. , provisioning a DigitalOcean Load Balancer). Application Developers manage Route resources (like HTTPRoute), defining how traffic is routed to their specific applications.</description></item><item><title>Kubernetes v1.34: Snapshottable API server cache</title><link>https://kubermates.org/docs/2025-09-09-kubernetes-v1-34-snapshottable-api-server-cache/</link><pubDate>Tue, 09 Sep 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-09-09-kubernetes-v1-34-snapshottable-api-server-cache/</guid><description>Kubernetes v1.34: Snapshottable API server cache Evolving the cache for performance and stability Consistent reads from cache (Beta in v1.31) Taming large responses with streaming (Beta in v1.33) The missing piece Kubernetes 1.34: snapshots complete the picture A new era of API Server performance 🚀 How to get started Acknowledgements For years, the Kubernetes community has been on a mission to improve the stability and performance predictability of the API server. A major focus of this effort has been taming list requests, which have historically been a primary source of high memory usage and heavy load on the etcd datastore. With each release, we&amp;rsquo;ve chipped away at the problem, and today, we&amp;rsquo;re thrilled to announce the final major piece of this puzzle. etcd The snapshottable API server cache feature has graduated to Beta in Kubernetes v1.34, culminating a multi-release effort to allow virtually all read requests to be served directly from the API server&amp;rsquo;s cache. The path to the current state involved several key enhancements over recent releases that paved the way for today&amp;rsquo;s announcement. While the API server has long used a cache for performance, a key milestone was guaranteeing consistent reads of the latest data from it. This v1.31 enhancement allowed the watch cache to be used for strongly-consistent read requests for the first time, a huge win as it enabled filtered collections (e. g. &amp;ldquo;a list of pods bound to this node&amp;rdquo;) to be safely served from the cache instead of etcd, dramatically reducing its load for common workloads. Another key improvement was tackling the problem of memory spikes when transmitting large responses. The streaming encoder, introduced in v1.33, allowed the API server to send list items one by one, rather than buffering the entire multi-gigabyte response in memory. This made the memory cost of sending a response predictable and minimal, regardless of its size.</description></item><item><title>Navigating DORA with Calico: Strengthening Kubernetes Operational Resilience in Financial Services</title><link>https://kubermates.org/docs/2025-09-09-navigating-dora-with-calico-strengthening-kubernetes-operational-resilience-in-f/</link><pubDate>Tue, 09 Sep 2025 17:58:02 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-09-navigating-dora-with-calico-strengthening-kubernetes-operational-resilience-in-f/</guid><description>The Challenge DORA Seeks to Solve What is DORA? Why DORA Matters for Kubernetes 5 Ways Calico Products Can Help with DORA Compliance Granular Network Microsegmentation and Policy Enforcement Comprehensive Network Observability Threat Detection and Mitigation Automated Compliance Reporting and Audit Trails Secure Multi-Cluster and Hybrid Cloud Operations Mapping DORA Requirements to Calico Capabilities Summary A single cyberattack or system outage can threaten not just one financial institution, but the stability of a vast portion of the entire financial sector. For today’s financial enterprises, securing dynamic infrastructure like Kubernetes is a core operational and regulatory challenge. The solution lies in achieving DORA compliance for Kubernetes, which transforms your cloud-native infrastructure into a resilient, compliant, and secure backbone for critical financial services. Before DORA (Digital Operational Resilience Act), rules for financial companies primarily focused on making sure they had enough financial capital to cover losses. But what if a cyberattack or tech failure brought a large part of the financial system down? Even with plenty of financial capital, a major outage could stop most operations and cause big problems for the whole financial market. DORA steps in to fix this. It’s all about making sure financial firms can withstand, respond to, and recover quickly from cyberattacks and other digital disruptions. The Digital Operational Resilience Act (DORA) is a European Union (EU) regulation that came into effect on January 17, 2025 and is designed to strengthen the security of financial entities. It establishes uniform requirements across the financial sector for managing Information and Communication Technology (ICT) risk, reporting major ICT-related incidents, conducting digital operational resilience testing, and overseeing ICT third-party risk. DORA applies to a broad range of financial entities, including banks, insurance companies, investment firms, payment service providers, and their ICT third-party service providers, ensuring a consolidated and robust approach to digital resilience across the European Union. Kubernetes is the standard for deploying and managing cloud-native containerized applications, offering unparalleled agility, scalability, and efficiency. However, this power comes bundled with complexity.</description></item><item><title>How to build highly available Kubernetes applications with Amazon EKS Auto Mode</title><link>https://kubermates.org/docs/2025-09-09-how-to-build-highly-available-kubernetes-applications-with-amazon-eks-auto-mode/</link><pubDate>Tue, 09 Sep 2025 17:49:16 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-09-how-to-build-highly-available-kubernetes-applications-with-amazon-eks-auto-mode/</guid><description>How to build highly available Kubernetes applications with Amazon EKS Auto Mode Solution overview Test scenarios Pod fail scenario Node fail scenario AZ fail scenario Cluster version upgrade scenario Karpenter node disruption Conclusion About the authors As organizations scale their Kubernetes deployments, many find themselves facing critical operational challenges. Consider how DevOps teams spend countless hours planning and executing cluster upgrades, managing add-ons, and making sure that security patches are applied consistently. There is a clear need for reliable, automated cluster lifecycle management with teams struggling to maintain consistent cluster configurations and security postures across environments. Amazon Elastic Kubernetes Service (Amazon EKS) Auto Mode addresses these challenges by automating control plane updates, streamlining add-on management, and making sure that clusters maintain current best practices. This post explores the capabilities of EKS Auto Mode in depth, subjecting it to a series of challenging scenarios such as failure simulations, node recycling, and cluster upgrades—all while maintaining uninterrupted service traffic. This guide delves into strategies for achieving high availability in the face of the dynamic nature of EKS Auto Mode by using a range of Kubernetes features to maximize uptime. The goal is to provide a comprehensive guide that shows how to harness the potential of EKS Auto Mode, thus making sure that your services remain robust and resilient in the demanding environments. Although there is a wealth of comprehensive literature on the broader subject of reliability in container ecosystems, this post specifically narrows its scope to the nuanced considerations of operating reliable workloads within EKS Auto Mode environments. Before delving into the specifics of Amazon EKS and its Auto Mode feature , you must understand key Kubernetes concepts that are instrumental in maximizing service uptime during different cluster events. These foundational elements form the bedrock of resilient application architectures in Kubernetes environments, regardless of the specific cloud provider or management mode. Mastering these concepts enables you to use EKS Auto Mode effectively and build highly available systems that withstand various operational challenges. In the rest of this section we explore these essential Kubernetes features that play a pivotal role in maintaining service continuity during both planned and unplanned events.</description></item><item><title>Unlocking the Power of GenAI</title><link>https://kubermates.org/docs/2025-09-09-unlocking-the-power-of-genai/</link><pubDate>Tue, 09 Sep 2025 14:33:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-09-unlocking-the-power-of-genai/</guid><description>Related Articles Unlocking the Power of GenAI Analyst Insight Series: Conquering Complexity—the Bedrock of Successful Platform Engineering Transfer VMware Cloud Foundation (VCF) Operations Integration SDK Builds to a Private Container Registry in the deployed VCF Operations Environment The future of technology is no longer a distant dream, but a reality that’s rapidly unfolding before our eyes. Artificial intelligence (AI) has been making waves in various industries, but the latest breakthrough—generative AI (GenAI)—is poised to revolutionize the way we live, work, and interact with each other. Our new white paper, “Unlocking the Power of GenAI,” goes in depth with the building blocks for AI applications and how you can move beyond the basics. From the challenges of private GenAI to the latest innovations in model accuracy and evaluation, this comprehensive guide provides a deep dive into the latest advancements in the field. Broadcom and NVIDIA have collaborated to develop the joint AI platform called VMware Private AI Foundation with NVIDIA. This platform enables enterprises to fine-tune LLM models, deploy RAG workflows, and run inference workloads in their data centers, addressing privacy, choice, cost, performance, and compliance concerns. This joint platform simplifies AI deployments for enterprises by offering Model Store, Air-Gapped Support, Model Runtime, NVIDIA NIM TM , NVIDIA Blueprints, and more. In this white paper, we’ll take you on a journey through the world of GenAI, highlighting its key features, benefits, and use cases. You’ll learn how to: Select the right foundation model for your AI applications Deploy and secure your AI applications in production Evaluate and improve the accuracy of your AI models Leverage the latest innovations in GenAI to drive business success Whether you’re an AI enthusiast, a tech leader, or simply someone interested in the future of technology, this white paper is a must-read. Download our white paper today and unlock the power of GenAI for yourself. Ready to get started on your AI journey? Check out these helpful resources: Complete this form to contact us! Read the VMware Private AI Foundation with NVIDIA solution brief. Learn more about VMware Private AI Foundation with NVIDIA.</description></item><item><title>KubeCon + CloudNativeCon North America 2025 Co-located Event Deep Dive: Istio Day</title><link>https://kubermates.org/docs/2025-09-09-kubecon-cloudnativecon-north-america-2025-co-located-event-deep-dive-istio-day/</link><pubDate>Tue, 09 Sep 2025 13:21:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-09-kubecon-cloudnativecon-north-america-2025-co-located-event-deep-dive-istio-day/</guid><description>Who will get the most out of attending this event? What is new and different this year? What will the day look like? Should I do any homework first? Find your community! Posted on September 9, 2025 by Co-chairs: Fatih Degirmenci &amp;amp; Denis Jannot CNCF projects highlighted in this post November 10, 2025 Atlanta, Georgia Istio Day is the biannual community event for the industry’s most widely adopted and feature rich service mesh, where attendees will find lessons learned from running Istio in production, the latest updates on Istio’s ambient mode developments, and the opportunity to meet and learn from maintainers across the Istio ecosystem. This will be the fourth Istio Day. The first Istio Day was co-located with KubeCon Europe 2023, in Amsterdam. This colocated event replaces ServiceMeshCon, which was first held in 2019, before Istio was a CNCF project. Everyone from newcomers to seasoned professionals will benefit from this event. Whether you’re just starting with Istio or have extensive experience, you’ll gain new insights, discover emerging trends, and have the opportunity to engage directly with industry experts and fellow practitioners. This year, we’ve curated a selection of topics that provide fresh and diverse perspectives on Istio. The event features new voices, emerging technologies, and real-world use cases that reflect the latest industry trends, offering attendees valuable insights they won’t find elsewhere. The event will begin with a welcome from the chairs, followed by a series of technical sessions covering various aspects of Istio, including case studies, new features, and practical recipes. Throughout the event, there will be dedicated breaks for coffee and networking, giving attendees the chance to connect with speakers and fellow practitioners. The event will conclude with closing remarks summarizing key takeaways. Attendees are encouraged to familiarize themselves with the basics of Istio and review the schedule to identify sessions of interest.</description></item><item><title>A guide to Rekor Monitor and its integration with Red Hat Trusted Artifact Signer</title><link>https://kubermates.org/docs/2025-09-09-a-guide-to-rekor-monitor-and-its-integration-with-red-hat-trusted-artifact-signe/</link><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-09-a-guide-to-rekor-monitor-and-its-integration-with-red-hat-trusted-artifact-signe/</guid><description>A guide to Rekor Monitor and its integration with Red Hat Trusted Artifact Signer Rekor-monitor functionality Enabling Rekor-monitor in Red Hat Trusted Artifact Signer About the author Firas Ghanmi More like this Blog post Blog post Original podcast Original podcast Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Securing the software supply chain is paramount in today’s digital world. As more organizations adopt practices like keyless signing to verify the integrity of their software artifacts, the need for robust monitoring against the systems that maintain the software supply chain infrastructure is essential. One such system when signing and verifying content where monitoring can be applied is within the transparency log. While logs are tamper-evident, they are not tamper-proof, making a monitoring solution essential for an immutable and append-only record. This is where the Rekor Monitor comes in, providing an easy-to-use solution for verifying log consistency within a Red Hat Trusted Artifact Signer deployment. Red Hat Trusted Artifact Signer offers a keyless solution for signing and verifying software artifacts, making it a crucial element within Red Hat’s Trusted Software Supply Chain portfolio. By integrating Rekor-monitor, we can extend the principles of supply chain security by actively verifying the integrity of the private Rekor logs. This powerful combination of a verifiable log and continuous monitoring creates a robust audit trail, ensuring that the software supply chain remains secure, transparent, and compliant. Rekor-monitor, also known as Rekor Log Monitor, is a tool that continuously checks the integrity of Rekor transparency logs. It is designed to run periodically at a specified interval to verify that the Rekor transparency log remains append-only and immutable. For easy observability, the monitor also exposes Prometheus based metrics, which can be used by monitoring systems to create dashboards and set up alerts based on key metrics. For example, you could configure an alert to notify you immediately if a consistency check fails, indicating a potential compromise of the log.</description></item><item><title>Senior Machine Learning Engineer on Red Hat's AI Inference Team</title><link>https://kubermates.org/docs/2025-09-09-senior-machine-learning-engineer-on-red-hat-s-ai-inference-team/</link><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-09-senior-machine-learning-engineer-on-red-hat-s-ai-inference-team/</guid><description>Senior Machine Learning Engineer on Red Hat&amp;rsquo;s AI Inference Team Red Hat Learning Subscription | Product Trial About the authors Brian Dellabetta Holly Bailey Vanshika Arora More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Brian D. is a senior machine learning (ML) engineer on our AI Inference team, which is part of the broader AI Engineering team at Red Hat. Based remotely in Chicago, Brian helps maintain LLM Compressor , a key component of vLLM (an open source inference server originally developed at UC Berkeley, and now supported by a global community). vLLM is designed to make AI inference —in other words, responses from models—more efficient. Through LLM Compressor, Brian and his team make it possible to optimize and deploy LLMs so they run faster, consume less energy, and operate on fewer GPUs, without compromising performance. The resulting impact? Lower computational barriers to working with AI—which, in turn, opens the door for more organizations, researchers, and innovators to use these models in meaningful ways. We sat down with Brian to learn more about his journey, his team, and life as a ML engineer. Tell us about your journey to Red Hat and AI I joined Red Hat in January through the acquisition of Neural Magic. I was actually in the middle of the interview process with Neural Magic when the acquisition was announced. It was great timing: my first week was the same week the entire Neural Magic team met in Boston for Red Hat’s new hire orientation. I’ve been in the AI/ML field for several years, but I’m still new to this role at Red Hat. My career path has been a gradual shift toward AI.</description></item><item><title>Analyst Insight Series: Conquering Complexity—the Bedrock of Successful Platform Engineering</title><link>https://kubermates.org/docs/2025-09-08-analyst-insight-series-conquering-complexity-the-bedrock-of-successful-platform-/</link><pubDate>Mon, 08 Sep 2025 22:07:04 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-08-analyst-insight-series-conquering-complexity-the-bedrock-of-successful-platform-/</guid><description>About the Author Related Articles Analyst Insight Series: Conquering Complexity—the Bedrock of Successful Platform Engineering Transfer VMware Cloud Foundation (VCF) Operations Integration SDK Builds to a Private Container Registry in the deployed VCF Operations Environment VMware Cloud Foundation - Cloud on Your Terms Complexity has always been a core challenge when deploying cloud-native software such as Kubernetes. By extension, it is also a key consideration when implementing platform engineering practices and processes to manage cloud-native environments. To overcome the array of complexity-related challenges across infrastructures, applications, tools, and teams, enterprise organizations must enable simplified operations and maintain consistency with federated platforms that can integrate and evolve with a variety of technologies. Consolidated, unified platforms that support integration across venues and software components are critical to overcoming complexity. For Kubernetes, this also means supporting the many different open source components that make up an effective deployment, including orchestration, monitoring and observability, and security functions. Further, as cloud-native constructs and containers are deployed across a broader swath of enterprise applications, teams must centrally and consistently manage both container-based and VM-based applications on a single platform. Much like DevOps, platform engineering relies on breaking down silos within an organization to support teams and personas beyond software developers and IT admins (e. g. , cloud admins, platform engineers, security teams) with self-service capabilities, as well as aligned objectives and incentives to foster collaboration. Comprehensive, end-to-end platforms that seamlessly connect compute, storage, networking, and security resources with unified lifecycle management, and that also facilitate integrations across the software development and deployment process, can help enterprises to better support their broad array of applications and teams. This can help address complexity issues by allowing teams to centrally and consistently manage platforms using a common skillset, further reducing costs by reducing the need for new hires and talent acquisition. Conquering complexity and integration challenges associated with platform engineering can enable improved developer productivity, faster delivery time, improved security and higher-quality code—sought-after benefits that are frequently identified in our Voice of the Enterprise surveys.</description></item><item><title>Kubernetes v1.34: VolumeAttributesClass for Volume Modification GA</title><link>https://kubermates.org/docs/2025-09-08-kubernetes-v1-34-volumeattributesclass-for-volume-modification-ga/</link><pubDate>Mon, 08 Sep 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-09-08-kubernetes-v1-34-volumeattributesclass-for-volume-modification-ga/</guid><description>Kubernetes v1.34: VolumeAttributesClass for Volume Modification GA What is VolumeAttributesClass? What is new from Beta to GA Cancel support from infeasible errors Quota support based on scope Drivers support VolumeAttributesClass Contact The VolumeAttributesClass API, which empowers users to dynamically modify volume attributes, has officially graduated to General Availability (GA) in Kubernetes v1.34. This marks a significant milestone, providing a robust and stable way to tune your persistent storage directly within Kubernetes. At its core, VolumeAttributesClass is a cluster-scoped resource that defines a set of mutable parameters for a volume. Think of it as a &amp;ldquo;profile&amp;rdquo; for your storage, allowing cluster administrators to expose different quality-of-service (QoS) levels or performance tiers. Users can then specify a volumeAttributesClassName in their PersistentVolumeClaim (PVC) to indicate which class of attributes they desire. The magic happens through the Container Storage Interface (CSI): when a PVC referencing a VolumeAttributesClass is updated, the associated CSI driver interacts with the underlying storage system to apply the specified changes to the volume. volumeAttributesClassName This means you can now: Dynamically scale performance: Increase IOPS or throughput for a busy database, or reduce it for a less critical application. Optimize costs: Adjust attributes on the fly to match your current needs, avoiding over-provisioning. Simplify operations: Manage volume modifications directly within the Kubernetes API, rather than relying on external tools or manual processes. There are two major enhancements from beta. To improve resilience and user experience, the GA release introduces explicit cancel support when a requested volume modification becomes infeasible. If the underlying storage system or CSI driver indicates that the requested changes cannot be applied (e.</description></item><item><title>Combatting Sophisticated Cybersecurity Threats with AI</title><link>https://kubermates.org/docs/2025-09-08-combatting-sophisticated-cybersecurity-threats-with-ai/</link><pubDate>Mon, 08 Sep 2025 16:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-08-combatting-sophisticated-cybersecurity-threats-with-ai/</guid><description>Smarter, faster, and more dangerous Intrusion Detection Systems (IDS) as your early warning system Introducing an Intrusion Prevention System (IPS) to complement your IDS as a proactive defence Leveraging Cloud-Native Endpoint Security for fully-remote or hybrid organisational setups User Behaviour Analytics (UBA) Security is about layering, not luck Posted on September 8, 2025 by Ana Ferreira, Software Engineer, YLD As AI continues to evolve, businesses are rapidly integrating it into their operations. But with this growth comes an urgent need to prioritise cybersecurity because ignoring security risks in an AI-driven landscape can leave businesses vulnerable to sophisticated threats. Cybersecurity isn’t about relying on a single tool but rather building layered defences. A strategic combination of firewalls, encryption, and advanced threat detection systems can significantly reduce cyber risks while ensuring your organisation runs smoothly. When security is effectively managed, uncertainty is reduced, and your organisation can operate with confidence, allowing you to focus on what truly matters: scaling your business, driving innovation, and staying ahead of the competition. This article explores key cybersecurity tools and how they work together to build a robust security strategy. Whether you’re looking to enhance your existing defences or implement new solutions, these insights will help you strengthen your organisation’s security posture. Cybersecurity is no longer just an IT concern, but rather a boardroom priority. With AI-driven cyber threats no longer limited to generic phishing emails or outdated malware, today’s attacks are smarter, more precise, and alarmingly convincing. In 2023, Darktrace reported a staggering 135% increase in social engineering attacks. These aren’t just your average phishing attempts – they’re highly targeted, AI-generated, and built to exploit human trust. Tools like wormGPT and FraudGPT are being used by attackers to create realistic, personalised, and highly deceptive campaigns that can slip past traditional security filters.</description></item><item><title>Accelerate your Red Hat Enterprise Linux 10 skills with Red Hat Learning Subscription</title><link>https://kubermates.org/docs/2025-09-08-accelerate-your-red-hat-enterprise-linux-10-skills-with-red-hat-learning-subscri/</link><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-08-accelerate-your-red-hat-enterprise-linux-10-skills-with-red-hat-learning-subscri/</guid><description>Accelerate your Red Hat Enterprise Linux 10 skills with Red Hat Learning Subscription Introducing Lessons: Your path to accelerated skills development Where to start Red Hat Enterprise Linux technical overview (RH024) New features in Red Hat Enterprise Linux (RH304) Red Hat system administration (RH124) Accelerate your skills Red Hat Enterprise Linux | Product trial About the author Mary Margaret Barnes More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Announced at Red Hat Summit earlier this year, Red Hat Enterprise Linux 10 is designed to meet the demands of hybrid cloud, automation, and AI-driven IT environments. RHEL10 provides a strategic and intelligent foundation for modern IT operations. With enhanced capabilities to navigate complexity, accelerate innovation, and strengthen security, organizations can confidently build a more resilient and future-ready infrastructure. With new capabilities powered by Red Hat Enterprise Linux Lightspeed you can streamline complex tasks with confidence using the skills learned in the content, freeing up more time to focus on driving innovation and a resilient infrastructure. Now is the perfect time to leverage the Red Hat Learning Subscription to update your skills and explore a wide range of training options. A key highlight of Red Hat Learning Subscription is the new lessons feature—topic-based, concise training sessions designed to help you quickly gain targeted skills. Lessons offer the flexibility to learn in manageable segments while addressing skill gaps for your role, specific tasks, or certification preparation. Each Lesson is also supported by the same comprehensive hands-on lab environments, giving you the practical experience, techniques, and tips needed to confidently apply what you learn. With the Red Hat Learning Subscription, you gain access to a complete training solution designed to help you stay ahead. Red Hat Learning Subscription includes a catalog of Red Hat Enterprise Linux training courses, focused lessons for quick learning, and hands-on labs to practice. Whether your goal is to earn Red Hat certifications or sharpen your Linux expertise, Red Hat Learning Subscription provides the tools to learn faster, practice smarter, and build Red Hat Enterprise Linux 10 skills with confidence. Discover what’s new in Red Hat Enterprise Linux 10 and why upgrading your skills is critical to support modern IT strategies in hybrid cloud and AI environments.</description></item><item><title>Establishing a sustainable automation community of practice</title><link>https://kubermates.org/docs/2025-09-08-establishing-a-sustainable-automation-community-of-practice/</link><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-08-establishing-a-sustainable-automation-community-of-practice/</guid><description>Establishing a sustainable automation community of practice What is a community of practice? Real-world automation community of practice success Post-community of practice implementation outcomes Recommendations for your community of practice A unified automation platform Leadership sponsorship and resource commitment Designated team representation Organizational alignment and buy-in Implementation timeline Months 1–2: Foundation building Months 3–4: Joint execution and learning Months 5–6+: Maturing and scaling Ongoing expectations Additional resources Red Hat Ansible Automation Platform | Product Trial About the author Lee Armbuster More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share A community of practice is a collaborative framework in which stakeholders across teams unify efforts to build automation capabilities. The community of practice fosters cross-functional communication, standardizes automation practices, and accelerates innovation through the sharing of knowledge, ideas, and lessons learned. It serves as a hub across disparate yet adjacent teams for the reuse of solutions and the development of consistent operational models. Note: The terms center of excellence, community of excellence, and community of practice are often used interchangeably within the industry. You can learn more about how Red Hat differentiates between a center of excellence and a community of practice in this blog. Regardless of terminology used, what&amp;rsquo;s most important is to focus on the right approaches, activities, and outcomes. Recently, Red Hat partnered with a government organization to establish a structured automation community of practice that would unify automation efforts across disparate technical teams. Initial challenges Before the implementation of the community of practice, the organization experienced significant challenges due to a lack of standardization, including redundant automation development use cases, minimal cross-team collaboration and knowledge sharing, and increased time and budget costs associated with supporting multiple tools and frameworks. Once the community of practice was in place, designated representatives from different domains began meeting regularly to: Share automation progress, blockers, and lessons learned. Collaboratively troubleshoot and review each other’s work. Align on reusable automation content and development standards. Using the learnings from the government organization, you can establish your own automation communities of practice.</description></item><item><title>Is your RHEL installation getting old? Here's what to do</title><link>https://kubermates.org/docs/2025-09-08-is-your-rhel-installation-getting-old-here-s-what-to-do/</link><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-08-is-your-rhel-installation-getting-old-here-s-what-to-do/</guid><description>Is your RHEL installation getting old? Here&amp;rsquo;s what to do Understanding the RHEL Lifecycle Special scenarios Extended update support (EUS) Extended and enhanced update support Extended lifecycle support (ELS) and the long life add-on RHEL Ssecurity Sselect Add-On The importance of keeping your systems up to date Your upgrade options LEAPP (in-place upgrade) Redeploy (clean install) Automation for updates and upgrades How Red Hat can help Red Hat Enterprise Linux | Product trial About the author Alessandro Rossi More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Red Hat Enterprise Linux (RHEL) is the enterprise platform for mission-critical workloads and it allows companies across the globe to run their businesses in a secure, reliable and supported way. But running an outdated version of any operating system can expose your organization to security risks and prevent you from leveraging the latest features and performance improvements. We know it can sometimes be challenging and time-consuming to stay updated, so in this article I explain the RHEL lifecycle , the importance of staying current, and your options for upgrading. Every version of RHEL has a lifecycle that defines the level of support it receives over time. We can easily summarize its lifecycle as divided into three main phases: Full support phase: During this phase, your RHEL system receives all security updates, bug fixes, and new features. This is the ideal phase for your production systems to be in. Maintenance support phase: In this phase, Red Hat provides critical and important security updates and urgent bug fixes. However, during maintenance support, no new features or hardware enablement are provided. Extended life phase (ELP): After the maintenance support phase, your RHEL version enters the extended life phase. During this time, you have access to a limited set of software maintenance services. RHEL is used in a diverse range of environments, asd sometimes there are specific scenarios that require special attention. For example, you may be involved with: Third-party software vendors certifying and supporting specific RHEL minor versions.</description></item><item><title>Taming hybrid cloud complexity: A path to true agility</title><link>https://kubermates.org/docs/2025-09-08-taming-hybrid-cloud-complexity-a-path-to-true-agility/</link><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-08-taming-hybrid-cloud-complexity-a-path-to-true-agility/</guid><description>Taming hybrid cloud complexity: A path to true agility Learn more about Red Hat OpenShift for Cloud-Native Application Platforms Want guidance on other tech challenges? Red Hat OpenShift Container Platform | Product Trial About the author Marco Bill More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share IT leaders face many challenges today, but few are as daunting as this: How do you keep up with the pace of change when your existing IT infrastructure is already so complex? For many enterprises, the primary source of that complexity is the struggle to manage containerized applications at scale across diverse environments—from on-premises data centers to multiple public clouds. This fragmentation can lead to inconsistent operations and vendor lock-in, which stifles agility and innovation. As we navigate the era of AI, IT leaders need a unified, predictable way to manage it all so that your teams can focus on higher-value work. I&amp;rsquo;ve had the unique opportunity to see this problem from both sides. In my previous roles leading customer-facing teams, I saw firsthand how our customers leveraged Red Hat OpenShift to tackle their most complex challenges, such as streamlining their hybrid cloud experiences and reducing operational complexity. It was profoundly rewarding to see them use our platform to break free from the status quo. Now, as the CIO of Red Hat, my perspective has shifted to that of an internal customer. I see the immense value of OpenShift even more clearly because we also use it to run Red Hat’s business. We are our own customer zero, not because our leadership team mandates it, but because the product works for us. Our experience has been a testament to the platform&amp;rsquo;s power to create consistency across our hybrid cloud environment, which has been critical for managing our global operations. Our teams have seen significant improvements, including shorter cycle times from code to production and better standardization of application architectures. By using OpenShift, our applications have improved cloud agility, allowing them to move across our hybrid cloud environment without concern for the underlying infrastructure.</description></item><item><title>Why upgrade to Red Hat Enterprise Linux 9.6 or 10 now</title><link>https://kubermates.org/docs/2025-09-08-why-upgrade-to-red-hat-enterprise-linux-9-6-or-10-now/</link><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-08-why-upgrade-to-red-hat-enterprise-linux-9-6-or-10-now/</guid><description>Why upgrade to Red Hat Enterprise Linux 9.6 or 10 now RHEL Lightspeed Image mode Post-quantum cryptography RHEL lifecycle Red Hat Enterprise Linux | Product trial About the author Gil Cattelain More like this Blog post Blog post Original podcast Original podcast Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share My name is Gil Cattelain, and I&amp;rsquo;m a Product Marketing Manager here at Red Hat. I joined the company over four years ago, and I manage the Red Hat Enterprise Linux (RHEL) marketing releases and launches. You&amp;rsquo;ve likely read some of my blog posts, which I typically publish with every minor or major release, but this article highlights the importance of upgrading. If you&amp;rsquo;re still on RHEL 6, RHEL 7, or even RHEL 8, then you should consider upgrading to RHEL 9.6 or a later version to take advantage of all the cool new features included in those versions. Why should you seriously consider upgrading to RHEL 9.6 or 10? That&amp;rsquo;s easy: To immediately leverage the groundbreaking technologies we&amp;rsquo;ve recently shipped. All new features included in RHEL 10 are also in RHEL 9.6, such as image mode and RHEL Lightspeed. While post-quantum cryptography isn&amp;rsquo;t yet in RHEL 9. x, it is planned for the upcoming RHEL 9.7 release. As you plan your upgrade from previous versions, make the leap to RHEL 9.6, RHEL 9.7, or even RHEL 10 to unlock these cutting-edge technologies! A significant new feature in both RHEL 9.6 and RHEL 10 is the RHEL Lightspeed command-line assistant. This powerful and optional AI assistant is directly integrated into the RHEL command-line interface, offering a revolutionary way to work. It can instantly and accurately answer RHEL-related questions, assist with troubleshooting to resolve issues faster, and easily decipher complex log entries, along with many other tasks to streamline daily operations. This command-line assistant provides a natural language interface, incorporating information from valuable resources like the RHEL documentation and release notes, thereby empowering organizations with an incredibly powerful tool for easier management, troubleshooting, and overall interaction with their RHEL systems.</description></item><item><title>GSoC 2025: Meet Our Projects and Contributors 🚀</title><link>https://kubermates.org/docs/2025-09-06-gsoc-2025-meet-our-projects-and-contributors/</link><pubDate>Sat, 06 Sep 2025 00:00:00 -0500</pubDate><guid>https://kubermates.org/docs/2025-09-06-gsoc-2025-meet-our-projects-and-contributors/</guid><description>Introduction 📚 Project Highlights Project 1: Kubeflow Platform Enhancements Project 2: KServe Models Web Application Modernization Project 3: Istio CNI and Ambient Mesh Project 4: Deploying Kubeflow with Helm Charts Project 5: JupyterLab Plugin for Kubeflow Project 6: Spark Operator with Kubeflow Notebooks Project 7: GPU Testing for LLM Blueprints Project 10: Support Volcano Scheduler in Kubeflow Trainer Project 12: Empowering Kubeflow Documentation with LLMs 🤖 🎉 Wrapping Up 👩‍💻 Want to Get Involved? Introduction 📚 Project Highlights Project 1: Kubeflow Platform Enhancements Project 2: KServe Models Web Application Modernization Project 3: Istio CNI and Ambient Mesh Project 4: Deploying Kubeflow with Helm Charts Project 5: JupyterLab Plugin for Kubeflow Project 6: Spark Operator with Kubeflow Notebooks Project 7: GPU Testing for LLM Blueprints Project 10: Support Volcano Scheduler in Kubeflow Trainer Project 12: Empowering Kubeflow Documentation with LLMs 🤖 Project 1: Kubeflow Platform Enhancements Project 2: KServe Models Web Application Modernization Project 3: Istio CNI and Ambient Mesh Project 4: Deploying Kubeflow with Helm Charts Project 5: JupyterLab Plugin for Kubeflow Project 6: Spark Operator with Kubeflow Notebooks Project 7: GPU Testing for LLM Blueprints Project 10: Support Volcano Scheduler in Kubeflow Trainer Project 12: Empowering Kubeflow Documentation with LLMs 🤖 🎉 Wrapping Up 👩‍💻 Want to Get Involved? Google Summer of Code (GSoC) 2025 has been an exciting journey for the Kubeflow community! We are very grateful for Google and the open source community members dedication and effort. 🎉 This year, 9 contributors from around the world collaborated with mentors to improve different parts of the Kubeflow ecosystem — from infrastructure and CI/CD, to notebooks, ML workflows, and beyond. In this blog, we are highlighting all the projects that were part of GSoC 2025 , their goals, the impact they’ve created, and the amazing contributors behind them. 👉 You can explore the full list on our GSoC 2025 page. Below are the projects from this year’s GSoC. Each section includes a short summary, contributor details, and links to project resources. Contributor: Harshvir Potpose ( @akagami-harsh ) Mentors: Julius von Kohout ( @juliusvonkohout ) Overview: We need an up to date S3 storage with hard multi-tenancy and run our containers with PodSecurityStandards restricted. MinIO transitioned to the AGPLv3 license in 2021, creating significant compliance challenges for the project. This project addressed this critical blocker by implementing SeaweedFS as a production-ready replacement for MinIO. SeaweedFS offers a more permissive Apache 2.0 license while providing superior performance characteristics and enterprise-grade security and reliability. Key Outcomes: Provided S3 storage with hard multi-tenancy Successfully migrated to SeaweedFS as a secure replacement for MinIO and integrated it into Kubeflow Pipelines Eliminated MinIO’s licensing constraints by adopting SeaweedFS’s more permissive license model Implemented comprehensive CI tests for SeaweedFS deployment and namespace isolation functionality Strengthened the manifests repository’s CI pipeline and contributed to the dashboard migration efforts Enforcing PodSecurityStandards baseline/restricted Resources: 📄 Project Page ✍️ Personal Blog: Kubeflow Pipelines Embraces SeaweedFS Contributor: (GitHub: @LogicalGuy77 ) Mentors: Griffin Sullivan ( @Griffin-Sullivan ), Julius von Kohout ( @juliusvonkohout ) Overview: This project revived and modernized the KServe Models Web Application (Angular + Flask), the UI used to manage machine learning inference services in Kubeflow via KServe. What began as a small Node.</description></item><item><title>Kubernetes v1.34: Pod Replacement Policy for Jobs Goes GA</title><link>https://kubermates.org/docs/2025-09-05-kubernetes-v1-34-pod-replacement-policy-for-jobs-goes-ga/</link><pubDate>Fri, 05 Sep 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-09-05-kubernetes-v1-34-pod-replacement-policy-for-jobs-goes-ga/</guid><description>Kubernetes v1.34: Pod Replacement Policy for Jobs Goes GA About Pod Replacement Policy How Pod Replacement Policy works Example How can you learn more? Acknowledgments Get involved In Kubernetes v1.34, the Pod replacement policy feature has reached general availability (GA). This blog post describes the Pod replacement policy feature and how to use it in your Jobs. By default, the Job controller immediately recreates Pods as soon as they fail or begin terminating (when they have a deletion timestamp). As a result, while some Pods are terminating, the total number of running Pods for a Job can temporarily exceed the specified parallelism. For Indexed Jobs, this can even mean multiple Pods running for the same index at the same time. This behavior works fine for many workloads, but it can cause problems in certain cases. For example, popular machine learning frameworks like TensorFlow and JAX expect exactly one Pod per worker index. If two Pods run at the same time, you might encounter errors such as: /job:worker/task:4: Duplicate task registration with task_name=/job:worker/replica:0/task:4 /job:worker/task:4: Duplicate task registration with task_name=/job:worker/replica:0/task:4 Additionally, starting replacement Pods before the old ones fully terminate can lead to: Scheduling delays by kube-scheduler as the nodes remain occupied. Unnecessary cluster scale-ups to accommodate the replacement Pods. Temporary bypassing of quota checks by workload orchestrators like Kueue. With Pod replacement policy, Kubernetes gives you control over when the control plane replaces terminating Pods, helping you avoid these issues. This enhancement means that Jobs in Kubernetes have an optional field.</description></item><item><title>Considerations when doing AI on Kubernetes</title><link>https://kubermates.org/docs/2025-09-05-considerations-when-doing-ai-on-kubernetes/</link><pubDate>Fri, 05 Sep 2025 16:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-05-considerations-when-doing-ai-on-kubernetes/</guid><description>Final Thoughts Posted on September 5, 2025 by Drishti Gupta, Senior Cloud Architect for Google Cloud CNCF projects highlighted in this post As more teams start weaving generative AI (GenAI) into their apps and workflows, Kubernetes naturally comes up as the go-to platform. It’s a tried-and-tested solution for managing containerized workloads, but AI workloads are a different beast. Here’s a rundown of what you should think about—and which tools can help—when running AI workloads in cloud-native environments. GenAI Workloads Need Event-Driven Infrastructure GenAI features often hinge on user prompts, streaming data, or background jobs. That means you need infrastructure that’s reactive, scalable, and lean. Knative Serving : Great for HTTP-based GenAI services (like LLM APIs). It automatically scales up when requests come in, and scales to zero when they don’t. Perfect for saving money on GPU-bound workloads. KEDA : Adds event-driven autoscaling based on external sources like Kafka, RabbitMQ, or Prometheus. It complements Knative by widening the scope of what can trigger scaling. Together, they give you a nimble setup that reacts fast and keeps infra costs manageable. Things to consider when serving LLMs in Cloud-Native Environments Cloud-native tooling provides robust building blocks to tackle the considerations below.</description></item><item><title>Beyond the docs: Kartik's journey to Brno and Red Hat</title><link>https://kubermates.org/docs/2025-09-05-beyond-the-docs-kartik-s-journey-to-brno-and-red-hat/</link><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-05-beyond-the-docs-kartik-s-journey-to-brno-and-red-hat/</guid><description>Beyond the docs: Kartik&amp;rsquo;s journey to Brno and Red Hat Red Hat Learning Subscription | Product Trial About the authors Holly Bailey Vanshika Arora More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share We recently caught up with Kartikeya Baid Dwivedi (known as Kartik), a longtime Red Hatter with a varied career path and a passion for shaping great user experiences. Kartik currently leads the Core Platforms and Hybrid Cloud Infrastructure documentation teams. Additionally, Kartik is a member of the Czech Leadership Council. Along with sharing about his work in technical documentation and what led him to Red Hat, Kartik gives us some insights into his relocation from India to the Czech Republic, the impact of his work in the Czech Leadership Council, and his hobbies as a hobby pilot and sous chef. Consider us intrigued! Could you tell us about your current role and team? In my role in Customer Content Services (CCS), also known as the Docs team, I manage the Core Platforms and Hybrid Cloud Infrastructure documentation teams, which create product documentation for many products including Red Hat Enterprise Linux (RHEL), Red Hat In-Vehicle Operating System (RHIVOS), Red Hat Satellite, Red Hat OpenStack, and Red Hat OpenShift Virtualization. I also dedicate time to the Czech Leadership Council, which supports associate engagement, strategic alignment, and leadership development across our Czech Republic office sites. Where are you currently based? Do you work remotely, in-person, or a bit of both? I live in lovely Brno, having moved there specifically for Red Hat in 2016. My wife and I fell in love with it. When I’m not traveling for work to the U. S. or India, I work in the Brno office between 3 and 4 days a week. Also, I’ve had the opportunity to visit approximately 20+ Red Hat offices worldwide so far! How did you start your career, and how did your journey lead you to Red Hat? I started my 20+ career as a web developer.</description></item><item><title>Friday Five — September 5, 2025</title><link>https://kubermates.org/docs/2025-09-05-friday-five-september-5-2025/</link><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-05-friday-five-september-5-2025/</guid><description>Friday Five — September 5, 2025 Analytics Insight : The Blind Spots in AI Security That Could Cost Us All: Brian Stevens, SVP &amp;amp; CTO – AI, Red Hat, Explains the Unsolved Challenges Techzine : Red Hat strives for simplicity in an ever more complex IT world Technically Speaking : Taming AI agents with observability ft. Bernd Greifeneder Security beyond the model: Introducing AI system cards ITWeb : Ready or not – quantum computing is coming About the author Red Hat Corporate Communications More like this Blog post Blog post Blog post Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share An interview with Red Hat&amp;rsquo;s Brian Stevens explores how to securely and efficiently scale AI in the enterprise. He highlights the &amp;ldquo;last mile of AI adoption&amp;rdquo; and Red Hat&amp;rsquo;s new AI Inference Server, which helps streamline deployment from months to days. Learn more This article from Techzine examines how Red Hat is making technologies enterprise-ready and helping customers achieve &amp;ldquo;digital autonomy. &amp;quot; It covers innovations like &amp;lsquo;bootc in image mode&amp;rsquo; in RHEL 10 and the AI-powered Red Hat Enterprise Linux Lightspeed. Learn more This episode explores the shift in enterprise IT from using AI as a tool to building and managing systems powered by autonomous AI agents. Learn more Red Hat is introducing a more comprehensive approach to AI documentation with the new AI system cards, which go beyond model cards to provide critical security and safety information about an entire AI system. Learn more Red Hat&amp;rsquo;s Bruce Busansky discusses how quantum computing is an inevitable force in digital transformation. He explains how Red Hat is leveraging its technology, including the acquisition of Neural Magic, to help organizations future-proof with open source and AI. Learn more Red Hat is the world’s leading provider of enterprise open source software solutions, using a community-powered approach to deliver reliable and high-performing Linux, hybrid cloud, container, and Kubernetes technologies. Red Hat helps customers integrate new and existing IT applications, develop cloud-native applications, standardize on our industry-leading operating system, and automate, secure, and manage complex environments. Award-winning support, training, and consulting services make Red Hat a trusted adviser to the Fortune 500.</description></item><item><title>Calico Egress Gateway: A Cost-Effective NAT for Kubernetes</title><link>https://kubermates.org/docs/2025-09-04-calico-egress-gateway-a-cost-effective-nat-for-kubernetes/</link><pubDate>Thu, 04 Sep 2025 20:51:58 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-04-calico-egress-gateway-a-cost-effective-nat-for-kubernetes/</guid><description>The Need for a Kubernetes NAT Gateway The Challenge With Cloud NAT Gateways Calico’s Built-in NAT for Kubernetes Egress How Calico Egress Gateway works as a NAT gateway – a simple example: Advantages of Calico Egress Gateway Used as a NAT Use Cases for Calico Egress Gateway and NAT Examples of Calico Egress Gateway in Action Preventing Data Exfiltration with Firewalls Enabling Trusted Access to Databases Behind a Firewall The Bottom Line When Kubernetes workloads need to connect to the outside world, whether to access external APIs, integrate with external systems, or connect to partner networks, they often face a unique challenge. The problem? Pod IP addresses inside Kubernetes clusters are dynamic and non-routable. For external systems to recognize and trust this traffic, workloads need a consistent, dependable identity. This means outbound connections require fixed, routable IP addresses that external services can rely on. This is where Network Address Translation (NAT) becomes essential. It assigns Kubernetes pods with a static, consistent IP for all outbound traffic, ensuring those connections work properly. If you’re running Kubernetes in the cloud, a common solution is to use your cloud provider’s managed NAT gateway service. These are easy to use, but they can come at a cost. In AWS, Azure, and Google Cloud, cloud-managed NAT gateways charge both an hourly fee and a per-gigabyte data processing fee. For high-traffic deployments, those charges can quickly add up, sometimes even exceeding your compute costs. The good news: with Calico, you can handle NAT from inside your Kubernetes cluster, avoiding cloud NAT gateway fees and giving you more control over how egress works. Managed NAT gateways from cloud providers are designed for convenience, but they come with a few limitations: Ongoing hourly charges – even if you’re not sending much traffic.</description></item><item><title>New Research Reveals: Cloud Maturity is the Key to Private Cloud Success</title><link>https://kubermates.org/docs/2025-09-04-new-research-reveals-cloud-maturity-is-the-key-to-private-cloud-success/</link><pubDate>Thu, 04 Sep 2025 19:04:09 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-04-new-research-reveals-cloud-maturity-is-the-key-to-private-cloud-success/</guid><description>Enterprises at mature stages of cloud adoption are 3x more likely to achieve their cloud goals and see dramatically better results across satisfaction, cost efficiency, and performance. The Private Cloud Renaissance The Maturity Advantage: A Game-Changing Discovery The Strategic Repatriation Trend What Separates the Leaders The Path Forward Ready to Assess Your Cloud Maturity? Related Articles New Research Reveals: Cloud Maturity is the Key to Private Cloud Success Broadcom and Canonical Partner to Fast-track and Secure Containerized Workload Deployments on VMware Cloud Foundation Strengthened Cyber-Risk Management and Compliance for Large-Scale VMware Cloud Foundation Environments Today, we’re excited to announce the release of our special edition report: “ Private Cloud Outlook: The Maturity Advantage. ” The research reveals a pivotal shift in how enterprises approach cloud strategy—and uncovers a critical success factor that separates high-performing organizations from the rest. Enterprises are fundamentally rethinking their cloud strategies, placing private cloud at the center to address today’s most pressing business challenges. While security, cost control, and data governance remain primary drivers, our research shows that private cloud has evolved into a powerful enabler for modern workloads, including AI initiatives. But here’s what’s truly fascinating: the benefits of private cloud aren’t just about the technology—they are about deployment maturity which also brings along people and process. When we analyzed responses through the lens of cloud maturity, a striking pattern emerged that reveals why some enterprises are pulling dramatically ahead of their peers. Enterprises that are mature in their cloud journey experience significantly superior results in strategic impact and operational excellence. They are investing in their infrastructure to drive business goals: 59% are increasing their private cloud investments, compared to 38% of developing-stage peers 60% rank private cloud workloads as their top IT priority, compared to 41% of developing-stage peers 90%+ report satisfaction in infrastructure operations, application deployment, security and compliance, compared to 50-65% satisfaction for developing-stage peers Perhaps most telling is the workload repatriation trend we discovered. Mature enterprises are 3x more likely to have repatriated workloads from public cloud , with 50% having brought modern, cloud-native applications back to private infrastructure. This isn’t about reversing failed migrations—it’s a deliberate, strategic move driven by heightened security concerns, rising costs, and the growing demands of modern workloads like AI. Our research identified six critical dimensions that determine cloud maturity progress across people, process and technology: Workload diversity – Successfully running both traditional and modern applications Self-service provisioning – Enabling tenant self-provisioning via catalogs and APIs Policy-based guardrails – Implementing automated governance for security and compliance Cost transparency – Providing chargeback/showback information to internal customers Platform model – Organizing around platform teams vs.</description></item><item><title>PSI Metrics for Kubernetes Graduates to Beta</title><link>https://kubermates.org/docs/2025-09-04-psi-metrics-for-kubernetes-graduates-to-beta/</link><pubDate>Thu, 04 Sep 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-09-04-psi-metrics-for-kubernetes-graduates-to-beta/</guid><description>PSI Metrics for Kubernetes Graduates to Beta What is Pressure Stall Information (PSI)? PSI metrics in Kubernetes How to enable PSI metrics What&amp;rsquo;s next? As Kubernetes clusters grow in size and complexity, understanding the health and performance of individual nodes becomes increasingly critical. We are excited to announce that as of Kubernetes v1.34, Pressure Stall Information (PSI) Metrics has graduated to Beta. Pressure Stall Information (PSI) is a feature of the Linux kernel (version 4.20 and later) that provides a canonical way to quantify pressure on infrastructure resources, in terms of whether demand for a resource exceeds current supply. It moves beyond simple resource utilization metrics and instead measures the amount of time that tasks are stalled due to resource contention. This is a powerful way to identify and diagnose resource bottlenecks that can impact application performance. PSI exposes metrics for CPU, memory, and I/O, categorized as either some or full pressure: some full some full These metrics are aggregated over 10-second, 1-minute, and 5-minute rolling windows, providing a comprehensive view of resource pressure over time. With the KubeletPSI feature gate enabled, the kubelet can now collect PSI metrics from the Linux kernel and expose them through two channels: the Summary API and the /metrics/cadvisor Prometheus endpoint. This allows you to monitor and alert on resource pressure at the node, pod, and container level. KubeletPSI /metrics/cadvisor The following new metrics are available in Prometheus exposition format via /metrics/cadvisor : /metrics/cadvisor container_pressure_cpu_stalled_seconds_total container_pressure_cpu_stalled_seconds_total container_pressure_cpu_waiting_seconds_total container_pressure_cpu_waiting_seconds_total container_pressure_memory_stalled_seconds_total container_pressure_memory_stalled_seconds_total container_pressure_memory_waiting_seconds_total container_pressure_memory_waiting_seconds_total container_pressure_io_stalled_seconds_total container_pressure_io_stalled_seconds_total container_pressure_io_waiting_seconds_total container_pressure_io_waiting_seconds_total These metrics, along with the data from the Summary API, provide a granular view of resource pressure, enabling you to pinpoint the source of performance issues and take corrective action. For example, you can use these metrics to: Identify memory leaks: A steadily increasing some pressure for memory can indicate a memory leak in an application. some Optimize resource requests and limits: By understanding the resource pressure of your workloads, you can more accurately tune their resource requests and limits. Autoscale workloads: You can use PSI metrics to trigger autoscaling events, ensuring that your workloads have the resources they need to perform optimally.</description></item><item><title>How to run AI model inference with GPUs on Amazon EKS Auto Mode</title><link>https://kubermates.org/docs/2025-09-04-how-to-run-ai-model-inference-with-gpus-on-amazon-eks-auto-mode/</link><pubDate>Thu, 04 Sep 2025 16:12:23 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-04-how-to-run-ai-model-inference-with-gpus-on-amazon-eks-auto-mode/</guid><description>How to run AI model inference with GPUs on Amazon EKS Auto Mode Key features that make EKS Auto Mode ideal for AI/ML workloads Walkthrough Prerequisites Set up environment variables Set up EKS Auto Mode cluster and run a model Reducing model cold start time in AI inference workloads Conclusion About the authors AI model inference using GPUs is becoming a core part of modern applications, powering real-time recommendations, intelligent assistants, content generation, and other latency-sensitive AI features. Kubernetes has become the orchestrator of choice for running inference workloads, and organizations want to use its capabilities while still maintaining a strong focus on rapid innovation and time-to-market. But here’s the challenge: while teams see the value of Kubernetes for its dynamic scaling and efficient resource management, they often get slowed down by the need to learn Kubernetes concepts, manage cluster configurations, and handle security updates. This shifts focus away from what matters most: deploying and optimizing AI models. That is where Amazon Elastic Kubernetes Service (Amazon EKS) Auto Mode comes in. EKS Auto Mode Automates node creation, manages core capabilities , and handles upgrades and security patching. In turn, this enables to run your inference workloads without the operational overhead. In this post, we show you how to swiftly deploy inference workloads on EKS Auto Mode. We also demonstrate key features that streamline GPU management, show best practices for model deployment, and walk through a practical example by deploying open weight models from OpenAI using vLLM. Whether you’re building a new AI/machine learning (ML) platform or optimizing existing workflows, these patterns help you accelerate development while maintaining operational efficiency. In this section, we take a closer look at the GPU-specific features that come pre-configured and ready to use with an EKS Auto Mode cluster. These capabilities are also available in self-managed Amazon EKS environments, but they typically need manual setup and tuning.</description></item><item><title>The Debug Trap: Why Smart Engineers Waste Hours on Trivial Problems</title><link>https://kubermates.org/docs/2025-09-04-the-debug-trap-why-smart-engineers-waste-hours-on-trivial-problems/</link><pubDate>Thu, 04 Sep 2025 16:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-04-the-debug-trap-why-smart-engineers-waste-hours-on-trivial-problems/</guid><description>The Question Nobody Asks The Change Principle (It’s Not Rocket Science) The Revert Rebellion The Exotic Bug Fallacy The Debug Stack The Meta Problem Posted on September 4, 2025 by Anshul Sao, Co-Founder &amp;amp; CTO at Facets. cloud Last month, I watched three senior engineers burn four hours debugging a “mysterious” Kubernetes issue that turned out to be a kubectl version upgrade. The same week, another team spent an entire night hunting phantom load balancer bugs when a certificate rotation had broken mobile clients with certificate pinning. These aren’t stories about incompetent engineers. These are stories about brilliant people falling into the same cognitive trap that catches everyone: diving deep instead of looking broad. Here’s what happened with the kubectl disaster. The symptoms looked serious: cron jobs failing, secrets not updating across namespaces, image pulls crashing left and right. Classic distributed systems chaos, right? The team immediately went into full forensic mode. Pod logs. RBAC permissions. Service account configurations. Network policies.</description></item><item><title>Dynamic Kubernetes request right sizing with Kubecost</title><link>https://kubermates.org/docs/2025-09-03-dynamic-kubernetes-request-right-sizing-with-kubecost/</link><pubDate>Wed, 03 Sep 2025 18:52:27 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-03-dynamic-kubernetes-request-right-sizing-with-kubecost/</guid><description>Dynamic Kubernetes request right sizing with Kubecost What are container requests? Kubecost savings insights Customizing recommendations Acting on Kubecost recommendations One-time resizing Scheduled right sizing Automating resizing with Helm Conclusion About the authors This post was co-written with Kai Wombacher, Founding Product Manager at Kubecost. In this post we show you how to use the Kubecost Amazon Elastic Kubernetes Service (Amazon EKS) add-on to lower infrastructure costs and boost Kubernetes efficiency. The Container Request Right Sizing feature allows you to find how container requests are configured, look for inefficiencies, and fix them either manually or through automated remediation. Specifically, we cover how to review Kubecost’s right sizing recommendations and take action on them using one-time updates or scheduled, automated resizing within your Amazon EKS environment to continuously optimize resource usage. Over-requested containers are one of the most common sources of cloud resource waste in Kubernetes environments. Without visibility and automation, development teams can request far more resources than their applications use, which leads to overprovisioned nodes and higher costs. In Kubernetes, a container request is a declared amount of CPU and memory that a workload needs. It plays a crucial role in how workloads are scheduled and how nodes are used. When a container specifies a CPU or memory request, the scheduler looks for a node that has at least that amount of unallocated capacity. When a pod is placed on a node, the requested resources are essentially reserved, regardless of whether the container uses them in practice. Although this reservation behavior makes sure that workloads have access to the resources they need, it can also lead to inefficient resource usage if requests are set too high. For example, if a container requests 1 CPU but only uses 200 millicores (0.2 CPU), then that added 0.8 CPU goes unused, yet the node capacity is still reserved and charged for.</description></item><item><title>Kubernetes v1.34: Service Account Token Integration for Image Pulls Graduates to Beta</title><link>https://kubermates.org/docs/2025-09-03-kubernetes-v1-34-service-account-token-integration-for-image-pulls-graduates-to-/</link><pubDate>Wed, 03 Sep 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-09-03-kubernetes-v1-34-service-account-token-integration-for-image-pulls-graduates-to-/</guid><description>Kubernetes v1.34: Service Account Token Integration for Image Pulls Graduates to Beta What&amp;rsquo;s new in beta? Required cacheType field Isolated image pull credentials How it works Configuration Image pull flow Audience restriction Getting started with beta Prerequisites Migration from alpha Example setup What&amp;rsquo;s next? Call to action How to get involved The Kubernetes community continues to advance security best practices by reducing reliance on long-lived credentials. Following the successful alpha release in Kubernetes v1.33 , Service Account Token Integration for Kubelet Credential Providers has now graduated to beta in Kubernetes v1.34, bringing us closer to eliminating long-lived image pull secrets from Kubernetes clusters. This enhancement allows credential providers to use workload-specific service account tokens to obtain registry credentials, providing a secure, ephemeral alternative to traditional image pull secrets. The beta graduation brings several important changes that make the feature more robust and production-ready: cacheType Breaking change from alpha : The cacheType field is required in the credential provider configuration when using service account tokens. This field is new in beta and must be specified to ensure proper caching behavior. cacheType # CAUTION: this is not a complete configuration example, just a reference for the &amp;rsquo;tokenAttributes. cacheType&amp;rsquo; field. tokenAttributes : serviceAccountTokenAudience : &amp;ldquo;my-registry-audience&amp;rdquo; cacheType : &amp;ldquo;ServiceAccount&amp;rdquo; # Required field in beta requireServiceAccount : true # CAUTION: this is not a complete configuration example, just a reference for the &amp;rsquo;tokenAttributes. cacheType&amp;rsquo; field. tokenAttributes : serviceAccountTokenAudience : &amp;ldquo;my-registry-audience&amp;rdquo; cacheType : &amp;ldquo;ServiceAccount&amp;rdquo; # Required field in beta requireServiceAccount : true Choose between two caching strategies: Token : Cache credentials per service account token (use when credential lifetime is tied to the token). This is useful when the credential provider transforms the service account token into registry credentials with the same lifetime as the token, or when registries support Kubernetes service account tokens directly. Note: The kubelet cannot send service account tokens directly to registries; credential provider plugins are needed to transform tokens into the username/password format expected by registries.</description></item><item><title>Securing the Node: A Primer on Cilium’s Host Firewall</title><link>https://kubermates.org/docs/2025-09-03-securing-the-node-a-primer-on-cilium-s-host-firewall/</link><pubDate>Wed, 03 Sep 2025 14:02:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-03-securing-the-node-a-primer-on-cilium-s-host-firewall/</guid><description>The Node as a Blind Spot How Cilium’s Host Firewall Works Enabling Host Firewall Audit Mode Observe Network Traffic with Hubble Writing Host Network Policies Enforcing the Policy Best Practices and Troubleshooting Tips Conclusion Additional Resources: Posted on September 3, 2025 by Paul Arah, Isovalent @ Cisco When discussing Kubernetes network security, much of the attention focuses on pod-to-pod traffic, ingress controllers, and service meshes. But what about the underlying nodes themselves, the very foundation on which our workloads run? The attack surface that Kubernetes nodes expose is vast and, if left unprotected, can become a golden ticket for malicious actors. Cilium host firewall is built to lock down the host network namespace with precision, visibility, and control, extending the same familiar declarative Kubernetes network policy model to the underlying host. In this blog post, we’ll explore what Cilium Host Firewall is, how it works, and why it should be a core part of your Kubernetes security. Kubernetes native network policies don’t apply to host-level traffic. This means any communication that enters or leaves the host directly (for example, SSH, kubelet, or external monitoring agents) is largely invisible to traditional Kubernetes policy enforcement. While some firewalling is possible via firewalld or external systems, managing those rules is brittle and lacks integration with Kubernetes. At its core, this is the problem Cilium Host Firewall solves. Leveraging eBPF, Cilium introduces host firewalling directly into the fabric of the cluster. Cilium treats the node as a special type of endpoint with the label reserved:host. This lets us apply policies just like we would for pods, except these apply to traffic to and from the node(s) themselves. Cilium host firewall operates at the interface level.</description></item><item><title>Sharks of DigitalOcean: Archana Kamath, Senior Director, IaaS</title><link>https://kubermates.org/docs/2025-09-03-sharks-of-digitalocean-archana-kamath-senior-director-iaas/</link><pubDate>Wed, 03 Sep 2025 04:09:34 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-03-sharks-of-digitalocean-archana-kamath-senior-director-iaas/</guid><description>Sharks of DigitalOcean: Archana Kamath, Senior Director, IaaS What makes DigitalOcean the right place for you? What excites you most about DOâs mission and technology? How would you describe the way innovation happens at DO? How does your team help power DOâs mission? What does customer-centricity mean at DO? How has DO grown while staying true to its values? Dive into the future with DigitalOcean About the author Try DigitalOcean for free Related Articles Sharks of DigitalOcean: Darian Wilkin, Senior Manager, Solutions Engineering Sharks of DigitalOcean: Laura Schaffer, VP, Growth Sharks of DigitalOcean: Ali Munir, Staff Technical Account Manager By Sujatha R Technical Writer Published: September 3, 2025 3 min read Archana Kamath, Senior Director of Compute and Network (IaaS), leads our compute and networking teams at DigitalOcean. Throughout her six-plus years with the company, she and her team have been a driving force behind key product milestones like expanding GPU offerings with AMD MI300X accelerators , accelerating backup and snapshot performance , and scaling compute offerings. Her technical leadership has been instrumental in shaping DigitalOceanâs cloud platform development. âIâve been with DO for a little over six years, and there are a few things that make this the perfect place to work for me personally. First is the concept of DO Simple that goes into building all of our products. Taking super complex scenarios and simplifying them for customers is such an interesting challenge, and one that I love solving for. Second is the culture at DO. Our values drive velocity and iterative development in a collaborative environment, always focusing on solving customer pain points. That alignment with both innovation and empathy is what makes working here so meaningful for me. â ð¥ Have a look at Archana Kamathâs full conversation â¬ï¸ âWhat excites me most is the unique opportunity we have to disrupt the space of cloud computing, and more recently, AI/ML. The DO Simple approach is customer-centric and focused on making cloud computing accessible for a wide range of use cases. The AI/ML space is still in its early days, but itâs already revolutionizing the tech industry.</description></item><item><title>Learn about confidential clusters</title><link>https://kubermates.org/docs/2025-09-03-learn-about-confidential-clusters/</link><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-03-learn-about-confidential-clusters/</guid><description>Learn about confidential clusters Running Red Hat OpenShift clusters on confidential nodes How to set up OpenShift confidential clusters on Microsoft Azure How to install OpenShift with confidential nodes on Google Cloud Try confidential clusters Red Hat Product Security About the authors Nitesh Narayan Lal Meirav Dean More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The Confidential Clusters project integrates confidential computing technology into Kubernetes clusters. It&amp;rsquo;s an end-to-end solution that provides data confidentiality on cloud platforms by isolating a cluster from its underlying infrastructure. In a confidential cluster, all nodes run on top of confidential virtual machines (cVM). Before a node can join the cluster and access secrets, the platform and environment&amp;rsquo;s authenticity are verified through remote attestation. This process involves communication with a trusted remote server. Confidential Clusters enables you to use Red Hat OpenShift, a trusted platform to develop, modernize, and deploy applications at scale and leverage the convenience and flexibility of the cloud services without compromising on data security. This is critical for industries such as financial services, health care, and government that need to adhere to the regulatory requirements such as the European Digital Operational Resiliency Act (DORA). The general availability of OpenShift confidential nodes on cVM is now offered with AMD SEV-SNP and Intel TDX integration on Google Cloud Platform (GCP), as well as with AMD SEV-SNP on Azure in OpenShift version 4.19. Support for Intel TDX on Azure will be available in version 4.20 and above. Additionally, the integration of remote attestation is currently under development and will be included in future OpenShift releases. It’s a complex technology, but that doesn&amp;rsquo;t mean it&amp;rsquo;s complex to set up. Here are three articles to get you started.</description></item><item><title>Security beyond the model: Introducing AI system cards</title><link>https://kubermates.org/docs/2025-09-03-security-beyond-the-model-introducing-ai-system-cards/</link><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-03-security-beyond-the-model-introducing-ai-system-cards/</guid><description>Security beyond the model: Introducing AI system cards What are AI model cards? Introducing AI system cards Looking forward Learn more Get started with AI Inference About the author Huzaifa Sidhpurwala More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share AI is one of the most significant innovations to emerge in the last 5 years. Generative AI (gen AI) models are now smaller, faster, and cheaper to run. They can solve mathematical problems, analyze situations, and even reason about cause‑and‑effect relationships to generate insights that once required human expertise. On its own, an AI model is merely a set of trained weights and mathematical operations, an impressive engine, but one sitting idle on a test bench. Business value only emerges when that model is embedded within a complete AI system: data pipelines feed it clean, context‑rich inputs, application logic orchestrates pre‑ and post‑processing, guardrails and monitoring enforce safety, security, and compliance, and user interfaces deliver insights through chatbots, dashboards, or automated actions. In practice, end users engage with systems, not raw models, which is why a single foundational model can power hundreds of tailored solutions across domains. Without the surrounding infrastructure of an AI system, even the most advanced model remains untapped potential rather than a tool that solves real‑world problems. AI model cards are files that accompany and describe the model, helping AI system developers make informed decisions about which model to choose for their applications. Model cards present a concise, standardized snapshot of each model’s strengths, limitations, and training information, summarizing performance metrics across key benchmarks, detailing the data and methodology used for training and evaluation, highlighting known biases and failure modes, and spelling out licensing terms and governance contacts. With this information in one place, it&amp;rsquo;s easier to assess whether a model aligns with accuracy targets, fairness requirements, deployment constraints, and compliance obligations, reducing integration risk and accelerating responsible adoption. In November 2024, we authored a paper addressing the rapidly evolving ecosystem of publicly available AI models and their potential implications for security and safety. In this paper we proposed standardization of model cards and extensions to include safety, security, and data governance and pedigree information.</description></item><item><title>Understanding AI agent types: A guide to categorizing complexity</title><link>https://kubermates.org/docs/2025-09-03-understanding-ai-agent-types-a-guide-to-categorizing-complexity/</link><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-03-understanding-ai-agent-types-a-guide-to-categorizing-complexity/</guid><description>Understanding AI agent types: A guide to categorizing complexity 1. Functional agents: Prioritizing essential functionality 2. Simple reflex agents: The basics of reaction 3. Model-based reflex agents: Adding internal awareness 4. Goal-based agents: Planning for the future 5. Learning agents: Adaptation through experience 6. Utility-based agents: Optimizing trade-offs 7. Hierarchical agents: Structured decomposition 8. Multi-agent systems: Collaboration and emergence Conclusion: Choosing the right complexity for your AI agents Get started with AI agents Get started with AI Inference About the author Richard Naszcyniec More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share In their most advanced form, AI agents are autonomous systems designed to perceive their environment, make decisions, and take actions to achieve specific goals. As AI technology evolves, agents are becoming pervasive across many industries like finance, healthcare, manufacturing, and customer service. However, not all AI agents are created equal—their capabilities vary widely in terms of autonomy, decision-making, adaptability, and interaction with their environment. When thinking about AI agent use cases and the effort to implement them, it is important to understand the complexity of every agent being considered.</description></item><item><title>Unlocking next-generation AI performance with Dynamic Resource Allocation on Amazon EKS and Amazon EC2 P6e-GB200</title><link>https://kubermates.org/docs/2025-09-02-unlocking-next-generation-ai-performance-with-dynamic-resource-allocation-on-ama/</link><pubDate>Tue, 02 Sep 2025 23:33:59 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-02-unlocking-next-generation-ai-performance-with-dynamic-resource-allocation-on-ama/</guid><description>Unlocking next-generation AI performance with Dynamic Resource Allocation on Amazon EKS and Amazon EC2 P6e-GB200 The power behind P6e-GB200: NVIDIA GB200 Grace Blackwell architecture Understanding EC2 P6e-GB200 UltraServer architecture Integrating P6e-GB200 UltraServers with Amazon EKS The challenge: running distributed AI workloads on Kubernetes The solution: Kubernetes DRA and IMEX How DRA solves traditional GPU allocation problems Topology-aware scheduling and memory coherence Workload scheduling flow with DRA How to use p6e-GB200 with Kubernetes DRA with Amazon EKS Prerequisites Step 1: Reserve P6e-GB200 UltraServer capacity Step 2: Create the EKS cluster configuration file Step 3: Deploy the EKS cluster Step 4: Deploy the NVIDIA GPU Operator Step 5: Install the NVIDIA DRA Driver Step 6: Verify DRA resources Validating IMEX channel allocation Apply and validate Multi-node IMEX communication in action Conclusion About the authors The rapid evolution of agentic AI and large language models (LLMs), particularly reasoning models, has created unprecedented demand for computational resources. Today’s most advanced AI models span hundreds of billions to trillions of parameters and necessitate massive computational power, extensive memory footprints, and ultra-fast interconnects to function efficiently. Organizations developing applications for natural language processing, scientific simulations, 3D content generation, and multimodal inference need infrastructure that can scale from today’s billion-parameter models to tomorrow’s trillion-parameter frontiers while maintaining performance. In this post, we explore how the new Amazon Elastic Compute Cloud (Amazon EC2) P6e-GB200 UltraServers are transforming distributed AI workload through seamless Kubernetes integration. Amazon Web Services (AWS) introduced the EC2 P6e-GB200 UltraServers to meet the growing demand for large-scale AI model training and inference. They represent a significant architectural breakthrough for distributed AI workloads. Furthermore, the EC2 P6e-GB200 UltraServer launch includes support for Amazon Elastic Kubernetes Service (Amazon EKS) , providing a Kubernetes-native environment for deploying and scaling from hundreds-of-billions to trillion-parameter models as the AI landscape continues to evolve. At the heart of EC2 P6e-GB200 UltraServers is the NVIDIA GB200 Grace Blackwell Superchip , which integrates two NVIDIA Blackwell GPUs with a NVIDIA Grace CPU. Furthermore, it provides NVLink-Chip-to-Chip (C2C) connection between these components, delivering 900 GB/s of bidirectional bandwidth, which is substantially faster than traditional PCIe interfaces. When deployed at rack scale, EC2 P6e-GB200 UltraServers participate in NVIDIA’s GB200 NVL72 architecture , creating memory-coherent domains of up to 72 GPUs. Fifth-generation NVLink technology enables GPU-to-GPU communication across discrete servers within the same domain at up to 1.8 TB/s per GPU. Critical to this performance is Elastic Fabric Adapter (EFAv4) networking, which delivers up to 28.8 Tbps of total network bandwidth per UltraServer.</description></item><item><title>What's New on DigitalOcean App Platform</title><link>https://kubermates.org/docs/2025-09-02-what-s-new-on-digitalocean-app-platform/</link><pubDate>Tue, 02 Sep 2025 23:01:24 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-02-what-s-new-on-digitalocean-app-platform/</guid><description>What&amp;rsquo;s New on DigitalOcean App Platform Week of September 1st About the author Try DigitalOcean for free Related Articles Single Sign-On is Now Available, Strengthening Security and Simplifying Authentication Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform By Waverly Swinton Published: September 2, 2025 1 min read Welcome to Whatâs New on DigitalOcean App Platform â your weekly roundup of the latest updates for App Platform. Each week, weâll share new feature releases, technical tutorials, or video walkthroughs to help you stay ahead of the curve and keep building. Check back regularly for new insights and inspiration, or get started now: Deploy Your First App on App Platform Product Update App Platform Now Supports Integration with AI-coding Assistants using MCP Whatâs new Now you can integrate AI-powered coding assistants â including Claude Code, Cursor, and VS Code MCP plugins â directly with DigitalOcean App Platform using the new DigitalOcean MCP Server. This means that your AI assistant can deploy applications, spin up databases, manage Spaces, check billing, and more â without leaving your coding environment. Go from AI-generated code to production in just a few prompts with our MCP Server Setup Tutorial , Video Walkthrough , and DigitalOcean MCP Server GitHub. How to get started The AI development revolution shouldnât stop at your IDE. Whether youâre building your first AI-powered app or deploying your hundredth, App Platform + MCP integration takes you from code to production faster than ever. Weâve built a working tutorial to take you from idea to production using only prompts. It uses MCP, DigitalOcean MCP Server, GitHub MCP, and Claude Code so you can go from building the app â committing to GitHub â staging with preview deployments â all the way to merging in production. Check out our Claude Code Tutorial and GitHub repo. Share Product Updates September 2, 2025 3 min read Read more August 20, 2025 2 min read Read more August 14, 2025 2 min read Read more.</description></item><item><title>Transfer VMware Cloud Foundation (VCF) Operations Integration SDK Builds to a Private Container Registry in the deployed VCF Operations Environment</title><link>https://kubermates.org/docs/2025-09-02-transfer-vmware-cloud-foundation-vcf-operations-integration-sdk-builds-to-a-priv/</link><pubDate>Tue, 02 Sep 2025 21:14:29 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-02-transfer-vmware-cloud-foundation-vcf-operations-integration-sdk-builds-to-a-priv/</guid><description>Context Steps to transfer VCF Operations Integration SDK builds from build container registry to a private registry in deployed VCF Operations environment Steps to be performed in the Integration SDK’s build environment Steps to be performed in the deployed VCF Operations environment Credits Related Articles Transfer VMware Cloud Foundation (VCF) Operations Integration SDK Builds to a Private Container Registry in the deployed VCF Operations Environment VMware Cloud Foundation - Cloud on Your Terms VMware Cloud Services Portal migration to the Broadcom Cloud Console VMware Cloud Foundation (VCF) Operations offers comprehensive IT management, focusing on proactive operations and security. It monitors, manages, and optimizes IT operations by collecting data using management packs. VCF Operations includes built-in packs for components like vCenter and VCF. Additionally, it provides tools—a management pack builder and an Integration SDK—that enable users to develop custom management packs for monitoring applications and infrastructure components not covered by the built-in options. These user-developed management packs can be uploaded and deployed to specific environments, allowing organizations to tailor their monitoring capabilities and integrate diverse technologies within their VCF ecosystem. The Integration SDK provides tools and libraries in Python and Java to aid in development of management packs to add custom objects, data, and relationships from an endpoint into VCF Operations. The Integration SDK generates containerized management packs and hence requires a container registry for its storage and retrieval. This container registry must be accessible by the VCF Operations Cloud Proxies to download the container. By default, the Integration SDK’s build environment’s container registry must be accessible by the VCF Operations Cloud proxies in the intended deployment environment. In most scenarios, the container registry used by the Integration SDK’s build environment is accessible by the VCF Operations Cloud Proxies in the deployed environment. However, in the following situations, Integration SDKs build environment’s container registry may not be accessible by the VCF Operations Cloud proxies in the intended deployment environment: VCF Operations has been deployed in an air-gapped environment and there is no connectivity to the internet to access public container registry. The VCF Operations Integration SDK’s build environment has used a private container registry and there is no connectivity from the customer’s infrastructure to this private container registry.</description></item><item><title>Kubernetes v1.34: Introducing CPU Manager Static Policy Option for Uncore Cache Alignment</title><link>https://kubermates.org/docs/2025-09-02-kubernetes-v1-34-introducing-cpu-manager-static-policy-option-for-uncore-cache-a/</link><pubDate>Tue, 02 Sep 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-09-02-kubernetes-v1-34-introducing-cpu-manager-static-policy-option-for-uncore-cache-a/</guid><description>Kubernetes v1.34: Introducing CPU Manager Static Policy Option for Uncore Cache Alignment Understanding the feature What is uncore cache? Cache-aware workload placement Use cases Enabling the feature Further reading Getting involved A new CPU Manager Static Policy Option called prefer-align-cpus-by-uncorecache was introduced in Kubernetes v1.32 as an alpha feature, and has graduated to beta in Kubernetes v1.34. This CPU Manager Policy Option is designed to optimize performance for specific workloads running on processors with a split uncore cache architecture. In this article, I&amp;rsquo;ll explain what that means and why it&amp;rsquo;s useful. prefer-align-cpus-by-uncorecache Until relatively recently, nearly all mainstream computer processors had a monolithic last-level-cache cache that was shared across every core in a multiple CPU package. This monolithic cache is also referred to as uncore cache (because it is not linked to a specific core), or as Level 3 cache. As well as the Level 3 cache, there is other cache, commonly called Level 1 and Level 2 cache, that is associated with a specific CPU core. In order to reduce access latency between the CPU cores and their cache, recent AMD64 and ARM architecture based processors have introduced a split uncore cache architecture, where the last-level-cache is divided into multiple physical caches, that are aligned to specific CPU groupings within the physical package. The shorter distances within the CPU package help to reduce latency. Kubernetes is able to place workloads in a way that accounts for the cache topology within the CPU package(s). The matrix below shows the CPU-to-CPU latency measured in nanoseconds (lower is better) when passing a packet between CPUs, via its cache coherence protocol on a processor that uses split uncore cache. In this example, the processor package consists of 2 uncore caches. Each uncore cache serves 8 CPU cores.</description></item><item><title>Beyond Terraform Modules: Infrastructure Design by Contract</title><link>https://kubermates.org/docs/2025-09-02-beyond-terraform-modules-infrastructure-design-by-contract/</link><pubDate>Tue, 02 Sep 2025 17:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-02-beyond-terraform-modules-infrastructure-design-by-contract/</guid><description>The Hidden Tax of Module Stitching Four Pillars of Infrastructure Design by Contract 1. Contract-Led Composition: Explicit Interfaces Between Components 2. Environment-Aware by Design: Contracts Across Environments 3. Progressive Rollouts for Infrastructure: Versioned Contracts 4. Developer Experience That Preserves Governance: Preconditions and Postconditions From Configuration to Contract-Driven Infrastructure Posted on September 2, 2025 by Anshul Sao, Co-Founder &amp;amp; CTO at Facets. cloud CNCF projects highlighted in this post It’s 2AM. You’re staring at a failed deployment pipeline, trying to figure out why your EKS cluster can’t find the right subnets. You’ve checked the variables, verified the outputs, and triple-checked your depends_on statements. Everything looks correct, yet it still fails. This scenario plays out daily across organizations that have adopted Terraform. While modules promised composable, reusable infrastructure, the reality has become a complex web of string-matching, implicit dependencies, and tribal knowledge. How much of your team’s time is spent debugging module connections rather than delivering value? Every time a team provisions infrastructure for a new service or application, they pay an invisible tax in cognitive overhead: For each input a module needs, someone must make critical decisions: Is this value a hard-coded constant? A data lookup? An output from another module? The answer differs for every project and environment, so this decision tree must be rebuilt from scratch each time.</description></item><item><title>Single Sign-On is Now Available, Strengthening Security and Simplifying Authentication</title><link>https://kubermates.org/docs/2025-09-02-single-sign-on-is-now-available-strengthening-security-and-simplifying-authentic/</link><pubDate>Tue, 02 Sep 2025 15:43:19 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-02-single-sign-on-is-now-available-strengthening-security-and-simplifying-authentic/</guid><description>Single Sign-On is Now Available, Strengthening Security and Simplifying Authentication Single Sign-On is Now Available, Strengthening Security and Simplifying Authentication Why Single Sign-On matters How Single Sign-On can benefit your business How SSO on DigitalOcean works Get started with SSO on DigitalOcean About the author Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 By Nicole Ghalwash Published: September 2, 2025 3 min read Good news: Single Sign-On (SSO) is now available for all DigitalOcean customers via Okta OpenID Connect (OIDC). Single Sign-On represents a major step forward in strengthening security, simplifying user management, and empowering teams to be more productive on DigitalOcean. For growing teams, managing user accounts across multiple platforms can quickly become a challenge. Password resets, manual onboarding, and offboarding all add up, not just for the IT teamâbut developers feel it too in terms of lost productivity. That means: No more juggling multiple usernames and passwords. No more juggling multiple usernames and passwords. Centralized access management that scales with your team. No more wasting time being locked out of critical apps. Stronger, consistent security across your entire toolchain. As organizations grow, managing user access across multiple tools and platforms can become complex, time-consuming, and costly. Single Sign-On solves this problem by allowing employees to securely access all their applications with a single set of credentials. By centralizing authentication, SSO not only strengthens security but also streamlines onboarding, reduces IT workload, and gives teams a seamless login experience that helps them stay focused on building and innovating.</description></item><item><title>EMEA’s cloud sovereignty puzzle takes shape</title><link>https://kubermates.org/docs/2025-09-02-emea-s-cloud-sovereignty-puzzle-takes-shape/</link><pubDate>Tue, 02 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-02-emea-s-cloud-sovereignty-puzzle-takes-shape/</guid><description>EMEA’s cloud sovereignty puzzle takes shape More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Digital sovereignty is not a new topic, but recent events on the world stage have catapulted it to the top of the top of boardroom agendas and government priorities alike. According to a 2025 BARC study , 84% of surveyed companies view data sovereignty as a central point of their strategy, and 70 percent report significantly increased relevance in the past one to two years. Meanwhile, industry analyst firm Gartner forecasts the sovereign cloud IaaS market will be worth $169 billion in 2028 , up from $37 billion last year. Different countries and organisations are adopting different approaches to digital and cloud sovereignty, some are cautiously waiting and watching to see how things shake out, others are acting fast. While there remains a certain degree of uncertainty over exactly what digital and cloud sovereignty looks like, most industry watchers agree that it will be ‘need to have’ rather than ‘nice to have. ’ For a broader perspective on digital sovereignty, we caught up with Red Hat country and regional leaders across Europe to test the waters in local markets. Gregor von Jagow, Country Manager, Germany: “We’re seeing demand across all sectors. ” Germany has long prioritized digital sovereignty, partly due to the cautious nature of its businesses. With geopolitical tensions and market consolidation, local cloud providers play a vital role. We partner with local players like IONOS and StackIT (a subsidiary of Schwarz Group) that are driving the sovereignty conversation in Germany. These providers are often best positioned to deliver the level of control and adherence to local regulations that truly define sovereign cloud solutions for customers. We’re seeing demand across all sectors, not just government.</description></item><item><title>Protect and unify with Commvault and Red Hat OpenShift: A modern approach to disaster recovery for VMs and containers</title><link>https://kubermates.org/docs/2025-09-02-protect-and-unify-with-commvault-and-red-hat-openshift-a-modern-approach-to-disa/</link><pubDate>Tue, 02 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-02-protect-and-unify-with-commvault-and-red-hat-openshift-a-modern-approach-to-disa/</guid><description>Protect and unify with Commvault and Red Hat OpenShift: A modern approach to disaster recovery for VMs and containers A unified approach to backup and recovery Take the next step in your modernization journey Red Hat OpenShift Container Platform | Product Trial About the author Dan Bettinger More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The path to modernization isn&amp;rsquo;t always the least distance between two points. As enterprises embrace containerization for speed and scalability, a significant number of business-critical workloads continue to run on virtual machines (VM). Managing these two distinct environments is complex enough as it is, but then there&amp;rsquo;s also disaster recovery. How do you ensure rapid, reliable recovery for both platforms without doubling complexity and cost? What if you could unify your virtualized and containerized workloads on a single, modern platform with a more consistent, effective strategy for backup and disaster recovery? Red Hat OpenShift Virtualization enables organizations to do just that. It&amp;rsquo;s an included feature of Red Hat OpenShift that allows you to run and manage VMs and containers on the same Kubernetes-based platform. This modern approach streamlines IT infrastructure, but it&amp;rsquo;s the ability to protect it that brings peace of mind. Commvault has long supported container backup and disaster recovery on Red Hat OpenShift. Now Commvault is extending its support for Red Hat OpenShift to include Red Hat OpenShift Virtualization so that the same unified platform now also protects VMs running alongside containers. Specifically Commvault Long-Term Support Release 11.40 and Commvault Innovation Release 11.42 will support the versions of OpenShift Virtualization that are part of Red Hat OpenShift Container Platform 4.18 and 4.19. For years, IT leaders have struggled with a fragmented approach to disaster recovery. Using separate backup tools for VMs and containers creates operational silos, increases the risk of data loss, and complicates recovery efforts when a disaster (such as a ransomware attack) occurs. With Commvault’s enterprise-grade data protection now extended to VMs running on Red Hat OpenShift Virtualization, you can finally implement a single, cohesive backup and recovery strategy.</description></item><item><title>The Metadata Assistant: How Red Hat is using generative AI to make web content easier to find and use</title><link>https://kubermates.org/docs/2025-09-02-the-metadata-assistant-how-red-hat-is-using-generative-ai-to-make-web-content-ea/</link><pubDate>Tue, 02 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-02-the-metadata-assistant-how-red-hat-is-using-generative-ai-to-make-web-content-ea/</guid><description>The Metadata Assistant: How Red Hat is using generative AI to make web content easier to find and use Facing challenges with metadata accuracy How Red Hat&amp;rsquo;s Metadata Assistant helps How it works: Relieving Red Hatters from long forms and tedious tasks Conclusion Get started with AI Inference About the authors Anna McHugh Gail Vadia More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Most of us rely on good web metadata every day, perhaps hundreds of times a day, without ever realizing it. While we don’t all see metadata, it’s present on every webpage, providing details for categorizing content. Search and information retrieval software—from content management systems and search engines to generative AI (gen AI) chatbots—all depend on metadata to categorize, interpret, and display content to users. In some ways, metadata is like the classification system librarians use to place books on shelves so that visitors can find them. Much like a library catalog, good web metadata makes content easier to find and use because it has consistent, meaningful labels. At Red Hat, we maintain dozens of websites with tens of thousands of pages and millions of metadata selections. To help manage it all, we’ve developed a new internal generative AI (gen AI) tool: the Metadata Assistant. Despite our best efforts to stay organized, we struggle to make accurate and consistent tags. The overwhelming amount of content to classify can feel like a messy, towering pile of books instead of an organized library where you can browse the shelves by meaningful categories. Red Hatters must consider more than 150 taxonomy choices across several parent categories, like products, topics, industries, regions, and partners when classifying their content. Our product portfolio is complex, with products frequently being added, discontinued, or renamed. Metadata best practices can be tricky for teams to keep up with.</description></item><item><title>What’s new with data science pipelines in Red Hat OpenShift AI</title><link>https://kubermates.org/docs/2025-09-02-what-s-new-with-data-science-pipelines-in-red-hat-openshift-ai/</link><pubDate>Tue, 02 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-02-what-s-new-with-data-science-pipelines-in-red-hat-openshift-ai/</guid><description>What’s new with data science pipelines in Red Hat OpenShift AI So, what exactly are data science pipelines in OpenShift AI? Creating a pipeline (and then what?) Cool new things in data science pipelines (from Kubeflow Pipelines 2.4. x) Flexible resource limits with placeholders Better control over parallel loops Reliable SubDAG Output resolution How these updates help you in OpenShift AI An important heads-up: The data science pipelines 2.0 change Wrapping up Learn more Get started with AI Inference About the author Alex Handy More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share If you’ve ever struggled with manual, inconsistent, or expensive machine learning (ML) workflows (or if you&amp;rsquo;d love to have a formalized workflow at all), you’re not alone! Many teams hit roadblocks when trying to automate, scale, or manage their ML pipelines—especially as projects grow, scope creeps, and requirements inevitably change. Red Hat OpenShift AI now brings a set of updates to data science pipelines, making it easier than ever to control and optimize your ML processes. These improvements aren’t just about new features, they’re also about helping you optimize resource consumption and minimize the 2am on-call page-outs by building workflows you can trust in production. Think of a data science pipeline in OpenShift AI as the navigation system for your entire ML workflow. You can visually build this pipeline, which is powered by a structure called a directed acyclic graph (DAG). A DAG acts as the blueprint for your process, from data prep to deployment, by mapping out each task in a clear, one-way workflow. This structure is what makes sure that every step runs in the correct order and your pipeline doesn&amp;rsquo;t get stuck in an infinite loop. This blueprint or map is technically a graph where each component (a node) runs inside of a container on Red Hat OpenShift. These nodes are connected by edges (think of this as the arrows on a flow chart), which define their dependencies, so a task only runs after its prerequisites are met and the appropriate outputs go to the right places (the inputs of the next steps). The graph is &amp;ldquo;directed&amp;rdquo; because the workflow only moves forward and &amp;ldquo;acyclic&amp;rdquo; because it can&amp;rsquo;t circle back on itself. This workflow gives you precise control over your ML operations, making more efficient use of cluster resources.</description></item><item><title>Why defence organisations need resilience beyond sovereignty</title><link>https://kubermates.org/docs/2025-09-02-why-defence-organisations-need-resilience-beyond-sovereignty/</link><pubDate>Tue, 02 Sep 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-02-why-defence-organisations-need-resilience-beyond-sovereignty/</guid><description>Why defence organisations need resilience beyond sovereignty Why autonomy matters The three pillars of autonomy Open source is the foundation of autonomy Beyond sovereignty to resilience Learn more Red Hat Learning Subscription | Product Trial About the author Giuseppe Magnotta More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Amid rising geopolitical tensions, digital sovereignty has become a focal point for governments and enterprises alike. However, for defence organisations, sovereignty alone is not enough. The ability to maintain operational resilience in the face of conflict or severe geopolitical disruption requires the capacity to act independently when cooperation is impossible. Only true autonomy can provide that ability. Digital sovereignty , whether cloud, data, or total sovereignty, refers to control over digital assets, so data, hardware, people, and software remain within a specific jurisdiction. While sovereignty addresses regulatory and jurisdictional concerns, it does not guarantee operational continuity in crisis scenarios. Strategic autonomy, however, goes further. It means maintaining the ability to collaborate with international partners when possible, while retaining the independence to operate alone when necessary. For defence organisations, this is a matter of mission survival. In peacetime, reliance on hyperscale cloud providers and distributed IT infrastructure is manageable. But in conflict, the landscape shifts dramatically. Physical destruction can render data centres inoperable, while internet outages cripple cloud-dependent operations.</description></item><item><title>Kubernetes v1.34: DRA has graduated to GA</title><link>https://kubermates.org/docs/2025-09-01-kubernetes-v1-34-dra-has-graduated-to-ga/</link><pubDate>Mon, 01 Sep 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-09-01-kubernetes-v1-34-dra-has-graduated-to-ga/</guid><description>Kubernetes v1.34: DRA has graduated to GA The core of DRA is now GA Features promoted to beta New alpha features What’s next? Getting involved Acknowledgments Kubernetes 1.34 is here, and it has brought a huge wave of enhancements for Dynamic Resource Allocation (DRA)! This release marks a major milestone with many APIs in the resource. k8s. io group graduating to General Availability (GA), unlocking the full potential of how you manage devices on Kubernetes. On top of that, several key features have moved to beta, and a fresh batch of new alpha features promise even more expressiveness and flexibility. resource. k8s. io Let&amp;rsquo;s dive into what&amp;rsquo;s new for DRA in Kubernetes 1.34! The headline feature of the v1.34 release is that the core of DRA has graduated to General Availability. Kubernetes Dynamic Resource Allocation (DRA) provides a flexible framework for managing specialized hardware and infrastructure resources, such as GPUs or FPGAs. DRA provides APIs that enable each workload to specify the properties of the devices it needs, but leaving it to the scheduler to allocate actual devices, allowing increased reliability and improved utilization of expensive hardware. With the graduation to GA, DRA is stable and will be part of Kubernetes for the long run. The community can still expect a steady stream of new features being added to DRA over the next several Kubernetes releases, but they will not make any breaking changes to DRA. So users and developers of DRA drivers can start adopting DRA with confidence.</description></item><item><title>Jaeger at 10: Forged in Community, Reborn in OpenTelemetry</title><link>https://kubermates.org/docs/2025-09-01-jaeger-at-10-forged-in-community-reborn-in-opentelemetry/</link><pubDate>Mon, 01 Sep 2025 14:57:18 +0000</pubDate><guid>https://kubermates.org/docs/2025-09-01-jaeger-at-10-forged-in-community-reborn-in-opentelemetry/</guid><description>Most Challenging Part of Tracing Journey to v2 Fueling the Future: The Mentorship Pipeline Mature Ecosystem of Tracing Recognition of the Community Posted on September 1, 2025 by Yuri Shkuro and Jonah Kowall, Jaeger Maintainers In the fast-paced world of software, reaching 10 years is a testament to a project’s resilience, utility, and community strength. Half a decade ago, we celebrated Jaeger’s fifth anniversary , marveling at its growth from a nascent idea to a critical component in the observability stack. Today, as we mark 10 years, we’re not just celebrating longevity; we’re celebrating a profound evolution. Jaeger has been reborn, embracing a future built on collaboration, standardization, and the incredible momentum of OpenTelemetry. For years, the most significant barrier to adopting distributed tracing wasn’t the backend. It was instrumentation. Getting telemetry data out of applications was a complex, often proprietary, and labor-intensive process. Every tracing system had its own SDKs, its own agents, and its own way of doing things. This fragmentation created vendor lock-in and a steep learning curve, hindering widespread adoption. This is where OpenTelemetry changed everything. It didn’t just offer another SDK; it advanced the entire practice of instrumentation. By providing a single, vendor-neutral standard, OpenTelemetry unified both manual instrumentation (SDKs) and automatic instrumentation (agents).</description></item><item><title>Announcing Kyverno Release 1.15!</title><link>https://kubermates.org/docs/2025-08-30-announcing-kyverno-release-1-15/</link><pubDate>Sat, 30 Aug 2025 13:31:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-30-announcing-kyverno-release-1-15/</guid><description>TL;DR New Policy Types MutatingPolicy: Flexible Resource Transformation GeneratingPolicy: Intelligent Resource Creation DeletingPolicy: Controlled Cleanup Made Easy Pod Security Standard(PSS) Policy Performance: ClusterPolicy vs ValidatingPolicy Key Performance Improvements Why It Matters New OpenReports API Group Support Community &amp;amp; Contributions Getting Started What’s Next? Conclusion Posted on August 30, 2025 by The Kyverno Team CNCF projects highlighted in this post Kyverno 1.15 makes Kubernetes policy management more powerful, extensible, and user-friendly. We are thrilled to announce the release of Kyverno 1.15.0, continuing our mission to make policy management in Kubernetes environments more modular, performant, and user-friendly. This release introduces new capabilities across multiple policy types, enhances testing and CLI functionality, and brings many improvements that a vibrant community has contributed. New MutatingPolicy for dynamic resource transformation with native Kubernetes integration New GeneratingPolicy for intelligent resource creation and synchronization using CEL expressions New DeletingPolicy resource for controlled cleanup of Kubernetes resources Advanced CEL functions and enhanced policy exception support for fine-grained control Community milestone: over 850 changes merged from 70+ contributors, including many first-timers! Building on the foundation of ValidatingPolicy and ImageValidatingPolicy from previous releases, Kyverno 1.15 introduces three new CEL-based policy types that complete our comprehensive policy ecosystem. While Kyverno, created by Nirmata, maintains its traditional policy engine capabilities, these new policy types provide native Kubernetes integration by automatically converting to Kubernetes admission controllers – MutatingPolicy converts to MutatingAdmissionPolicy (MAP) and ValidatingPolicy converts to ValidatingAdmissionPolicy (VAP). This hybrid approach gives users the flexibility to choose between Kyverno’s rich policy engine features and native Kubernetes performance. The new MutatingPolicy type provides native Kubernetes integration through MutatingAdmissionPolicy, offering: Full support for all functions that a mutate rule of a traditional policy supports Easier for each loop iteration with CEL’s map() and filter() functions Full support of advanced custom CEL libraries for complex policy logic CLI support for offline resources mutation in CI/CD pipelines Here is an example of adding default labels to deployments. Previous ClusterPolicy approach: apiVersion: kyverno. io/v1 kind: ClusterPolicy metadata: name: add-default-labels spec: rules: - name: add-labels match: resources: kinds: - Deployment mutate: patchStrategicMerge: metadata: labels: environment: &amp;ldquo;production&amp;rdquo; managed-by: &amp;ldquo;kyverno&amp;rdquo; apiVersion: kyverno. io/v1 kind: ClusterPolicy metadata: name: add-default-labels spec: rules: - name: add-labels match: resources: kinds: - Deployment mutate: patchStrategicMerge: metadata: labels: environment: &amp;ldquo;production&amp;rdquo; managed-by: &amp;ldquo;kyverno&amp;rdquo; New MutatingPolicy approach: MutatingPolicy apiVersion: policies. kyverno. io/v1alpha1 kind: MutatingPolicy metadata: name: add-default-labels spec: mutations: - patchType: ApplyConfiguration applyConfiguration: expression: &amp;gt; Object{ metadata: Object.</description></item><item><title>Kubernetes v1.34: Finer-Grained Control Over Container Restarts</title><link>https://kubermates.org/docs/2025-08-29-kubernetes-v1-34-finer-grained-control-over-container-restarts/</link><pubDate>Fri, 29 Aug 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-08-29-kubernetes-v1-34-finer-grained-control-over-container-restarts/</guid><description>Kubernetes v1.34: Finer-Grained Control Over Container Restarts The problem with a single restart policy Introducing per-container restart policies Use cases In-place restarts for training jobs Try-once init containers Pods with multiple containers How to use it Example 1: Restarting on specific exit codes Example 2: A try-once init container Example 3: Containers with different restart policies Learn more Roadmap Your feedback is welcome! With the release of Kubernetes 1.34, a new alpha feature is introduced that gives you more granular control over container restarts within a Pod. This feature, named Container Restart Policy and Rules , allows you to specify a restart policy for each container individually, overriding the Pod&amp;rsquo;s global restart policy. In addition, it also allows you to conditionally restart individual containers based on their exit codes. This feature is available behind the alpha feature gate ContainerRestartRules. ContainerRestartRules This has been a long-requested feature. Let&amp;rsquo;s dive into how it works and how you can use it. Before this feature, the restartPolicy was set at the Pod level. This meant that all containers in a Pod shared the same restart policy ( Always , OnFailure , or Never ). While this works for many use cases, it can be limiting in others. restartPolicy Always OnFailure Never For example, consider a Pod with a main application container and an init container that performs some initial setup. You might want the main container to always restart on failure, but the init container should only run once and never restart. With a single Pod-level restart policy, this wasn&amp;rsquo;t possible.</description></item><item><title>From VLAN Tag to Segment: Using Guest VLAN Tagging in NSX</title><link>https://kubermates.org/docs/2025-08-29-from-vlan-tag-to-segment-using-guest-vlan-tagging-in-nsx/</link><pubDate>Fri, 29 Aug 2025 18:16:13 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-29-from-vlan-tag-to-segment-using-guest-vlan-tagging-in-nsx/</guid><description>Guest VLAN Tagging alone… not great with NSX Mapping VLAN Tags to NSX Segments Step 1 – Attach the VM to a segment Step 2 – Convert the VM’s port into a parent port Step 3 – Create child ports for each VLAN Notes Configuration steps VM Configuration Creation of a child port using API Step 1 – Retrieve the VM’s Port Step 2 – Convert the Port into a Parent Port Step 3 – Create a Child Port for VLAN 10 Creation of a Child Port for via UI Related Articles From VLAN Tag to Segment: Using Guest VLAN Tagging in NSX Save Costs and Scale Efficiently with vSAN Deduplication in VMware Cloud Foundation 9.0 How to Connect your VMware Private AI Services Agents to OpenWeb UI By default, a virtual machine sends traffic to its vNIC untagged. The virtual switch then receives that traffic into a single VLAN or NSX segment. This model limits a VM to connecting to at most 10 networks directly. But what if we need to connect the VM to more than 10 networks? With VLAN-backed networking, this is where Guest VLAN Tagging (GVT) comes in. When Guest VLAN Tagging is enabled on a port group, the VM itself is responsible for inserting VLAN tags (802. 1Q C-TAGs) into the frames it transmits. The virtual switch then uses the tag to direct the frame to the correct VLAN. With this approach, a single vNIC can access up to 4,094 VLAN-backed networks , dramatically increasing connectivity without adding more virtual NICs. Guest VLAN Tagging is enabled by specifying a VLAN ID range on the port group. For example, if you configure a range of 10–20, the vNIC will accept traffic tagged with VLAN IDs from 10 to 20, while dropping traffic tagged with IDs outside that range. The VM’s operating system must, of course, be configured to generate the appropriate VLAN-tagged subinterfaces. Guest VLAN Tagging can also be configured on NSX segments in a similar way, but there is an important difference: NSX segments are not identified by VLAN IDs.</description></item><item><title>The Signal in the Storm: Why Chasing More Data Misses the Point</title><link>https://kubermates.org/docs/2025-08-29-the-signal-in-the-storm-why-chasing-more-data-misses-the-point/</link><pubDate>Fri, 29 Aug 2025 17:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-29-the-signal-in-the-storm-why-chasing-more-data-misses-the-point/</guid><description>Posted on August 29, 2025 by Endre Sara, Co-Founder at Causely CNCF projects highlighted in this post As OpenTelemetry adoption has exploded, so has the volume of telemetry data moving through modern observability pipelines. But despite collecting more logs, metrics, and traces than ever before, teams are still struggling to answer the most basic questions during incidents: What broke? Where? And why? In my recent talk at OpenTelemetry Community Day, “ T he Signal in the Storm: Practical Strategies for Managing Telemetry Overload ,” I laid out a different path forward, one focused not on volume, but on meaning. More telemetry doesn’t guarantee more understanding. In many cases, it gives you the illusion of control while silently eroding your ability to reason about the system. That illusion becomes expensive, especially when telemetry pipelines are optimized for ingestion, not insight. Observability Needs a Better Model Traditional observability relies on emitting and aggregating raw signals (spans, logs, and metrics), then querying across that pile post-hoc. That model assumes the data will be useful after something goes wrong. But today’s distributed, dynamic, multi-tenant AI-driven systems don’t give you that luxury. The cost of collecting all raw signals without semantics and without context is too high. We need a shift: from streams of telemetry to structured, semantic representations of how systems behave. That starts by modeling the actual components of the system (entities) and the relationships between them. Not as metadata bolted onto spans, but as first-class signals.</description></item><item><title>VMware Cloud Foundation – Cloud on Your Terms</title><link>https://kubermates.org/docs/2025-08-29-vmware-cloud-foundation-cloud-on-your-terms/</link><pubDate>Fri, 29 Aug 2025 16:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-29-vmware-cloud-foundation-cloud-on-your-terms/</guid><description>Related Articles VMware Cloud Foundation - Cloud on Your Terms VMware Cloud Services Portal migration to the Broadcom Cloud Console Broadcom and Canonical Partner to Fast-track and Secure Containerized Workload Deployments on VMware Cloud Foundation At Broadcom, we are committed to empowering our customers to build and manage modern private clouds on their own terms. With VMware Cloud Foundation (VCF) 9.0, we’re giving our customers a consistent operating model that brings the agility and scalability of the public cloud together with the security, performance, and cost benefits of an on-premises environment. Our goal with this new version is to make our customers’ lives easier by delivering a single, unified platform that supports all applications — from traditional to modern and AI-based — and gives them a consistent experience across their data centers, edge, and cloud infrastructure. The VCF customer entitlement of license portability is a game-changer, introduced by Broadcom to give customers the freedom to use it where it makes the most sense for their business. It allows them to move their licenses and workloads between on-premises data centers and supported endpoints, including hyperscale clouds. This isn’t just a feature; it’s a fundamental shift that puts customer in control of their private cloud deployment preference. To ensure our customers have consistent user experience, the fastest access to our latest innovations, and maximum flexibility to move VCF environments anywhere they choose, Broadcom is transforming its hyperscaler go to market to a license portability-only operating model beginning in the new fiscal year. Customers will purchase VCF subscriptions from Broadcom, which includes license portability to any supported hyperscale cloud. This change simplifies and streamlines how customers consume and deploy VCF along with global hyperscaler cloud infrastructure offerings. By combining the powerful innovations of VCF 9.0 with the flexibility of license portability, we’re giving customers the ability to deliver a modern private cloud that aligns with their business goals. Customers can innovate on their own timeline, leverage the infrastructure they already have, and expand to the cloud when needed. Ultimately, with VCF and license portability, Broadcom is delivering the modern private cloud on our customers’ terms.</description></item><item><title>Bring your own knowledge to OpenShift Lightspeed</title><link>https://kubermates.org/docs/2025-08-29-bring-your-own-knowledge-to-openshift-lightspeed/</link><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-29-bring-your-own-knowledge-to-openshift-lightspeed/</guid><description>Bring your own knowledge to OpenShift Lightspeed 1. Start with documentation 2. Use the BYO knowledge tool 3. Configure OpenShift Lightspeed for your Knowledge Bring your own knowledge Red Hat OpenShift Container Platform | Product Trial About the authors Ben Cohen Erik Jacobs Diego Alvarez More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Many organizations possess a wealth of unique internal knowledge. This includes customized operational runbooks, environment-specific configurations, internal best practices, and stringent compliance protocols. This information may be critical for the organization&amp;rsquo;s day-to-day operations, but it sits outside public knowledge bases where large language models (LLM) are trained. There&amp;rsquo;s a clear need to bridge this gap, and to enable an AI assistant to understand and leverage proprietary context and provide specific and actionable guidance. In response to this need, we introduced the &amp;ldquo;bring your own knowledge&amp;rdquo; (BYO knowledge) capability to Red Hat OpenShift Lightspeed. BYO knowledge empowers you to augment Lightspeed&amp;rsquo;s intelligence with your organization&amp;rsquo;s private documentation and internal expertise. This transforms OpenShift Lightspeed from a generally knowledgeable OpenShift expert into a highly specialized, context-aware partner. It&amp;rsquo;s not just data, it&amp;rsquo;s your data. The benefit of bringing your own data to AI is immediate and impactful.</description></item><item><title>Don't let perfection stop progress when developing AI agents</title><link>https://kubermates.org/docs/2025-08-29-don-t-let-perfection-stop-progress-when-developing-ai-agents/</link><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-29-don-t-let-perfection-stop-progress-when-developing-ai-agents/</guid><description>Don&amp;rsquo;t let perfection stop progress when developing AI agents The agent purist’s dilemma Progress over perfection = allow function over form API-wrapping agents Physical device agents Event-driven agents Data aggregation agents Chatbot orchestrators Specialized functional agents Why this matters Wrapping up How to get started Get started with AI Inference About the author Richard Naszcyniec More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The AI revolution has ignited a debate about what constitutes an &amp;quot; AI agent. &amp;quot; Using the term “AI agent” these days commonly implies autonomous, self-learning systems that pursue complex goals, adapting over time. A very impressive goal, but this purist vision can alienate traditional developers and slow innovation. It’s time to expand the definition, and embrace a broader perspective: AI agents don’t always need to self-learn or chase lofty goals. Functional agents —a new term—that connect large language models (LLMs) to APIs, physical devices, or event-driven systems can be just as impactful. By prioritizing function over form, we enable a broader pool of developers to engage in building AI agents, empower both AI and traditional developers to collaborate, and build practical solutions that drive real-world value. Let’s make progress without always demanding perfection. The traditional definition of an AI agent—rooted in significant AI research—demands autonomy, reasoning, learning, and goal-oriented behavior. These agents, like those powering autonomous vehicles or reinforcement learning models, are impressive but complex. They require deep expertise in machine learning (ML), which can feel like a barrier to traditional developers skilled in APIs, databases, or event-driven architectures. This purist stance risks gatekeeping, sidelining practical agents that don’t learn, but still solve critical problems. Why should an agent that wraps an API call or responds to a sensor be considered inferior? Not every challenge needs a self-evolving neural network—sometimes, a reliable, lightweight solution is enough.</description></item><item><title>Friday Five — August 29, 2025</title><link>https://kubermates.org/docs/2025-08-29-friday-five-august-29-2025/</link><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-29-friday-five-august-29-2025/</guid><description>Friday Five — August 29, 2025 Edge Industry Review : Orbital data center heads to ISS to test real-time edge computing in space Financial services giant strengthens DR through automation What is AI security? Developing a standard AI OS: Unlocking production-grade AI at enterprise scale New product roadmaps on Red Hat TV About the author Red Hat Corporate Communications More like this Blog post Blog post Blog post Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share A new orbital data center being sent to the International Space Station (ISS) provided by the ISS National Lab will enhance space-based computing assets, considering data storage and real-time processing. This project is in partnership with Axiom Space and Red Hat and employs Red Hat Device Edge to provide in-orbit computing power. Learn more When a leading bank wanted to strengthen disaster recovery, it decided to integrate Red Hat Ansible Automation Platform with its internal system. Red Hat Services provided integration, training, and frameworks to automate disaster recovery plans and embed new skills in the financial service company’s team. Learn more To protect your AI systems, it’s important to understand them inside and out. The more you understand your AI technology, the better you can protect it. Learn more Many AI projects show promise, but scaling these successes, integrating AI into core operations, and consistently extracting value across the enterprise remain significant challenges. This is why an open, collaboratively-developed, standard AI operating system (AI OS) is vital. Learn more Red Hat TV is now streaming product roadmaps for Ansible, AI, OpenShift, and RHEL 10 that were recently released at Red Hat Summit. Learn more Red Hat is the world’s leading provider of enterprise open source software solutions, using a community-powered approach to deliver reliable and high-performing Linux, hybrid cloud, container, and Kubernetes technologies. Red Hat helps customers integrate new and existing IT applications, develop cloud-native applications, standardize on our industry-leading operating system, and automate, secure, and manage complex environments. Award-winning support, training, and consulting services make Red Hat a trusted adviser to the Fortune 500.</description></item><item><title>Red Hat Trusted Artifact Signer can now be hosted on RHEL</title><link>https://kubermates.org/docs/2025-08-29-red-hat-trusted-artifact-signer-can-now-be-hosted-on-rhel/</link><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-29-red-hat-trusted-artifact-signer-can-now-be-hosted-on-rhel/</guid><description>Red Hat Trusted Artifact Signer can now be hosted on RHEL Additional options without sacrificing functionality Simplified installation and configuration using Red Hat Ansible Automation Platform Red Hat Product Security About the author Andrew Block More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Organizations looking to better understand the lineage of their software artifacts have begun to adopt signing as a way to improve their security posture. By applying digital signatures to software artifacts, trust can be established to verify that assets have not been substituted or tampered with through the software development and delivery process. Red Hat Trusted Artifact Signer , a key component of Red Hat’s Trusted Software Supply Chain portfolio , provides a suite of tools that supports signing and verifying assets from first commit to deployment. Since Trusted Artifact Signer was first released, it has been available as a Day-2 operator on Red Hat OpenShift. With the release of version 1.2, you can now also deploy the entire Trusted Artifact Signer suite onto a Red Hat Enterprise Linux (RHEL) machine, providing another option for where to run the service. An installation of Trusted Artifact Signer within an RHEL environment will feel familiar to those who have previously deployed the service on OpenShift. Linux containers continue to be the primary delivery vehicle and it relies on the same content source, enabling a consistent experience wherever the service is deployed. One of the goals associated with the design and implementation of Trusted Artifact Signer on RHEL was to include the same core components and to remain as feature-compatible as possible with the existing OpenShift-based deployment. This includes: The entire suite of Trusted Artifact Signer based services, including Fulcio, Rekor, TUF, and a Timestamp Authority The ability to expose each of the services using a set of provided TLS certificates The ability to utilize external instances of MySQL and/or Redis You can also now use Cockpit to monitor the deployment of Trusted Artifact Signer on RHEL. Once enabled, the management of container instances can all be governed within a single console using tooling that is familiar with most RHEL administrators. Ansible Automation Platform is the underlying engine behind the installation and configuration of Trusted Artifact Signer on RHEL. The new redhat.</description></item><item><title>Why you should be using portable zero-touch provisioning on the edge</title><link>https://kubermates.org/docs/2025-08-29-why-you-should-be-using-portable-zero-touch-provisioning-on-the-edge/</link><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-29-why-you-should-be-using-portable-zero-touch-provisioning-on-the-edge/</guid><description>Why you should be using portable zero-touch provisioning on the edge Challenges of provisioning air-gapped edge environments Portable edge ZTP architecture pattern How to design an air-gapped zero-touch provisioning solution Embracing container-native OS with RHEL image mode Laptop deployment Edge device image deployment Modern application distribution for edge Day 2 management evolution Automated zero-touch deployment Red Hat Device Edge | Product Trial About the author Silvio Pérez Torres More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Being connected is important, but for a dispersed, global, and mobile organization that works with increasingly sensitive data, sometimes it&amp;rsquo;s better to stay disconnected for security, safety, operational, and other technical concerns. This type of deployment is often referred to as air-gapped. Common air-gapped environments include ships, vehicles, aircraft, remote industrial sites, nuclear facilities, and emergency field operations. These are locations where traditional network connectivity is unavailable or undesired, but where critical decision-making, predictive maintenance, resource estimation, safety monitoring, intrusion detection, and anomaly detection are essential. To address the challenges of operating in air-gapped environments (including lack of connectivity, limited hardware resources, manual update procedures, and the need for secure, autonomous systems) where agility in deployment and redeployment is critical, a portable edge zero touch provisioning (ZTP) solution can be adopted. Across various industry use cases, several essential requirements consistently emerge, which can be integrated into the ZTP solution: Self-contained: No dependency on cloud API or external data sources Long-lived: Must run for weeks or months without updates Secure by default: Threat models assume local tampering or sabotage Hardware-aware: Limited compute, storage, power, and rugged devices Manually updatable: Updates over USB, SD cards, or other physical media Network-constrained: Low, intermittent, or no connectivity (GSM, satellite, or fully offline) With common characteristics of air-gapped and resource-constrained environments identified, a portable edge ZTP architecture pattern can now be defined: Immutable infrastructure Description: The system, including operating system and services, is deployed as read-only, signed images. Benefits: Helps prevent drift and unauthorized changes, ensuring system integrity and security, which is crucial in an isolated environment. Description: The system, including operating system and services, is deployed as read-only, signed images. Benefits: Helps prevent drift and unauthorized changes, ensuring system integrity and security, which is crucial in an isolated environment. Hybrid workload: Description: Combine OCI-compliant containers with traditional package-based or virtualized workloads using Image Builder to enable seamless coexistence, with locally-hosted registries or USB transfers for modular, isolated deployment Benefits: Simplifies deployment, scalability, and management of applications, allowing for isolated updates without affecting other components. Description: Combine OCI-compliant containers with traditional package-based or virtualized workloads using Image Builder to enable seamless coexistence, with locally-hosted registries or USB transfers for modular, isolated deployment Benefits: Simplifies deployment, scalability, and management of applications, allowing for isolated updates without affecting other components. Offline, physical media, and network-isolated updates Description: Updates are delivered by physical media (USB, SD card) or through local network-restricted methods (DHCP/HTTP boot) using formats such as OSTree or container images.</description></item><item><title>Your essential reading list: Top 10 articles for your IT strategy</title><link>https://kubermates.org/docs/2025-08-29-your-essential-reading-list-top-10-articles-for-your-it-strategy/</link><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-29-your-essential-reading-list-top-10-articles-for-your-it-strategy/</guid><description>Your essential reading list: Top 10 articles for your IT strategy Model Context Protocol (MCP): Understanding security risks and controls Getting started with Red Hat Ansible Lightspeed with IBM watsonx Code Assistant Red Hat boosts partner engagement with program updates and new Partner Demand Center Now available: Red Hat Enterprise Linux Security Select Add-On Why agents are the new kingmakers Disaster recovery approaches for Red Hat OpenShift Virtualization From chaos to cohesion: How NC State is rebuilding IT around Red Hat OpenShift Virtualization 9 articles Red Hat customers are reading after Red Hat Summit Accelerate virtual machine migrations with the migration toolkit for virtualization 2.9 The Red Hat Ansible Certified Collection for Terraform has been updated to support HashiCorp Terraform Enterprise Beyond the roundup: Your next steps Red Hat Learning Subscription | Product Trial About the author Isabel Lee More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share As the pace of innovation across the IT industry accelerates, so does the need to stay informed. To help you stay ahead, we’ve gathered our top articles from July into one essential roundup. This collection of top stories highlights the technologies and strategies that are helping our customers build a more secure, productive, and forward-looking IT environment. From major product announcements to real-world customer stories, these insights will help youdiscover the critical tools and guidance that can strengthen your journey. Model Context Protocol (MCP): Understanding security risks and controls The rise of large language models (LLMs) and their use with external tools has brought a new set of security challenges. This article provides a clear, concise look at the security risks inherent in the Model Context Protocol (MCP), a new protocol by Anthropic. It explores common vulnerabilities like authentication issues and unauthorized command execution. The author shares practical advice on how to mitigate these risks, helping you take advantage of the power of LLMs safely while maintaining a robust security posture. The rise of large language models (LLMs) and their use with external tools has brought a new set of security challenges. This article provides a clear, concise look at the security risks inherent in the Model Context Protocol (MCP), a new protocol by Anthropic. It explores common vulnerabilities like authentication issues and unauthorized command execution. The author shares practical advice on how to mitigate these risks, helping you take advantage of the power of LLMs safely while maintaining a robust security posture.</description></item><item><title>Building a Scalable, Flexible, Cloud-Native GenAI Platform with Open Source Solutions</title><link>https://kubermates.org/docs/2025-08-28-building-a-scalable-flexible-cloud-native-genai-platform-with-open-source-soluti/</link><pubDate>Thu, 28 Aug 2025 23:57:26 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-28-building-a-scalable-flexible-cloud-native-genai-platform-with-open-source-soluti/</guid><description>Core Architecture: Two-Tier Gateway Design ​ Tier One Gateway ​ Tier Two Gateway ​ Design Benefits ​ Routing and Traffic Management ​ Self-Hosted Model Serving with KServe ​ Observability, Control, and Optimization for Production Readiness ​ Observability ​ Control ​ Optimization ​ Pluggable and Flexible ​ Summary ​ Posted on August 28, 2025 by Takeshi Yoneda, Envoy Maintainer and Open Source Software Engineer at Tetrate AI workloads are complex, and unmanaged complexity kills velocity. Your architecture is the key to mastering it. As generative AI (GenAI) becomes foundational to modern software products, developers face a chaotic new reality, juggling different APIs from various providers while also attempting to deploy self-hosted open-source models. This leads to credential sprawl, inconsistent security policies, runaway costs, and an infrastructure that is difficult to scale and govern. Your architecture doesn’t have to be this complex. Platform engineering teams need a secure, scalable way to serve both internal and external LLMs to their users. That’s where tools such as Envoy AI Gateway come in. This reference architecture outlines how to build a flexible GenAI platform using the open source solutions Envoy AI Gateway, KServe, and complementary tools. Whether you’re self-hosting models or integrating with model-serving services on cloud providers, such as OpenAI and Anthropic, this architecture enables a unified, governable interface for LLM traffic. The foundation of this platform is a Two-Tier Gateway Architecture: Deployed in a centralized Gateway Cluster. It serves as the main API traffic entry point for client applications. Its Job: To route traffic to external LLM providers (e.</description></item><item><title>Kubernetes v1.34: User preferences (kuberc) are available for testing in kubectl 1.34</title><link>https://kubermates.org/docs/2025-08-28-kubernetes-v1-34-user-preferences-kuberc-are-available-for-testing-in-kubectl-1-/</link><pubDate>Thu, 28 Aug 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-08-28-kubernetes-v1-34-user-preferences-kuberc-are-available-for-testing-in-kubectl-1-/</guid><description>Kubernetes v1.34: User preferences (kuberc) are available for testing in kubectl 1.34 How it works Defaults Aliases Debugging Get involved Have you ever wished you could enable interactive delete , by default, in kubectl ? Or maybe, you&amp;rsquo;d like to have custom aliases defined, but not necessarily generate hundreds of them manually ? Look no further. SIG-CLI has been working hard to add user preferences to kubectl , and we are happy to announce that this functionality is reaching beta as part of the Kubernetes v1.34 release. kubectl A full description of this functionality is available in our official documentation , but this blog post will answer both of the questions from the beginning of this article. Before we dive into details, let&amp;rsquo;s quickly cover what the user preferences file looks like and where to place it. By default, kubectl will look for kuberc file in your default kubeconfig directory, which is $HOME/. kube. Alternatively, you can specify this location using &amp;ndash;kuberc option or the KUBERC environment variable. kubectl kuberc $HOME/. kube &amp;ndash;kuberc KUBERC Just like every Kubernetes manifest, kuberc file will start with an apiVersion and kind : kuberc apiVersion kind apiVersion : kubectl. config. k8s. io/v1beta1 kind : Preference # the user preferences will follow here apiVersion : kubectl.</description></item><item><title>VMware Cloud Services Portal migration to the Broadcom Cloud Console</title><link>https://kubermates.org/docs/2025-08-28-vmware-cloud-services-portal-migration-to-the-broadcom-cloud-console/</link><pubDate>Thu, 28 Aug 2025 17:30:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-28-vmware-cloud-services-portal-migration-to-the-broadcom-cloud-console/</guid><description>Frequently Asked Questions Related Articles VMware Cloud Services Portal migration to the Broadcom Cloud Console Broadcom and Canonical Partner to Fast-track and Secure Containerized Workload Deployments on VMware Cloud Foundation Strengthened Cyber-Risk Management and Compliance for Large-Scale VMware Cloud Foundation Environments Last Updated: August 28, 2025 Editorial Note: this is a living blog featuring important service updates regarding the migration of the VMware Cloud Services Portal to the Broadcom Cloud Console. Please check back frequently for the latest updates and additional answered FAQs. As this feature is in active development, all information in this document is subject to change. As part of our ongoing efforts to integrate VMware systems with Broadcom, we are pleased to announce that the VMware Cloud Services Portal will be migrating to the Broadcom Cloud Console in the near future. We wanted to take this opportunity to share additional details regarding the transition to ensure you are well informed about this upcoming change to your online product experience: All your current system settings will be migrated to the new Broadcom experience to ensure a smooth and seamless transition including: Identity &amp;amp; Access Management Console Roles &amp;amp; Permissions API Tokens &amp;amp; OAuth Apps Organization Hold Functionality (*new Broadcom Cloud Console functionality) Identity &amp;amp; Access Management Console Roles &amp;amp; Permissions API Tokens &amp;amp; OAuth Apps Organization Hold Functionality (*new Broadcom Cloud Console functionality) Leading up to migration weekend, additional communications will be shared regarding system availability, anticipated down-times and new system readiness. Supporting resources including product documentation, knowledge base articles, etc. will be updated based on the new experience throughout the transition period so check back often as these resources are enhanced. Reference the below frequently asked questions to learn more about this effort and check back frequently for the latest updates and additional answered FAQs. Q: What is changing as part of this migration effort? A: As part of this migration effort, the VMware Cloud Services will be migrating to Broadcom systems to promote a more unified user experience across all Broadcom systems. Q: When will the migration occur? A: The migration of the VMware Cloud Services Portal is planned for mid-to-late September 2025. Additional communications regarding the exact date of the transition will be shared in the coming weeks. Q: Will I have to recreate my Cloud Console user profile in the new system? A: Because the VMware Cloud Services Console uses the same user authentication technology as the new Broadcom Cloud Console, there are no actions required to access the new portal once it’s live.</description></item><item><title>Red Hat contributes Trustify project to OpenSSF’s GUAC community</title><link>https://kubermates.org/docs/2025-08-28-red-hat-contributes-trustify-project-to-openssf-s-guac-community/</link><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-28-red-hat-contributes-trustify-project-to-openssf-s-guac-community/</guid><description>Red Hat contributes Trustify project to OpenSSF’s GUAC community Managing software security data in the open About the author Red Hat More like this Blog post Blog post Blog post Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share With cyberattacks on the rise, increasing software supply chain visibility is crucial for organizations to proactively identify and mitigate vulnerabilities within their applications and infrastructure. However, handling diverse security data sources such as software bill of materials (SBOMs), critical vulnerabilities and exploits (CVEs), and vendor advisories remains a major challenge due to inconsistent formats, varying levels of detail, and the lack of standardized integration points. Addressing this challenge requires not only better tools, but also open collaboration across the entire ecosystem, demanding transparency and trust. In an effort to create a more unified and scalable solution for managing security metadata, Red Hat is proud to contribute Trustify to the Graph for Understanding Artifact Composition (GUAC), an Open Source Security Foundation (OpenSSF) incubating project. This contribution reflects Red Hat’s belief that transparent, upstream-first innovation is essential to building security solutions that are more scalable, interoperable, and community-driven. Under the OpenSSF umbrella, end-users will be able to contribute and collaborate to Trustify, helping to grow the project adoption and mature the technology. Trustify is an open source project, developed by Red Hat, that provides a high-performance, searchable backend for software supply chain metadata. It supports SBOM and advisory formats such as SPDX, CycloneDX, and OSV, and is designed for integration into modern continuous integration and continuous delivery (CI/CD) workflows. The GUAC open source project aggregates and connects software security metadata into a unified graph. It enables developers and security teams to answer complex questions about software provenance, vulnerability impact, and supply chain integrity at scale. Both Trustify and GUAC are designed to tackle the overwhelming challenge of managing vast amounts of software security data that can lead to unmanageable vulnerability handling for security engineers (also known as “alert fatigue”). While Trustify focuses on providing a single, searchable database for SBOMs, CVEs and advisories, GUAC&amp;rsquo;s strength lies in its ability to normalize data from multiple sources into a rich graph database, providing deeper insights and actionable intelligence.</description></item><item><title>Extreme Performance Series 2025: VMware Cloud Foundation 9.0 in the Hands-On Labs</title><link>https://kubermates.org/docs/2025-08-27-extreme-performance-series-2025-vmware-cloud-foundation-9-0-in-the-hands-on-labs/</link><pubDate>Wed, 27 Aug 2025 21:29:32 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-27-extreme-performance-series-2025-vmware-cloud-foundation-9-0-in-the-hands-on-labs/</guid><description>Related Articles Extreme Performance Series 2025: VMware Cloud Foundation 9.0 in the Hands-On Labs Extreme Performance Series 2025: vSAN ESA vs Traditional Storage Array Extreme Performance Video Blog Series 2025 The Extreme Performance Series is back for 2025! This video blog series highlights recent performance work on VMware technology. In this episode, Todd Muirhead talks with Josh Schnee about the inclusion of VMware Cloud Foundation 9.0 (VCF 9.0) in the Hands-on Labs at VMware Explore 2025. Links to additional resources: Hands-on Labs VMmark 4 Extreme Performance Series 2024 – VMmark 4 New and Improved Extreme Performance Series 2023 – Behind the Scenes of the Performance Hands-on Labs Extreme Performance Series Playlist VMware Explore 2025.</description></item><item><title>Introducing Seekable OCI Parallel Pull mode for Amazon EKS</title><link>https://kubermates.org/docs/2025-08-27-introducing-seekable-oci-parallel-pull-mode-for-amazon-eks/</link><pubDate>Wed, 27 Aug 2025 19:13:44 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-27-introducing-seekable-oci-parallel-pull-mode-for-amazon-eks/</guid><description>Introducing Seekable OCI Parallel Pull mode for Amazon EKS Introducing Parallel Pull mode for the SOCI snapshotter Understanding image pulls SOCI Parallel Pull details Performance consideration SOCI Parallel Pull in action Tuning configuration Benchmark Getting started with SOCI Parallel Pull Mode About the authors Containerization has transformed how customers build and deploy modern cloud native applications, offering unparalleled benefits in portability, scalability, and operational efficiency. Containers provide integrated dependency management and enable a standard distribution and deployment model for any workload. With Amazon Elastic Kubernetes Service (Amazon EKS), Kubernetes has emerged as a go-to solution for customers running large-scale containerized workloads that need to efficiently scale to meet evolving needs. However, one persistent challenge continues to impact specific deployment and scaling aspects of Kubernetes workload operations. Container image pulls, particularly when working with large and complex container images, can directly impact the responsiveness and agility of your systems. With the growth of AI/ML workloads, where we see particularly large images, this directly impacts operations as images may take several minutes to pull and prepare. In our recent Under the Hood post for EKS Ultra Scale Clusters, we briefly touched on our evolving solution for this problem, Seekable OCI (SOCI) Parallel Pull. In this post, we’ll explain how container image pulls work and how they impact deployment and scaling operations, we’ll dive deeper into how SOCI parallel pull works, and finally show how it can help you improve image pull performance with your workloads on Amazon EKS. As average container image sizes have grown in recent years, container startup performance has become a critical element of modern cloud native system performance. Image pull and preparation can account for more than 75% of total startup time for new and scaling workloads. This challenge is particularly acute with the rise of AI/ML workloads on Amazon EKS. These workloads have driven significant growth in container image sizes, where images are commonly tens of gigabytes in size.</description></item><item><title>Metal3.io becomes a CNCF incubating project</title><link>https://kubermates.org/docs/2025-08-27-metal3-io-becomes-a-cncf-incubating-project/</link><pubDate>Wed, 27 Aug 2025 19:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-27-metal3-io-becomes-a-cncf-incubating-project/</guid><description>What is Metal3. io? Key Milestones and Ecosystem Growth Integrations Across the Cloud Native Landscape Technical Components Community Highlights Maintainer Perspective From the TOC Looking Ahead Posted on August 27, 2025 by Metal3. io Project Maintainers CNCF projects highlighted in this post The CNCF Technical Oversight Committee (TOC) has voted to accept Metal3. io as a CNCF incubating project. Metal3. io joins a growing ecosystem of technologies tackling real-world challenges at the edge of cloud native infrastructure. The Metal3. io project (pronounced: “Metal Kubed”) provides components for bare metal host management with Kubernetes. You can enroll your bare metal machines, provision operating system images, and then, if you like, deploy Kubernetes clusters to them. From there, operating and upgrading your Kubernetes clusters can be handled by Metal3. io. Moreover, Metal3.</description></item><item><title>Kubernetes v1.34: Of Wind &amp; Will (O' WaW)</title><link>https://kubermates.org/docs/2025-08-27-kubernetes-v1-34-of-wind-will-o-waw/</link><pubDate>Wed, 27 Aug 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-08-27-kubernetes-v1-34-of-wind-will-o-waw/</guid><description>Kubernetes v1.34: Of Wind &amp;amp; Will (O&amp;rsquo; WaW) Release theme and logo Spotlight on key updates Stable: The core of DRA is GA Beta: Projected ServiceAccount tokens for kubelet image credential providers Alpha: Support for KYAML, a Kubernetes dialect of YAML Features graduating to Stable Delayed creation of Job’s replacement Pods Recovery from volume expansion failure VolumeAttributesClass for volume modification Structured authentication configuration Finer-grained authorization based on selectors Restrict anonymous requests with fine-grained controls More efficient requeueing through plugin-specific callbacks Ordered Namespace deletion Streaming list responses Resilient watch cache initialization Relaxing DNS search path validation Support for Direct Service Return (DSR) in Windows kube-proxy Sleep action for Container lifecycle hooks Linux node swap support Allow special characters in environment variables Taint management is separated from Node lifecycle New features in Beta Pod-level resource requests and limits. kuberc file for kubectl user preferences External ServiceAccount token signing DRA features in beta kube-scheduler non-blocking API calls Mutating admission policies Snapshottable API server cache Tooling for declarative validation of Kubernetes-native types Streaming informers for list requests Graceful node shutdown handling for Windows nodes In-place Pod resize improvements New features in Alpha Pod certificates for mTLS authentication &amp;ldquo;Restricted&amp;rdquo; Pod security standard now forbids remote probes Use. status. nominatedNodeName to express Pod placement DRA features in alpha Container restart rules Load environment variables from files created in runtime Graduations, deprecations, and removals in v1.34 Graduations to stable Deprecations and removals Release notes Availability Release Team Project Velocity Event Update Upcoming Release Webinar Get Involved Editors: Agustina Barbetta, Alejandro Josue Leon Bellido, Graziano Casto, Melony Qin, Dipesh Rawat Similar to previous releases, the release of Kubernetes v1.34 introduces new stable, beta, and alpha features. The consistent delivery of high-quality releases underscores the strength of our development cycle and the vibrant support from our community. This release consists of 58 enhancements. Of those enhancements, 23 have graduated to Stable, 22 have entered Beta, and 13 have entered Alpha. There are also some deprecations and removals in this release; make sure to read about those. A release powered by the wind around us — and the will within us. Every release cycle, we inherit winds that we don&amp;rsquo;t really control — the state of our tooling, documentation, and the historical quirks of our project. Sometimes these winds fill our sails, sometimes they push us sideways or die down. What keeps Kubernetes moving isn&amp;rsquo;t the perfect winds, but the will of our sailors who adjust the sails, man the helm, chart the courses and keep the ship steady.</description></item><item><title>How OCI Artifacts Will Drive Future AI Use Cases</title><link>https://kubermates.org/docs/2025-08-27-how-oci-artifacts-will-drive-future-ai-use-cases/</link><pubDate>Wed, 27 Aug 2025 14:24:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-27-how-oci-artifacts-will-drive-future-ai-use-cases/</guid><description>The role of OCI artifacts in the AI ecosystem How Kubernetes and OCI Artifacts fit together CRI-O and OCI Artifact support Help to shape an AI native Kubernetes Posted on August 27, 2025 by Sascha Grunert, CNCF Member Project Maintainer (Graduate Project) CNCF | Special Interest Group (SIG) | CNCF Ambassador In recent years, the software industry has seen a strong shift toward enabling and supporting Artificial Intelligence (AI) workloads. While a variety of high level tools like Large Language Models (LLMs) already exist to support generic use cases, many domain specific solutions are either not yet available or come with significant development costs and risks, particularly when targeting more niche problems. This raises an important challenge: how can we avoid building a fragmented AI tooling landscape with limited real world applicability? To truly support the next wave of AI innovation, especially in cloud native environments, we need to rethink and reinforce our software component foundation. This includes standardizing formats, improving cross platform interoperability, and evolving Kubernetes with AI native features in mind. One of the most promising developments in this space revolves around Open Container Initiative (OCI) artifacts. OCI artifacts enable users to store and distribute arbitrary files and metadata using OCI compliant container registries. While originally used for generic purposes (like ORAS supports them), they’re now finding critical roles in AI/ML workflows, especially with the rise of specifications like the CNCF ModelPack. The CNCF ModelPack Specification builds on top of OCI artifacts and aims to standardize the packaging, distribution, and execution of AI models in cloud native environments. By moving away from proprietary formats, ModelPack facilitates reproducibility, portability, and vendor neutrality in machine learning workflows. This opens the door to several important use cases: Standardized AI/ML model packaging : With OCI artifacts, models can be versioned, distributed, and tracked like container images. This promotes consistency and traceability across environments. Secure model delivery : Well established solutions like sigstore signatures offer mechanisms to sign and verify OCI artifacts, improving model integrity and trustworthiness.</description></item><item><title>Simplify Linux management across your systems’ lifecycles with Red Hat Insights</title><link>https://kubermates.org/docs/2025-08-27-simplify-linux-management-across-your-systems-lifecycles-with-red-hat-insights/</link><pubDate>Wed, 27 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-27-simplify-linux-management-across-your-systems-lifecycles-with-red-hat-insights/</guid><description>Simplify Linux management across your systems’ lifecycles with Red Hat Insights Build more robust systems from the start Move away from a &amp;ldquo;break-fix&amp;rdquo; approach Simplify day-to-day operations About the authors Mary Mackey McKibbin Brady More like this Blog post Blog post Blog post Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share As a system administrator, keeping a Linux fleet running efficiently and securely can feel like you’re constantly putting out fires. Just when you address one issue, another one pops up, often because of a lack of visibility across your environment and too many manual, repetitive tasks. But what if you could spend less time reacting to problems and more time on the work that drives your business forward? That&amp;rsquo;s why we&amp;rsquo;re committed to building and improving new capabilities in Red Hat Insights for Red Hat Enterprise Linux (RHEL) to help you stay ahead of the curve. Insights for RHEL helps you proactively manage your environment across the entire system lifecycle. With exciting new features – some you may have already heard about at Red Hat Summit this past May - you can make better decisions, access targeted information when you need it, and automate repetitive tasks to increase efficiency. Planning for new systems can be difficult when you don&amp;rsquo;t have a clear, centralized view of the future. The new Insights planning for RHEL capability provides a centralized view of future roadmap details, package lifecycle information, and deprecations. This helps you proactively plan system builds, anticipate potential impacts of updates, and align your infrastructure with your long-term strategy. You can build new images with confidence, avoiding last-minute scrambling and unexpected issues. We&amp;rsquo;re also making it easier to create more complete and functional images with package recommendations in image builder. This new feature provides proactive package recommendations based on user inputs and real-world usage patterns, helping you discover valuable components you might otherwise overlook. This is like having a second set of eyes on your work, helping to minimize post-deployment headaches.</description></item><item><title>Use Envoy Gateway as the Unified Ingress Gateway and Waypoint Proxy for Ambient Mesh</title><link>https://kubermates.org/docs/2025-08-26-use-envoy-gateway-as-the-unified-ingress-gateway-and-waypoint-proxy-for-ambient-/</link><pubDate>Tue, 26 Aug 2025 13:09:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-26-use-envoy-gateway-as-the-unified-ingress-gateway-and-waypoint-proxy-for-ambient-/</guid><description>Why use Envoy Gateway with Ambient Mesh? How Envoy Gateway Works in Ambient Mesh? Setting Up Envoy Gateway as the Ingress Gateway Enabling Envoy Gateway as the Waypoint Proxy Should You Use Envoy Gateway with Ambient Mesh? What’s Next? References Posted on August 26, 2025 by Huabing (Robin) Zhao, Software Engineer &amp;amp; Ric Hincapié, DevOps and Support Engineer at Tetrate In this article, we’ll look at how you can use Envoy Gateway , an Envoy project open source solution, together with Istio when running in Ambient mode. This allows you to easily leverage the power of Envoy’s L7 capabilities for Ingress and east-west traffic in your mesh with easy-to-use CRDs. To understand how this integration works, let’s first take a quick look at Ambient Mesh itself. Also known as Istio Ambient mode , it’s a sidecar-less service mesh architecture that aims to simplify deployments and can boost efficiency for specific use cases. Unlike sidecar-based meshes, Ambient splits the data plane into two key components: the ztunnel , which secures service-to-service communication, and the Waypoint Proxy , which handles Layer 7 traffic routing and policy enforcement. On the other side, Envoy Gateway is a Kubernetes-native API gateway built on top of Envoy Proxy. It’s designed to work seamlessly with the Kubernetes Gateway API and takes a batteries-included approach—offering built-in support for authentication, authorization, rate limiting, CORS handling, header manipulation, and more. These capabilities are exposed through familiar Kubernetes-style APIs, letting you fully tap into Envoy’s power without needing complex configurations. Because both Ambient Mesh and Envoy Gateway are built on top of Envoy, they share a common foundation. This makes integration straightforward and allows Envoy Gateway to act as both the Ingress Gateway and Waypoint Proxy —giving you a consistent and powerful way to manage traffic and apply Layer 7 policies across your mesh. While Ambient Mesh simplifies service mesh operations by removing sidecars, its feature set doesn’t yet match the maturity of the sidecar-based model. Some advanced Layer 7 capabilities are either missing, considered experimental, or require extra complexity to configure in native Ambient mode.</description></item><item><title>Broadcom and Canonical Partner to Fast-track and Secure Containerized Workload Deployments on VMware Cloud Foundation</title><link>https://kubermates.org/docs/2025-08-26-broadcom-and-canonical-partner-to-fast-track-and-secure-containerized-workload-d/</link><pubDate>Tue, 26 Aug 2025 12:35:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-26-broadcom-and-canonical-partner-to-fast-track-and-secure-containerized-workload-d/</guid><description>Related Articles Broadcom and Canonical Partner to Fast-track and Secure Containerized Workload Deployments on VMware Cloud Foundation Strengthened Cyber-Risk Management and Compliance for Large-Scale VMware Cloud Foundation Environments Building your GenAI Agents on VCF with Private AI Services A survey on the State of Developers found that most developers spend roughly one day each week tackling development and IT inefficiencies 1. This is way too much overhead for an organization’s most important source of innovation. That’s why we are very excited about the newly expanded partnership between Broadcom and Canonical announced today at Explore 2025 in Las Vegas that will power high velocity, no friction containerized workload deployments on VMware Cloud Foundation (VCF). It brings together the #1 Private Cloud platform with the #1 Cloud OS to help customers who are building Kubernetes-based modern applications streamline support, improve developer efficiency, manage security risks, and simplify AI workload deployment. VCF 9.0 delivers a unified, AI-ready private cloud platform to manage traditional as well as modern containerized applications. Customers get a unified cloud experience reducing friction and significantly increasing developer productivity and experience. This partnership will further streamline the deployment of container-based and AI applications. Fueling Private Cloud Momentum VCF with vSphere Kubernetes Service (VKS) is the leading platform of choice for modern private clouds. VKS is Broadcom’s enterprise-grade upstream conformant CNCF certified Kubernetes distribution, offered as part of VCF. It is easy to install, upgrade, scale, and manage as a multi-cluster deployment delivering a seamless self-service experience. Centralized policy management for access control, networking, security, image registries, and runtime configurations across diverse Kubernetes environments simplifies compliance and governance at scale. Broadcom’s expanded partnership with Canonical, a trusted leader in open-source innovation, will accelerate the deployment of modern containerized and AI workloads with greater efficiency and security.</description></item><item><title>Advancing AI for enterprises: Announcing Expanded Collaboration between Broadcom and AMD on AI</title><link>https://kubermates.org/docs/2025-08-26-advancing-ai-for-enterprises-announcing-expanded-collaboration-between-broadcom-/</link><pubDate>Tue, 26 Aug 2025 12:30:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-26-advancing-ai-for-enterprises-announcing-expanded-collaboration-between-broadcom-/</guid><description>Announcement of Our Collaboration Addressing Key Enterprise AI Challenges Solution Architecture Capability Details 1. Enable Privacy &amp;amp; Security of AI Models: 2. Simplify Infrastructure Management 3. Streamline Model Deployment Unlocking New Use Cases Related Articles Broadcom and Canonical Partner to Fast-track and Secure Containerized Workload Deployments on VMware Cloud Foundation Strengthened Cyber-Risk Management and Compliance for Large-Scale VMware Cloud Foundation Environments Building your GenAI Agents on VCF with Private AI Services Artificial Intelligence (AI) is rapidly transforming industries, and Generative AI (Gen AI) is pushing the boundaries of what’s possible, creating new content and redefining value creation. However, enterprises face significant challenges in AI adoption, especially concerning privacy, data security, and the need for adaptable infrastructure. This is why Broadcom announced VMware Private AI. Today Broadcom and AMD are announcing the expansion of our collaboration to Advance AI for enterprises with a private, secure, and high-performance AI infrastructure. We will release in the future a joint platform bringing VMware Cloud Foundation (VCF) together with AMD Enterprise AI software and AMD Instinct™ GPUs. Together this will enable privacy and security of infrastructure, simplify infrastructure management and streamline AI model deployment. We will also enable Broadcom’s Enhanced DirectPath (I/O) driver models with AMD GPUs to take advantage of VCF’s unique virtualization capabilities for delivering a scalable and efficient platform for AI workloads. Our expanded efforts directly tackle the core issues enterprises encounter with AI deployments: Privacy &amp;amp; Security: Training AI models on public platforms risks exposing sensitive confidential data. Our solution will ensure increased protection of intellectual property, confidential data and access control.</description></item><item><title>Building your GenAI Agents on VCF with Private AI Services</title><link>https://kubermates.org/docs/2025-08-26-building-your-genai-agents-on-vcf-with-private-ai-services/</link><pubDate>Tue, 26 Aug 2025 12:30:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-26-building-your-genai-agents-on-vcf-with-private-ai-services/</guid><description>Model Gallery and Model Governance Model Runtime and ML API Gateway Data Indexing and Retrieval Agent Builder Want to do this yourself? Here are the docs! Related Articles Broadcom and Canonical Partner to Fast-track and Secure Containerized Workload Deployments on VMware Cloud Foundation Strengthened Cyber-Risk Management and Compliance for Large-Scale VMware Cloud Foundation Environments Building your GenAI Agents on VCF with Private AI Services Today at VMware Explore’s general session you saw Chris Wolf demonstrate Intelligent Assist for VMware Cloud Foundation, providing AI-powered assistance for our users. In this blog, we’ll take a step behind the curtain to see how these capabilities are running in VCF, using AI features that our customers can also use to build their own AI experiences with their own private data. VMware Private AI services enable administrators to safely and securely import and share approved AI models (Model Gallery and Model Governance); scale and run Models as a Service for their organization (Model Runtime and ML API gateway); create Knowledge bases and regularly refresh data in a fully supported vector database for creating RAG applications (Data Indexing and Retrieval Service in partnership with Data Services Manager); and provide developers a UI where they can compose models, knowledge bases, and tools together to create Agents (Agent Builder). The Intelligent Assist service is using these capabilities to run the Intelligent Assist agent, and VCF engineering teams are using these services as a common AI platform to deliver joint services and AI workflows. Customers can also use these same capabilities for their own teams. These features give private cloud administrators what they need to safely download, validate, and share models with teams across their cloud. Learn about how to safely onboard popular models from upstream and ensure the model’s behavior meets your enterprises’ expectations and requirements – and behavior doesn’t drift over time in this blog post. Now that you have models securely imported and shared with the right folks in your organization, you will want to run them in an efficient and scalable way. Gone are the days of every division running their own separate copies of the same popular models – instead your team can provide Models as a Service using the Model Runtime. Deploy models on a fully maintained runtime stack from directly within VCF, and then horizontally scale them as they come under load with no end user impact, as users broker their requests via the ML API gateway. This also gives you flexibility to do rolling upgrades of models with zero end user impact. This method of deploying models allows separate lines of business or tenants within a Cloud Service Provider to keep their data separate from each other while ensuring high GPU utilization.</description></item><item><title>Engineering the Next Generation of VMware Cloud Foundation</title><link>https://kubermates.org/docs/2025-08-26-engineering-the-next-generation-of-vmware-cloud-foundation/</link><pubDate>Tue, 26 Aug 2025 12:30:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-26-engineering-the-next-generation-of-vmware-cloud-foundation/</guid><description>VMware Cloud Foundation – The Platform For What’s Next Infrastructure at the Speed of the Developer Private AI as a Service Taking It Further with VCF Advanced Cyber Compliance The Path Forward Related Articles Broadcom and Canonical Partner to Fast-track and Secure Containerized Workload Deployments on VMware Cloud Foundation Strengthened Cyber-Risk Management and Compliance for Large-Scale VMware Cloud Foundation Environments Building your GenAI Agents on VCF with Private AI Services The private cloud is entering a new era. The workloads our infrastructure must serve have shifted from traditional applications only to also include distributed systems, data-intensive analytics, and AI-driven models that demand unprecedented agility and scale. VMware Cloud Foundation (VCF) has always been about giving customers a single, integrated platform to build and operate their private cloud with consistency. But the requirements are evolving—and so must the platform. And VMware Cloud Foundation isn’t just adapting to enterprise transformation – it’s leading it. VCFis the engine driving how modern businesses reimagine their infrastructure, applications, and data strategies. Each new innovation isn’t a reaction to where the industry is going—it sets the pace for what’s possible in the private cloud – whether it’s in a central datacenter, edge or in the hyperscaler environment. VCF 9.0 set the benchmark for the modern private cloud: Sovereign by Design – Data residency tags, geo-fencing, and automated certificate rotation to embed compliance into the platform. Operational Simplicity – New installer, unified fleet management, and streamlined lifecycle operations reduce deployment and maintenance overhead. Modern App Platform – vSphere Kubernetes Service and VM Service deliver VMs and containers side by side with GitOps-driven automation. Performance &amp;amp; Efficiency – NVMe memory tiering and global vSAN deduplication boost scale and flash efficiency at lower cost. Cost Transparency – Built-in showback/chargeback with predictive modeling turns infrastructure consumption into clear financial insight.</description></item><item><title>Strengthened Cyber-Risk Management and Compliance for Large-Scale VMware Cloud Foundation Environments</title><link>https://kubermates.org/docs/2025-08-26-strengthened-cyber-risk-management-and-compliance-for-large-scale-vmware-cloud-f/</link><pubDate>Tue, 26 Aug 2025 12:30:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-26-strengthened-cyber-risk-management-and-compliance-for-large-scale-vmware-cloud-f/</guid><description>Related Articles Broadcom and Canonical Partner to Fast-track and Secure Containerized Workload Deployments on VMware Cloud Foundation Strengthened Cyber-Risk Management and Compliance for Large-Scale VMware Cloud Foundation Environments Building your GenAI Agents on VCF with Private AI Services Is data an organization’s most valuable asset, or biggest liability? The ever-evolving risks presented by today’s modern threat landscape pose unprecedented challenges that most today are ill-equipped to address. With ransomware attacks becoming increasingly sophisticated and regulatory guidelines more stringent, cyber-risk management and compliance lie at the core of every strategic IT decision made by C-Suite executives and board members. 51% of leaders reported security, data protection and privacy as key priorities to enable compliant operations 1. However, the path to achieve successful outcomes is beset with obstacles. Why Organizations Struggle to Manage Cyber-Risk and Maintain Compliance Reactive vs. Proactive Response: data is scattered across different applications in multiple geographic locations, each with its own rules and regulations. 63% of organizations report that complexity from the disaggregated nature of data as the main challenge to maintain compliance 2. Policy monitoring for virtual machines (VMs), containers, databases, infrastructure and technology stack components is siloed, which hinders early detection of drifts and leads to a reactive versus proactive response. Manual Remediation and Recovery: policy management and recovery lack automation, which directly impacts an organization’s ability to scale operations and preserve compliance as applications are changed, moved or deleted. Intense manual intervention inevitably delays time to audit-readiness and leaves unaddressed gaps that could expose critical workloads to increased damage. Stitching of Multiple Tools: consolidation of cyber-risk management, resilience and recovery is easier said than done. Most organizations rely on a piecemeal approach, plagued with operational complexity and inefficiencies that increase chances of human error.</description></item><item><title>Unleashing the Power of Private AI: New Innovations from Broadcom with NVIDIA</title><link>https://kubermates.org/docs/2025-08-26-unleashing-the-power-of-private-ai-new-innovations-from-broadcom-with-nvidia/</link><pubDate>Tue, 26 Aug 2025 12:30:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-26-unleashing-the-power-of-private-ai-new-innovations-from-broadcom-with-nvidia/</guid><description>VMware Private AI Services Now Included in VCF Subscription New Capabilities Supported now NVIDIA Blackwell Architecture Support Future Releases Enable Privacy and Security Multi-tenant models as a service Simplify Infrastructure Management DirectPath enablement for GPUs with VMware Private AI Foundation with NVIDIA Intelligent Assist with VCF Streamline Model Deployment Model Context Protocol (MCP) High-Speed Networking with Enhanced DirectPath I/O VCF Support for NVIDIA HGX Platform with Blackwell GPUs and NVSwitch VCF Support for NVIDIA HGX B200 New Partners in the VMware Private AI Ecosystem Related Articles Broadcom and Canonical Partner to Fast-track and Secure Containerized Workload Deployments on VMware Cloud Foundation Strengthened Cyber-Risk Management and Compliance for Large-Scale VMware Cloud Foundation Environments Building your GenAI Agents on VCF with Private AI Services Enterprises can get tremendous productivity and business transformation from AI. With VMware Private AI Foundation with NVIDIA , Broadcom and NVIDIA aim to unlock AI and unleash productivity with lower TCO. Recently with VCF 9.0 , Broadcom and NVIDIA released several features in VMware Private AI Foundation with NVIDIA to further our mission of providing private and secure AI models for enterprises. Today we are happy to announce additional capabilities to help enterprises in this mission. Previously sold separately, VCF will now include VCF Private AI services as part of the platform. Private AI services enable privacy and security, simplify infrastructure management and streamline model deployment. They include capabilities such as GPU Monitoring, Model Store, Model Runtime, Agent Builder, Vector Database and Data Indexing and Retrieval. By embedding all the benefits of Private AI into VMware Cloud Foundation, enterprises will get a unified platform for their AI and non-AI workloads without an additional purchase. We are releasing an exciting new capability in the platform. NVIDIA Blackwell GPUs unlock the potential of generative, agentic and physical AI by delivering exceptional performance, efficiency and scale for enterprises. VCF will now support the NVIDIA Blackwell architecture, enabling enterprises to get the industry-leading AI training and inference capabilities at unprecedented scale. Let’s look at the supported GPUs.</description></item><item><title>DigitalOcean MCP Server is now available</title><link>https://kubermates.org/docs/2025-08-26-digitalocean-mcp-server-is-now-available/</link><pubDate>Tue, 26 Aug 2025 02:32:43 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-26-digitalocean-mcp-server-is-now-available/</guid><description>DigitalOcean MCP Server is now available What is Model Context Protocol? 1. Broad Service Coverage 2. Natural Language to API Translation 3. Simple, Transparent Setup 4. Scoped Access for Security &amp;amp; Simplicity 5. Seamless Integration Why This Matters How to Get Started Important things to remember Availability &amp;amp; Pricing Related Resources Feedback &amp;amp; Next Steps About the author(s) Try DigitalOcean for free Related Articles Choosing the Right GPU Droplet for your AI/ML Workload By Mavis Franco , Nicole Ghalwash , Amit Jotwani , and Bikram Gupta Published: August 26, 2025 5 min read The new DigitalOcean MCP (Model Context Protocol) Server enables you to manage your cloud resources with simple, natural language commands through AI-powered tools like Cursor, Claude, or your own custom Large Language Models (LLM). Running locally, it connects seamlessly to 9 services â making cloud operations faster, easier, and more intuitive for developers. â Try the new DigitalOcean MCP Server today by following the specific configuration guidelines for the MCP client from the DigitalOcean MCP GitHub repo â Watch our most recent MCP Server video walkthrough: â Explore the Model Context Protocol (MCP) Overview Model Context Protocol (MCP) is an open-source standard that streamlines how AI systems like large language models (LLMs) connect with external tools, systems, and data sources. It defines a standard and consistent way to manage and share context across machine learning components, replacing fragmented integrations. An MCP Server acts as a bridge between an AI application and these external resources. Learn more about the DigitalOcean MCP Server Now supports 9 services (and growing): Accounts , App Platform , Databases , DOKS , Droplets , Insights , Marketplace , Networking , and Spaces Storage. Instead of juggling multiple dashboards or tools, you can manage common cloud operations right inside your favorite MCP-compatible tools.</description></item><item><title>Accelerating 5G standalone rollout: continuous testing to enhance robustness, interoperability and efficiency</title><link>https://kubermates.org/docs/2025-08-26-accelerating-5g-standalone-rollout-continuous-testing-to-enhance-robustness-inte/</link><pubDate>Tue, 26 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-26-accelerating-5g-standalone-rollout-continuous-testing-to-enhance-robustness-inte/</guid><description>Accelerating 5G standalone rollout: continuous testing to enhance robustness, interoperability and efficiency The challenge of 5G core interoperability Testing within a cloud-native environment Using a comprehensive testing regime Introducing the autonomous continuous testing framework Benefits of the automated testing Streamlining functional testing Next steps and how Red Hat can help Red Hat Learning Subscription | Product Trial About the authors Mark Longwell Hanen Garcia Saad Ahmed Rob McManus More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share In this post: Learn about the importance of interoperability testing between the various 5G core network functions. Read about the challenges associated with 5G core lifecycle management, software versioning, and its impact on testing and verification. Find out how Red Hat and Rebaca’s autonomous continuous testing framework helps service providers with the consistent delivery of compliant and interoperable 5G core cloud-native network functions (CNFs). Deploying a 5G core network involves several key components, including the Access and Mobility Management Function (AMF) for connection and mobility management, the Session Management Function (SMF) for managing user data sessions, the User Plane Function (UPF) for service delivery, the Network Repository Function (NRF) for service discovery, and the Policy Control Function (PCF) for policy enforcement among others. A significant challenge for telecommunication (telco) service providers is the management of different software versions of these network functions, especially in a multivendor environment. A particular version may include new features, bug fixes, or require specific configurations. Consequently, testing of the network functions is complex, going beyond the verification of each component&amp;rsquo;s functionality to ensure that various versions of these network functions can interoperate. Dealing with this complexity necessitates extensive testing automation to guarantee streamlined operations across the service provider’s network. Adopting a cloud-native design for the 5G core gives service providers a competitive edge in building networks that easily scale and adapt. A distributed and disaggregated 5G core network introduces additional challenges, particularly around ensuring that all the different components can work together. As 5G core cloud-native network functions (CNFs) are made up of microservices that are constantly being updated and deployed in a dynamic environment, keeping tabs on security and adherence to standards becomes an overwhelming task. To ensure that all independently developed components of the 5G core operate effectively, the various network functions within the 5G core must interoperate efficiently, adhering to 3GPP standards.</description></item><item><title>What’s New in Calico – Summer 2025</title><link>https://kubermates.org/docs/2025-08-25-what-s-new-in-calico-summer-2025/</link><pubDate>Mon, 25 Aug 2025 22:17:49 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-25-what-s-new-in-calico-summer-2025/</guid><description>Security at Scale with a Unified Platform Calico Ingress Gateway with Integrated WAF Granular Dashboard access with new RBAC and UI enhancements Simplified Operations for Kubernetes, VM, and bare metal workloads Policy Recommendations Now Available in Calico Cloud Free Tier Centralized log forwarding for VM and bare metal hosts Improved visualization of VM and bare metal hosts in Calico Service Graph Summary As Kubernetes adoption scales across enterprise architectures, platform architects face mounting pressure to implement consistent security guardrails across distributed, multi-cluster environments while maintaining operational velocity. Modern infrastructure demands a security architecture that can adapt without introducing complexity or performance penalties. Traditional approaches force architects to cobble together separate solutions for ingress protection, network policies, and application-layer security, creating operational friction and increasing attack surface. Today, we’re announcing significant enhancements to Calico that eliminate this architectural complexity. This release introduces native Web Application Firewall (WAF) capabilities integrated directly into Calico’s Ingress Gateway, enabling platform architects to deploy a single technology stack for both ingress management and HTTP-layer threat protection. Combined with enhanced Role-Based Access Controls (RBAC) controls, and centralized observability across heterogeneous workloads, platform architects can now design and implement comprehensive security all within a unified platform. The new features in this release can be grouped under two main categories: Security at Scale with a Unified Platform: This release introduces critical security features that make it easier to secure and scale Kubernetes workloads. Simplified Operations for Kubernetes, VM, and bare metal workloads: Reducing complexity is key to scaling Kubernetes, VM, and bare metal workloads, and this release introduces features that make security management more automated and scalable. Ingress traffic into a Kubernetes cluster is a common entry point for attacks, so it’s critical to inspect and proactively secure it. Since clusters often receive traffic directly from the public internet, analyzing application-layer protocols like HTTP and gRPC for threats is a fundamental security requirement. While there are options to deploy a standalone Web Application Firewall (WAF) with your ingress controller, using an integrated WAF simplifies operations and can reduce both complexity and cost. Calico Ingress Gateway , our implementation of the Kubernetes Gateway API, now includes a built-in WAF that allows you to inspect, authorize, and secure ingress traffic at runtime.</description></item><item><title>Extreme Performance Series 2025: vSAN ESA vs Traditional Storage Array</title><link>https://kubermates.org/docs/2025-08-25-extreme-performance-series-2025-vsan-esa-vs-traditional-storage-array/</link><pubDate>Mon, 25 Aug 2025 20:34:53 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-25-extreme-performance-series-2025-vsan-esa-vs-traditional-storage-array/</guid><description>Related Articles Extreme Performance Series 2025: VMware Cloud Foundation 9.0 in the Hands-On Labs Extreme Performance Series 2025: vSAN ESA vs Traditional Storage Array Extreme Performance Video Blog Series 2025 The Extreme Performance Series is back for 2025! This video blog series highlights recent performance work on VMware technology. In this video, Todd Muirhead talks with Pete Koehler about the performance advantage of vSAN ESA over a traditional storage array that he discussed in a recent blog. Links to additional resources: vSAN Performance: Express Storage Architecture Extreme Performance Series 2023 Extreme Performance Series Playlist VMware Explore 2025.</description></item><item><title>Extreme Performance Video Blog Series 2025</title><link>https://kubermates.org/docs/2025-08-25-extreme-performance-video-blog-series-2025/</link><pubDate>Mon, 25 Aug 2025 20:34:24 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-25-extreme-performance-video-blog-series-2025/</guid><description>Related Articles Extreme Performance Series 2025: VMware Cloud Foundation 9.0 in the Hands-On Labs Extreme Performance Series 2025: vSAN ESA vs Traditional Storage Array Extreme Performance Video Blog Series 2025 Back for a fifth year, the Extreme Performance Series will continue the tradition of exploring some of the exciting work being done at Broadcom. The following series of blog posts is all new for 2025 and will be released starting during the week of VMware Explore. vSAN ESA vs Traditional Storage Array Performance – 8/25/25 VMware Cloud Foundation 9 in the Hands-on Labs – 8/27/25 Performance of Memory Tiering in VMware Cloud Foundation 9 – coming 9/25 Balanced vs High Performance on Modern CPUs – coming 9/25 More to be announced… Looking forward to a great year of extreme performance! Previous years: Extreme Performance 2024 Extreme Performance 2023 Extreme Performance 2022 Extreme Performance 2021.</description></item><item><title>How Should Prometheus Handle OpenTelemetry Resource Attributes? – A UX Research Report</title><link>https://kubermates.org/docs/2025-08-25-how-should-prometheus-handle-opentelemetry-resource-attributes-a-ux-research-rep/</link><pubDate>Mon, 25 Aug 2025 19:42:02 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-25-how-should-prometheus-handle-opentelemetry-resource-attributes-a-ux-research-rep/</guid><description>Project background Research approach User interviews insights Survey insights What I didn’t expect to learn (but did) Recommended solutions Short-term solutions Long-term vision Acknowledgments What’s next for me Posted on August 25, 2025 by Victoria Nduka, User Experience Designer CNCF projects highlighted in this post On May 29th, 2025, I wrapped up my mentorship with Prometheus through the Linux Foundation Mentorship Program. My project focused on understanding how Prometheus handles OpenTelemetry resource attributes and how that experience could be improved for users. My job was to conduct user research to get the user perspective on this challenge. In three months, I conducted user and stakeholder interviews, ran a survey, and analyzed the findings. In this article, I’ll share how I conducted the research, what I uncovered and where the communities involved could go from here. OpenTelemetry (OTel) has something called a resource attribute, which is extra information about the source of a metric, like the service, host, or environment that generated it. Prometheus, a time-series database, uses labels to identify and query metrics. If resource attributes are converted to labels, they can cause what’s known as “a cardinality explosion”, essentially creating too many unique combinations that overwhelm the system. This usually happens if the attributes change often or include a lot of unique values, like user IDs or pod names. Currently, there are three main approaches to handling this challenge: Map all resource attributes to labels: This creates cardinality explosion problems, especially for applications with large numbers of attributes or frequently changing attribute values. Selective promotion: Users manually choose which resource attributes are important enough to be converted to labels in Prometheus. Target info pattern: Put all resource attributes in a separate metric called target_info.</description></item><item><title>VCF Breakroom Chats Episode 56: FinOps in VMware Cloud Foundation</title><link>https://kubermates.org/docs/2025-08-25-vcf-breakroom-chats-episode-56-finops-in-vmware-cloud-foundation/</link><pubDate>Mon, 25 Aug 2025 04:32:39 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-25-vcf-breakroom-chats-episode-56-finops-in-vmware-cloud-foundation/</guid><description>Related Articles Broadcom and Canonical Partner to Fast-track and Secure Containerized Workload Deployments on VMware Cloud Foundation Strengthened Cyber-Risk Management and Compliance for Large-Scale VMware Cloud Foundation Environments Building your GenAI Agents on VCF with Private AI Services Welcome to the next episode of the VCF Breakroom Chats. Today, we are happy to present this vLog with Soumya Kapoor, Product Manager, VCF Division, Broadcom. In this episode, Soumya Kapoor and Sachin Alex discuss all things FinOps and show you how to implement cost controls in your VMware Cloud Foundation environment. Want to learn more about VCF Operations? Check out the VCF Operations web page for more resources. Read blog post Operations in VMware Cloud Foundation 9.0. About the VCF Breakroom Chat Series This webinar series focuses on VMware Cloud Foundation. In this series, we share conversations with industry-recognized experts from Broadcom and with solution partners and customers. You’ll gain relevant insights whether you’re an IT practitioner, IT admin, cloud or platform architect, developer, DevOps, senior IT manager, IT executive, or AI/ML professional. If you are new to the series, please check out our previous episodes.</description></item><item><title>Accelerate issue resolution with a Dedicated Operations Technical Account Manager</title><link>https://kubermates.org/docs/2025-08-25-accelerate-issue-resolution-with-a-dedicated-operations-technical-account-manage/</link><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-25-accelerate-issue-resolution-with-a-dedicated-operations-technical-account-manage/</guid><description>Accelerate issue resolution with a Dedicated Operations Technical Account Manager What is a Dedicated Operations Technical Account Manager (TAM)? Unlock efficiency with a Dedicated Operations TAM Red Hat Learning Subscription | Product Trial About the author Imed Chihi More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Building and managing applications and IT infrastructure securely is a complex task for even the most skilled individual or team. While Red Hat makes it easier to work across platforms and environments with strengthened open source solutions, organizations may lack the time and expertise required to take full advantage of their current Red Hat investments. With a Dedicated Operations Technical Account Manager (TAM), organizations have an embedded resource that serves as an extension of their team within Red Hat. Focused on Day 2 diagnostics, remediation, and risk mitigation, a Dedicated Operations TAM is the ideal resource for complex, high-uptime, or regulated environments. Unlike Red Hat’s standard Technical Account Management subscriptions, the Dedicated Operations TAM offers hands-on issue resolution, optional on-site presence, and security-cleared personnel, making it a value add for OpenShift, Ansible, and compliance-driven customers. TAMs help ensure that you have the tools and knowledge needed to continually innovate while maintaining regulatory compliance and mitigating risk. Quickly reacting to production downtime and minimizing disruptions while building a reliable IT infrastructure and deploying applications is no easy task. Today’s IT teams are under pressure to deliver more with fewer resources. Bringing in a dedicated expert can provide much-needed support, thereby increasing flexibility, improving scalability, and streamlining the management of complex systems. Your Dedicated Operations TAM allows you to address your specific needs more effectively. Additionally, they provide: On-location assistance and flexibility : Dedicated Operations TAMs meet you where you are, offering flexible service options tailored to your needs. You can opt for fully remote TAM service to minimize facility access concerns, or your TAM can be fully embedded on-premise, in your day-to-day operations.</description></item><item><title>From core to tactical edge: A unified platform for defense innovation</title><link>https://kubermates.org/docs/2025-08-25-from-core-to-tactical-edge-a-unified-platform-for-defense-innovation/</link><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-25-from-core-to-tactical-edge-a-unified-platform-for-defense-innovation/</guid><description>From core to tactical edge: A unified platform for defense innovation The rise of edge computing Key challenges in defence digital transformation Proven use cases in defence Sovereignty, systems security, and speed Red Hat Learning Subscription | Product Trial About the author Giuseppe Magnotta More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Driven by the need for agility, security, and sovereignty, the defence sector is undergoing a rapid digital transformation. Military organizations are increasingly operating across a hybrid infrastructure, spanning the strategic core, deployed edge, and tactical edge, while maintaining absolute control over their systems. However, this evolution presents significant challenges, from siloed technologies to cybersecurity threats. Defence organizations must navigate these obstacles by adopting a unified platform approach, leveraging an open framework based on open standards to strengthen autonomy, security, and seamless operations across all environments. Traditionally, defence operations began in the strategic core, which were large, on-premise data centers disconnected from the internet. These handled everything from mission planning to logistics. The first wave of transformation introduced private cloud solutions, allowing military organizations to scale infrastructure dynamically while maintaining digital sovereignty. Many defence entities remain cautious about using the public cloud due to security concerns, opting instead for air-gapped private clouds—physically isolated environments within their facilities—so only screened personnel and verified secure networks have access. Today, the focus has shifted to edge computing, highlighting the need for smarter, real-time, and autonomous decision-making in the field. Defence organizations now operate across two key edge environments, the deployed edge and the tactical edge. The deployed edge consists of mini data centres, capable of running mission-critical applications, processing intelligence and supporting AI workloads. Meanwhile, the tactical edge encompasses lightweight, ruggedized devices such as drones, autonomous vehicles, and soldier-worn systems that can collect, analyse, and act on data in real time, often completely disconnected from central networks.</description></item><item><title>KubeCon India 2025 with Kubeflow: Our Community Experience</title><link>https://kubermates.org/docs/2025-08-23-kubecon-india-2025-with-kubeflow-our-community-experience/</link><pubDate>Sat, 23 Aug 2025 00:00:00 -0500</pubDate><guid>https://kubermates.org/docs/2025-08-23-kubecon-india-2025-with-kubeflow-our-community-experience/</guid><description>Introduction Featured Talks Kubeflow Booth Highlights Our experience Want to help? KubeCon + CloudNativeCon India 2025 in Hyderabad was an absolute blast! As a second-time attendee ( Akash Jaiswal ) and a first-time attendee ( Yash Pal ), we couldn’t help but be blown away by the incredible energy at one of world’s biggest cloud native gatherings. We were super excited seeing Kubeflow get a special shoutout during the opening keynote for its role in cloud native AI/ML and MLOps - definitely made us proud to be part of the community! (Above image shows the keynote moment) We also got super lucky with the chance to volunteer at the Kubeflow booth this year. We also met Johnu George in person, who delivered two amazing talks on Kubeflow’s latest capabilities. It was really exciting to finally meet community members face-to-face whom we’ve only seen in community calls and Slack! This blog shares all the exciting bits from our packed 2 days at KubeCon - from awesome booth conversations to technical deep-dives. We hope this motivates more community members to not just contribute but also attend and help Kubeflow at events like KubeCon. Trust me, you won’t want to miss the next one! 😊 Cloud Native GenAI using KServe and OPEA Speakers: Johnu George , Gavrish Prabhu (Nutanix) Sched Link: View on Sched Bridging Big Data and Machine Learning Ecosystems Speakers: Johnu George , Shiv Jha (Nutanix) Sched Link: View on Sched Here’s a picture of our Kubeflow booth volunteer team. It was really great to meet and interact with audiences who had dozens of questions about Kubeflow, contributors who wanted to help, and developers who were already using it and shared their experiences. Here are some key highlights from our booth conversations: Community Engagement: Discussions on real-world use cases and deployment strategies. Few users shared their experience of using Kubeflow in their companies and how its benefiting them. Many of the audience wants to learn more about how to explore and contribute to Kubeflow. (Answers: Join community calls, and check out GitHub for open issues) Several companies expressed interest in adopting projects like Kubeflow. Few senior engineers were already using it for some of their workloads, now they want to use it for production workload.</description></item><item><title>How I Team Up with GenAI to Craft Conference Talk Proposals</title><link>https://kubermates.org/docs/2025-08-22-how-i-team-up-with-genai-to-craft-conference-talk-proposals/</link><pubDate>Fri, 22 Aug 2025 20:09:07 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-22-how-i-team-up-with-genai-to-craft-conference-talk-proposals/</guid><description>An Incomplete List of GenAI Tells My Process Step One: Brainstorm Buddy Step Two: Shape the Story Step Three: Write the Abstract Prompt 1: Brainstorm Buddy Prompt 2: Shape the Story Prompt 3: Write the Abstract Enjoy the Result! Before you Submit LMKWYT! References Posted on August 22, 2025 by Whitney Lee, CNCF Ambassador and Senior Technical Advocate at Datadog TL;DR: GenAI can help you write conference abstracts, but if you use it without injecting your own curiosity and humanness, reviewers can tell. This post walks through my personal process for co-writing abstracts with GenAI in a way that preserves my voice and experience. I developed this process for technical talks, but it adapts well to other formats too. I’ve spoken at a lot of conferences, big and small. I’ve given keynotes. Breakouts. Workshops. I’ve been around the block and written many conference talk proposals. I’ve also been on program committees and reviewed others’ proposals. There’s always room for improvement, but I like to think I know what good is. In my view, here are some elements of a good talk proposal: – A clear, engaging idea (or a run-of-the-mill idea with a compelling angle) – Some clear details about the talk, like concrete examples or stories, or talk format and tools covered – Clear audience – Clear takeaways As you can imagine, the widespread use of GenAI to generate talk proposals has changed the game for both submitters and reviewers. As a submitter, you can type just one line into ChatGPT, for example: Write me a KubeCon + CloudNativeCon talk abstract on Validating Admission Policy And it produces something that, on the surface, looks pretty good!* Title: Enforcing What Matters: Building Safer Kubernetes with Validating Admission Policies Abstract: As Kubernetes adoption continues to grow across organizations, ensuring workloads meet security, compliance, and operational standards before hitting the cluster is more critical than ever.</description></item><item><title>Virtually Speaking Podcast: Explore Las Vegas 2025 Preview</title><link>https://kubermates.org/docs/2025-08-22-virtually-speaking-podcast-explore-las-vegas-2025-preview/</link><pubDate>Fri, 22 Aug 2025 14:20:04 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-22-virtually-speaking-podcast-explore-las-vegas-2025-preview/</guid><description>Get ready for VMware Explore 2025 in Las Vegas! In this episode of Virtually Speaking, hosts Pete Flecha and John Nicholson are joined by Brad Tompkins, Executive Director of VMUG, to preview what attendees can expect at this year’s event. They cover: Whether you’re coming for the deep technical content, the networking, or the community, VMware Explore 2025 has something for everyone. We hope to see you there! The Virtually Speaking Podcast is a technical podcast dedicated to discussing VMware topics related to private and hybrid cloud. Each week Pete Flecha and John Nicholson bring in various subject matter experts from within the industry to discuss their respective areas of expertise. If you’re new to the Virtually Speaking Podcast check out all episodes on vspeakingpodcast. com and follow on Twitter\X @VirtSpeaking.</description></item><item><title>DenizBank drives AI innovation with Red Hat OpenShift AI</title><link>https://kubermates.org/docs/2025-08-22-denizbank-drives-ai-innovation-with-red-hat-openshift-ai/</link><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-22-denizbank-drives-ai-innovation-with-red-hat-openshift-ai/</guid><description>DenizBank drives AI innovation with Red Hat OpenShift AI The challenge: Overcoming barriers to productivity and innovation The approach: Building a foundation for AI at scale OpenShift AI in action: What was accomplished Measurable success: The compelling results Pioneering the future of banking Get started with AI for enterprise: A beginner’s guide About the authors Erkan Ercan Will McGrath More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share In the competitive world of modern finance, staying ahead means embracing innovation. For DenizBank, one of Türkiye&amp;rsquo;s leading private banks, this means a strong commitment to integrating artificial intelligence (AI) and machine learning (ML) into the core of its operations. Driving this technological evolution within DenizBank is Intertech, DenizBank&amp;rsquo;s IT subsidiary, which has played a key role in redefining what&amp;rsquo;s possible in banking. Before this modernization, DenizBank&amp;rsquo;s data science teams faced significant hurdles that hindered their ability to innovate quickly. Their workflows were characterized by: Manual and inflexible environments : Over 120 data scientists were using a Virtual Desktop Infrastructure (VDI) for model development. This VDI-based environment was not only slow and resource-intensive, but it also made lifecycle management of continuously changing Python libraries incredibly difficult. It forced them to rely on cumbersome digital workstations where each new model required a complex, manual setup. A lack of standardization : Without a unified platform, consistency was a major issue. Data access methods, model environments, database integrations, and code repositories varied from one data scientist to another, making collaboration and management difficult. Slow time-to-market and deployment : The manual processes created a development bottleneck. It was also taking a significant amount of time to decide which model to deploy, as business teams struggled to understand and compare which models performed better. This indecision meant that critical business opportunities were being missed.</description></item><item><title>Friday Five — August 22, 2025</title><link>https://kubermates.org/docs/2025-08-22-friday-five-august-22-2025/</link><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-22-friday-five-august-22-2025/</guid><description>Friday Five — August 22, 2025 An open world: Why Red Hat supports the United Nations Open Source Principles Production AI success: From gen AI promise to business impact More than meets the eye: Behind the scenes of Red Hat Enterprise Linux 10 Evolve your Citrix Infrastructure: Unlocking agility and efficiency with Red Hat OpenShift Virtualization Agentic AI vs. generative AI About the author Red Hat Corporate Communications More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Red Hat is formally endorsing the United Nations (UN) Open Source Principles, which align closely with their own long-standing practices of open collaboration and community building. This endorsement aims to further drive open source adoption and innovation globally, emphasizing transparency, sustainability, and accountability. Learn more Gen AI has the potential to significantly improve customer engagement, reduce costs, and boost productivity. However, many enterprises struggle to move beyond initial experiments to widespread and scalable implementations. This article examines 6 common obstacles to enterprise AI adoption and how Red Hat can help you overcome them. Learn more This is the first post in a new series that looks at the people and planning that went into building and releasing Red Hat Enterprise Linux 10. From the earliest conceptual stages to the launch at Red Hat Summit 2025, we’ll hear firsthand accounts of how RHEL 10 came into being. Learn more The infrastructure decisions you make today shape your organization&amp;rsquo;s flexibility tomorrow. But for many IT leaders, the challenge isn&amp;rsquo;t choosing the next technology, it&amp;rsquo;s figuring out how to move forward without breaking what already works. Running your Citrix Virtual Apps and Desktops on a unified platform with OpenShift Virtualization respects your existing investment while setting you up for what&amp;rsquo;s next. Learn more The line between agentic AI and gen AI can feel blurry because they both begin with a prompt from a user and typically exist in a chatbot-like format.</description></item><item><title>Optimize your virtualization platform: IBM Turbonomic now manages VMs on Red Hat OpenShift</title><link>https://kubermates.org/docs/2025-08-22-optimize-your-virtualization-platform-ibm-turbonomic-now-manages-vms-on-red-hat-/</link><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-22-optimize-your-virtualization-platform-ibm-turbonomic-now-manages-vms-on-red-hat-/</guid><description>Optimize your virtualization platform: IBM Turbonomic now manages VMs on Red Hat OpenShift What is IBM Turbonomic? What is OpenShift Virtualization? How Turbonomic enhances OpenShift Virtualization Key business drivers Red Hat OpenShift Virtualization Engine | Product Trial About the author Simon Seagrave More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share With many businesses rethinking their reliance on their traditional virtualization platform, IT teams are looking for ways to manage costs without compromising performance or control. For organizations migrating VM workloads or running them side by side with containers, IBM Turbonomic and Red Hat OpenShift Virtualization now offer a unified, intelligent platform to optimize and enable innovation. The powerful new integration between IBM Turbonomic, a leader in application resource management (ARM) and Red Hat OpenShift Virtualization extends OpenShift by adding support for virtual machines (VMs) alongside containers. Powered by KubeVirt and the KVM hypervisor, OpenShift Virtualization lets you run, manage, and migrate Linux and Windows VMs. For organizations focused solely on virtualization, Red Hat also offers OpenShift Virtualization Engine , a dedicated edition of OpenShift that provides the same proven virtualization capabilities without the container platform and application development and delivery features found in the broader OpenShift platform. IBM Turbonomic already helps OpenShift users automatically manage and optimize container workloads. With this new integration, Turbonomic extends those same capabilities to VMs running on OpenShift Virtualization, providing teams with unified control of resource optimization across both VMs and containers in hybrid environments. IBM Turbonomic is a platform designed to manage and optimize application resources automatically. It continuously checks resource demands and provides clear recommendations and automated adjustments to keep applications running smoothly and efficiently. At its core, Turbonomic takes a unique application-centric approach to resource management. It automatically discovers your entire environment, from applications to infrastructure, and maps the dependencies between them. Using real-time analytics and AI-driven decision making, it determines the exact resources each application needs and takes action to ensure those needs are met.</description></item><item><title>Migrate to Amazon EKS: Data plane cost modeling with Karpenter and KWOK</title><link>https://kubermates.org/docs/2025-08-21-migrate-to-amazon-eks-data-plane-cost-modeling-with-karpenter-and-kwok/</link><pubDate>Thu, 21 Aug 2025 18:18:42 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-21-migrate-to-amazon-eks-data-plane-cost-modeling-with-karpenter-and-kwok/</guid><description>Migrate to Amazon EKS: Data plane cost modeling with Karpenter and KWOK Solution overview Solution walkthrough Prerequisites Step 1: Create the source EKS cluster Step 2: Deploy an example workload Step 3: Extract cluster configuration with Velero Step 4: Create the destination EKS cluster Step 5: Deploy Karpenter with the KWOK provider Step 6: Restore the backup Clean up Conclusion About the authors When migrating Kubernetes clusters to Amazon Elastic Kubernetes Service (Amazon EKS) , organizations typically follow three phases: assessment, mobilize, and migrate and modernize. The assessment phase involves evaluating technical feasibility for Amazon EKS workloads, analyzing current Kubernetes environments, identifying compatibility issues, estimating costs, and determining timelines with business impact considerations. During the mobilize phase, organizations create detailed migration plans, establish EKS environments with proper networking and security, train teams, and develop testing procedures. The final migrate and modernize phase involves transferring applications and data, validating functionality, implementing cloud-centered features, optimizing resources and costs, and enhancing observability to fully use AWS capabilities. One of the most significant challenges organizations face during the process is cost estimation, which happens in the assessment phase. Karpenter is an open source Kubernetes node autoscaler that efficiently provisions just-in-time compute resources to match workload demands. Unlike traditional autoscalers, Karpenter directly integrates with cloud providers to make intelligent, real-time decisions about instance types, availability zones, and capacity options. It evaluates pod requirements and constraints to select optimal instances, considering factors such as CPU, memory, price, and availability. Karpenter can consolidate workloads for cost efficiency and rapidly scale from zero to handle sudden demand spikes. It supports both spot and on-demand instances, and automatically terminates nodes when they’re no longer needed, optimizing cluster resource utilization and reducing cloud costs. Karpenter uses the concept of Providers to interact with different infrastructure platforms for provisioning and managing compute resources. KWOK (Kubernetes WithOut Kubelet) is a toolkit that simulates data plane nodes without allocating actual infrastructure, and can be used as a provider to create lightweight testing environments that enable developers to validate provisioning decisions, try various (virtual) instance types, and debug scaling behaviors.</description></item><item><title>Stop Building SaaS from Scratch: Meet the SeaNotes Starter Kit</title><link>https://kubermates.org/docs/2025-08-21-stop-building-saas-from-scratch-meet-the-seanotes-starter-kit/</link><pubDate>Thu, 21 Aug 2025 14:20:21 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-21-stop-building-saas-from-scratch-meet-the-seanotes-starter-kit/</guid><description>Stop Building SaaS from Scratch: Meet the SeaNotes Starter Kit What is SeaNotes? Who Itâs For It Solves the Right User Stories (So You Donât Have To) Works Great with LLMs Complete Feature Set ð§¾ Billing and Invoice Generation ð§âð¼Admin Dashboard ð§ Generate a note with Gradient Serverless Inference ð¤User Profile Settings System Status Try It Out Get Started About the author(s) Try DigitalOcean for free Related Articles Powered by DigitalOcean Hatch: Why Uxifyâs Founders Always Choose DigitalOcean Powered by DigitalOcean Hatch: Ontra Mobility is Building Smarter Cities Powered by DigitalOcean Hatch: How Ex-human uses GPU Droplets to Build Empathetic AI that Serves Customers By Amit Jotwani and Haimantika Mitra Published: August 21, 2025 3 min read Thereâs a moment in every SaaS project where you realizeâ¦ Youâre not building your product yet. Youâre setting up auth. Youâre wiring up Stripe. Youâre figuring out how to send emails, where to store files, how to deploy it â and now, how to sprinkle in just enough AI to make it feel modern. Even in 2025, LLMs still struggle with this part. Theyâre great at scaffolding UI and generating business logic. But they donât know how to spin up a database, integrate Stripe, or deploy an actual app. Thatâs exactly what SeaStack solves. SeaStack is a new series of open-source starter kits and reference apps from DigitalOcean â built to help developers ship real apps, faster. Itâs our way of saying: â Hereâs how you can build real things with DigitalOcean â and hereâs the source code to get started. â And SeaNotes is the first one. Live Demo View the GitHub Repo SeaNotes SaaS Starter Kit is an open source GitHub repo that gives developers a simple, production-ready foundation to build real SaaS apps â fast.</description></item><item><title>Celebrating 100 Golden Kubestronauts</title><link>https://kubermates.org/docs/2025-08-21-celebrating-100-golden-kubestronauts/</link><pubDate>Thu, 21 Aug 2025 13:59:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-21-celebrating-100-golden-kubestronauts/</guid><description>What is a Golden Kubestronaut? Once a Golden, Always a Golden Growing Together as a Community Thank You! Posted on August 21, 2025 by Christophe Sauthier, Cloud Native Training and Certification Lead The Kubestronauts program has been growing at an incredible pace. We recently celebrated over 2000 kubestronauts across the globe, and today we’re excited to celebrate another milestone: over 100 Golden Kubestronauts have achieved Golden status in less than five months, thanks to their dedication and hard work. One of the most inspiring aspects of the Golden Kubestronauts community is its global reach. With more than 100 Golden badges earned so far, we see representation from every corner of the world—spanning North and South America, Europe, Asia, and beyond. From Austria, Brazil, and Canada to India, Japan, and Singapore , and from Germany, the Netherlands, France and the United States to places like Bangladesh, Mongolia, and Vietnam , Golden Kubestronauts truly embody the global spirit of CNCF. This diversity highlights not only the worldwide adoption of cloud native technologies, but also the strength of a community that learns, grows, and achieves together—regardless of geography. Becoming a Golden Kubestronaut is the highest level of recognition in the program. It requires completing all CNCF certifications as well as the Linux Foundation Certified Sysadmin (LFCS). Starting October 15th, the Certified Cloud Native Platform Engineering Associate (CNPA) will also be added as part of this journey—just as all future CNCF certifications will be included as the program continues to evolve. Golden Kubestronauts are proof of what’s possible when passion, knowledge, persistence, and a love of cloud native come together. They embody the breadth of skills across the ecosystem, from Kubernetes administration to observability, security, platform engineering, and more. It’s important to highlight that once you’ve earned Golden Kubestronaut status, it’s yours for life.</description></item><item><title>An open world: Why Red Hat supports the United Nations Open Source Principles</title><link>https://kubermates.org/docs/2025-08-21-an-open-world-why-red-hat-supports-the-united-nations-open-source-principles/</link><pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-21-an-open-world-why-red-hat-supports-the-united-nations-open-source-principles/</guid><description>An open world: Why Red Hat supports the United Nations Open Source Principles About the author Shuchi Sharma More like this Blog post Blog post Blog post Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share At Red Hat, we have always believed that the best technology comes from open collaboration. It’s no secret that the components of Red Hat’s products, from the world’s leading enterprise Linux platform to the latest AI innovations of Red Hat AI Inference Server , are created in the upstream, in the open, with communities and projects that make them possible. For three decades, open source has been a core tenet for Red Hat. While the broader technology world was slower to embrace it, more and more organizations are turning to open source-first approaches each year. This is why Red Hat, as a champion of open source technologies and practices, is pleased to formally endorse the United Nations (UN) Open Source Principles. These principles echo how we have always worked by building in the open, contributing back, and nurturing communities that thrive because of inclusion and collaboration. These guidelines aim to drive collaboration and open source adoption within the UN and globally, and were produced by the Open Source United Community as part of the UN Chief Executive Board’s Digital Technology Network. The principles encompass: Open by default: Making open source the standard approach for projects. Open by default: Making open source the standard approach for projects. Contribute back: Encouraging active participation in the open source ecosystem. Secure by design: Making security a priority in all software projects. Foster inclusive participation and community building: Enabling and facilitating diverse and inclusive contributions.</description></item><item><title>Production AI success: From gen AI promise to business impact</title><link>https://kubermates.org/docs/2025-08-21-production-ai-success-from-gen-ai-promise-to-business-impact/</link><pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-21-production-ai-success-from-gen-ai-promise-to-business-impact/</guid><description>Production AI success: From gen AI promise to business impact 1. Defining a strategic path and clear business value for AI 2. Addressing the AI talent gap 3. Establishing a robust and scalable AI infrastructure 4. Addressing AI risks and regulatory compliance 5. Cultivating executive sponsorship 6. Overcoming employee reluctance Building a foundation for AI success Get started with AI for enterprise: A beginner’s guide About the author Brian Stevens More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Generative AI (gen AI) has the potential to significantly improve customer engagement, reduce costs, and boost productivity. However, many enterprises struggle to move beyond initial experiments to widespread and scalable implementations. This article examines 6 common obstacles to enterprise AI adoption —as highlighted in this Harvard Business Review report —and how Red Hat can help you overcome them. Many organizations initiate gen AI projects without a defined strategy or roadmap. This can lead to fragmented efforts that make it difficult to demonstrate a return on investment (ROI). A well-articulated strategy is crucial for successful adoption.</description></item><item><title>Red Hat: a leader in driving sustainability efforts within the IT industry</title><link>https://kubermates.org/docs/2025-08-21-red-hat-a-leader-in-driving-sustainability-efforts-within-the-it-industry/</link><pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-21-red-hat-a-leader-in-driving-sustainability-efforts-within-the-it-industry/</guid><description>Red Hat: a leader in driving sustainability efforts within the IT industry Red Hat&amp;rsquo;s solutions for energy efficiency Supporting small language models (SLMs) for energy efficiency Business benefits for customers Partnership initiatives Conclusion Get started with AI for enterprise: A beginner’s guide About the author Dan Schnitzer More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Red Hat makes it a strategic priority to offer energy-efficient products, addressing the growing demand of our customers for sustainable IT solutions. Our efforts are driven by minimizing the increase in energy consumption of AI, cloud computing, and IT infrastructure. Red Hat focuses on open source technologies designed to improve energy efficiency across IT environments. This includes: AI optimization : With Red Hat AI, you have access to Red Hat AI Inference Server to optimize model inference across the hybrid cloud for faster, cost-effective deployments. Powered by vLLM , the inference server maximizes GPU utilization and enables faster response times, thereby reducing the computational resources and energy required for AI workloads. Virtualization : Red Hat OpenShift Virtualization consolidates multiple virtual machines (VMs) onto fewer physical servers, directly translating to lower energy consumption and increased hardware utilization. System monitoring : Red Hat Enterprise Linux (RHEL) supports performance Co-Pilot (PCP), which provides detailed, real-time visibility into system performance and energy usage, enabling customers to identify and address inefficiencies. Power monitoring : Power monitoring for Red Hat OpenShift provides detailed insights into power consumption, improving adoption of energy-aware computing within cloud-native ecosystems. Intelligent automation : Red Hat Ansible Automation Platform boosts operational efficiency, reducing system down times, and intelligently optimizing resource utilization, which all contributes to a lower carbon footprint and enhanced energy efficiency. InstructLab is an open source project that enhances large language models (LLMs) used in generative AI (gen AI), upgrading an LLM with less human input and fewer resources than retraining. One of the key aspects of InstructLab is its focus on smaller, fine-tuned language models. This approach directly translates to improved energy efficiency compared to larger, more resource-intensive LLMs deployed by competitors.</description></item><item><title>Honoring Styra’s Contributions To OPA</title><link>https://kubermates.org/docs/2025-08-20-honoring-styra-s-contributions-to-opa/</link><pubDate>Wed, 20 Aug 2025 23:03:50 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-20-honoring-styra-s-contributions-to-opa/</guid><description>Honoring Styra’s Contributions To OPA As the creator of Open Policy Agent (OPA), Styra has played an extraordinary role in shaping the Policy as Code (PaC) movement and advancing cloud-native security and governance. This week, the founders and core team behind Styra announced that they are joining Apple. I first met the Styra team at KubeCon 2017 in Austin, when Brian Grant saw an early demo of what we were building, and suggested that I talk to them. Although we have significantly different approaches to PaC, one of my favorite moments at each KubeCon since has been catching up with folks from Styra and exchanging notes on the space. We here at Nirmata, want to take a moment to recognize and thank the entire Styra team – their founders Teemu, Tim, Torin, along with Anders, Charlie and many others – for their pioneering work. Without their vision and drive, there would be no PaC ecosystem as we know it today. At Nirmata, we know that Kyverno’s journey, and our own, would not have been possible without the foundation laid by the OPA community. The early adoption of OPA proved the need for declarative policies and automated governance. It set the standard for how organizations think about declarative policies across cloud and infrastructure environments and it will continue to play an important role in the CNCF landscape. We are also encouraged that the Styra team has found a great home at Apple. With such a strong technology leader supporting them, the OPA community can look forward to ongoing innovation, stability, and new opportunities for growth. We are excited to see what comes next in the OPA roadmap and how the project continues to evolve as a critical part of the cloud native landscape.</description></item><item><title>Announcing OpenAI gpt-oss Models on the DigitalOcean Gradient™ AI Platform</title><link>https://kubermates.org/docs/2025-08-20-announcing-openai-gpt-oss-models-on-the-digitalocean-gradient-ai-platform/</link><pubDate>Wed, 20 Aug 2025 16:21:33 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-20-announcing-openai-gpt-oss-models-on-the-digitalocean-gradient-ai-platform/</guid><description>Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Whatâs new How to get started What you can build Deploy now About the author Try DigitalOcean for free Related Articles Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 Now Live: GPT-5 on the DigitalOcean Gradientâ¢ AI Platform By Grace Morgan Updated: August 20, 2025 2 min read OpenAIâs first open-source GPT models (20b and 120b) are now available on the Gradient AI Platform. This launch brings even more flexibility and choice to developers building AI-powered applications, whether youâre starting with a quick prototype or scaling a production agent. Open-source GPT models: Access gpt-oss 20b and 120b directly on the Gradient AI Platform. Code + UI support: Call the models through our Serverless Inference API or select them in the Gradient dashboard when creating agents, or try them out in the model playground. Integrated experience: Unified billing, observability, and traceability built into the platformâno need to stitch together multiple vendors, billing, or monitoring tools. With code: Call the models directly through our Serverless Inference API. import os import sys from gradient import Gradient model_access_key = os. environ. get( &amp;ldquo;GRADIENT_MODEL_ACCESS_KEY&amp;rdquo; ) if not model_access_key: sys. stderr. write( &amp;ldquo;Error: GRADIENT_MODEL_ACCESS_KEY environment variable is not set. \n &amp;quot; ) sys.</description></item><item><title>Uniting the Cloud Native Community at the Inaugural KCD SF Bay Area</title><link>https://kubermates.org/docs/2025-08-20-uniting-the-cloud-native-community-at-the-inaugural-kcd-sf-bay-area/</link><pubDate>Wed, 20 Aug 2025 14:23:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-20-uniting-the-cloud-native-community-at-the-inaugural-kcd-sf-bay-area/</guid><description>Posted on August 20, 2025 by Lisa-Marie Namphy and Matthew Cascio, CNCF Ambassadors The cloud native landscape is constantly evolving, and staying ahead of the curve means more than just reading documentation—it means connecting with the people who are shaping the future. That’s exactly what the inaugural Kubernetes Community Day (KCD) SF Bay Area event is all about. This full-day gathering, proudly sponsored by the Cloud Native Computing Foundation (CNCF), is the ultimate opportunity for enthusiasts, developers, and industry leaders to come together for a day of insightful talks and unparalleled networking. On September 9th , the historic Computer History Museum in Mountain View will open its doors to the Bay Area’s thriving cloud native community. This isn’t just a conference; it’s a celebration of collaboration and knowledge sharing. As an added bonus, your event registration grants you access to the museum’s fascinating exhibits, allowing you to explore the history of technology during your breaks. The KCD SF Bay Area event boasts an incredible lineup of speakers and visionaries. You’ll have the chance to hear directly from some of the most influential minds in the cloud native space. Confirmed speakers include: Solomon Hykes , the founder of Docker and Dagger. io, whose work has fundamentally changed the way we build and deploy applications. Jim Bugwadia , founder of Nirmata and a key maintainer of the popular policy engine Kyverno. Ramiro Berrelleza , founder of Okteto, who is dedicated to simplifying cloud native development workflows.</description></item><item><title>Disaster Recovery: Achieving Instantaneous Hot-Hot with OpenShift</title><link>https://kubermates.org/docs/2025-08-20-disaster-recovery-achieving-instantaneous-hot-hot-with-openshift/</link><pubDate>Wed, 20 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-20-disaster-recovery-achieving-instantaneous-hot-hot-with-openshift/</guid><description>Disaster Recovery: Achieving Instantaneous Hot-Hot with OpenShift Infrastructure Agility Through the Disaster Recovery Lens Automating Disaster Recovery with OpenShift 1. Networking and Scaling Automation 2. Monitoring 3. User Validation Tooling That Supports Infrastructure Agility Where Can OpenShift Take Your Infrastructure? Red Hat OpenShift Container Platform | Product Trial About the author Derrick Sutherland More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share The biggest challenge for disaster recovery in traditional environments is that every environment looks and feels different. If you&amp;rsquo;re moving from a colo that someone manages to VMware, the cloud, or a different VMware data center, they all look and feel different. Even if you&amp;rsquo;re using the same virtualization provider, storage is probably handled differently. There has to be a lot of planning and strategy around how to copy data from one area to another since that’s not something natively built into your virtualization provider. You also have to figure out networking, DNS, routing, etc. With Red Hat OpenShift , all of these components are software-defined. You get software-defined DNS, networking, and storage layers all because it’s based on Kubernetes. Out-of-the-box OpenShift won’t “automagically” failover without additional levels of configuration, much like VMware or other traditional infrastructure environments. But the tools required are mostly there for you, either pre-packaged and open-source or robustly tested and a standard in the space.</description></item><item><title>Tuning Linux Swap for Kubernetes: A Deep Dive</title><link>https://kubermates.org/docs/2025-08-19-tuning-linux-swap-for-kubernetes-a-deep-dive/</link><pubDate>Tue, 19 Aug 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-08-19-tuning-linux-swap-for-kubernetes-a-deep-dive/</guid><description>Tuning Linux Swap for Kubernetes: A Deep Dive Introduction to Linux swap Anonymous vs File-backed memory Key kernel parameters for swap tuning Swap tests and results Test setup Test methodology Findings Risks and recommendations Kubernetes context Recommended starting point The Kubernetes NodeSwap feature , likely to graduate to stable in the upcoming Kubernetes v1.34 release, allows swap usage: a significant shift from the conventional practice of disabling swap for performance predictability. This article focuses exclusively on tuning swap on Linux nodes, where this feature is available. By allowing Linux nodes to use secondary storage for additional virtual memory when physical RAM is exhausted, node swap support aims to improve resource utilization and reduce out-of-memory (OOM) kills. However, enabling swap is not a &amp;ldquo;turn-key&amp;rdquo; solution. The performance and stability of your nodes under memory pressure are critically dependent on a set of Linux kernel parameters. Misconfiguration can lead to performance degradation and interfere with Kubelet&amp;rsquo;s eviction logic. In this blogpost, I&amp;rsquo;ll dive into critical Linux kernel parameters that govern swap behavior. I will explore how these parameters influence Kubernetes workload performance, swap utilization, and crucial eviction mechanisms. I will present various test results showcasing the impact of different configurations, and share my findings on achieving optimal settings for stable and high-performing Kubernetes clusters. At a high level, the Linux kernel manages memory through pages, typically 4KiB in size. When physical memory becomes constrained, the kernel&amp;rsquo;s page replacement algorithm decides which pages to move to swap space. While the exact logic is a sophisticated optimization, this decision-making process is influenced by certain key factors: Page access patterns (how recently pages are accessed) Page dirtyness (whether pages have been modified) Memory pressure (how urgently the system needs free memory) It is important to understand that not all memory pages are the same.</description></item><item><title>Introducing langchain-gradient: Seamless LangChain Integration with DigitalOcean Gradient™ AI Platform</title><link>https://kubermates.org/docs/2025-08-19-introducing-langchain-gradient-seamless-langchain-integration-with-digitalocean-/</link><pubDate>Tue, 19 Aug 2025 17:37:45 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-19-introducing-langchain-gradient-seamless-langchain-integration-with-digitalocean-/</guid><description>Introducing langchain-gradient: Seamless LangChain Integration with DigitalOcean Gradientâ¢ AI Platform Why langchain-gradient? Example Use Cases Get Started Whatâs Next Get Involved About the author Try DigitalOcean for free Related Articles Agentic Cloud: Reinventing the Cloud with AI Agents How to optimize your cloud architecture for business growth Choosing the Right DigitalOcean Offering for Your AI/ML Workload By Narasimha Badrinath Updated: August 19, 2025 2 min read Weâre excited to introduce langchain-gradient , a new open-source integration that brings the power of LangChain to DigitalOcean Gradient AI Platform. LangChain is a popular framework for building applications powered by large language models (LLMs), with tools for chaining prompts, managing context, and connecting to external data sources. With this package, you can now seamlessly connect LangChainâs flexible orchestration framework to DigitalOceanâs scalable, developer-friendly AI infrastructureâmaking it easier than ever to build, and scale AI-powered applications. As AI adoption accelerates, developers are looking for ways to streamline the process of building and deploying intelligent applications. LangChain has been a go-to framework for chaining together LLMs, tools, and data sources. DigitalOcean Gradient AI, meanwhile, offers a simple, cost-effective platform for running AI workloads at scale. With langchain-gradient , you can now: Connect LangChain to Gradient AI Platform : Use Gradient AI Platform as a provider within your LangChain pipelines, unlocking ideas to production in just a few lines of code. Choose from multiple model options: Instantly access multiple AI models with GPU-accelerated, serverless inference on DigitalOcean. Stay open and flexible : The package is fully open-source and designed to work with the latest versions of LangChain and Gradient AI Platform. With LangChain and Gradient AI Platform working together, you can: Build retrieval-augmented chatbots that pull knowledge from your own files, databases, or cloud storage. Build retrieval-augmented chatbots that pull knowledge from your own files, databases, or cloud storage. Automate customer support workflows by chaining together intent detection, document search, and AI-powered responses.</description></item><item><title>Celebrating 100 Days of Kagent</title><link>https://kubermates.org/docs/2025-08-19-celebrating-100-days-of-kagent/</link><pubDate>Tue, 19 Aug 2025 13:30:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-19-celebrating-100-days-of-kagent/</guid><description>Accepted into CNCF as a sandbox project 🌟 100 Contributors &amp;amp; 1000+ GitHub Stars! Users advocating for kagent Talk to Infrastructure using kagent, A2A, kgateway and others! Integrate kagent with Argo MCP server Integrate kagent with home labs and MCP servers Use kagent for AI reliability engineering Fast delivery with kagent: A2A agent with discord: GCP terraform agent on kagent: 🌍 Kagent travels around the world 📢KubeCon + CloudNativeCon EU 2025 🇫🇷 GOSIM AI Paris 2025 🤠 KCD Texas 2025 🇬🇧London DevOps Meetup May 2025 🇨🇳KubeCon + CloudNativeCon China 2025 🇯🇵 OpenSSF Community – Japan 🇺🇸Open Source Summit NA Beyond cloud native operations 🎉 Wrapping up Posted on August 19, 2025 by Lin Sun, VP of Open Source at Solo. io CNCF projects highlighted in this post When we first introduced kagent on March 17th, 2025, we had a bold vision: to bring agentic AI to cloud native—empowering platforms and DevOps engineers to harness AI agents for solving real operational challenges. Fast forward 100 days, and today we’re celebrating a major milestone: 100 days of the kagent project ! 🎉Thank you everyone for being part of this journey! 🚀 What began as a tool to address our own customer challenges has grown into a thriving open-source project. Kagent is now a CNCF Sandbox project , and it’s evolving into a powerful declarative agentic AI framework —enabling like-minded engineers to run AI agents in Kubernetes, automating complex operations and streamlining troubleshooting workflows. Source: LinkedIn post In just 100 days, kagent has hit two incredible milestones: 🚀 100 contributors , with over 85% from outside Solo. io ⭐ 1000+ GitHub stars from our amazing community! We’ve been actively connecting with contributors through GitHub, Discord, weekly community calls, livestreams, and Contributor Spotlights. We’re incredibly grateful for everyone who’s explored, supported, and contributed to kagent. A huge shoutout to our top 20 contributors , and a special thanks to our top 3 : @eitanya, @peterj, and @sbx03 — we deeply appreciate your dedication and impact!! 🙌 Source: https://kagent. devstats. cncf. io/d/66/developer-activity-counts-by-companies?orgId=1 Over the past 100 days, we’ve been blown away by how our early adopters are using kagent in creative and powerful ways—including in production environments ! Across social media, community meetings , and beyond, users are not just experimenting—they’re innovating. 💡 Here are just a few examples of how the community is putting kagent to work: Source: Linkedin post Source: Linkedin post Source: LinkedIn post Source: LinkedIn post Source: LinkedIn post Source: LinkedIn post In just 100 days, kagent has made its mark on the global stage.</description></item><item><title>Combining GenAI &amp; Agentic AI to build scalable, autonomous systems</title><link>https://kubermates.org/docs/2025-08-18-combining-genai-agentic-ai-to-build-scalable-autonomous-systems/</link><pubDate>Mon, 18 Aug 2025 15:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-18-combining-genai-agentic-ai-to-build-scalable-autonomous-systems/</guid><description>Understanding Agentic AI GenAI is great at creation, while Agentic AI brings decision-making and execution Strategic considerations for tech leaders Posted on August 18, 2025 by Marta Fernandes, Engineering Operations Manager, YLD A common pattern in today’s AI adoption is that businesses are investing heavily in GenAI capabilities, yet many are leaving significant value on the table by failing to pair it with Agentic AI. This matters because while GenAI can generate content, ideas, or responses, it can’t act on them. However, when combined with Agentic AI, your systems shift from being reactive and prompt-driven to autonomous and outcome-oriented. This article aims to help senior technology leaders better understand the value Agentic AI can add to existing AI infrastructure, while outlining key considerations to determine whether its implementation aligns with your business needs. Image caption: Core components of an Agentic AI Architecture from Markovate. Agentic AI systems are designed to operate autonomously, perceiving their environment, making decisions, and executing actions without continuous human oversight. This autonomy enables businesses to streamline operations, reduce costs, and enhance scalability. The image above illustrates how the following systems function: Perception layer: This layer involves collecting data from various sources i. e. sensors, cameras, microphones, and digital inputs, to understand the current state of the environment. For businesses, this means real-time monitoring of operations, customer interactions, and market trends, facilitating proactive decision-making and rapid response to changing conditions. Cognition layer: Once data is collected, the cognition layer processes this information to interpret context, recognise patterns, and determine the best course of action.</description></item><item><title>Gear Up for the 5th Annual KCD Washington DC!</title><link>https://kubermates.org/docs/2025-08-16-gear-up-for-the-5th-annual-kcd-washington-dc/</link><pubDate>Sat, 16 Aug 2025 13:04:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-16-gear-up-for-the-5th-annual-kcd-washington-dc/</guid><description>Posted on August 16, 2025 by Matthew Cascio, CNCF Ambassador and KCD Washington DC Organizer We’re so thrilled to be organizing another year of cloud native community in the Nation’s Capital. Past years have witnessed amazing presentations ranging from core Kubernetes topics to emerging trends in ML/AI, sustainability, virtual clusters and more. This year promises to be even more amazing. Our program is set, and includes two new innovations for our KCD: For the first time, we’re offering a hands-on workshop covering basic and intermediate topics on using Prometheus in your observability practice. The curriculum covers 20-25% of the Linux Foundation’s Prometheus Certified Associate (PCA) course. While this workshop does not lead to certification, and is not a substitute for the PCA course, it will give you a solid working knowledge for getting started and perhaps help you decide if you’d like to pursue certification separately – if you do, you’ll be ahead of the game with this solid foundation. We’re adding a government-focused cybersecurity keynote by our special guest, Ashley Jones, Cybersecurity Advisor for Region III of the Department of Homeland Security, Cybersecurity and Infrastructure Security Agency (CISA. ) Special Government &amp;amp; Cybersecurity Focus – and free admission for our government employee community members! You won’t want to miss Ashley Jones’ keynote if you’re a cybersecurity professional or involved in government technology in any way. A limited number of free tickets will be available soon for our community members working for federal, state or local government agencies. You can request a free government ticket here: https://forms. gle/aLNfRwpvFn3vQ5Vo8 Plus our regular program is amazing! All of that rests on top of our regular program that includes presentations like: Industry perspective: Scaling Cloud Native: Reaching the Next 10 Million Users by Lin Sun at Solo. io K8s in the Enterprise: From Silos to SIGs: Building Kubernetes Community in the Enterprise by Jason Stryker Inclusivity: Unleash Your Inner Ally: Let’s Make Open Source Radically Welcoming! by Catherine Paganini and Christopher Khanoyan advocate for inclusivity in open source.</description></item><item><title>Calico at KubeCon + CloudNativeCon North America 2025!</title><link>https://kubermates.org/docs/2025-08-15-calico-at-kubecon-cloudnativecon-north-america-2025/</link><pubDate>Fri, 15 Aug 2025 13:34:02 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-15-calico-at-kubecon-cloudnativecon-north-america-2025/</guid><description>CalicoCon North America 2025 Happy Hour with Calico KubeCon + CloudNativeCon North America 2025 See you in Atlanta! Get ready, North America! The Calico team is thrilled to announce our participation in KubeCon + CloudNativeCon North America 2025, where we’ll be showcasing the latest advancements in Kubernetes networking, security, and observability. We’re excited to connect with the vibrant cloud-native community, share insights, and demonstrate how Calico Open Source continues to empower organizations worldwide. We have a packed agenda designed to offer you multiple ways to engage with our team and learn more about Calico. Mark your calendars for these exciting opportunities! Join us at CalicoCon North America 2025 , your go-to event for the latest in Kubernetes networking, security, and observability. Hosted by the Calico team, this hybrid event is your chance to hear directly from Calico engineers and leadership, get hands-on with new features, and take an in-depth look at the state of Project Calico. We’ll dive into Calico 3.30, Calico eBPF, and Calico Whisker: open source observability for Kubernetes. Add CalicoCon to your existing KubeCon + CloudNativeCon registration ‌to secure your spot. If you are not attending KubeCon + CloudNativeCon North America but would still like to attend CalicoCon, please reach out to us ‌on the Calico User Slack. Event Details Date : November 10, 2025 Time : 1:00pm to 5:00pm EST Location : Virtual | The Westin Peachtree Plaza Atlanta Register Now After a day of deep dives and technical discussions, unwind and network with other Calico users and the team at our exclusive Happy Hour with Calico ! This is a fantastic opportunity to relax, mingle with fellow Kubernetes enthusiasts, and connect with Calico engineers in a casual setting. Enjoy good food, drinks, and great company as we celebrate the cloud-native community. Event Details Date : November 10, 2025 Time : 5:00pm to 7:00pm EST Location : The Sun Dial Restaurant, Atlanta, GA Register Now Be sure to stop by booth #521 to discuss Calico’s latest Kubernetes network security and observability advancements. Our experts will be on hand to provide demos of Calico’s leading capabilities in Kubernetes network security and observability.</description></item><item><title>Why KubeCon India 2025 Meant More to KodeKloud</title><link>https://kubermates.org/docs/2025-08-15-why-kubecon-india-2025-meant-more-to-kodekloud/</link><pubDate>Fri, 15 Aug 2025 07:29:57 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-15-why-kubecon-india-2025-meant-more-to-kodekloud/</guid><description>&lt;ol&gt;
&lt;li&gt;A Landmark Partnership for India: CNCF + KodeKloud + Linux Foundation 2. A Special Conversation: Mumshad Meets Chris Aniszczyk, CTO of CNCF 3. Book Signing: Knowledge Meets Connection 4. Meeting the Faces Behind the Screens 5. Why KubeCon is a Networking Goldmine for Tech Professionals 6. Key Takeaways for Our Community Closing the Loop: Why This Year Matters Exploring System Architecture for DevOps Engineers Linux: List Disks Linux: &amp;ldquo;cat&amp;rdquo; Command Linux Made Easy for DevOps Beginners From CFP to Stage: Win Your Tech Talk Slot MCP Explained Simply: How AI Can Actually Do Things Now Still Not Job-Ready After Learning DevOps? What Is Kubernetes? Finally, a Simple Explanation! Brilliant. It’s exactly what KodeKloud is about. When attendees, speakers, and industry peers tell you that at KubeCon India, it’s a clear sign that what KodeKloud is doing is making a real impact. For KodeKloud, KubeCon India 2025 was more than a tech conference. It was a celebration of community, connection, and the power of practical learning in the Cloud Native space. From conversations with global leaders to meeting hundreds of learners face-to-face, this year meant more than ever before.&lt;/li&gt;
&lt;/ol&gt;</description></item><item><title>Build smarter AI agents: new tools now available for the DigitalOcean Gradient™ AI Platform</title><link>https://kubermates.org/docs/2025-08-14-build-smarter-ai-agents-new-tools-now-available-for-the-digitalocean-gradient-ai/</link><pubDate>Thu, 14 Aug 2025 14:37:02 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-14-build-smarter-ai-agents-new-tools-now-available-for-the-digitalocean-gradient-ai/</guid><description>Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Gradient AI SDK (Python) Log Stream Insights Dropbox Data Connector A better way to build with AI About the author Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 Now Live: GPT-5 on the DigitalOcean Gradientâ¢ AI Platform By Grace Morgan Updated: August 14, 2025 2 min read Building with AI should be fast, flexible, and frustration-free. Thatâs why weâve leveled up the Gradient AI Platform with new tools that let you ship smarter agents, debug in real time, and tap into the knowledge sources you already use. Now you can: Streamline AI app development with the unified Gradient AI Python SDK -&amp;gt; Streamline AI app development with the unified Gradient AI Python SDK -&amp;gt; Optimize agent performance via log stream insights -&amp;gt; Optimize agent performance via log stream insights -&amp;gt; Augment your agents with Dropbox knowledge bases -&amp;gt; Augment your agents with Dropbox knowledge bases -&amp;gt; Whether youâre launching your first AI MVP or scaling production workloads, these new tools help you build real, production-ready AI apps. Access all major Gradient AI services, like Agents, Inference APIs, Knowledge Bases, and even GPU Droplets , through one modern Python SDK, now available on PyPI. Fully typed with Pydantic and TypedDict for IDE support and reliability Async and sync support, including real-time streaming via SSE Built-in error handling, retries, and custom transport configuration One install: pip install do-gradientai No more jumping between tools or stitching together API calls. The Gradient AI SDK gives you a unified way to access end-to-end AI building blocks, from bare metal GPU infrastructure to a complete agent builder, perfect for quick prototyping, scalable backends, or custom internal tools. Explore the SDK -&amp;gt; To install the SDK: pip install &amp;ndash;pre gradient pip install &amp;ndash;pre gradient When your agent isnât behaving how it should, combing through logs can slow you down. Log Stream Insights helps you skip the guesswork by analyzing your agent trace data in real timeâand surfacing optimization recommendations automatically. Reduce costs and latency with real-time, AI-generated suggestions Improve agent performance with actionable insights for prompt design, model selection, and token use Pay-as-you-go with usage-based pricing Integrate seamlessly with existing Agent Traceability tools for complete visibility If youâre already storing traces, you can turn on Insights with a single toggleâand start seeing recommendations instantly. Add Log Stream Insights â Your team already stores knowledge in Dropbox, why not use it to power your agents? With the new Dropbox Data Connector, you can ingest content directly from Dropbox into your Gradient knowledge bases. Supports 7 formats: PDF, DOC, TXT, CSV, HTML, JSON, and Markdown Ingest in bulk or sync updates via API OAuth-secured access and encrypted data transfer Indexed automatically in OpenSearch for fast querying Use it to build onboarding assistants, search agents, or custom copilots grounded in your docs. Connect Dropbox data â We built these tools to meet you where you areâand help you level up.</description></item><item><title>Introducing GPU Droplets accelerated by NVIDIA HGX H200</title><link>https://kubermates.org/docs/2025-08-14-introducing-gpu-droplets-accelerated-by-nvidia-hgx-h200/</link><pubDate>Thu, 14 Aug 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-14-introducing-gpu-droplets-accelerated-by-nvidia-hgx-h200/</guid><description>Introducing GPU Droplets accelerated by NVIDIA HGX H200 NVIDIA H200s on DigitalOcean: Deploy complex inference workloads at competitive pricing Simple, Scalable, Reliable Customers are experiencing the power of H200s on DigitalOcean Expanding our GPU lineup Get started today About the author Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Now Live: GPT-5 on the DigitalOcean Gradientâ¢ AI Platform By Waverly Swinton Published: August 14, 2025 2 min read Key Takeaways NVIDIA HGX H200 is now available as a DigitalOcean GPU Droplet (virtual, on-demand machines). NVIDIA HGX H200 is now available as a DigitalOcean GPU Droplet (virtual, on-demand machines). NVIDIA HGX H200 GPU Droplets offer significant performance improvements - up to 2x faster inference speeds and double the memory capacity - compared to the H100. NVIDIA H200 GPU Droplets are designed for simplicity, scalability, and cost-effectiveness, with on-demand pricing at just $3.44/GPU/hr. Weâre expanding our AI/ML offerings with the introduction of new DigitalOcean Gradientâ¢ AI GPU Droplets , virtual machines accelerated by NVIDIA HGX H200. This is a significant step forward in our mission to provide the simplest, most scalable cloud for builders everywhere. For developers working on the cutting edge of AI, the right hardware can make all the difference. NVIDIA HGX H200 is built to handle the most demanding tasks in generative AI and high-performance computing (HPC). Weâre making it easier and more affordable for you to tap into this power without the complexity and high costs often seen with other providers. NVIDIA H200 is a state-of-the-art GPU that brings some serious advantages to the table: Iterate and deploy faster: H200 GPU offers up to 2x faster inference speeds than the NVIDIA H100 on large language models like Llama 2 70B. Iterate and deploy faster: H200 GPU offers up to 2x faster inference speeds than the NVIDIA H100 on large language models like Llama 2 70B. Access larger memory capacity: First GPU to feature HBM3e memory, delivering a substantial increase in memory bandwidth and capacity for better inference performance.</description></item><item><title>Sharks of DigitalOcean: Darian Wilkin, Senior Manager, Solutions Engineering</title><link>https://kubermates.org/docs/2025-08-14-sharks-of-digitalocean-darian-wilkin-senior-manager-solutions-engineering/</link><pubDate>Thu, 14 Aug 2025 04:29:13 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-14-sharks-of-digitalocean-darian-wilkin-senior-manager-solutions-engineering/</guid><description>Sharks of DigitalOcean: Darian Wilkin, Senior Manager, Solutions Engineering What makes DigitalOcean a place where people grow? How do you help customers succeed in the cloud? How does DigitalOcean turn ideas into real customer solutions? What does customer-centricity mean to you at DigitalOcean? Which DigitalOcean value resonates with you the most? Dive into the future with DigitalOcean About the author Try DigitalOcean for free Related Articles Sharks of DigitalOcean: Laura Schaffer, VP, Growth Sharks of DigitalOcean: Ali Munir, Staff Technical Account Manager Sharks of DigitalOcean: Jason Dobry, Staff IT Project Specialist By Sujatha R Technical Writer Published: August 14, 2025 3 min read Darian Wilkin has been part of DigitalOceanâs story for over eight years, witnessing firsthand the companyâs transformation from scaling startup to public company. As a Senior Manager of Solutions Architecture, he helps customers build smarter, more scalable cloud solutions. Darian has seen how DigitalOcean has grown to serve more complex workloads for digital native enterprises, while staying true to its roots as a developer-friendly cloud. For Darian, the answer is simple: the people and the culture that bring them together. âI think youâll hear this from just about anyone at DigitalOcean, the people are what make this place special. Not only do I get to work with extremely talented folks, but everyone genuinely enjoys being here. That creates an environment and culture thatâs really pleasant to be a part of. â Itâs this mix of collaboration, shared passion, and a love for the work that keeps Darian energized after nearly a decade at DO. ð¥ Have a look at Darian Wilkinâs full conversation â¬ï¸ âI like to think of our team as the lubricant in the gears that keep things moving when businesses are adopting cloud technologies. We meet with customers one-on-one to help them design robust, fault-tolerant, and performant architectures, while keeping costs in check. If our customers canât expand their cloud presence in a reliable and cost-effective way, their business is going to struggle. So they rely on us for best practices and forward-looking advice on how to get better in the cloud.</description></item><item><title>Canary delivery with Argo Rollout and Amazon VPC Lattice for Amazon EKS</title><link>https://kubermates.org/docs/2025-08-12-canary-delivery-with-argo-rollout-and-amazon-vpc-lattice-for-amazon-eks/</link><pubDate>Tue, 12 Aug 2025 23:55:07 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-12-canary-delivery-with-argo-rollout-and-amazon-vpc-lattice-for-amazon-eks/</guid><description>Canary delivery with Argo Rollout and Amazon VPC Lattice for Amazon EKS Solution overview Walkthrough: progressive delivery with VPC Lattice and Argo Rollouts Integration highlights Conclusion About the authors Modern application delivery demands agility and reliability, where updates are rolled out progressively while making sure of the minimal impact on end users. Progressive delivery strategies, such as canary deployments, allow organizations to release new features by shifting traffic incrementally between old and new versions of a service. This allows organizations to first release features to a small subset of users, monitor system behavior and performance in real time, and automatically roll back if anomalies are detected. This is particularly valuable in modern microservices environments running on platforms such as Amazon Elastic Kubernetes Service (Amazon EKS) , where service meshes and traffic routers provide the necessary infrastructure for fine-grained control over traffic routing. This post explores an architectural approach to implementing progressive delivery using Amazon VPC Lattice, Amazon CloudWatch Synthetics , and Argo Rollouts. The solution uses VPC Lattice for enhanced traffic control across microservices, CloudWatch Synthetics for real-time health and validation monitoring, and Argo Rollouts for orchestrating canary updates. The content in this post addresses readers who are already familiar with networking constructs on Amazon Web Services (AWS), such as Amazon Virtual Private Cloud (Amazon VPC) , CloudWatch Synthetics and Amazon EKS. Instead of defining these services, we focus on their capabilities and integration with VPC Lattice. We also build upon your existing understanding of VPC Lattice concepts and Argo Rollouts. For more background on Amazon VPC Lattice, we recommend that you review the post, Build secure multi-account multi-VPC connectivity for your applications with Amazon VPC Lattice , and the collection of resources in the VPC Lattice Getting started guide. The architecture integrates multiple AWS services and Kubernetes-native components, providing a comprehensive solution for progressive delivery: Amazon EKS : A fully managed Kubernetes service to host microservices. VPC Lattice : A service networking layer that enables consistent traffic routing, authentication, and observability across services.</description></item><item><title>Red Hat Named a Leader in 2025 Gartner® Magic Quadrant™ for Container Management for the Third Consecutive Year</title><link>https://kubermates.org/docs/2025-08-11-red-hat-named-a-leader-in-2025-gartner-magic-quadrant-for-container-management-f/</link><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-11-red-hat-named-a-leader-in-2025-gartner-magic-quadrant-for-container-management-f/</guid><description>Red Hat Named a Leader in 2025 Gartner® Magic Quadrant™ for Container Management for the Third Consecutive Year What is the Gartner Magic Quadrant? How was the Magic Quadrant judged? What criteria were used? Why is this significant to us? Learn more about Red Hat OpenShift for Container Management Red Hat OpenShift Container Platform | Product Trial About the authors Sara Buffington Debbie Margulies More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share This week we announced that for the third consecutive year, Red Hat has been named a Leader in the Gartner® Magic Quadrant™ for Container Management. We’re thrilled by this recognition and believe it represents continued validation of Red Hat OpenShift’s strong execution and strategy for delivering an industry leading application platform across hybrid environments from the data center, to the cloud, and to the edge. Red Hat OpenShift is recognized for its ability to execute and completeness of vision. Red Hat’s investments in OpenShift as a platform for AI workloads, and in building AI capabilities directly into the product to make users’ jobs easier. We believe it is also reflected in Red Hat’s position in the Gartner Magic Quadrant. OpenShift provides a comprehensive application platform with container management capabilities, including support for virtual machine workloads and AI-powered services. Magic Quadrant reports are a culmination of rigorous, fact-based research in specific markets, providing a wide-angle view of the relative positions of the providers in markets where growth is high and provider differentiation is distinct. Providers are positioned into four quadrants: Leaders, Challengers, Visionaries and Niche Players. Leaders execute well against their current vision and are well positioned for tomorrow. The research enables you to get the most from market analysis in alignment with your unique business and technology needs. Gartner defines container management as offerings that support the deployment and operation of containerized workloads and associated resources. It uses a combination of technologies (many open source) that enable agile application deployments and infrastructure modernization.</description></item><item><title>Now Live: GPT-5 on the DigitalOcean Gradient™ AI Platform</title><link>https://kubermates.org/docs/2025-08-07-now-live-gpt-5-on-the-digitalocean-gradient-ai-platform/</link><pubDate>Thu, 07 Aug 2025 23:00:48 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-07-now-live-gpt-5-on-the-digitalocean-gradient-ai-platform/</guid><description>Now Live: GPT-5 on the DigitalOcean Gradientâ¢ AI Platform Get started now Why GPT-5? Whatâs New Deploy today About the author(s) Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 By Grace Morgan , Yogesh Sharma , and Amit Jotwani Updated: August 11, 2025 2 min read Weâre excited to announce that GPT-5 is now available on the DigitalOcean Gradientâ¢ AI Platform. With this update, developers can start using GPT-5 immediately via serverless inference APIs or the Gradient AI Platform SDK. Alternatively, you can bring your own OpenAI API key to integrate the new model into your Gradient AI Platform agent workflow. Curl command curl https://inference. do-ai. run/v1/chat/completions \ -H &amp;ldquo;Authorization: Bearer YOUR_API_KEY&amp;rdquo; \ -H &amp;ldquo;Content-Type: application/json&amp;rdquo; \ -d &amp;rsquo; { &amp;ldquo;model&amp;rdquo;: &amp;ldquo;openai-gpt-5&amp;rdquo;, &amp;ldquo;messages&amp;rdquo;: [ { &amp;ldquo;role&amp;rdquo;: &amp;ldquo;user&amp;rdquo;, &amp;ldquo;content&amp;rdquo;: &amp;ldquo;Explain quantum computing in simple terms&amp;rdquo; } ], &amp;ldquo;temperature&amp;rdquo;: 0.7, &amp;ldquo;max_tokens&amp;rdquo;: 1000 } &amp;rsquo; curl https://inference. do-ai. run/v1/chat/completions \ -H &amp;ldquo;Authorization: Bearer YOUR_API_KEY&amp;rdquo; \ -H &amp;ldquo;Content-Type: application/json&amp;rdquo; \ -d &amp;rsquo; { &amp;ldquo;model&amp;rdquo;: &amp;ldquo;openai-gpt-5&amp;rdquo;, &amp;ldquo;messages&amp;rdquo;: [ { &amp;ldquo;role&amp;rdquo;: &amp;ldquo;user&amp;rdquo;, &amp;ldquo;content&amp;rdquo;: &amp;ldquo;Explain quantum computing in simple terms&amp;rdquo; } ], &amp;ldquo;temperature&amp;rdquo;: 0.7, &amp;ldquo;max_tokens&amp;rdquo;: 1000 } &amp;rsquo; Gradient AI Platform SDK from gradient import Gradient inference_key = &amp;ldquo;YOUR_GRADIENT_INFERENCE_KEY&amp;rdquo; inference_client = Gradient( inference_key=inference_key, ) inference_response = inference_client. chat. completions. create( messages=[ { &amp;ldquo;role&amp;rdquo;: &amp;ldquo;user&amp;rdquo;, &amp;ldquo;content&amp;rdquo;: &amp;ldquo;What is the capital of France?&amp;rdquo;, } ], model=&amp;ldquo;openai-gpt-5&amp;rdquo;, ) print(inference_response. choices[0].</description></item><item><title>Introducing Headlamp AI Assistant</title><link>https://kubermates.org/docs/2025-08-07-introducing-headlamp-ai-assistant/</link><pubDate>Thu, 07 Aug 2025 20:00:00 +0100</pubDate><guid>https://kubermates.org/docs/2025-08-07-introducing-headlamp-ai-assistant/</guid><description>Introducing Headlamp AI Assistant Hopping on the AI train Context is everything Tools AI Plugins Try it out! This announcement originally appeared on the Headlamp blog. To simplify Kubernetes management and troubleshooting, we&amp;rsquo;re thrilled to introduce Headlamp AI Assistant : a powerful new plugin for Headlamp that helps you understand and operate your Kubernetes clusters and applications with greater clarity and ease. Whether you&amp;rsquo;re a seasoned engineer or just getting started, the AI Assistant offers: Fast time to value: Ask questions like &amp;ldquo;Is my application healthy?&amp;rdquo; or &amp;ldquo;How can I fix this?&amp;rdquo; without needing deep Kubernetes knowledge. Deep insights: Start with high-level queries and dig deeper with prompts like &amp;ldquo;List all the problematic pods&amp;rdquo; or &amp;ldquo;How can I fix this pod?&amp;rdquo; Focused &amp;amp; relevant: Ask questions in the context of what you&amp;rsquo;re viewing in the UI, such as &amp;ldquo;What&amp;rsquo;s wrong here?&amp;rdquo; Action-oriented: Let the AI take action for you, like &amp;ldquo;Restart that deployment&amp;rdquo; , with your permission. Here is a demo of the AI Assistant in action as it helps troubleshoot an application running with issues in a Kubernetes cluster: Large Language Models (LLMs) have transformed not just how we access data but also how we interact with it. The rise of tools like ChatGPT opened a world of possibilities, inspiring a wave of new applications. Asking questions or giving commands in natural language is intuitive, especially for users who aren&amp;rsquo;t deeply technical. Now everyone can quickly ask how to do X or Y, without feeling awkward or having to traverse pages and pages of documentation like before. Therefore, Headlamp AI Assistant brings a conversational UI to Headlamp , powered by LLMs that Headlamp users can configure with their own API keys. It is available as a Headlamp plugin, making it easy to integrate into your existing setup. Users can enable it by installing the plugin and configuring it with their own LLM API keys, giving them control over which model powers the assistant. Once enabled, the assistant becomes part of the Headlamp UI, ready to respond to contextual queries and perform actions directly from the interface.</description></item><item><title>Red Hat Named a Leader in 2025 Gartner® Magic Quadrant™ for Cloud-Native Application Platforms for the Second Consecutive Year</title><link>https://kubermates.org/docs/2025-08-07-red-hat-named-a-leader-in-2025-gartner-magic-quadrant-for-cloud-native-applicati/</link><pubDate>Thu, 07 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-07-red-hat-named-a-leader-in-2025-gartner-magic-quadrant-for-cloud-native-applicati/</guid><description>Red Hat Named a Leader in 2025 Gartner® Magic Quadrant™ for Cloud-Native Application Platforms for the Second Consecutive Year What is the Gartner Magic Quadrant? How was the evaluation done in this Magic Quadrant? What was the criteria for inclusion? Why is this significant to us? Learn more about Red Hat OpenShift as a Cloud-Native Application Platform Red Hat OpenShift Container Platform | Product Trial About the authors Alexa Overbay Anes Kim More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share This week we announced that Red Hat has been positioned as a Leader for the second year in a row in the 2025 Gartner® Magic Quadrant™ for Cloud-Native Application Platforms. We are proud of this recognition, as we believe this is a strong validation of Red Hat OpenShift cloud services as a flexible, complete, and secure cloud-native application platform for AI-enabled applications and virtualized and containerized workloads across the hybrid cloud. The Red Hat OpenShift cloud services portfolio consists of jointly engineered solutions with hyperscalers, including Red Hat OpenShift Service on AWS , Azure Red Hat OpenShift , Red Hat OpenShift on IBM Cloud and Red Hat OpenShift Dedicated on Google Cloud. Source: Gartner, “Magic Quadrant for Cloud-Native Application Platforms,” August 2025 To help further explain the evaluation of the Gartner Magic Quadrant from our point of view, we’re answering some of the most frequently asked questions. “Magic Quadrant reports are a culmination of rigorous, fact-based research in specific markets, providing a wide-angle view of the relative positions of the providers in markets where growth is high and provider differentiation is distinct. Providers are positioned into four quadrants: Leaders, Challengers, Visionaries and Niche Players. The research enables you to get the most from market analysis in alignment with your unique business and technology needs. ” “Gartner defines cloud-native application platforms as those that provide managed application runtime environments for applications and integrated capabilities to manage the life cycle of an application or application component. They typically enable distributed application deployments and support cloud style operations — such as elasticity, multitenancy and self-service — without requiring the development team to provision infrastructure or manage containers. Cloud-native application platforms are designed to facilitate the deployment, runtime execution, and management of modern cloud-native or cloud-optimized applications without the need to manage any underlying infrastructure. Also, they are designed to enhance developer productivity, accelerate development and deployment cycles, and increase operational effectiveness by making it easier to scale on demand. Cloud-native application platforms offer a structured execution environment for applications, effectively hiding the complexities of the underlying infrastructure and computing resources.</description></item><item><title>Top 10 articles Red Hat customers are reading right now</title><link>https://kubermates.org/docs/2025-08-07-top-10-articles-red-hat-customers-are-reading-right-now/</link><pubDate>Thu, 07 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-07-top-10-articles-red-hat-customers-are-reading-right-now/</guid><description>Top 10 articles Red Hat customers are reading right now 1. Red Hat OpenShift 4.19 accelerates virtualization and enterprise AI innovation 2. What&amp;rsquo;s new in Red Hat Enterprise Linux 9.6? 3. Top 10 Linux blog posts that deserve a spot on your reading list 4. Ansible Automation Platform and HashiCorp Terraform integration for infrastructure lifecycle management 5. Red Hat’s commitments for sovereign cloud: Your cloud, your rules 6. Announcing OLM v1: Next-Generation Operator Lifecycle Management 7. What&amp;rsquo;s new in Red Hat OpenShift Virtualization 4.19 8. The 2024 Red Hat Product Security Risk Report: CVEs, XZ Backdoor, SSCAs, AI…oh my! 9. The new telco playbook: 4 trends shaping 2025 and beyond 10. Can you vibe code without knowing how to code? Powering your innovation journey with Red Hat Get started with AI for enterprise: A beginner’s guide About the author Isabel Lee More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Staying on top of the latest advancements in open source technology is more critical than ever. At Red Hat, we&amp;rsquo;re at the forefront of this innovation, continuously delivering solutions that empower enterprises to navigate complex challenges, encourage efficiency, and build a more secure future.</description></item><item><title>Simplify network connectivity using Tailscale with Amazon EKS Hybrid Nodes</title><link>https://kubermates.org/docs/2025-08-06-simplify-network-connectivity-using-tailscale-with-amazon-eks-hybrid-nodes/</link><pubDate>Wed, 06 Aug 2025 22:12:21 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-06-simplify-network-connectivity-using-tailscale-with-amazon-eks-hybrid-nodes/</guid><description>Simplify network connectivity using Tailscale with Amazon EKS Hybrid Nodes Prerequisites Cost and security Architecture overview Step 1: Define a remote pod, and node network Step 2: Install Tailscale on your EKS Hybrid Node Step 3: Add a Tailscale subnet router inside your Amazon VPC Step 4: Update subnet routes in the Amazon VPC Step 5: Verify connectivity between the two Tailscale devices AWS CloudShell connectivity test: Step 6: Instructions for EKS Hybrid Nodes Cleaning up Conclusion Next steps About the authors This post was co-authored with Lee Briggs, Director of Solutions Engineering at Tailscale. In this post, we guide you through integrating Tailscale with your Amazon Elastic Kubernetes Service (EKS) Hybrid Nodes environment. Amazon EKS Hybrid Nodes is a feature of Amazon EKS that enables you to streamline your Kubernetes management by connecting on-premises and edge infrastructure to an EKS cluster running in Amazon Web Services (AWS). This unified approach allows AWS to manage the Kubernetes control plane in the cloud while you maintain your hybrid nodes in on-premises or edge locations. We demonstrate how to configure a remote pod network and node address space. Install Tailscale on your hybrid nodes, set up a subnet router within your Amazon Virtual Private Cloud (Amazon VPC) , and update your AWS routes accordingly. This integration provides direct, encrypted connections that streamline the network architecture needed for EKS Hybrid Nodes. Although EKS Hybrid Nodes streamlines the Kubernetes management challenge, network connectivity between your on-premises infrastructure and AWS remains a critical requirement. Tailscale can help streamline this network connectivity between your EKS Hybrid Nodes data plane and Amazon EKS Kubernetes control plane. Unlike traditional VPNs, which tunnel all network traffic through a central gateway server, Tailscale creates a peer-to-peer mesh network (known as a tailnet ). It enables encrypted point-to-point connections using the open source WireGuard protocol, connecting devices and services across different networks with enhanced security features. However, you can still use Tailscale like a traditional VPN.</description></item><item><title>How 1&amp;1 Mail &amp; Media Scaled Kubernetes Networking with eBPF and Calico</title><link>https://kubermates.org/docs/2025-08-05-how-1-1-mail-media-scaled-kubernetes-networking-with-ebpf-and-calico/</link><pubDate>Tue, 05 Aug 2025 16:12:16 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-05-how-1-1-mail-media-scaled-kubernetes-networking-with-ebpf-and-calico/</guid><description>Challenge Solution Results What’s Next “We started in 2017 with Calico and never regretted it!” —Stephan Fudeus, Product Owner/Lead Architect, 1&amp;amp;1 Mail &amp;amp; Media 1&amp;amp;1 Mail &amp;amp; Media, part of the United Internet, powers popular European internet brands including GMX and Web. de, serving more than 50% of Germany’s population with critical identity and email infrastructure. With roughly 45 to 50 million users, network reliability is non-negotiable. Any downtime could affect millions. By 2022, the company had containerized 80% of its workloads on Kubernetes across three self-managed data centers. While the platform, backed by bare metal nodes and custom network layers, was highly scalable, network throughput bottlenecks began to emerge. Pods were limited to 2.5 Gbps of bandwidth due to IP encapsulation overhead, despite 10 Gbps network interfaces. The team needed a solution that: Improved pod-to-pod network performance Maintained strong network policy isolation across up to 40 tenants per cluster Scaled to millions of network connections and 1.4 million HTTP requests per second 1&amp;amp;1 Mail &amp;amp; Media had adopted Calico back in 2017, largely for its unique Kubernetes NetworkPolicy standard support. As their Kubernetes platform evolved, with clusters scaling to 300 bare metal nodes, 16,000 pods, and over 4 million conntrack entries, the team turned to Calico’s eBPF data plane to unlock performance gains. Following successful initial trials of eBPF in development and integration environments, the team moved forward with production migrations in 2023. While early versions of Calico on older Linux kernel versions presented some limitations, these challenges were quickly addressed with proactive collaboration between the Calico maintainers and the team. The Calico team introduced key improvements such as auto-scaling of connection tracking tables, which enhanced resilience under load.</description></item><item><title>Introducing OpenShift Service Mesh 3.1</title><link>https://kubermates.org/docs/2025-08-05-introducing-openshift-service-mesh-3-1/</link><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-08-05-introducing-openshift-service-mesh-3-1/</guid><description>Introducing OpenShift Service Mesh 3.1 Upgrading to OpenShift Service Mesh 3.1 Kubernetes Gateway API support in OCP 4.19+ Generally available dual-stack support for x86 clusters Moving to UBI Micro containers Toward a sidecar-less service mesh: Istio&amp;rsquo;s ambient mode Kiali updates Mesh Page Updates Performance and scalability with large meshes Get started with OpenShift Service Mesh Red Hat OpenShift Container Platform | Product Trial About the author Jamie Longmuir More like this Blog post Blog post Blog post Keep exploring Browse by channel Automation Artificial intelligence Open hybrid cloud Security Edge computing Infrastructure Applications Virtualization Share Red Hat OpenShift Service Mesh 3.1 has been released and is included with the Red Hat OpenShift Container Platform and Red Hat OpenShift Platform Plus. Based on the Istio, Envoy, and Kiali projects, this release updates the version of Istio to 1.26 and Kiali to 2.11 , and is supported on OpenShift Container Platform 4.16 and above. This is the first minor release following Red Hat OpenShift Service Mesh 3.0, a major update to converge OpenShift Service Mesh with the community Istio project, with installation and management using the Sail operator. This change helps ensure that OpenShift Service Mesh can offer the latest stable Istio features with Red Hat support. If you are running OpenShift Service Mesh 2.6 or earlier releases, you must upgrade to OpenShift Service Mesh 3.0 before upgrading to 3.1. We recommend migrating to OpenShift Service Mesh 3.0 promptly, because version 2.6 reaches its end of life on March 12, 2026. An in-depth migration guide is provided in the OpenShift Service Mesh 3.0 documentation , including an analysis of the differences between OpenShift Service Mesh 2.6 and 3.0. We&amp;rsquo;ve also recently published an article that describes how to use the Kiali console for migrating between OpenShift Service Mesh 2.6 and 3.0. For an example of OpenShift Service Mesh 3.0 in action, with fully configured metrics and the Kiali console, see this solution pattern. Kubernetes Gateway API is the next generation of Kubernetes Ingress, load balancer, and service mesh APIs. Istio plans to make it the default set of APIs for creating and managing traffic using a service mesh, and it is required for using Istio&amp;rsquo;s ambient mode. Note that there are no plans to remove the stable Istio APIs, such as VirtualService, DestinationRule, and others.</description></item><item><title>Top 5 Kubernetes Network Issues You Can Catch Early with Calico Whisker</title><link>https://kubermates.org/docs/2025-07-29-top-5-kubernetes-network-issues-you-can-catch-early-with-calico-whisker/</link><pubDate>Tue, 29 Jul 2025 18:54:43 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-29-top-5-kubernetes-network-issues-you-can-catch-early-with-calico-whisker/</guid><description>&lt;ol&gt;
&lt;li&gt;Policy Misconfigurations 2. Misconfigured ServiceAccount Trust Boundaries 3. Asymmetric or One-Sided Flow Patterns 4. Traffic to “Private Network” Destinations Without Explicit Source Policies 5. Zombie Connections and Stale Pod Traffic 🧠 Bonus: Why Whisker Stands Apart from Traditional Logs 🧪 Ready to Try It? Kubernetes networking is deceptively simple on the surface, until it breaks, silently leaks data, or opens the door to a full-cluster compromise. As modern workloads become more distributed and ephemeral, traditional logging and metrics just can’t keep up with the complexity of cloud-native traffic flows. That’s where Calico Whisker comes in. Whisker is a lightweight Kubernetes-native observability tool created by Tigera. It offers deep insights into real-time traffic flow patterns, without requiring you to deploy heavyweight service meshes or packet sniffer. And here’s something you won’t get anywhere else: Whisker is data plane-agnostic. Whether you run Calico eBPF data plane, nftables, or iptables, you’ll get the same high-fidelity flow logs with consistent fields, format, and visibility.&lt;/li&gt;
&lt;/ol&gt;</description></item><item><title>Innovating DigitalOcean Managed Databases: Our H1 Progress and Improvements</title><link>https://kubermates.org/docs/2025-07-29-innovating-digitalocean-managed-databases-our-h1-progress-and-improvements/</link><pubDate>Tue, 29 Jul 2025 13:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-29-innovating-digitalocean-managed-databases-our-h1-progress-and-improvements/</guid><description>Innovating DigitalOcean Managed Databases: Our H1 Progress and Improvements Managed PostgreSQL support for v17 [February] Managed MongoDB support for v8 [March] Support for up to 20TB for Managed MySQL and 30TB for PostgreSQL [March] Introducing DigitalOcean Managed Caching for Valkey [April] Introducing Role-Based Access Control to DigitalOcean Managed MonogDB with Predefined Roles [May] Database Observability, Monitoring, and Hardening Advancements [June] Support for Kafka Schema Registry [July] Want to see what weâre working on for H2? About the author Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 By Nicole Ghalwash Published: July 29, 2025 4 min read Databases are the cornerstone of modern applications, and at DigitalOcean, weâre committed to providing you with powerful, scalable, and easy-to-use managed database solutions. The first half of 2025 has been a busy and exciting time for our Managed Databases team, as weâve rolled out a series of significant new features and engines designed to enhance your development experience. From expanding support for popular database versions to introducing advanced observability tools, weâve been focused on delivering the innovations you need to build and scale with confidence. In chronological order, letâs take a look at some of the key launches and improvements weâve made within the first half of the 2025 fiscal year [H1]. Launched February 11, support for PostgreSQL 17 offers improved performance, expanded developer tools, enhanced high availability and replication, and advanced security and monitoring capabilities. To read more about all of the new features, such as indexing improvements and expanding monitoring and analysis tools, check out our blog post from this past February. Continuing the theme of database upgrades, as of March 13, DigitalOcean Managed MongoDB supports MongoDB v8 (8.0.12). This latest release brings significant improvements in performance, scalability, and security, including faster replication through improved concurrent writes, optimized time-series data handling, and enhanced client-side encryption with support for range queries. Developers also gain greater control over performance with features like persistent query settings and default maximum execution times. To learn more about this release and our Managed MongoDB service, check out our blog or visit the MongoDB homepage. Both Managed MySQL and PostgreSQL have doubled the amount of storage they support, with MySQL now supporting up to 20TB and PostgreSQL supporting up to 30TB. As of March 19, these expanded storage options are currently available in Singapore (SGP1), San Francisco (SFO2), Toronto (TOR1), New York City (NYC2, NYC3), and Frankfurt (FRA1), with more regions to come.</description></item><item><title>Kubernetes v1.34 Sneak Peek</title><link>https://kubermates.org/docs/2025-07-28-kubernetes-v1-34-sneak-peek/</link><pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-28-kubernetes-v1-34-sneak-peek/</guid><description>Kubernetes v1.34 Sneak Peek Featured enhancements of Kubernetes v1.34 The core of DRA targets stable ServiceAccount tokens for image pull authentication Pod replacement policy for Deployments Production-ready tracing for kubelet and API Server PreferSameZone and PreferSameNode traffic distribution for Services Support for KYAML: a Kubernetes dialect of YAML Fine-grained autoscaling control with HPA configurable tolerance Want to know more? Get involved Kubernetes v1.34 is coming at the end of August 2025. This release will not include any removal or deprecation, but it is packed with an impressive number of enhancements. Here are some of the features we are most excited about in this cycle! Please note that this information reflects the current state of v1.34 development and may change before release. The following list highlights some of the notable enhancements likely to be included in the v1.34 release, but is not an exhaustive list of all planned changes. This is not a commitment and the release content is subject to change. Dynamic Resource Allocation (DRA) provides a flexible way to categorize, request, and use devices like GPUs or custom hardware in your Kubernetes cluster. Since the v1.30 release, DRA has been based around claiming devices using structured parameters that are opaque to the core of Kubernetes. The relevant enhancement proposal, KEP-4381 , took inspiration from dynamic provisioning for storage volumes. DRA with structured parameters relies on a set of supporting API kinds: ResourceClaim, DeviceClass, ResourceClaimTemplate, and ResourceSlice API types under resource. k8s. io , while extending the. spec for Pods with a new resourceClaims field.</description></item><item><title>Kubernetes Is Powerful, But Not Secure (at least not by default)</title><link>https://kubermates.org/docs/2025-07-24-kubernetes-is-powerful-but-not-secure-at-least-not-by-default/</link><pubDate>Thu, 24 Jul 2025 19:38:47 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-24-kubernetes-is-powerful-but-not-secure-at-least-not-by-default/</guid><description>Why Is Microsegmentation So Hard? 🛡️Securing the Default Cluster Posture with Kubernetes Network Policy 🔐 What Makes Admin Network Policy (ANP) So Special? 🛡️Reach Even Further with Calico Network Policies 🧩 Delegating Policy Ownership with Kubernetes RBAC 🧪 Safely Testing and Troubleshooting Policies 👀 Calico Whisker: Observe Before You Act 🧪 Staged Network Policies: Test Without Breaking Things 🤖 Policy Recommendation Engine (Free with Calico Cloud Free Tier) 🗺️ Bonus: Visual Tools in Calico Cloud Free Tier Conclusion: Microsegmentation Doesn’t Have to Be a Nightmare Kubernetes has transformed how we deploy and manage applications. It gives us the ability to spin up a virtual data center in minutes, scaling infrastructure with ease. But with great power comes great complexities, and in the case of Kubernetes, that complexity is security. By default, Kubernetes permits all traffic between workloads in a cluster. This “allow by default” stance is convenient during development, and testing but it’s dangerous in production. It’s up to DevOps, DevSecOps, and cloud platform teams to lock things down. To improve the security posture of a Kubernetes cluster, we can use microsegmentation , a practice that limits each workload’s network reach so it can only talk to the specific resources it needs. This is an essential security method in today’s cloud-native environments. We all understand that network policies can achieve microsegmentation; or in other words, it can divide our Kubernetes network model into isolated pieces. This is important since Kubernetes is usually used to provide multiple teams with their infrastructural needs or host multiple workloads for different tenants. With that, you would think network policies are first citizens of clusters. However, when we dig into implementing them, three operational challenges make most practitioners reluctant about implementing policies.</description></item><item><title>Scaling beyond IPv4: integrating IPv6 Amazon EKS clusters into existing Istio Service Mesh</title><link>https://kubermates.org/docs/2025-07-22-scaling-beyond-ipv4-integrating-ipv6-amazon-eks-clusters-into-existing-istio-ser/</link><pubDate>Tue, 22 Jul 2025 18:54:25 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-22-scaling-beyond-ipv4-integrating-ipv6-amazon-eks-clusters-into-existing-istio-ser/</guid><description>Scaling beyond IPv4: integrating IPv6 Amazon EKS clusters into existing Istio Service Mesh Amazon EKS IPv6 interoperability with IPv4 in Istio Service Mesh Solution overview Istio Multi-Primary Multicluster deployment model on a single network Istio Multi-Primary Multicluster deployment model on multi-network Walkthrough Initial setup Conclusion About the authors Organizations are increasingly adopting IPv6 for their Amazon Elastic Kubernetes Service (Amazon EKS) deployments, driven by three key factors: depletion of private IPv4 addresses, the need to streamline or eliminate overlay networks, and improved network security requirements on Amazon Web Services (AWS). In IPv6-enabled EKS clusters, each pod receives a unique IPv6 address from the Amazon Virtual Private Cloud (Amazon VPC) IPv6 range, with seamless compatibility facilitated by the Amazon EKS VPC Container Network Interface (CNI). This solution effectively addresses two major IPv4 limitations: the scarcity of private addresses and the security vulnerabilities created by overlapping IPv4 spaces that need Network Address Translation (NAT) at the node level. When transitioning to IPv6, you likely need to run both IPv4 and IPv6 EKS clusters simultaneously. This is particularly important for organizations using Istio Service Mesh with Amazon EKS, because IPv6 clusters must integrate with the existing Service Mesh and work smoothly alongside IPv4 clusters. To streamline this transition, you can configure your Istio Service Mesh to support both your current IPv4 EKS clusters and your new IPv6 EKS clusters. If Istio Service Mesh isn’t part of your infrastructure, then we suggest exploring Amazon VPC Lattice as an alternative solution to speed up your IPv6 implementation on AWS. This post provides a step-by-step guide for combining IPv6-enabled EKS clusters with your existing Istio Service Mesh and IPv4 workloads, enabling a graceful transition to IPv6 on AWS. This guide covers detailed instructions for enabling communication between IPv6 and IPv4 EKS clusters, along with recommended practices for implementing IPv6 across both single and multiple VPC configurations. The functionality of Amazon EKS IPv6 builds on the native dual-stack capabilities of VPC. When you enable IPv6 in your VPC, it receives both IPv4 prefixes and a /56 IPv6 prefix. This IPv6 prefix can come from three sources: Amazon’s Global Unicast Address (GUA) space, your own IPv6 range (BYOIPv6), or a Unique Local Address (ULA) space.</description></item><item><title>Four Powerful, New Features to Help You Build and Deploy More Efficient Apps On DigitalOcean Kubernetes</title><link>https://kubermates.org/docs/2025-07-22-four-powerful-new-features-to-help-you-build-and-deploy-more-efficient-apps-on-d/</link><pubDate>Tue, 22 Jul 2025 16:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-22-four-powerful-new-features-to-help-you-build-and-deploy-more-efficient-apps-on-d/</guid><description>Four Powerful, New Features to Help You Build and Deploy More Efficient Apps On DigitalOcean Kubernetes DigitalOcean Managed Kubernetes support for new GPU Droplets types Nodepool Scale-to-zero DigitalOcean Kubernetes now available in our new, AI-optimized Atlanta data center (ATL1) DigitalOcean Kubernetes Routing Agent now generally available (GA) Get started About the author Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 By Nicole Ghalwash Published: July 22, 2025 4 min read Weâre adding to Marchâs updates with even more Managed Kubernetes features that will help you get even more utility out of the productâincluding newly supported Droplet types, the ability to automatically scale nodes to zero when youâre not using them, and more. Letâs walk through these new features and how they can benefit both your Kubernetes environment and your business. TL;DR: We built some new features for Kubernetes platform. Get started with some quick-links below: â&amp;gt;Not a customer yet? Spin up a Kubernetes cluster in minutes. â&amp;gt;Already a customer? Explore the new features by logging into your DigitalOcean account. You can now deploy GPU-accelerated workloads on DigitalOcean Managed Kubernetes using our latest GPU Droplet types (both NVIDIA and AMD ). These new instance types are ideal for AI/ML training and inference, image and video processing, and other compute-intensive workloads. With native support for GPU nodes in your Kubernetes clusters, you get the flexibility of containers with the raw power of high-performance GPUs-fully integrated into the DOKS experience. Here are the new GPU Droplet types: This feature allows a node pool to automatically scale down to zero nodes when there are no active workloads that require those nodes. You can now enable the node pools within your Kubernetes environment to automatically scale down to zero when idle, stopping compute charges during those periods of inactivity. This feature is optimal for development or testing environments, applications with usage patterns tied to business hours that naturally yield idle periods, or workloads that use specialized node pools like GPU or CPU-Optimized for intermittent jobs. The main components of this feature include: 1.</description></item><item><title>Introducing ERNIE 4.5-21B-A3B-Base</title><link>https://kubermates.org/docs/2025-07-22-introducing-ernie-4-5-21b-a3b-base/</link><pubDate>Tue, 22 Jul 2025 14:51:42 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-22-introducing-ernie-4-5-21b-a3b-base/</guid><description>Introducing ERNIE 4.5-21B-A3B-Base What is ERNIE 4.5 21B? ERNIE 4.5 21B in Action: Rapid translation from English to Chinese with Python Why Choose DigitalOcean 1-Click Models? How to Deploy ERNIE 4.5-21B-A3B Base on DigitalOcean Join thousands of developers who use 1-Click Models to dedicate their time to innovation rather than infrastructure About the author(s) Try DigitalOcean for free Related Articles Stop Building SaaS from Scratch: Meet the SeaNotes Starter Kit Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Introducing langchain-gradient: Seamless LangChain Integration with DigitalOcean Gradientâ¢ AI Platform By Waverly Swinton and Quinn Eckart Published: July 22, 2025 2 min read TL;DR: Baidu recently released ERNIE 4.5-21B-A3B-Base, a powerful open-source LLM Baidu recently released ERNIE 4.5-21B-A3B-Base, a powerful open-source LLM You can launch ERNIE 4.5-21B directly on DigitalOcean GPU Droplets with only 1-click - deploy in the cloud console You can launch ERNIE 4.5-21B directly on DigitalOcean GPU Droplets with only 1-click - deploy in the cloud console We put ERNIE 4.5-21B to the test for translation and it outperformed models like Qwen3 - check out our demo We put ERNIE 4.5-21B to the test for translation and it outperformed models like Qwen3 - check out our demo ERNIE (Enhanced Representation through kNowledge IntEgration) 4.5-21B-A3B-Base represents a significant leap in large language model development. Originating from Baiduâs extensive research, this model is celebrated for its unique knowledge-enhanced architecture, which allows for robust performance across complex natural language processing tasks, including sophisticated text generation, nuanced conversational AI, and comprehensive summarization. Its ability to integrate real-world knowledge into its understanding sets it apart, making it a compelling choice for intricate AI applications. Deploying ERNIE 4.5-21B on DigitalOcean empowers you to rapidly prototype and scale applications powered by a model that goes beyond surface-level text processing, without the overhead of intricate infrastructure configuration. ERNIE 4.5-21B is praised for its advanced multimodal capabilities, handling text, images, audio, and video, as well as its remarkable computational efficiency, allowing you to save significantly on AI infrastructure deployment costs while achieving high performance. We specifically put it to the test for translation, using an ERNIE 4.5 1-Click GPU Droplet to rapidly translate English files to Chinese. ERNIE mastered the task, translating line-by-line in mere moments. This efficient, line-by-line translation can be adapted for various file types, including markdown and doc files. Instant Deployment: Get models running with a single clickâno manual installation required. Optimized Performance: DigitalOcean GPU Droplets provide powerful, cost-effective compute resources for AI workloads. Seamless Scalability: Easily scale your AI applications as your needs grow. Developer-Friendly: Simple, intuitive deployment through the DigitalOcean Cloud Console.</description></item><item><title>From CFP to Stage: Win Your Tech Talk Slot</title><link>https://kubermates.org/docs/2025-07-22-from-cfp-to-stage-win-your-tech-talk-slot/</link><pubDate>Tue, 22 Jul 2025 06:19:59 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-22-from-cfp-to-stage-win-your-tech-talk-slot/</guid><description>Step 1: It All Starts with the &amp;ldquo;Why&amp;rdquo; - Choosing Your Topic Step 2: The Title - Your Ten-Word Sales Pitch Step 3: The Abstract — The Heart of Your Proposal Step 4: The Details Matter — Track, Level, and Benefits Final Pro-Tip: Think Like a Reviewer Exploring System Architecture for DevOps Engineers Why KubeCon India 2025 Meant More to KodeKloud Linux: List Disks Linux: &amp;ldquo;cat&amp;rdquo; Command Linux Made Easy for DevOps Beginners MCP Explained Simply: How AI Can Actually Do Things Now Still Not Job-Ready After Learning DevOps? What Is Kubernetes? Finally, a Simple Explanation! You find the perfect conference, open the Call for Proposals (CFP) form, and pour your heart into it. You hit &amp;ldquo;submit. &amp;quot; And then you wait… That rejection email always stings. We’ve all been there. But what if you could change the odds? What if you could move from hoping your talk gets chosen to expecting it? After speaking at numerous global conferences and analyzing hundreds of accepted and rejected talks, I’ve found that a winning proposal isn’t about luck. It’s about strategy. This guide will break down that strategy, making it applicable for any tech event, from a local Google Community Day to the global stages of KubeCon or a Linux Foundation Summit. Before you write a single word of your abstract, you need to pick the right topic. The best topics sit at the intersection of three things: It Solves a Painful, Urgent Problem: Reviewers are looking for talks that solve a real-world problem their audience is facing right now. A talk on “migrating to Kubernetes” was a guaranteed hit in 2019, but in 2025, the problem is no longer urgent. Conversely, a talk on “ securing a Retrieval Augmented Generation (RAG) system” is incredibly urgent today because it’s a new, complex technology with high stakes and few established best practices. Ask yourself: What are people complaining about on social media or in community Slacks? What new technology is causing both excitement and anxiety? That’s where you’ll find winning topics.</description></item><item><title>Deep dive into cluster networking for Amazon EKS Hybrid Nodes</title><link>https://kubermates.org/docs/2025-07-21-deep-dive-into-cluster-networking-for-amazon-eks-hybrid-nodes/</link><pubDate>Mon, 21 Jul 2025 22:22:52 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-21-deep-dive-into-cluster-networking-for-amazon-eks-hybrid-nodes/</guid><description>Deep dive into cluster networking for Amazon EKS Hybrid Nodes Architecture overview CNI considerations Load balancing considerations Prerequisites Walkthrough BGP routing (Cilium example) Static routing (Calico example) On-premises load balancer (MetalLB example) External load balancer (AWS Load Balancer Controller example) Cleaning up Conclusion About the author Amazon Elastic Kubernetes Service ( Amazon EKS ) Hybrid Nodes enables organizations to integrate their existing on-premises and edge computing infrastructure into EKS clusters as remote nodes. EKS Hybrid Nodes provides you with the flexibility to run your containerized applications wherever needed, while maintaining standardized Kubernetes management practices and addressing latency, compliance, and data residency needs. EKS Hybrid Nodes accelerates infrastructure modernization by repurposing existing hardware investments. Organizations can harness the elastic scalability, high availability, and fully managed advantages of Amazon EKS, while making sure of operational consistency through unified workflows and toolsets across hybrid environments. One of the key aspects of the EKS Hybrid Nodes solution is the hybrid network architecture between the cloud-based Amazon EKS control plane and your on-premises nodes. This post dives deep into the cluster networking configurations, guiding you through the process of integrating an EKS cluster with hybrid nodes in your existing infrastructure. In this walkthrough, we set up different Container Network Interface (CNI) options and load balancing solutions on EKS Hybrid Nodes to meet your networking requirements. EKS Hybrid Nodes needs private network connectivity between the cloud-hosted Amazon EKS control plane and the hybrid nodes running in your on-premises environment. This connectivity can be established using either Amazon Web Services (AWS) Direct Connect or AWS Site-to-Site VPN , through an AWS Transit Gateway or the Virtual Private Gateway into your Amazon Virtual Private Cloud (Amazon VPC). For an optimal experience, AWS recommends reliable network connectivity with at least 100 Mbps bandwidth, and a maximum of 200ms round-trip latency, for hybrid nodes connecting to the AWS Region. This is general guidance rather than a strict requirement, and specific bandwidth and latency requirements may differ based on the quantity of hybrid nodes and your application’s unique characteristics. The node and pod Classless Inter-Domain Routing (CIDR) blocks for your hybrid nodes and container workloads must be within the IPv4 RFC-1918 ranges.</description></item><item><title>Democratizing AI Model Training on Kubernetes: Introducing Kubeflow Trainer V2</title><link>https://kubermates.org/docs/2025-07-21-democratizing-ai-model-training-on-kubernetes-introducing-kubeflow-trainer-v2/</link><pubDate>Mon, 21 Jul 2025 00:00:00 -0500</pubDate><guid>https://kubermates.org/docs/2025-07-21-democratizing-ai-model-training-on-kubernetes-introducing-kubeflow-trainer-v2/</guid><description>Background and Evolution User Personas Python SDK Simplified API Extensibility and Pipeline Framework LLMs Fine-Tuning Support Dataset and Model Initializers Use of JobSet API Kueue Integration MPI Support Gang-Scheduling Fault Tolerance Improvements What’s Next? Migration from Training Operator v1 Resources and Community Background and Evolution User Personas Python SDK Simplified API Extensibility and Pipeline Framework LLMs Fine-Tuning Support Dataset and Model Initializers Use of JobSet API Kueue Integration MPI Support Gang-Scheduling Fault Tolerance Improvements What’s Next? Migration from Training Operator v1 Resources and Community Running machine learning workloads on Kubernetes can be challenging. Distributed training and LLMs fine-tuning, in particular, involves managing multiple nodes, GPUs, large datasets, and fault tolerance, which often requires deep Kubernetes knowledge. The Kubeflow Trainer v2 (KF Trainer) was created to hide this complexity, by abstracting Kubernetes from AI Practitioners and providing the easiest, most scalable way to run distributed PyTorch jobs. The main goals of Kubeflow Trainer v2 include: Make AI/ML workloads easier to manage at scale Provide a Pythonic interface to train models Deliver the easiest and most scalable PyTorch distributed training on Kubernetes Add built-in support for fine-tuning large language models Abstract Kubernetes complexity from AI Practitioners Consolidate efforts between Kubernetes Batch WG and Kubeflow community We’re deeply grateful to all contributors and community members who made the Trainer v2 possible with their hard work and valuable feedback. We’d like to give special recognition to andreyvelich , tenzen-y , electronic-waste , astefanutti , ironicbo , mahdikhashan , kramaranya , harshal292004 , akshaychitneni , chenyi015 and the rest of the contributors. We would also like to highlight ahg-g , kannon92 , and vsoch whose feedback was essential while we designed the Kubeflow Trainer architecture together with the Batch WG. See the full contributor list for everyone who helped make this release possible. Kubeflow Trainer v2 represents the next evolution of the Kubeflow Training Operator , building on over seven years of experience running ML workloads on Kubernetes. The journey began in 2017 when the Kubeflow project introduced TFJob to orchestrate TensorFlow training on Kubernetes. At that time, Kubernetes lacked many of the advanced batch processing features needed for distributed ML training, so the community had to implement these capabilities from scratch. Over the years, the project expanded to support multiple ML frameworks including PyTorch , MXNet , MPI , and XGBoost through various specialized operators. In 2021, these were consolidated into the unified Training Operator v1.</description></item><item><title>Blog: Post-Quantum Cryptography in Kubernetes</title><link>https://kubermates.org/docs/2025-07-18-blog-post-quantum-cryptography-in-kubernetes/</link><pubDate>Fri, 18 Jul 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-18-blog-post-quantum-cryptography-in-kubernetes/</guid><description>Post-Quantum Cryptography in Kubernetes What is Post-Quantum Cryptography Key exchange vs. digital signatures: different needs, different timelines State of PQC key exchange mechanisms (KEMs) today Post-quantum KEMs in Kubernetes: an unexpected arrival The Go version mismatch pitfall Limitations: packet size State of Post-Quantum Signatures Conclusion The world of cryptography is on the cusp of a major shift with the advent of quantum computing. While powerful quantum computers are still largely theoretical for many applications, their potential to break current cryptographic standards is a serious concern, especially for long-lived systems. This is where Post-Quantum Cryptography (PQC) comes in. In this article, I&amp;rsquo;ll dive into what PQC means for TLS and, more specifically, for the Kubernetes ecosystem. I’ll explain what the (suprising) state of PQC in Kubernetes is and what the implications are for current and future clusters. Post-Quantum Cryptography refers to cryptographic algorithms that are thought to be secure against attacks by both classical and quantum computers. The primary concern is that quantum computers, using algorithms like Shor&amp;rsquo;s Algorithm , could efficiently break widely used public-key cryptosystems such as RSA and Elliptic Curve Cryptography (ECC), which underpin much of today&amp;rsquo;s secure communication, including TLS. The industry is actively working on standardizing and adopting PQC algorithms. One of the first to be standardized by NIST is the Module-Lattice Key Encapsulation Mechanism ( ML-KEM ), formerly known as Kyber, and now standardized as FIPS-203 (PDF download). ML-KEM It is difficult to predict when quantum computers will be able to break classical algorithms. However, it is clear that we need to start migrating to PQC algorithms now, as the next section shows.</description></item><item><title>Post-Quantum Cryptography in Kubernetes</title><link>https://kubermates.org/docs/2025-07-18-post-quantum-cryptography-in-kubernetes/</link><pubDate>Fri, 18 Jul 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-18-post-quantum-cryptography-in-kubernetes/</guid><description>Post-Quantum Cryptography in Kubernetes What is Post-Quantum Cryptography Key exchange vs. digital signatures: different needs, different timelines State of PQC key exchange mechanisms (KEMs) today Post-quantum KEMs in Kubernetes: an unexpected arrival The Go version mismatch pitfall Limitations: packet size State of Post-Quantum Signatures Conclusion The world of cryptography is on the cusp of a major shift with the advent of quantum computing. While powerful quantum computers are still largely theoretical for many applications, their potential to break current cryptographic standards is a serious concern, especially for long-lived systems. This is where Post-Quantum Cryptography (PQC) comes in. In this article, I&amp;rsquo;ll dive into what PQC means for TLS and, more specifically, for the Kubernetes ecosystem. I&amp;rsquo;ll explain what the (suprising) state of PQC in Kubernetes is and what the implications are for current and future clusters. Post-Quantum Cryptography refers to cryptographic algorithms that are thought to be secure against attacks by both classical and quantum computers. The primary concern is that quantum computers, using algorithms like Shor&amp;rsquo;s Algorithm , could efficiently break widely used public-key cryptosystems such as RSA and Elliptic Curve Cryptography (ECC), which underpin much of today&amp;rsquo;s secure communication, including TLS. The industry is actively working on standardizing and adopting PQC algorithms. One of the first to be standardized by NIST is the Module-Lattice Key Encapsulation Mechanism ( ML-KEM ), formerly known as Kyber, and now standardized as FIPS-203 (PDF download). ML-KEM It is difficult to predict when quantum computers will be able to break classical algorithms. However, it is clear that we need to start migrating to PQC algorithms now, as the next section shows.</description></item><item><title>Elevate Your AI Workloads: AMD Instinct™ MI325X GPU Droplets are Now Available on DigitalOcean</title><link>https://kubermates.org/docs/2025-07-17-elevate-your-ai-workloads-amd-instinct-mi325x-gpu-droplets-are-now-available-on-/</link><pubDate>Thu, 17 Jul 2025 15:07:55 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-17-elevate-your-ai-workloads-amd-instinct-mi325x-gpu-droplets-are-now-available-on-/</guid><description>Elevate Your AI Workloads: AMD Instinctâ¢ MI325X GPU Droplets are Now Available on DigitalOcean What Makes AMD Instinctâ¢ MI325X GPUs a Game Changer? The DigitalOcean Advantage with MI325X GPU Droplets Getting Started About the author Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 By Waverly Swinton Published: July 17, 2025 2 min read At DigitalOcean, weâre constantly striving to equip developers and digital native enterprises with the most powerful and accessible tools to fuel AI innovation. Following the introduction of our AMD Instinctâ¢ MI300X GPU Droplets last month, weâre thrilled to announce the availability of the next generation of AI accelerators - AMD Instinctâ¢ MI325X. Built on the advanced AMD CDNAâ¢ 3 architecture, the AMD Instinctâ¢ MI325X accelerators are engineered to deliver exceptional performance for AI workloads, including large model training, fine-tuning, inference, and high-performance computing (HPC). These GPUs are available as GradientAI GPU Droplets , powerful virtual machines with a simplified setup. Key benefits of the AMD Instinctâ¢ MI325X that enable you to accelerate complex computations, ensure your inference tasks run faster, and provide more flexibility and control: Memory capacity and bandwidth : With 256GB of HBM3E memory and 6. 0TB/s of bandwidth, the MI325X can hold massive models entirely in memory, significantly reducing the need for model splitting across GPUs and accelerating complex computations. This represents a substantial leap in memory capabilities, offering 1. 8x more capacity and 1. 3x more bandwidth compared to previous generations. Memory capacity and bandwidth : With 256GB of HBM3E memory and 6. 0TB/s of bandwidth, the MI325X can hold massive models entirely in memory, significantly reducing the need for model splitting across GPUs and accelerating complex computations. This represents a substantial leap in memory capabilities, offering 1.</description></item><item><title>Under the hood: Amazon EKS ultra scale clusters</title><link>https://kubermates.org/docs/2025-07-16-under-the-hood-amazon-eks-ultra-scale-clusters/</link><pubDate>Wed, 16 Jul 2025 00:14:37 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-16-under-the-hood-amazon-eks-ultra-scale-clusters/</guid><description>This post was co-authored by Shyam Jeedigunta, Principal Engineer, Amazon EKS; Apoorva Kulkarni, Sr. Specialist Solutions Architect, Containers and Raghav Tripathi, Sr. Software Dev Manager, Amazon EKS. Today, Amazon Elastic Kubernetes Service (Amazon EKS) announced support for clusters with up to 100,000 nodes. With Amazon EC2’s new generation accelerated computing instance types, this translates to 1. 6 million AWS Trainium chips or 800,000 NVIDIA GPUs in a single Kubernetes cluster. This unlocks ultra scale artificial intelligence (AI) and machine leaning (ML) workloads such as state-of-the-art model training, fine-tuning and agentic inference. Besides customers directly consuming Amazon EKS today, these improvements also extend to other AI/ML services like Amazon SageMaker HyperPod with EKS that leverage EKS as their compute layer, advancing AWS’s overall ultra scale computing capabilities. Our customers have made it clear that containerization of training jobs and operators such as Kubeflow, the ability to streamline resource provisioning and lifecycle through projects like Karpenter, support for pluggable scheduling strategies, and access to a vast ecosystem of cloud-native tools is critical for their success in the AI/ML domain. Kubernetes has emerged as a key enabler here due to its powerful and extensible API model along with robust container orchestration capabilities, allowing accelerated workloads to scale quickly and run reliably. Through multiple technical innovations, architectural improvements and open-source collaboration, Amazon EKS has built the next generation of its cluster control plane and data plane for ultra scale, with full Kubernetes conformance. At AWS, we recommend customers running general-purpose applications with low coupling and horizontal scalability to follow a cell-based architecture as the strategy to sustain growth.</description></item><item><title>Amazon EKS enables ultra scale AI/ML workloads with support for 100K nodes per cluster</title><link>https://kubermates.org/docs/2025-07-16-amazon-eks-enables-ultra-scale-ai-ml-workloads-with-support-for-100k-nodes-per-c/</link><pubDate>Wed, 16 Jul 2025 00:14:26 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-16-amazon-eks-enables-ultra-scale-ai-ml-workloads-with-support-for-100k-nodes-per-c/</guid><description>&lt;p&gt;Open the original post ↗ &lt;a href="https://aws.amazon.com/blogs/containers/amazon-eks-enables-ultra-scale-ai-ml-workloads-with-support-for-100k-nodes-per-cluster/"&gt;https://aws.amazon.com/blogs/containers/amazon-eks-enables-ultra-scale-ai-ml-workloads-with-support-for-100k-nodes-per-cluster/&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Dry Run: Your Kubernetes network policies with Calico staged network policies</title><link>https://kubermates.org/docs/2025-07-15-dry-run-your-kubernetes-network-policies-with-calico-staged-network-policies/</link><pubDate>Tue, 15 Jul 2025 14:01:01 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-15-dry-run-your-kubernetes-network-policies-with-calico-staged-network-policies/</guid><description>Impact Resolution Let’s Set the Scene The Power of Staged Network Policies Where to Observe Staged Policies? Go Even Further With Calico Cloud Free Tier Outcome Kubernetes Network Policies (KNP) are powerful resources that help secure and isolate workloads in a cluster. By defining what traffic is allowed to and from specific pods, KNPs provide the foundation for zero-trust networking and least-privilege access in cloud-native environments. But there’s a problem: KNPs are risky, and applying them without a clear game plan can be potentially disruptive. Without deep insight into existing traffic flows, applying a restrictive policy can instantly break connectivity killing live workloads, user sessions, or critical app dependencies. An even scarier scenario is when we implement policies that we think cover everything and workloads actually work, but after a restart or scaling operation we hit new problems. Kubernetes, with all of its features, has no built-in “dry run” mode for policies, and no first-class observability to show what would be blocked or allowed which is the right decision since Kubernetes is an orchestrator not an implementer. This forces platform teams into a difficult choice, deploy permissive or no policies and weaken security, or Risk service disruption while debugging restrictive ones. As a result, many teams delay implementing network policies entirely only to regret it after a zero-day exploit like Log4Shell, XZ backdoor, or other vulnerabilities that can impact production. The fear of breaking something becomes the top reason why Kubernetes environments go unsegmented. You can’t enforce what you can’t test safely. For instance, let’s say you want to secure a workload deployed by another team. You don’t control how it was configured.</description></item><item><title>Nirmata Teams Dashboard Gets a Makeover: A Cleaner, Smarter Experience for Managing Kubernetes and Cloud Environments</title><link>https://kubermates.org/docs/2025-07-15-nirmata-teams-dashboard-gets-a-makeover-a-cleaner-smarter-experience-for-managin/</link><pubDate>Tue, 15 Jul 2025 08:00:09 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-15-nirmata-teams-dashboard-gets-a-makeover-a-cleaner-smarter-experience-for-managin/</guid><description>Nirmata Teams Dashboard Gets a Makeover: A Cleaner, Smarter Experience for Managing Kubernetes and Cloud Environments What’s New in the Teams Dashboard? Redesigned Team Cards Better Visibility Into Policy Assignments Pagination for Better Performance Redesigned Team Details Page Major UI Cleanup New Tabbed Layout Smarter Member Management Why It Matters We’re excited to announce a refreshed Teams Dashboard in Nirmata Control Hub. This redesign delivers a cleaner, more intuitive interface that not only looks better, it works better too. Whether you’re managing a few teams or dozens, the new dashboard streamlines how you view and interact with team-related data across your Kubernetes and cloud environments. To improve visibility and team-level accountability, the dashboard now shows how infrastructure resources are allocated across teams. You can easily see which Kubernetes clusters, namespaces, and repositories belong to each team – such as Security, Platform, or Development – and who the team members are. This makes it simpler to understand team footprints, manage access, and align infrastructure usage with organizational structure. Team cards have been completely redesigned with a cleaner layout that makes key information immediately visible. Each card now includes: Total Members in the team Total Clusters assigned Total Namespaces assigned Total Repositories assigned This update gives you a high-level snapshot of your teams’ footprint across your infrastructure , helping you stay informed at a glance. We’ve improved how policy report assignments are surfaced on each team card, offering better visibility and preparing the groundwork for even more powerful insights and statistics in future releases. Instead of listing all teams on one long scroll, we now paginate the dashboard. You’ll see 12 teams at a time, making navigation faster and more manageable, especially for larger organizations. We’ve also made significant improvements to the Team Details page, focusing on clarity and ease of use.</description></item><item><title>What Is Kubernetes? Finally, a Simple Explanation!</title><link>https://kubermates.org/docs/2025-07-15-what-is-kubernetes-finally-a-simple-explanation/</link><pubDate>Tue, 15 Jul 2025 05:26:35 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-15-what-is-kubernetes-finally-a-simple-explanation/</guid><description>Why Do We Need Kubernetes? It&amp;rsquo;s Hard to Run a Large Number of Containers What Is Kubernetes Used For? Automation Doing a Lot with Very Few Instructions Sticking to the Plan You Provide What Is a Kubernetes Cluster? What Is a Pod in Kubernetes? Kubernetes vs. Docker Learn More about Kubernetes with a Beginner-Friendly Course Exploring System Architecture for DevOps Engineers Why KubeCon India 2025 Meant More to KodeKloud Linux: List Disks Linux: &amp;ldquo;cat&amp;rdquo; Command Linux Made Easy for DevOps Beginners From CFP to Stage: Win Your Tech Talk Slot MCP Explained Simply: How AI Can Actually Do Things Now Still Not Job-Ready After Learning DevOps? Well, you&amp;rsquo;re about to read something different. Finally, a blog that will explain &amp;ldquo;What is Kubernetes?&amp;rdquo; in very simple words ; an explanation for normal human beings. But Kubernetes is a solution to a problem. And to understand the solution, you first have to look at the problem it solves. Containers, containers, containers… in the voice of Steve Ballmer. You probably heard about Docker. But even if you didn&amp;rsquo;t, it&amp;rsquo;s no big deal to understand it. Docker is a set of tools that allow you to create, edit, and run containers. And a container is just an application in a small little box. What&amp;rsquo;s so cool about containers? Well, one thing is that the application has all that it needs (the so-called dependencies ) inside that little box. So you can take that container, run it on Windows, MacOS, Linux, whatever.</description></item><item><title>From Raw Data to Model Serving: A Blueprint for the AI/ML Lifecycle with Kubeflow</title><link>https://kubermates.org/docs/2025-07-15-from-raw-data-to-model-serving-a-blueprint-for-the-ai-ml-lifecycle-with-kubeflow/</link><pubDate>Tue, 15 Jul 2025 00:00:00 -0500</pubDate><guid>https://kubermates.org/docs/2025-07-15-from-raw-data-to-model-serving-a-blueprint-for-the-ai-ml-lifecycle-with-kubeflow/</guid><description>Project Overview A Note on the Data Why Kubeflow? Key Benefits Getting Started: Prerequisites and Cluster Setup Prerequisites 1. Create a Local Kubernetes Cluster 2. Deploy Kubeflow Pipelines 3. Upload the Raw Data to MinIO 4. Install Model Registry, KServe, Spark Operator, and Set Policies Building and Understanding the Pipeline Images Image Locations How to Build Entry points Pushing Images The Kubeflow Pipeline 1. Data Preparation with Spark 2. Feature Engineering with Feast 3. Model Training 4. Model Registration 5. Real-Time Inference with KServe Importing and Running the Pipeline Import the Pipeline Run the Pipeline Testing the Live Endpoint Conclusion Are you looking for a practical, reproducible way to take a machine learning project from raw data all the way to a deployed, production-ready model? This post is your blueprint for the AI/ML lifecycle: you’ll learn how to use Kubeflow and open source tools such as Feast to build a workflow you can run on your laptop and adapt to your own projects. We’ll walk through the entire ML lifecycle—from data preparation to live inference—leveraging the Kubeflow platform to create a cohesive, production-grade MLOps workflow. The project implements a complete MLOps workflow for a fraud detection use case.</description></item><item><title>Kubernetes Monitoring backend 2.2: better cluster observability through new alert and recording rules</title><link>https://kubermates.org/docs/2025-07-15-kubernetes-monitoring-backend-2-2-better-cluster-observability-through-new-alert/</link><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-15-kubernetes-monitoring-backend-2-2-better-cluster-observability-through-new-alert/</guid><description>Configure and customize Kubernetes Monitoring easier with Alloy Operator Pete Wall Â· 17 Jun 2025 Â· 5 min read Our latest Kubernetes Monitoring Helm chart offers easier, dynamic configuration with Alloy Operator.</description></item><item><title>Kubernetes Governance with Nirmata &amp; SUSE</title><link>https://kubermates.org/docs/2025-07-14-kubernetes-governance-with-nirmata-suse/</link><pubDate>Mon, 14 Jul 2025 08:05:54 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-14-kubernetes-governance-with-nirmata-suse/</guid><description>Kubernetes Governance with Nirmata &amp;amp; SUSE Seamless Integration with Rancher Manager Here’s how to find it: Why This Partnership Matters for You Happy Monday, cloud-native enthusiasts! We’re thrilled to share some exciting news that marks a significant milestone for Nirmata and the broader Kubernetes community. We are proud to announce that Nirmata is now officially approved and listed in the SUSE Partner Certification &amp;amp; Solutions Catalog! This achievement isn’t just a badge; it’s a testament to our commitment to providing robust, integrated solutions that simplify Kubernetes governance and management to enhance security for enterprises. For users of SUSE and Rancher, this partnership opens up new avenues for streamlined operations and greater control over their cloud-native environments. One of the most immediate benefits of this partnership is the seamless availability of Nirmata within the Rancher ecosystem. You can now easily access Nirmata directly from your Rancher Manager interface. Navigate to Apps → Charts within your Rancher Manager, and you’ll find Nirmata listed in the Rancher Partner Catalog. This direct integration makes it incredibly easy for Rancher users to discover, deploy, and leverage Nirmata’s capabilities for Kubernetes governance, policy management, governance, and security. This collaboration between Nirmata and SUSE/Rancher brings together two powerful platforms to address critical needs in the cloud-native landscape: Enhanced K8s Governance and Security: Nirmata provides a comprehensive policy engine that allows organizations to define, enforce, and audit policies across their Kubernetes clusters. This is crucial for maintaining security posture, ensuring compliance, and enforcing operational best practices, especially in multi-cluster and hybrid cloud environments managed by Rancher. Simplified Operations: By integrating with Rancher Manager, Nirmata makes it even easier for platform teams to gain visibility and control over their Kubernetes deployments. From admission control to runtime security, Nirmata streamlines complex tasks , allowing teams to focus on innovation rather than operational overhead. Accelerated Adoption: For enterprises already invested in the SUSE/Rancher ecosystem, this partnership means they can confidently extend their capabilities with Nirmata, knowing that the solutions are certified, integrated, and designed to work together seamlessly.</description></item><item><title>Powered by DigitalOcean Hatch: Why Uxify’s Founders Always Choose DigitalOcean</title><link>https://kubermates.org/docs/2025-07-10-powered-by-digitalocean-hatch-why-uxify-s-founders-always-choose-digitalocean/</link><pubDate>Thu, 10 Jul 2025 18:29:23 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-10-powered-by-digitalocean-hatch-why-uxify-s-founders-always-choose-digitalocean/</guid><description>Powered by DigitalOcean Hatch: Why Uxifyâs Founders Always Choose DigitalOcean Building beyond the original blueprint The cloud platform they trust, again and again Chasing purpose, not profit About the author Try DigitalOcean for free Related Articles Stop Building SaaS from Scratch: Meet the SeaNotes Starter Kit Powered by DigitalOcean Hatch: Ontra Mobility is Building Smarter Cities Powered by DigitalOcean Hatch: How Ex-human uses GPU Droplets to Build Empathetic AI that Serves Customers By Martin Nguyen Published: July 10, 2025 4 min read Hatch is DigitalOceanâs global program for startups that provides select technology companies with cloud infrastructure credits and discounts, direct access to DigitalOceanâs product experts for personalized guidance, priority technical support, and access to a thriving community of like-minded founders. By keeping infrastructure and cost management simple, the Hatch program enables startups to build tech solutions that make an impact. For Georgi Petrov and Mihail Stoychev, the co-founders and serial entrepreneurs behind Uxify , choosing DigitalOcean to host their startupâs infrastructure was a no-brainer. They built their previous ventures NitroPack, SMSBump, and iSenseLabs on DigitalOcean and could trust the platformâs simplicity, reliability, and user experience. When it came time to power Uxifyâs AI-enabled website performance solution and AI agent Uxi, they opted for DigitalOcean again With Hatch program benefits like cloud credits and GPU Droplet discounts, Uxify could focus on building and scaling their underlying AI infrastructure. Uxifyâs story begins over a decade ago with iSenseLabs, an agency that provided services but also built and sold its own products. One of those products was NitroPack, a performance optimization tool designed to help websites load faster. As NitroPack gained traction, the team began seeing possibilities beyond their current solution. Fast forward to 2024, the team behind NitroPack reached a pivotal moment. They began rethinking web performance and userâs perception of site responsivenessâ¦ This new philosophy led them to develop Navigation AI, a performance solution focused on user experience rather than purely technical metrics. Around the same time, NitroPack was acquired by WP Engine. As part of that acquisition, the emerging Navigation AI project was spun out into a new company: Uxify.</description></item><item><title>Sharks of DigitalOcean: Laura Schaffer, VP, Growth</title><link>https://kubermates.org/docs/2025-07-10-sharks-of-digitalocean-laura-schaffer-vp-growth/</link><pubDate>Thu, 10 Jul 2025 15:25:33 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-10-sharks-of-digitalocean-laura-schaffer-vp-growth/</guid><description>Sharks of DigitalOcean: Laura Schaffer, VP, Growth What makes DigitalOcean a unique place to work? Can you tell us about the team you lead and how it supports DigitalOceanâs mission? What excites you most about your work? Dive into the future with DigitalOcean About the author Try DigitalOcean for free Related Articles Sharks of DigitalOcean: Darian Wilkin, Senior Manager, Solutions Engineering Sharks of DigitalOcean: Ali Munir, Staff Technical Account Manager Sharks of DigitalOcean: Jason Dobry, Staff IT Project Specialist By Sujatha R Technical Writer Published: July 10, 2025 2 min read Laura Schaffer, Vice President of Growth at DigitalOcean, leads the team responsible for helping customers onboard and grow with DigitalOcean. In this edition of Shark Tales, she shares what excites her about building a self-serve experience, the energy that fuels her team, and why DigitalOcean stands out as a special place to work. Thereâs a ton of opportunity here, and Iâm excited about everything weâre building together. But what really sets DigitalOcean apart is how driven and united we are. We show up each day ready to collaborate, eager to deliver value to our customers, and motivated to make things happen. Itâs incredibly energizing to work in an environment where everyone shares that same hunger. You feel it in every conversation and project; itâs inspiring, itâs motivating, and it makes this a truly special place to be. ð¥ Have a look at Laura Schafferâs full conversation â¬ï¸ I lead the Growth team, and our focus is on helping customers get started with DigitalOcean. Itâs such a fun space to be in because we get to witness that first spark, when someone signs up, explores what we offer, and has that âAha!â moment. Our platform is simple and easy to use, and seeing new users discover that for themselves is incredibly rewarding. Every day, my team helps users reach that point of discovery, and thatâs a powerful thing to be a part of. As someone focused on growth and self-serve, Iâm especially excited about how our technology enables simplicity.</description></item><item><title>What's New on DigitalOcean Gradient™ AI Platform</title><link>https://kubermates.org/docs/2025-07-08-what-s-new-on-digitalocean-gradient-ai-platform/</link><pubDate>Tue, 08 Jul 2025 18:59:38 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-08-what-s-new-on-digitalocean-gradient-ai-platform/</guid><description>What&amp;rsquo;s New on DigitalOcean Gradientâ¢ AI Platform Week of July 28th Week of July 21st Week of July 7th Week of June 30th Week of June 17th Week of June 9th Week of May 19th Week of May 5th Week of April 28th Week of April 21st Week of April 14th Week of April 7th Week of March 31st Week of March 24th Week of March 17th Week of March 10th Week of February 24th Week of February 17th Week of February 10th Week of February 3rd Week of January 27th Previous tutorials About the author Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 By Grace Morgan Updated: August 11, 2025 10 min read DigitalOceanâs GenAI Platform is now DigitalOcean Gradientâ¢ AI Platform. Learn more about the GA release and features. Welcome to Whatâs New on DigitalOcean Gradientâ¢ AI Platform âyour weekly roundup of the latest updates for the Gradient AI Platform. Each week, weâll share new feature releases, technical tutorials, or video walkthroughs to help you stay ahead of the curve and keep building. Check back regularly for new insights and inspiration, or get started now: build an agent or call a model with serverless inference. Product Update Add Dropbox folders as a data source On Gradient AI Platform, you can now add a Dropbox folder as a data source to your knowledge bases. This allows you to index and use files stored in your Dropbox account within your knowledge base. Product Update Log Stream Insights on Gradient AI Platform DigitalOcean GradientAI is a unified AI cloud platform that combines GPU infrastructure, intelligent agent development, and prebuilt applications to help developers seamlessly build, deploy, and scale AI solutions from prototype to production. Product Update Gradient AI Platform SDK Now Available in Public Preview The official DigitalOcean Platform SDK is now in Public Preview. You can use the SDK to manage Gradient AI Platform resources, including knowledge bases and generative AI agents, from Python applications. Product Update Introducing DigitalOcean Gradientâ¢ AI DigitalOcean GradientAI is a unified AI cloud platform that combines GPU infrastructure, intelligent agent development, and prebuilt applications to help developers seamlessly build, deploy, and scale AI solutions from prototype to production. Product Update Gradient AI Platform is now GA Gradient AI Platform is now generally available, offering developers a fully managed environment to build, scale, and deploy AI-powered applications and agentsâcomplete with integrated tools for data integration, evaluation, observability, and serverless access to top LLMsâall without managing infrastructure.</description></item><item><title>Introducing Gradient: DigitalOcean’s Unified AI Cloud</title><link>https://kubermates.org/docs/2025-07-08-introducing-gradient-digitalocean-s-unified-ai-cloud/</link><pubDate>Tue, 08 Jul 2025 18:36:43 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-08-introducing-gradient-digitalocean-s-unified-ai-cloud/</guid><description>Introducing Gradient: DigitalOceanâs Unified AI Cloud Why DigitalOcean Gradient? Whatâs Included What You Can Expect Whatâs Next Get Started Today About the author Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 By Bratin Saha Updated: July 30, 2025 3 min read At DigitalOcean, weâve always focused on delivering simple, powerful infrastructure that helps digital native enterprises build and scale their applications. Over the past year, weâve extended that commitment into the AI space by investing in compute, tooling, and workflows that make it easier to build with AI from the ground up. We believe that to truly unlock the value of AI, developers need more than just infrastructure, they need an integrated experience that spans the full development lifecycle. Thatâs why weâre bringing together GPU infrastructure, intelligent agent development, and pre-built AI applications into one cohesive platform. Today, weâre taking the next step in that journey with the introduction of DigitalOcean Gradient, our unified AI cloud. It gives you everything you need to train, fine-tune, deploy, and scale AI workloadsânot just as isolated components, but as a seamless, full-stack platform purpose-built for modern AI development. As our AI portfolio has grown, weâve recognized the need for a dedicated identity that reflects the scale and ambition of what weâre building. The name DigitalOcean Gradient captures both the technical roots of machine learning and the philosophy behind our platform. In model training, gradients guide each step of learningâadjusting weights, reducing error, and moving toward better outcomes. In the same way, DigitalOcean Gradient is designed to guide developers from idea to production, with the right tools, abstractions, and infrastructure at every stage. Whether youâre experimenting with foundation models, building intelligent agents, or deploying production workloads at scale, DigitalOcean Gradient is your end-to-end AI cloud. With DigitalOcean Gradient, weâre unifying our AI offerings under one umbrella to make them easier to discover, adopt, and scale: DigitalOcean Gradient Infrastructure: Building blocks like GPU Droplets, Bare Metal GPUs, vector databases, and DO optimized software that helps improve model performance and infrastructure scale out.</description></item><item><title>DigitalOcean Gradient Platform is now Generally Available</title><link>https://kubermates.org/docs/2025-07-08-digitalocean-gradient-platform-is-now-generally-available/</link><pubDate>Tue, 08 Jul 2025 18:36:14 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-08-digitalocean-gradient-platform-is-now-generally-available/</guid><description>DigitalOcean Gradient Platform is now Generally Available Big Upgrades, Even Bigger Possibilities Just need model access? Weâve got you covered. Go from idea to production fastâget started now Get started today: About the author Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 By Grace Morgan Updated: July 30, 2025 4 min read TL;DR: Gradient Platform (previously GenAI Platform) is now generally available, offering a fully managed way to build and deploy AI apps with agents and serverless inference. New features like external data integration, traceability, and evaluations make it easy to go from prototype to productionâno infrastructure management required. -&amp;gt; Build an agent -&amp;gt; Call a model with serverless inference -&amp;gt; Check out our tutorial on how to use serverless inference and agents with the OpenAI SDK: Today, weâre excited to announce the General Availability (GA) of DigitalOceanâs Gradient Platform , a developer-first platform that makes it easy to build, scale, and ship AI-powered applications. Gradient Platform brings together everything you need to go from prototype to production with AI, all in one fully managed experience. You can create intelligent agents that reason over your data, integrate cutting-edge LLMs into your app with a single API, or experiment with prebuilt tools, like evaluations or versioning, to accelerate your AI development. Whether youâre automating workflows, enhancing customer support, or building new AI products from the ground up, the Platform gives you all the power without the complexity of managing infrastructure. Thousands of developers explored the platform during its public preview (formerly known as the GenAI Platform). Their feedback helped shape the product launching today. Todayâs release brings you a comprehensive platform that makes it easier to build, debug, and scale AI applications. New features like external data integration, agent traceability, customer conversation logs, and agent evaluation give you deeper visibility into how your agents behave, more control over their data and logic, and better ways to collaborate across teamsâall without adding infrastructure overhead. Since the initial release in January 2025 , weâve launched a wide range of enhancements driven by real-world adoption and evolving use cases.</description></item><item><title>Calico Whisker &amp; Staged Network Policies: Secure Kubernetes Workloads Without Downtime</title><link>https://kubermates.org/docs/2025-07-07-calico-whisker-staged-network-policies-secure-kubernetes-workloads-without-downt/</link><pubDate>Mon, 07 Jul 2025 20:00:17 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-07-calico-whisker-staged-network-policies-secure-kubernetes-workloads-without-downt/</guid><description>Deploying a Kubernetes Cluster Deploying the yaobank Application Installing Calico for Policy Connect to Whisker UI Deploy Staged Network Policy Summary Rolling out network policies in a live Kubernetes cluster can feel like swapping wings mid-flight—one typo or overly broad rule and critical traffic is grounded. Calico’s Staged Network Policies remove the turbulence by letting you deploy policies in staged mode, so you can observe their impact before enforcing anything. Add Whisker , the open-source policy enforcement and testing tool (introduced as part of Calico Open Source 3.30 ) that captures every flow and tags it with a policy verdict, and you’ve got a safety harness that proves your change is sound long before you flip the switch. In this post, we’ll walk you through how you can leverage these capabilities to tighten security, validate intent, and ship changes confidently—without a single packet of downtime. Calico for Policy is a CNI agnostic tool. Refer to the Calico Open Source docs for a list of supported CNIs. The git repository for this blog post can be found here. For this post, let’s deploy a simple AKS cluster with Azure CNI. ## Configure az group create &amp;ndash;name calicooss &amp;ndash;location eastus2 ## Create a 3 node AKS cluster with Azure CNI az aks create &amp;ndash;resource-group calicooss &amp;ndash;name calico-whisker &amp;ndash;node-count 3 &amp;ndash;network-plugin azure &amp;ndash;kubernetes-version 1.31.8 ## Retrieve the kubeconfig file az aks get-credentials &amp;ndash;resource-group calicooss &amp;ndash;name calico-whisker Now that our cluster is deployed. Let’s provision a demo application. yaobank For this post, we will deploy a three-tier web application called “yet-another-bank” (yaobank). The manifest for the application can be found at this link.</description></item><item><title>Introducing Kafka Schema Registry for DigitalOcean Managed Kafka</title><link>https://kubermates.org/docs/2025-07-07-introducing-kafka-schema-registry-for-digitalocean-managed-kafka/</link><pubDate>Mon, 07 Jul 2025 17:15:04 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-07-introducing-kafka-schema-registry-for-digitalocean-managed-kafka/</guid><description>Introducing Kafka Schema Registry for DigitalOcean Managed Kafka Why use Schema Registry? Features of Kafka Schema Registry Common Developer Use Cases Getting Started About the author Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 By Nicole Ghalwash Published: July 7, 2025 3 min read Weâre excited to announce support for Kafka Schema Registry in DigitalOceanâs Managed Kafka service, giving developers a powerful way to manage and validate schemas in their event-driven applications. Kafka Schema Registry, also referred to as Karapace, is a centralized service for managing and validating schemas for Kafka messages. It helps to ensure that data produced to and consumed from Kafka topics adheres to a defined structure, preventing data compatibility issues. For a developer, Kafka Schema Registry provides robust schema governance and standardized HTTP access for Kafka services, improving data integrity, developer productivity, and system interoperability. This helps to empower DevOps teams to build reliable, evolving event-driven applications by ensuring data integrity through centralized schema management and validation. Schema Registry simplifies the Kafka integration across diverse systems in a more secure way. Ultimately, this tool brings structure to your Kafka topics. It lets you define, version, and validate message schemas so that producers and consumers can stay in sync, even as your data evolves. It helps prevent breaking changes, making debugging easier, and keeps your Kafka pipelines reliable at scale. Please note that Kafka Schema Registry is only available for Kafka customers with a dedicated CPU environment. To learn more about our CPU-optimized Droplets, visit our Droplets homepage or our [product documentation. ]( https://docs.</description></item><item><title>Quick Fixes for Common Kubernetes Issues</title><link>https://kubermates.org/docs/2025-07-06-quick-fixes-for-common-kubernetes-issues/</link><pubDate>Sun, 06 Jul 2025 18:26:31 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-06-quick-fixes-for-common-kubernetes-issues/</guid><description>&lt;ol&gt;
&lt;li&gt;CrashLoopBackOff — The Pod That Won’t Stay Alive 2. Pending Pods — Scheduling Never Happens 3. Service Not Routing to Pods 4. ImagePullBackOff / ErrImagePull — Container Won’t Start 5. Deployment Rollout Stuck 6. Secrets Visible in YAML — A Security Misstep 7. Port Forwarding Doesn’t Work 8. RBAC: Access Denied Quick Commands for Any Issue Final Thoughts: Embrace the Errors Exploring System Architecture for DevOps Engineers Why KubeCon India 2025 Meant More to KodeKloud Linux: List Disks Linux: &amp;ldquo;cat&amp;rdquo; Command Linux Made Easy for DevOps Beginners From CFP to Stage: Win Your Tech Talk Slot MCP Explained Simply: How AI Can Actually Do Things Now Still Not Job-Ready After Learning DevOps? What Is Kubernetes? Finally, a Simple Explanation! The good news? You’re not alone. This guide walks you through the most common Kubernetes problems developers and DevOps teams face — and more importantly, how to fix them quickly. Whether you&amp;rsquo;re new to Kubernetes or scaling your first production app, consider this your essential cheat sheet. Symptoms: Your Pod starts, crashes, restarts, and repeats the loop.&lt;/li&gt;
&lt;/ol&gt;</description></item><item><title>Navigating Failures in Pods With Devices</title><link>https://kubermates.org/docs/2025-07-03-navigating-failures-in-pods-with-devices/</link><pubDate>Thu, 03 Jul 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-03-navigating-failures-in-pods-with-devices/</guid><description>Navigating Failures in Pods With Devices The AI/ML boom and its impact on Kubernetes Understanding AI/ML workloads Why Kubernetes still reigns supreme The current state of device failure handling Failure modes: K8s infrastructure Failure modes: device failed Failure modes: container code failed Failure modes: device degradation Roadmap Roadmap for failure modes: K8s infrastructure Roadmap for failure modes: device failed Roadmap for failure modes: container code failed Roadmap for failure modes: device degradation Join the conversation Kubernetes is the de facto standard for container orchestration, but when it comes to handling specialized hardware like GPUs and other accelerators, things get a bit complicated. This blog post dives into the challenges of managing failure modes when operating pods with devices in Kubernetes, based on insights from Sergey Kanzhelev and Mrunal Patel&amp;rsquo;s talk at KubeCon NA 2024. You can follow the links to slides and recording. The rise of AI/ML workloads has brought new challenges to Kubernetes. These workloads often rely heavily on specialized hardware, and any device failure can significantly impact performance and lead to frustrating interruptions. As highlighted in the 2024 Llama paper , hardware issues, particularly GPU failures, are a major cause of disruption in AI/ML training. You can also learn how much effort NVIDIA spends on handling devices failures and maintenance in the KubeCon talk by Ryan Hallisey and Piotr Prokop All-Your-GPUs-Are-Belong-to-Us: An Inside Look at NVIDIA&amp;rsquo;s Self-Healing GeForce NOW Infrastructure ( recording ) as they see 19 remediation requests per 1000 nodes a day! We also see data centers offering spot consumption models and overcommit on power, making device failures commonplace and a part of the business model. However, Kubernetes’s view on resources is still very static. The resource is either there or not. And if it is there, the assumption is that it will stay there fully functional - Kubernetes lacks good support for handling full or partial hardware failures. These long-existing assumptions combined with the overall complexity of a setup lead to a variety of failure modes, which we discuss here. Generally, all AI/ML workloads require specialized hardware, have challenging scheduling requirements, and are expensive when idle.</description></item><item><title>A Detailed Look at Calico Cloud Free Tier</title><link>https://kubermates.org/docs/2025-07-02-a-detailed-look-at-calico-cloud-free-tier/</link><pubDate>Wed, 02 Jul 2025 20:23:35 +0000</pubDate><guid>https://kubermates.org/docs/2025-07-02-a-detailed-look-at-calico-cloud-free-tier/</guid><description>Why Calico Cloud Free Tier? What is Calico Cloud Free Tier? Enhanced Observability Simplified Microsegmentation Intuitive Dashboards Intelligent Policy Recommendations Seamless Path to Calico Cloud Free Tier How Calico Cloud Free Tier Enhances Calico Open Source Conclusion As Kubernetes environments grow in scale and complexity, platform teams face increasing pressure to secure workloads without slowing down application delivery. But managing and enforcing network policies in Kubernetes is notoriously difficult—especially when visibility into pod-to-pod communication is limited or nonexistent. Teams are often forced to rely on manual traffic inspection, standalone logs, or trial-and-error policy changes, increasing the risk of misconfiguration and service disruption. Safe policy management and microsegmentation becomes a daunting task without clear knowledge or insight into which services should communicate with each other. In this detailed look, we’ll explore how Calico Cloud Free Tier builds upon Calico Open Source , and helps platform teams visualize traffic with a dynamic service graph, simplifies policy management, and even analyzes actual traffic to recommend policies. Calico Cloud Free Tier is a managed SaaS, no-cost offering that extends the capabilities of Calico Open Source 3.30 and higher to help Kubernetes teams improve network visibility, simplify policy management, and improve security by simplifying microsegmentation. Designed for single-cluster environments, it provides platform engineers and operators with powerful observability and policy management tools. With a seamless onboarding experience for users already running Calico Open Source 3.30 or higher, Calico Cloud Free Tier empowers teams to take control of their Kubernetes traffic—without additional cost or vendor lock-in. Let’s take a closer look at the key features that make Calico Cloud Free Tier a powerful solution for Kubernetes network security and observability: Calico’s primary observability solution is Dynamic Service Graph , a powerful visualization tool that maps real-time pod-to-pod communication across your cluster. This Service Graph, which is available in Calico Cloud Free Tier, gives you an immediate understanding of how workloads interact, making it far easier to identify unexpected traffic patterns or missing connections. Need to troubleshoot a failed service call? Simply drill down into the graph to access real-time flow logs with detailed packet and policy metadata. This eliminates the guesswork from debugging and speeds up root-cause analysis dramatically.</description></item><item><title>Expanding DigitalOcean’s Role-Based Access Controls with custom roles</title><link>https://kubermates.org/docs/2025-06-30-expanding-digitalocean-s-role-based-access-controls-with-custom-roles/</link><pubDate>Mon, 30 Jun 2025 17:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-30-expanding-digitalocean-s-role-based-access-controls-with-custom-roles/</guid><description>Expanding DigitalOceanâs Role-Based Access Controls with custom roles What are custom roles? Key features of custom roles When to use custom roles vs. predefined roles How custom roles benefit your team Resources to get started About the author Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 By Nicole Ghalwash Published: June 30, 2025 4 min read Today, we are excited to announce our latest Role-Based Access Control (RBAC) feature, custom roles. With custom roles, teams can now assign permissions to individuals that are precisely aligned with their operational and security requirements, reinforcing the principle of least privilege. This allows for more precise permission management, which helps to enhance overall infrastructure security by reducing the risk of over-privileged accounts. Custom roles give you full control over who can do what on your projects, improving the overall security of your cloud resources. In this blog post, we will walk through what custom roles are, how they work, key features, when to use them, and how they can help your team. Custom roles are user-defined sets of permissions that allow organizations to tailor access control to their specific needs, beyond whatâs available in predefined roles. In other words, custom roles let you create your own set of permissions instead of relying only on default, predefined roles (like Viewer, Billing Viewer, etc. ) that may not work for you. Now, users can define more detailed custom permissions that target specific resources and needs. For example, a user may only need read access to Droplets, but write access to Kubernetes. Hear what a DigitalOcean customer had to say about using custom roles.</description></item><item><title>Is Your Kyverno Healthy? Now You Can Know for Sure.</title><link>https://kubermates.org/docs/2025-06-30-is-your-kyverno-healthy-now-you-can-know-for-sure/</link><pubDate>Mon, 30 Jun 2025 08:00:13 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-30-is-your-kyverno-healthy-now-you-can-know-for-sure/</guid><description>Is Your Kyverno Healthy? Now You Can Know for Sure. Why Kyverno Health Matters What’s New: Kyverno Health Check Real Example: NetworkPolicy for Kyverno Getting Started Why Kyverno Health Check Matters Running Kyverno is essential for enforcing Kubernetes governance and security policies. But is your Kyverno setup secure, scalable, and resilient? With the latest enhancement in Nirmata Control Hub , you no longer have to guess. We’re excited to introduce the Kyverno Health Check – a new capability that provides a clear, actionable view of Kyverno’s configuration in your clusters. Whether you’re a platform engineer managing dozens of clusters or a security lead enforcing policy compliance, this feature helps to ensure that Kyverno is running optimally at all times. Kyverno by Nirmata enforces critical security, compliance, and operational policies in Kubernetes. But like any controller, its effectiveness depends on its own configuration and health. A misconfigured or unhealthy Kyverno deployment can : Allow lateral network traffic to Kyverno pods. Fail under load due to insufficient resource settings Go unnoticed during outages due to missing observability. Or worse, be vulnerable to privilege escalation or unauthorized cluster-admin bindings Kyverno Health Check ensures none of this happens by continuously and vigilantly evaluating your deployments for best practices and known risks. The new Kyverno Health Check feature in Nirmata Control Hub evaluates Kyverno deployments based on four critical categories: Security Detects wildcard permissions, cluster-admin bindings, and missing NetworkPolicies Detects wildcard permissions, cluster-admin bindings, and missing NetworkPolicies Availability Checks for resource configurations, pod disruption budgets, and runtime stability of Kyverno. Checks for resource configurations, pod disruption budgets, and runtime stability of Kyverno.</description></item><item><title>Cut Through the DevSecOps Noise: Smart Violation Prioritization in Nirmata Control Hub</title><link>https://kubermates.org/docs/2025-06-27-cut-through-the-devsecops-noise-smart-violation-prioritization-in-nirmata-contro/</link><pubDate>Fri, 27 Jun 2025 14:54:55 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-27-cut-through-the-devsecops-noise-smart-violation-prioritization-in-nirmata-contro/</guid><description>Cut Through the DevSecOps Noise: Smart Violation Prioritization in Nirmata Control Hub A TL;DR for Your K8s Cluster’s Health What to Fix First – and Why How to Fix It: In-Line Remediation Guidance Tie It All Together with Jira Integration Get One Step Closer to a Secure, Governed K8s Cluster When managing security and compliance in Kubernetes, it’s easy to get overwhelmed. As a platform engineer or administrator, you might look at your cluster or namespace and find hundreds or even thousands of violations. But not all violations are created equal. Some are critical security risks that demand immediate attention, while others are best practices that can be addressed later. The problem? Without clear guidance, most teams end up doing nothing. Not because they don’t care – but because they don’t know where to start. That’s exactly the problem we solve with Violation Summarization and Prioritization in Nirmata Control Hub. With Nirmata Control Hub, you don’t have to dig through pages of findings and all the time spent therein. With a timely glance, you get a summary report for your entire cluster or specific namespaces, showing: Total number of violations Affected namespaces Top security risks Recommended actions Nirmata Control Hub won’t just tell you what’s wrong. We prioritize violations into clear buckets: 🔥 Priority 1: Immediate Attention High-impact misconfigurations that expose your workloads to real risk – fix these ASAP. 🚨 Priority 2: Important but Not Urgent Still important, but can be queued up for your next sprint. ℹ️ Priority 3: Minor / Best Practices Clean-up tasks and optimizations – ideal for long-term hygiene.</description></item><item><title>More resilient, flexible networking for the cloud workloads that matter</title><link>https://kubermates.org/docs/2025-06-26-more-resilient-flexible-networking-for-the-cloud-workloads-that-matter/</link><pubDate>Thu, 26 Jun 2025 16:50:42 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-26-more-resilient-flexible-networking-for-the-cloud-workloads-that-matter/</guid><description>More resilient, flexible networking for the cloud workloads that matter Partner Network Connect now supports high availability and DOKS Reserved IPv6 on Droplets Bring Your Own IP (BYOIP) Public preview Watch the webinar to scale smarter with all Q2 2025 IaaS updates About the author(s) Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 By Anantha Ramachandran and Udhay Ravindran Published: June 26, 2025 5 min read Building and scaling cloud infrastructure often means navigating fragile network configurations, unpredictable IP behavior, and rigid networking provider limitations. Whether itâs dealing with flaky multi-cloud connections, scrambling to update IP allow-lists after a restart, or hesitating to migrate workloads due to reputation-bound IPs, these challenges add unnecessary risk and complexity to modern deployments. Weâre excited to announce three new updates designed to simplify cloud networking and enhance deployment flexibility: Partner Network Connect supports high availability and DOKS, general availability of Reserved IPv6 on Droplets, and public preview of Bring Your Own IP (BYOIP). These improvements address real-world infrastructure challenges and are built with simplicity, scalability, and reliability in mind. Multi-cloud architectures are great, until the private links between them become a single point of failure. For teams routing traffic between multiple cloud providers like DigitalOcean, AWS, Microsoft Azure, or Google Cloud Platform, even brief disruptions can stall ML pipelines, gaming platforms, or analytics services. Traditional setups offer limited redundancy, and recovering from outages usually involves manual intervention or rerouting traffic over public networks. High Availability for Partner Network Connect helps solve this by letting you provision redundant links across two separate DigitalOcean gateway routers. Traffic automatically fails over if one link goes down, with no manual steps required and no impact to your customers. With fully managed failover, this update removes operational overhead while helping meet the needs of latency-sensitive or distributed systems. Key benefits include: Automatic failover: Eliminate downtime from single link failures. Automatic failover: Eliminate downtime from single link failures.</description></item><item><title>Switching to eBPF One Step at a Time with Calico DNS Inline Policy</title><link>https://kubermates.org/docs/2025-06-25-switching-to-ebpf-one-step-at-a-time-with-calico-dns-inline-policy/</link><pubDate>Wed, 25 Jun 2025 16:05:49 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-25-switching-to-ebpf-one-step-at-a-time-with-calico-dns-inline-policy/</guid><description>Calico iptables – DNS Inline policy The outcome NFTABLES Conclusion Calico Enterprise lets users write network policies using domain names instead of IP addresses. This is done by dynamically mapping domain names to IP addresses and matching the egress traffic against these IPs. We have discussed this feature in detail when we introduced the Inline mode for the eBPF data plane in Calico Enterprise 3.20 release! It addresses the latency and performance issues of the various modes used by Calico in iptables/nftables data planes. It is a shame that Calico users who are not yet ready to switch completely to eBPF would miss out on this big DNS policy improvement. Don’t worry! We found a way to port it to iptables to enhance our users’ experience without forcing users to make a huge leap. In Calico Enterprise v3.21, we have extended the Inline DNS policy mode to iptables. In this mode, DNS policies are updated in real time as DNS responses are parsed by eBPF within the data plane, thus improving the performance. In all the existing modes in the iptables data plane, the DNS response packets are sent to Felix – Calico’s userspace agent. It parses the packets and updates the data plane since advanced packet parsing is not feasible with standard iptables rules. However, iptables has an xt_bpf extension which lets us process and match the packets by an eBPF program the same way we do that in the eBPF data plane! xt_bpf An iptables rule that allows it may look something like this: iptables -A INPUT -m bpf –object-pinned /sys/fs/bpf/dns_parser -j ACCEPT The iptables rule calls the eBPF DNS parser program on the response packet and updates the data plane inline. When the client connects for the first time, the policy is enforced without any delays, thus avoiding any retransmits or any impact on application performance. The parser program is the exact same as in the case of the eBPF data plane and is called in the exact spot in which the packets were formerly sent to userspace.</description></item><item><title>Image Compatibility In Cloud Native Environments</title><link>https://kubermates.org/docs/2025-06-25-image-compatibility-in-cloud-native-environments/</link><pubDate>Wed, 25 Jun 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-25-image-compatibility-in-cloud-native-environments/</guid><description>Image Compatibility In Cloud Native Environments The need for image compatibility specification Dependencies between containers and host OS Multi-cloud and hybrid cloud challenges Image compatibility initiative Implementation in Node Feature Discovery Compatibility specification Client implementation for node validation Examples of usage Conclusion Get involved In industries where systems must run very reliably and meet strict performance criteria such as telecommunication, high-performance or AI computing, containerized applications often need specific operating system configuration or hardware presence. It is common practice to require the use of specific versions of the kernel, its configuration, device drivers, or system components. Despite the existence of the Open Container Initiative (OCI) , a governing community to define standards and specifications for container images, there has been a gap in expression of such compatibility requirements. The need to address this issue has led to different proposals and, ultimately, an implementation in Kubernetes&amp;rsquo; Node Feature Discovery (NFD). NFD is an open source Kubernetes project that automatically detects and reports hardware and system features of cluster nodes. This information helps users to schedule workloads on nodes that meet specific system requirements, which is especially useful for applications with strict hardware or operating system dependencies. A container image is built on a base image, which provides a minimal runtime environment, often a stripped-down Linux userland, completely empty or distroless. When an application requires certain features from the host OS, compatibility issues arise. These dependencies can manifest in several ways: Drivers : Host driver versions must match the supported range of a library version inside the container to avoid compatibility problems. Examples include GPUs and network drivers. Libraries or Software : The container must come with a specific version or range of versions for a library or software to run optimally in the environment. Examples from high performance computing are MPI, EFA, or Infiniband.</description></item><item><title>See More, Worry Less: Managed Database Observability, Monitoring, and Hardening Advancements</title><link>https://kubermates.org/docs/2025-06-24-see-more-worry-less-managed-database-observability-monitoring-and-hardening-adva/</link><pubDate>Tue, 24 Jun 2025 17:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-24-see-more-worry-less-managed-database-observability-monitoring-and-hardening-adva/</guid><description>See More, Worry Less: Managed Database Observability, Monitoring, and Hardening Advancements Whatâs new? Datadog Integration Default resource alerts and advanced cluster event notifications Trusted sources IP Labeling Resources to get started About the author Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 By Nicole Ghalwash Published: June 24, 2025 4 min read As your applications scale, keeping a close eye on them becomes more important than everâit also becomes harder the more data you have. Thatâs why weâre pleased to announce that we have made several new observability, monitoring, and hardening advancements to DigitalOcean Managed Databasesâto give you better visibility, understanding, and peace of mind. To be exact, we have made three advancements that will improve your Managed Database observability experience here at DigitalOcean. Letâs walk through all three of them. DigitalOcean Managed Databases now supports log forwarding to Datadog , enabling seamless integration to your existing Datadog observability dashboard. You can now send, view, and analyze your Managed Database logs directly within Datadog. This feature will enhance your overall observability and monitoring. Key features of this integration include: Direct log forwarding: This feature automatically pushes your database service logs to Datadogâs log intake endpoints, eliminating the need to manage a separate log collection agent. By automating log forwarding, you no longer have to worry about deploying, managing, or scaling additional logging agents, which helps to reduce operational complexity. Direct log forwarding: This feature automatically pushes your database service logs to Datadogâs log intake endpoints, eliminating the need to manage a separate log collection agent. By automating log forwarding, you no longer have to worry about deploying, managing, or scaling additional logging agents, which helps to reduce operational complexity. UI-based configuration: The integration is fully configurable through the management console, allowing you to set it up for most database types directly within the log forwarding section.</description></item><item><title>New Spaces features make it easier to stay secure, compliant, and in control</title><link>https://kubermates.org/docs/2025-06-18-new-spaces-features-make-it-easier-to-stay-secure-compliant-and-in-control/</link><pubDate>Wed, 18 Jun 2025 19:05:05 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-18-new-spaces-features-make-it-easier-to-stay-secure-compliant-and-in-control/</guid><description>New Spaces features make it easier to stay secure, compliant, and in control Spaces access logs are now generally available New Spaces connection wizard makes third-party tools easier to use No additional cost. Just a better experience. Watch the webinar to scale smarter with all Q2 2025 IaaS updates About the author(s) Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 By Anantha Ramachandran and Keshav Attrey Published: June 18, 2025 4 min read As teams grow and their cloud storage needs scale, seemingly small issues can snowball into bigger problems. Weâve seen common pain points emerge, like struggling to track who accessed files through CDN endpoints, or wrestling with tricky configurations when using third-party tools like the AWS CLI or s3cmd, plus missed audit trails, prolonged troubleshooting, or accidental misconfigurations that can lead to data loss. Thatâs why weâve made two important updates to help simplify and strengthen the way you manage object storage with DigitalOcean Spaces , our S3-compatible object storage solution, built for simplicity, scalability, and affordability. Whether youâre serving up images on a website, storing large media files, managing data backups, or powering developer workflows, Spaces helps you move fast without managing infrastructure. These new featuresâthe general availability of Spaces access logs and the introduction of the connection wizardâare designed to enhance your visibility, minimize risk, and empower you to work with greater confidence. With access logs, you can get detailed, time-stamped records of all file read and write requests, including traffic that flows through both Spaces origin and CDN endpoints. Previously, there was no easy way to monitor who was accessing your files over the CDN. That made it difficult to troubleshoot, detect suspicious behavior, or meet compliance and audit requirements. Whatâs new Logs now capture both direct and CDN traffic, so you have a complete view of object activity. Logs now capture both direct and CDN traffic, so you have a complete view of object activity.</description></item><item><title>Securing Kubernetes Traffic with Calico Ingress Gateway</title><link>https://kubermates.org/docs/2025-06-17-securing-kubernetes-traffic-with-calico-ingress-gateway/</link><pubDate>Tue, 17 Jun 2025 16:26:04 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-17-securing-kubernetes-traffic-with-calico-ingress-gateway/</guid><description>Wait a second, is this the ‘Ingress vs. Gateway API’ debate? What makes Gateway API different? The Ingress Rut The purpose of this blog post Requirements Spin up a Kubernetes Cluster Install Calico with Operator Deploy a Demo Application Enable Calico Ingress Gateway Deploy Gateway API Resources Gateway HTTPRoute SSL Certificate and Automated Certification Process with Cert-Manager Gateway API integration ClusterIssuer Enabling HTTPS using Calico Ingress Gateway Force Redirect to HTTPS Clean up Conclusion If you’ve managed traffic in Kubernetes, you’ve likely navigated the world of Ingress controllers. For years, Ingress has been the standard way of getting our HTTP/S services exposed. But let’s be honest, it often felt like a compromise. We wrestled with controller-specific annotations to unlock critical features, blurred the lines between infrastructure and application concerns, and sometimes wished for richer protocol support or a more standardized approach. This “pile of vendor annotations,” while functional, highlighted the limitations of a standard that struggled to keep pace with the complex demands of modern, multi-team environments and even led to security vulnerabilities. Yes, and it’s a crucial one. The Kubernetes Gateway API isn’t just an Ingress v2; it’s a fundamental redesign, the “future” of Kubernetes ingress, built by the community to address these very challenges head-on. There are three main points that I came across while evaluating GatewayAPI and Ingress controllers: Standardization &amp;amp; Portability: It aims to provide a core, standard way to manage ingress, reducing reliance on vendor-specific hacks and making it easier to switch implementations – change the class, and it should “just work. ” Role-Based Architecture: Its biggest win is arguably the separation of concerns. Infrastructure teams can manage the Gateway (the entry point, TLS, ports), while application teams manage their HTTPRoutes (or TCPRoutes, etc. ), defining where their specific traffic should go.</description></item><item><title>The Definitive Guide to Microservices</title><link>https://kubermates.org/docs/2025-06-16-the-definitive-guide-to-microservices/</link><pubDate>Mon, 16 Jun 2025 04:23:11 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-16-the-definitive-guide-to-microservices/</guid><description>Key Takeaways Understanding Microservices How Microservices Operate Key Components of Microservices Architecture Advantages of Microservices Common Design Patterns in Microservices Challenges and Criticisms of Microservices Microservices vs. Monolithic Architectures Migrating from Monolithic to Microservices Service-Oriented Architecture (SOA) vs. Microservices Real-World Examples and Case Studies Tools and Technologies for Microservices Best Practices for Building Microservices Summary Frequently Asked Questions What are microservices, and how do they differ from monolithic architectures? How do microservices communicate with each other? What are some common design patterns used in microservices? What challenges can arise when adopting microservices? What tools and technologies are commonly used for building microservices? Exploring System Architecture for DevOps Engineers Why KubeCon India 2025 Meant More to KodeKloud Linux: List Disks Linux: &amp;ldquo;cat&amp;rdquo; Command Linux Made Easy for DevOps Beginners From CFP to Stage: Win Your Tech Talk Slot MCP Explained Simply: How AI Can Actually Do Things Now Still Not Job-Ready After Learning DevOps? What Is Kubernetes? Finally, a Simple Explanation! They ensure scalability, flexibility, and ease of maintenance by allowing individual services to operate and be updated autonomously. In this guide, we will break down what microservices are, how they operate, their advantages, key components, and best practices, offering a comprehensive overview for anyone looking to understand this modern architectural approach. Microservices architecture allows applications to be built as a collection of independent, modular services that enhance scalability and flexibility. Key components of microservices include API gateways, service registries, and individual data stores for each service, which streamline communication and data management. While microservices offer advantages like fault isolation and faster development cycles, they also introduce complexity and require robust tools for management and orchestration. Microservices are an architectural style that organizes an application as a collection of small, independently deployable services. Unlike traditional monolithic architectures, where a single service encapsulates all functionalities, microservices based applications break down applications into individual services, each focusing on a specific business capability. This modularity allows for independent deployment and scaling, making microservice architecture a popular choice for modern applications, particularly when considering internal microservices. One of the key characteristics of microservices is their cloud-native approach. Each service operates independently but is part of a larger application framework, often utilizing cloud resources to ensure scalability and fault tolerance.</description></item><item><title>Blog: Changes to Kubernetes Slack</title><link>https://kubermates.org/docs/2025-06-16-blog-changes-to-kubernetes-slack/</link><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-16-blog-changes-to-kubernetes-slack/</guid><description>Changes to Kubernetes Slack UPDATE : We’ve received notice from Salesforce that our Slack workspace WILL NOT BE DOWNGRADED on June 20th. Stand by for more details, but for now, there is no urgency to back up private channels or direct messages. Kubernetes Slack will lose its special status and will be changing into a standard free Slack on June 20, 2025. Sometime later this year, our community may move to a new platform. If you are responsible for a channel or private channel, or a member of a User Group, you will need to take some actions as soon as you can. For the last decade, Slack has supported our project with a free customized enterprise account. They have let us know that they can no longer do so, particularly since our Slack is one of the largest and more active ones on the platform. As such, they will be downgrading it to a standard free Slack while we decide on, and implement, other options. On Friday, June 20, we will be subject to the feature limitations of free Slack. The primary ones which will affect us will be only retaining 90 days of history, and having to disable several apps and workflows which we are currently using. The Slack Admin team will do their best to manage these limitations. Responsible channel owners, members of private channels, and members of User Groups should take some actions to prepare for the upgrade and preserve information as soon as possible.</description></item><item><title>Changes to Kubernetes Slack</title><link>https://kubermates.org/docs/2025-06-16-changes-to-kubernetes-slack/</link><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-16-changes-to-kubernetes-slack/</guid><description>Changes to Kubernetes Slack UPDATE : We’ve received notice from Salesforce that our Slack workspace WILL NOT BE DOWNGRADED on June 20th. Stand by for more details, but for now, there is no urgency to back up private channels or direct messages. Kubernetes Slack will lose its special status and will be changing into a standard free Slack on June 20, 2025. Sometime later this year, our community may move to a new platform. If you are responsible for a channel or private channel, or a member of a User Group, you will need to take some actions as soon as you can. For the last decade, Slack has supported our project with a free customized enterprise account. They have let us know that they can no longer do so, particularly since our Slack is one of the largest and more active ones on the platform. As such, they will be downgrading it to a standard free Slack while we decide on, and implement, other options. On Friday, June 20, we will be subject to the feature limitations of free Slack. The primary ones which will affect us will be only retaining 90 days of history, and having to disable several apps and workflows which we are currently using. The Slack Admin team will do their best to manage these limitations. Responsible channel owners, members of private channels, and members of User Groups should take some actions to prepare for the upgrade and preserve information as soon as possible.</description></item><item><title>Kubernetes HPA: Mastering Horizontal Pod Autoscaler Basics and Best Practices</title><link>https://kubermates.org/docs/2025-06-15-kubernetes-hpa-mastering-horizontal-pod-autoscaler-basics-and-best-practices/</link><pubDate>Sun, 15 Jun 2025 04:51:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-15-kubernetes-hpa-mastering-horizontal-pod-autoscaler-basics-and-best-practices/</guid><description>Key Takeaways Understanding Horizontal Pod Autoscaler (HPA) How HPA Works Setting Up Metrics Server Configuring HPA in Your Kubernetes Cluster Practical Example: Implementing HPA Deploy Sample Application Create Kubernetes Service Apply Horizontal Pod Autoscaler Testing HPA Functionality Increase Load Monitor Scaling Events Decrease Load HPA Limitations Best Practices for HPA Configuration Integrating HPA with Other Autoscalers Usage and Cost Reporting Summary Frequently Asked Questions What is the primary purpose of the Horizontal Pod Autoscaler (HPA)? How does HPA determine the number of replicas to scale? What are some common limitations of HPA? How can integrating HPA with other autoscalers improve scaling efficiency? What tools can help track resource usage and manage costs in a Kubernetes cluster with HPA? Exploring System Architecture for DevOps Engineers Why KubeCon India 2025 Meant More to KodeKloud Linux: List Disks Linux: &amp;ldquo;cat&amp;rdquo; Command Linux Made Easy for DevOps Beginners From CFP to Stage: Win Your Tech Talk Slot MCP Explained Simply: How AI Can Actually Do Things Now Still Not Job-Ready After Learning DevOps? What Is Kubernetes? Finally, a Simple Explanation! This capability ensures your application remains responsive and performs well under varying traffic loads. In this article, we will cover the basics of Kubernetes HPA, how it works, best practices, and a hands-on example to help you master this critical Kubernetes feature. The Horizontal Pod Autoscaler (HPA) automatically adjusts the number of pod replicas in Kubernetes based on CPU and memory metrics, ensuring stable application performance under varying loads. Proper setup of the Metrics Server is essential, as it provides the necessary resource metrics for HPA to make informed scaling decisions; accurate HPA configuration specifies minimum and maximum replicas along with target utilization. Integrating HPA with other autoscalers such as the Vertical Pod Autoscaler (VPA) and Cluster Autoscaler enhances scaling strategies, allowing for efficient resource management and responsiveness to varying application demands. At its core, the Kubernetes Horizontal Pod Autoscaler (HPA) is a Kubernetes resource that automatically adjusts the number of pod replicas based on observed CPU and memory usage metrics. This dynamic adjustment ensures your application maintains stable performance even as traffic fluctuates, making it a critical component for production workloads. Horizontal pod autoscaling HPA operates by continuously monitoring specified metrics and making scaling decisions to match the demand. The HPA utilizes resource metrics like CPU and memory, as well as custom metrics and cpu metrics, to determine the appropriate number of replicas needed. For example, if the average CPU utilization exceeds a predefined threshold, HPA will increase the number of replicas to distribute the load. Conversely, when the demand decreases, HPA reduces the number of replicas, optimizing resource usage and reducing costs. Additionally, the custom metrics api can be leveraged to enhance monitoring capabilities.</description></item><item><title>kubectl logs: How to Get Pod Logs in Kubernetes (With Examples)</title><link>https://kubermates.org/docs/2025-06-14-kubectl-logs-how-to-get-pod-logs-in-kubernetes-with-examples/</link><pubDate>Sat, 14 Jun 2025 14:09:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-14-kubectl-logs-how-to-get-pod-logs-in-kubernetes-with-examples/</guid><description>Prerequisites Create a Pod How to View Pod Logs View Pod logs in real time Fetch a specific number of lines of Pod logs View logs of an exited container Fetch Pod logs from a specific time period Fetch the logs of a specific container in a multi-container Pod Conclusion Exploring System Architecture for DevOps Engineers Why KubeCon India 2025 Meant More to KodeKloud Linux: List Disks Linux: &amp;ldquo;cat&amp;rdquo; Command Linux Made Easy for DevOps Beginners From CFP to Stage: Win Your Tech Talk Slot MCP Explained Simply: How AI Can Actually Do Things Now Still Not Job-Ready After Learning DevOps? What Is Kubernetes? Finally, a Simple Explanation! Like any software, these applications can sometimes fail or not perform as expected due to various reasons. When such failures occur, it’s important to identify and rectify the issue quickly. One key aspect of troubleshooting involves analyzing the application logs, which can provide valuable information about the root cause of the problem. Logs are essentially records of events happening within your application. By examining these logs, we can often gain insights into what went wrong. In this blog post, we’ll learn how to access Pod logs in Kubernetes using the kubectl logs command. Note that when we say Pod logs, we’re generally referring to the logs of the applications running in containers inside the Pod. kubectl logs To easily follow along with the examples in this post, we recommend using KodeKloud’s Kubernetes playground. This playground will provide you instant access to a running Kubernetes cluster with kubectl already installed. No need for you to install any software. With just one click, you&amp;rsquo;ll be ready to run the example code snippets and start experimenting right away. kubectl Alternatively, if you prefer to set up your own Kubernetes cluster, you can use a tool such as minikube.</description></item><item><title>Introducing AMD Instinct™ MI300X GPU Droplets</title><link>https://kubermates.org/docs/2025-06-12-introducing-amd-instinct-mi300x-gpu-droplets/</link><pubDate>Thu, 12 Jun 2025 13:30:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-12-introducing-amd-instinct-mi300x-gpu-droplets/</guid><description>Introducing AMD Instinctâ¢ MI300X GPU Droplets What are AMD Instinct â¢ MI300X GPUs? What are the benefits of AMD Instinct MI300X â¢ GPUs? What are the benefits of accessing GPU Droplets with DigitalOcean? What happens next? About the author Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 By Waverly Swinton Updated: June 11, 2025 2 min read GPU Droplets are now DigitalOcean GradientAI GPU Droplets. Learn more about DigitalOcean GradientAI , our suite of AI products. At DigitalOcean, weâre committed to giving you even more options to power your AI/ML workloads. Today, weâre excited to announce that DigitalOcean customers now have access to AMD Instinctâ¢ MI300X as DigitalOcean GPU Droplets. AMD Instinctâ¢ MI300X GPUs deliver leadership performance for accelerated high-performance computing (HPC) applications and the newly exploding demands of generative AI. Use-cases for these GPUs include large model training, fine-tuning, inference, and HPC. With the AMD ROCmâ¢ software platform, you can develop powerful HPC and AI production-ready systems faster than ever before. AMD Instinctâ¢ MI300Xâs large memory capacity allows it to hold models with hundreds of billions of parameters entirely in memory, reducing the need for model splitting across multiple GPUs. By combining powerful AMD AI compute engines and DigitalOceanâs cloud technologies, we aim to empower the massive community of developers like you to integrate AI into your applications and support your most demanding AI workloads at scale. Memory performance : High memory bandwidth (3.35 TB/s) and capacity (192 GB of HBM3) to efficiently handle larger models and datasets. Value: Offered at a competitive price point ($1.99 GPU/hr/on-demand) Value: Offered at a competitive price point ($1.99 GPU/hr/on-demand) Customization: AMD Instinctâ¢ MI300X is available both as single and eight GPU configurations and in bare metal configurations Customization: AMD Instinctâ¢ MI300X is available both as single and eight GPU configurations and in bare metal configurations Virtual instances to manage cost Virtual instances to manage cost Seamless integration with the broader DigitalOcean ecosystem, including access to our Kubernetes service Seamless integration with the broader DigitalOcean ecosystem, including access to our Kubernetes service Pre-installed Python and Deep Learning software packages Pre-installed Python and Deep Learning software packages HIPAA-eligibility and SOC 2 compliance (all GPU Droplets) HIPAA-eligibility and SOC 2 compliance (all GPU Droplets) Later this year, weâll also offer AMD Instinctâ¢ MI325X GPUs, further expanding access to powerful and affordable GPU models. AMD Instinctâ¢ MI325X GPU accelerators set new AI performance standards, delivering incredible performance and efficiency for training and inference.</description></item><item><title>Choosing the Right GPU Droplet for your AI/ML Workload</title><link>https://kubermates.org/docs/2025-06-11-choosing-the-right-gpu-droplet-for-your-ai-ml-workload/</link><pubDate>Wed, 11 Jun 2025 21:54:17 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-11-choosing-the-right-gpu-droplet-for-your-ai-ml-workload/</guid><description>Choosing the Right GPU Droplet for your AI/ML Workload DigitalOcean Gradient AIâ¢ GPU Droplets for large model training, fine-tuning, and high-performance computing (HPC) DigitalOcean Gradient AIâ¢ GPU Droplets for cost-effective inference and graphical workloads Benefits of GPU Droplets About the author Try DigitalOcean for free By Waverly Swinton Updated: June 11, 2025 4 min read GPU Droplets are now DigitalOcean GradientAI GPU Droplets. Learn more about DigitalOcean GradientAI , our suite of AI products. Whether youâre new to AI and machine learning (ML) or a seasoned expert, looking to train a large language model (LLM) or run cost-effective inference, DigitalOcean has a GPU Droplet for you. We currently offer seven different GPU Droplet types from industry-leading brands - AMD and Nvidia - with more GPU Droplet types to come. Read on to learn more about how to choose the right GPU Droplet for your workload. AMD Instinctâ¢ MI325X Use cases: Large model training, fine-tuning, inference, and HPC Why choose: AMD Instinctâ¢ MI325Xâs large memory capacity allows it to hold models with hundreds of billions of parameters entirely in memory, reducing the need for model splitting across multiple GPUs. Key benefits: Memory performance: High memory capacity to hold models with hundreds of billions of parameters, reducing the need for model splitting across multiple GPUs Memory performance: High memory capacity to hold models with hundreds of billions of parameters, reducing the need for model splitting across multiple GPUs Value: Offered at a competitive price point ($1.69/GPU/hr/contract) for a HPC GPU. Contact us to reserve capacity. Value: Offered at a competitive price point ($1.69/GPU/hr/contract) for a HPC GPU. Contact us to reserve capacity. Key performance benchmark: With 256 GB of HBM3E memory (vs. MI300Xâs 192 GB), MI325X can handle significantly larger models and datasets entirely on a single GPU AMD Instinctâ¢ MI300X Use cases: Generative AI LLM training, fine-tuning, inference, and HPC Why choose: AMD Instinctâ¢ MI300Xâs large memory capacity allows it to hold models with hundreds of billions of parameters entirely in memory, reducing the need for model splitting across multiple GPUs.</description></item><item><title>Secure and Scalable Kubernetes for Multi-Cluster Management</title><link>https://kubermates.org/docs/2025-06-10-secure-and-scalable-kubernetes-for-multi-cluster-management/</link><pubDate>Tue, 10 Jun 2025 20:03:12 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-10-secure-and-scalable-kubernetes-for-multi-cluster-management/</guid><description>The Multi-Cluster Challenge: When Complexity Takes Over Calico’s Approach: Seamless Security, Streamlined Operations, and Crystal-Clear Visibility Enhanced Security Across the Board Intelligent Traffic Management: North/South and East/West Unified Observability: Seeing is Securing The Outcome: A Robust and Efficient Kubernetes Infrastructure This story is becoming more and more common in the Kubernetes world. What starts as a manageable cluster or two can quickly balloon into a sprawling, multi-cluster architecture spanning public clouds, private data centers, or a bit of both. And with that growth comes a whole new set of headaches. How do you keep tabs on compliance across wildly different configurations? When a service goes down across multiple clusters, how do you pinpoint the cause amidst the chaos? And what about those hard-to-diagnose latency issues that seem to crop up between regions? The truth is, achieving secure and scalable multi-cluster Kubernetes isn’t about throwing more tools at the problem. It’s about having the right tools and adopting the right best practices. This is where a solution like Calico Cluster Mesh shines, offering those essential capabilities for a seamless multi-cluster experience without the complexity or overhead that you expect with traditional service meshes. So, why are so many organizations finding themselves in this multi-cluster maze? Often, it’s driven by solid business reasons: High Availability and Disaster Recovery: Spreading workloads across multiple regions or clusters means that if one goes down, your users shouldn’t notice. Performance Optimization: Shifting compute resources to take advantage of lower pricing or bringing processing closer to the data at the edge can make a big difference. Regulatory and Compliance Requirements: Sometimes, data simply has to reside in specific geographies. Hybrid Cloud Strategies: The reality is, not everything can or should move to the cloud. A hybrid approach allows organizations to keep sensitive or legacy systems on-premises while still leveraging the flexibility and scalability of public cloud. While these motivations are sound, the challenges that emerge in these multi-cloud environments are remarkably consistent: Inter-cluster communication is a beast.</description></item><item><title>K3s vs K8s: What are the Differences &amp; Use Cases</title><link>https://kubermates.org/docs/2025-06-10-k3s-vs-k8s-what-are-the-differences-use-cases/</link><pubDate>Tue, 10 Jun 2025 07:48:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-10-k3s-vs-k8s-what-are-the-differences-use-cases/</guid><description>What is K8s? Why Use K8s What is K3s? Why Use K3s? When To Use K3s vs Kubernetes Ease of Operations of K8s vs. K3s Disadvantages of K3s Key Differences Wrapping Up Exploring System Architecture for DevOps Engineers Why KubeCon India 2025 Meant More to KodeKloud Linux: List Disks Linux: &amp;ldquo;cat&amp;rdquo; Command Linux Made Easy for DevOps Beginners From CFP to Stage: Win Your Tech Talk Slot MCP Explained Simply: How AI Can Actually Do Things Now Still Not Job-Ready After Learning DevOps? What Is Kubernetes? Finally, a Simple Explanation! But as organizations look to deploy containerized workloads to devices at the edge of their network or for Internet of Things (IoT) applications, the full Kubernetes distribution can be overkill. This is where K3s comes in. Developed by Rancher Labs, K3s is a lightweight Kubernetes distribution designed specifically for resource-constrained edge and IoT environments. In this article, we&amp;rsquo;ll explain the key differences between K3s and the upstream Kubernetes project to help you understand when each makes the most sense for your application architecture and deployment needs. Try the Kubernetes Deployments Lab for free. Kubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications. It allows you to define your application&amp;rsquo;s desired state and ensures that it runs consistently in a cluster of machines. Kubernetes automates tasks such as load balancing, self-healing, and scaling, making it easier to manage and maintain container-based applications. It has become the industry standard for container orchestration, simplifying the management of complex, distributed applications. To learn more about how it works, check out this blog: Kubernetes Architecture Explained: Overview for DevOps Enthusiasts. Scalability and Resource Efficiency - Kubernetes enables easy scaling of your applications up or down based on demand.</description></item><item><title>Enhancing Kubernetes Event Management with Custom Aggregation</title><link>https://kubermates.org/docs/2025-06-10-enhancing-kubernetes-event-management-with-custom-aggregation/</link><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-10-enhancing-kubernetes-event-management-with-custom-aggregation/</guid><description>Enhancing Kubernetes Event Management with Custom Aggregation The challenge with Kubernetes events Real-World value Building an Event aggregation system Architecture overview Event processing and classification Implementing Event correlation Event storage and retention Good practices for Event management Advanced features Pattern detection Real-time alerts Conclusion Next steps Kubernetes Events provide crucial insights into cluster operations, but as clusters grow, managing and analyzing these events becomes increasingly challenging. This blog post explores how to build custom event aggregation systems that help engineering teams better understand cluster behavior and troubleshoot issues more effectively. In a Kubernetes cluster, events are generated for various operations - from pod scheduling and container starts to volume mounts and network configurations. While these events are invaluable for debugging and monitoring, several challenges emerge in production environments: Volume : Large clusters can generate thousands of events per minute Retention : Default event retention is limited to one hour Correlation : Related events from different components are not automatically linked Classification : Events lack standardized severity or category classifications Aggregation : Similar events are not automatically grouped To learn more about Events in Kubernetes, read the Event API reference. Consider a production environment with tens of microservices where the users report intermittent transaction failures: Traditional event aggregation process: Engineers are wasting hours sifting through thousands of standalone events spread across namespaces. By the time they look into it, the older events have long since purged, and correlating pod restarts to node-level issues is practically impossible. With its event aggregation in its custom events: The system groups events across resources, instantly surfacing correlation patterns such as volume mount timeouts before pod restarts. History indicates it occurred during past record traffic spikes, highlighting a storage scalability issue in minutes rather than hours. The beneﬁt of this approach is that organizations that implement it commonly cut down their troubleshooting time significantly along with increasing the reliability of systems by detecting patterns early. This post explores how to build a custom event aggregation system that addresses these challenges, aligned to Kubernetes best practices. I&amp;rsquo;ve picked the Go programming language for my example. This event aggregation system consists of three main components: Event Watcher : Monitors the Kubernetes API for new events Event Processor : Processes, categorizes, and correlates events Storage Backend : Stores processed events for longer retention Here&amp;rsquo;s a sketch for how to implement the event watcher: package main import ( &amp;ldquo;context&amp;rdquo; metav1 &amp;ldquo;k8s.</description></item><item><title>Introducing Serverless Inference on the GenAI Platform</title><link>https://kubermates.org/docs/2025-06-09-introducing-serverless-inference-on-the-genai-platform/</link><pubDate>Mon, 09 Jun 2025 17:48:18 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-09-introducing-serverless-inference-on-the-genai-platform/</guid><description>Introducing Serverless Inference on the GenAI Platform A simpler way to integrate AI Ideal use cases Start building today About the author Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 By Grace Morgan Updated: June 9, 2025 2 min read DigitalOceanâs GenAI Platform is now DigitalOcean Gradient Platform. Learn more about the GA release and features. In order to scale AI applications, developers often end up spending more time wrangling infrastructure, scaling for unpredictable traffic, or juggling multiple model providers than actually building. Donât even get us started on fragmented billing. Serverless inference, now available on the DigitalOcean GenAI Platform , removes all of that complexity. It gives you a fast, low-friction way to integrate powerful models from providers like OpenAI, Anthropic, and Meta, without provisioning infrastructure or managing multiple keys and accounts. Serverless inference is one of the simplest ways to integrate AI models into your application. No infrastructure, no setup, no hassle. Whether youâre building a recommendation engine, chatbot, or another AI-powered feature, you get direct access to powerful models through a single API. Itâs built for simplicity and scalability: nothing to provision, no clusters to manage, and automatic scaling to handle unpredictable workloads. You stay focused on building, while we handle the rest. With the newest feature, you get: Unified simple model access with one API key Fixed endpoints for reliable integration Centralized usage monitoring and billing Support for unpredictable workloads without pre-provisioning Usage-based pricing with no idle infrastructure costs Itâs a low-friction, cost-efficient way to embed AI features into your product, ideal for teams who want full control over the experience and integration.</description></item><item><title>Should I Use Kubernetes?</title><link>https://kubermates.org/docs/2025-06-07-should-i-use-kubernetes/</link><pubDate>Sat, 07 Jun 2025 07:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-07-should-i-use-kubernetes/</guid><description>When Should You Use Kubernetes? Microservices architecture Automatic scaling Resource optimization When Should You Avoid Kubernetes? Simple applications Resource constraints Learning curve vs. project timeline Alternative Solutions to Kubernetes Docker Swarm Apache Mesos with Marathon Virtual Machines Conclusion Exploring System Architecture for DevOps Engineers Why KubeCon India 2025 Meant More to KodeKloud Linux: List Disks Linux: &amp;ldquo;cat&amp;rdquo; Command Linux Made Easy for DevOps Beginners From CFP to Stage: Win Your Tech Talk Slot MCP Explained Simply: How AI Can Actually Do Things Now Still Not Job-Ready After Learning DevOps? What Is Kubernetes? Finally, a Simple Explanation! Since its debut in 2014, Kubernetes has seen a meteoric rise in its adoption and popularity. Despite being a relatively new technology, it is now used by a vast number of organizations, from small start-ups to large enterprises, so much so that it has become a standard in the industry. Kubernetes benefits from strong community support and boasts a rich ecosystem of tools and extensions. It is continuously being developed and improved, and judging by the current trends, Kubernetes is only going to get stronger in the future. In this blog post, we’ll discuss why you should consider using Kubernetes , scenarios where Kubernetes might not be the ideal solution, and explore some of its alternatives. Let’s get started! Kubernetes offers several key features that cater to a wide range of application deployment and management needs. Here are the top three features of Kubernetes that make it particularly suitable for modern software development: In recent years, the microservices approach ( where software systems are developed as a collection of small, independent services ) has become increasingly popular in software development. Kubernetes provides many abstractions and APIs that are particularly well-suited to the requirements and characteristics of a microservices architecture. Here&amp;rsquo;s how Kubernetes simplifies the deployment and management of microservices: Containerization support : Kubernetes is designed to manage containers, which are ideal for microservices. Each microservice can be packaged into its own container, encapsulating its dependencies and runtime environment. This makes it easy to deploy and manage microservices consistently and reliably.</description></item><item><title>Sharks of DigitalOcean: Ali Munir, Staff Technical Account Manager</title><link>https://kubermates.org/docs/2025-06-06-sharks-of-digitalocean-ali-munir-staff-technical-account-manager/</link><pubDate>Fri, 06 Jun 2025 04:14:12 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-06-sharks-of-digitalocean-ali-munir-staff-technical-account-manager/</guid><description>Sharks of DigitalOcean: Ali Munir, Staff Technical Account Manager Why did you choose DigitalOcean? What makes DigitalOcean unique in the tech industry? What does customer-centricity mean at DigitalOcean? What excites you about the work you do? How would you describe DigitalOceanâs work and collaboration style? Dive into the future with DigitalOcean About the author Try DigitalOcean for free Related Articles Sharks of DigitalOcean: Darian Wilkin, Senior Manager, Solutions Engineering Sharks of DigitalOcean: Laura Schaffer, VP, Growth Sharks of DigitalOcean: Jason Dobry, Staff IT Project Specialist By Sujatha R Technical Writer Published: June 6, 2025 3 min read Ali Munir, a Staff Technical Account Manager at DigitalOcean, is driven by one core mission: helping customers succeed. From day one, heâs embraced a culture rooted in honesty, collaboration, and a deep commitment to putting customers first. DigitalOcean stood out to me in a crowded cloud industry of enterprise giants. Here, we lead with simplicity, transparency, and a genuine commitment to customer experience. I was most impressed with the companyâs ability to offer powerful technology without overwhelming users with complexity. When I researched DO, I immediately knew it was the right place for me. Itâs a place where your voice matters, where youâre empowered to act, and where collaboration feels seamless. If you have an idea that can make a difference, people here listen, align with you, and help bring it to life. Thatâs powerful. To me, DigitalOcean isnât just a workplace, itâs a launchpad for innovation. Weâre not just building tools; weâre helping builders around the world solve real problems and bring meaningful software to life. What I love most is our customer-centric mindset.</description></item><item><title>Introducing Gateway API Inference Extension</title><link>https://kubermates.org/docs/2025-06-05-introducing-gateway-api-inference-extension/</link><pubDate>Thu, 05 Jun 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-05-introducing-gateway-api-inference-extension/</guid><description>Introducing Gateway API Inference Extension Gateway API Inference Extension How it works Request flow Benchmarks Key results Roadmap Summary Modern generative AI and large language model (LLM) services create unique traffic-routing challenges on Kubernetes. Unlike typical short-lived, stateless web requests, LLM inference sessions are often long-running, resource-intensive, and partially stateful. For example, a single GPU-backed model server may keep multiple inference sessions active and maintain in-memory token caches. Traditional load balancers focused on HTTP path or round-robin lack the specialized capabilities needed for these workloads. They also don’t account for model identity or request criticality (e. g. , interactive chat vs. batch jobs). Gateway API Inference Extension was created to address this gap by building on the existing Gateway API , adding inference-specific routing capabilities while retaining the familiar model of Gateways and HTTPRoutes. By adding an inference extension to your existing gateway, you effectively transform it into an Inference Gateway , enabling you to self-host GenAI/LLMs with a “model-as-a-service” mindset. The project’s goal is to improve and standardize routing to inference workloads across the ecosystem. Key objectives include enabling model-aware routing, supporting per-request criticalities, facilitating safe model roll-outs, and optimizing load balancing based on real-time model metrics.</description></item><item><title>Is It Time to Migrate? A Practical Look at Kubernetes Ingress vs. Gateway API</title><link>https://kubermates.org/docs/2025-06-04-is-it-time-to-migrate-a-practical-look-at-kubernetes-ingress-vs-gateway-api/</link><pubDate>Wed, 04 Jun 2025 14:10:48 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-04-is-it-time-to-migrate-a-practical-look-at-kubernetes-ingress-vs-gateway-api/</guid><description>The Situation: The Ingress Rut Kubernetes Gateway API vs. Ingress: The Core Differences 🤔 The Implication: Tied Hands and Increased Risk The Resolution: Gateway API Meets Calico The Outcome: Secure, Streamlined, and Standardized If you’ve managed traffic in Kubernetes, you’ve likely navigated the world of Ingress controllers. For years, Ingress has been the standard way of getting HTTP/S services exposed. But let’s be honest, it often felt like a compromise. We wrestled with controller-specific annotations to unlock critical features, blurred the lines between infrastructure and application concerns, this complexity didn’t just make portability more difficult, it sometimes led to security vulnerabilities and other complications. As part of Calico Open Source v3.30 , we have released a free and open source Calico Ingress Gateway that implements a custom built Envoy proxy with the Kubernetes Gateway API standard to help you navigate Ingress complexities with style. This blog post is designed to get you up to speed on why such a change might be the missing link in your environment. The challenge with traditional Ingress wasn’t a lack of effort, since the landscape is full of innovative solutions. However, the problem was the lack of a unified, expressive, and role-aware standard. Existing ingress controllers were capable, implemented advanced features, however at the same time tied you to a specific project/vendor. This meant: Vendor Lock-In: Migrating from one ingress controller to another became a painful exercise in translating a web of custom annotations. Configurations were tied to the implementation, not the intent.</description></item><item><title>Introducing ATL1: DigitalOcean’s new AI-optimized data center in Atlanta</title><link>https://kubermates.org/docs/2025-06-03-introducing-atl1-digitalocean-s-new-ai-optimized-data-center-in-atlanta/</link><pubDate>Tue, 03 Jun 2025 12:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-03-introducing-atl1-digitalocean-s-new-ai-optimized-data-center-in-atlanta/</guid><description>Introducing ATL1: DigitalOceanâs new AI-optimized data center in Atlanta Built to power the future of AI/ML Bringing Cloud and AI infrastructure closer to the Southern U. S. About the author Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 By Grace Morgan Published: June 3, 2025 2 min read Weâre excited to announce the launch of ATL1, DigitalOceanâs newest and largest data center, opening in Atlanta. The new data center is purpose-built to deliver high-density GPU infrastructure optimized for the future of AI/ML and is a major step forward in our mission to simplify cloud computing and AI for growing tech companies. Starting now, you will be able to deploy DigitalOcean products, including Premium Droplets, Spaces Object Storage, Load Balancers, Kubernetes clusters, managed databases, and Bare Metal GPUs, from our Atlanta data center, offering you even greater performance, scalability, and flexibility. ATL1 expands DigitalOceanâs footprint to 17 data centers across 10 global regions and marks a major investment into supporting the next generation of AI, machine learning, and high-performance applications. ATL1 isnât just another data centerâitâs built to power the next generation of AI/ML, providing the infrastructure needed for the worldâs most advanced workloads. Key highlights about the new data center: Massive capacity Two data halls, each offering 4.5 MW of power (9 MW total) Designed for high-density GPU deployments with flexible, scalable expansion capabilities Cutting-edge hardware Over 300 GPUs deployed in the first data hallâincluding NVIDIA H200 and AMD Instinct MI300X clustersâfor high-performance AI/ML workloads Second data hall planned to launch with more GPU infrastructure in 2025 Built to support the rapid expansion of GPU clusters DigitalOcean infrastructure services Providing scalable compute, storage, networking, and Kubernetes Seamless integration into DigitalOceanâs private global backbone With ATL1, DigitalOcean is expanding our footprint to better serve developers, startups, and digital native enterprises across the Southern U. S. region. By adding local cloud and AI infrastructure in Atlanta, weâre making it easier for customers to access the high-performance compute, storage, and GPU resources they need, with lower latency, greater reliability, and the same simplicity and cost-effectiveness they expect from DigitalOcean. Located at Flexentialâs Atlanta-Douglasville campus , ATL1 benefits from a high-density, enterprise-grade facility optimized for AI/ML workloads, further enhancing performance, scalability, and security for our customers.</description></item><item><title>Start Sidecar First: How To Avoid Snags</title><link>https://kubermates.org/docs/2025-06-03-start-sidecar-first-how-to-avoid-snags/</link><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-06-03-start-sidecar-first-how-to-avoid-snags/</guid><description>Start Sidecar First: How To Avoid Snags A gentle refresher The problem Readiness probe Maybe a startup probe? What about the postStart lifecycle hook? Liveness probe Findings summary From the Kubernetes Multicontainer Pods: An Overview blog post you know what their job is, what are the main architectural patterns, and how they are implemented in Kubernetes. The main thing I’ll cover in this article is how to ensure that your sidecar containers start before the main app. It’s more complicated than you might think! I&amp;rsquo;d just like to remind readers that the v1.29.0 release of Kubernetes added native support for sidecar containers , which can now be defined within the. spec. initContainers field, but with restartPolicy: Always. You can see that illustrated in the following example Pod manifest snippet:. spec. initContainers restartPolicy: Always initContainers : - name : logshipper image : alpine:latest restartPolicy : Always # this is what makes it a sidecar container command : [ &amp;lsquo;sh&amp;rsquo; , &amp;lsquo;-c&amp;rsquo; , &amp;rsquo;tail -F /opt/logs. txt&amp;rsquo; ] volumeMounts : - name : data mountPath : /opt initContainers : - name : logshipper image : alpine:latest restartPolicy : Always # this is what makes it a sidecar container command : [ &amp;lsquo;sh&amp;rsquo; , &amp;lsquo;-c&amp;rsquo; , &amp;rsquo;tail -F /opt/logs. txt&amp;rsquo; ] volumeMounts : - name : data mountPath : /opt What are the specifics of defining sidecars with a. spec. initContainers block, rather than as a legacy multi-container pod with multiple.</description></item><item><title>Gateway API v1.3.0: Advancements in Request Mirroring, CORS, Gateway Merging, and Retry Budgets</title><link>https://kubermates.org/docs/2025-06-02-gateway-api-v1-3-0-advancements-in-request-mirroring-cors-gateway-merging-and-re/</link><pubDate>Mon, 02 Jun 2025 09:00:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-06-02-gateway-api-v1-3-0-advancements-in-request-mirroring-cors-gateway-merging-and-re/</guid><description>Gateway API v1.3.0: Advancements in Request Mirroring, CORS, Gateway Merging, and Retry Budgets Graduation to Standard channel Percentage-based request mirroring Additions to Experimental channel CORS filtering XListenerSets (standardized mechanism for Listener and Gateway merging) Retry budgets (XBackendTrafficPolicy) Try it out Get involved Related Kubernetes blog articles Join us in the Kubernetes SIG Network community in celebrating the general availability of Gateway API v1.3.0! We are also pleased to announce that there are already a number of conformant implementations to try, made possible by postponing this blog announcement. Version 1.3.0 of the API was released about a month ago on April 24, 2025. Gateway API v1.3.0 brings a new feature to the Standard channel (Gateway API&amp;rsquo;s GA release channel): percentage-based request mirroring , and introduces three new experimental features: cross-origin resource sharing (CORS) filters, a standardized mechanism for listener and gateway merging, and retry budgets. Also see the full release notes and applaud the v1.3.0 release team next time you see them. Graduation to the Standard channel is a notable achievement for Gateway API features, as inclusion in the Standard release channel denotes a high level of confidence in the API surface and provides guarantees of backward compatibility. Of course, as with any other Kubernetes API, Standard channel features can continue to evolve with backward-compatible additions over time, and we (SIG Network) certainly expect further refinements and improvements in the future. For more information on how all of this works, refer to the Gateway API Versioning Policy. Leads: Lior Lieberman , Jake Bennert GEP-3171: Percentage-Based Request Mirroring Percentage-based request mirroring is an enhancement to the existing support for HTTP request mirroring , which allows HTTP requests to be duplicated to another backend using the RequestMirror filter type. Request mirroring is particularly useful in blue-green deployment. It can be used to assess the impact of request scaling on application performance without impacting responses to clients. The previous mirroring capability worked on all the requests to a backendRef. Percentage-based request mirroring allows users to specify a subset of requests they want to be mirrored, either by percentage or fraction.</description></item><item><title>Leveling Up Policy Enforcement in Kubernetes: A Look at Kyverno 1.14 and CEL</title><link>https://kubermates.org/docs/2025-05-29-leveling-up-policy-enforcement-in-kubernetes-a-look-at-kyverno-1-14-and-cel/</link><pubDate>Thu, 29 May 2025 17:37:06 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-29-leveling-up-policy-enforcement-in-kubernetes-a-look-at-kyverno-1-14-and-cel/</guid><description>Leveling Up Policy Enforcement in Kubernetes: A Look at Kyverno 1.14 and CEL What is Kyverno? Evolving Challenges and the Rise of CEL Introducing New Policy Types in Kyverno 1.14 Validating Policy Image Validating Policy Future Directions Kyverno’s Strengths Recently, Cloud Native Live featured a session diving into the powerful integration of Common Expression Language (CEL) in Kyverno 1.14. Hosted by Eric Durmishi and presented by Kyverno maintainers Mariam Fahmy and Charles Edward, the session introduced exciting new capabilities that enhance Kyverno’s flexibility and consistency as a policy engine for Kubernetes. At its core, Kyverno is a policy engine for Kubernetes. It enables you to define policies for validating, mutating, generating, or verifying images for resources within your cluster. When an API request, like creating a Pod, is sent to the Kubernetes API server, it passes through several stages, including authentication and authorization. Kyverno integrates as an admission webhook. The API request is forwarded to the Kyverno server, which fetches relevant policies, applies them to the resource, and then either accepts or rejects the request based on whether the resource violates the policy. Kyverno also includes components such as the Reports Controller, which generates policy reports to visualize policy results, and a Cleanup Controller, which manages unused resources based on cleanup policies. Traditionally, Kyverno used a single policy type with different rule types nested within it, such as validate, mutate, generate, and verifyImages. Policies could define rules to match or exclude specific resources. For validation, you could use patterns, deny rules, Pod Security subrules, or assertion trees. If a resource violated a policy, Kyverno’s action was determined by the failureAction field, which could be set to enforce (reject the request) or audit (allow the request, generating a warning and a policy report).</description></item><item><title>No More kubectl Commands — Just Talk to Your Kubernetes Cluster in Natural Language</title><link>https://kubermates.org/docs/2025-05-28-no-more-kubectl-commands-just-talk-to-your-kubernetes-cluster-in-natural-languag/</link><pubDate>Wed, 28 May 2025 17:52:01 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-28-no-more-kubectl-commands-just-talk-to-your-kubernetes-cluster-in-natural-languag/</guid><description>Getting Started Install kubectl-ai in Seconds Example 1: Get the Pod Count — Just Ask Example 2: Launch an Nginx Pod in Seconds Example 3: Ask for Nginx Pod Details—No Commands Needed Why This Changes the Game Key Takeaway Exploring System Architecture for DevOps Engineers Why KubeCon India 2025 Meant More to KodeKloud Linux: List Disks Linux: &amp;ldquo;cat&amp;rdquo; Command Linux Made Easy for DevOps Beginners From CFP to Stage: Win Your Tech Talk Slot MCP Explained Simply: How AI Can Actually Do Things Now Still Not Job-Ready After Learning DevOps? What Is Kubernetes? Finally, a Simple Explanation! …and your terminal figured out the rest? &amp;raquo;&amp;gt; how&amp;rsquo;s my nginx app doing? Thanks to kubectl-ai , Kubernetes can now understand natural language. You type what you want in plain English, and it responds by executing real, context-aware kubectl commands — without needing to memorize flags, write YAML, or guess namespaces. kubectl-ai kubectl Let’s explore how this works in action. Before you can chat with your cluster, make sure you have the basics in place: kubectl must be installed and configured for your environment. If you can already run kubectl get pods , you’re good to go. kubectl get pods The fastest way (Linux &amp;amp; macOS): Skip the manual steps and install in one line: curl -sSL https://raw. githubusercontent. com/GoogleCloudPlatform/kubectl-ai/main/install. sh | bash curl -sSL https://raw. githubusercontent. com/GoogleCloudPlatform/kubectl-ai/main/install. sh | bash You’ll be ready to talk to your cluster in under a minute.</description></item><item><title>Why we need a unified approach to Kubernetes environments</title><link>https://kubermates.org/docs/2025-05-26-why-we-need-a-unified-approach-to-kubernetes-environments/</link><pubDate>Mon, 26 May 2025 16:46:48 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-26-why-we-need-a-unified-approach-to-kubernetes-environments/</guid><description>Challenges Managing Multiple Technologies Deploying Holistic Solutions to Drive Better Outcomes Meeting the Demands of Modern Application Architectures Today, organizations struggle managing disparate technologies for their Kubernetes networking and network security needs. Leveraging multiple technologies for networking and security for in-cluster, ingress, egress, and traffic across clusters creates challenges, including operational complexities and increased costs. For example, to manage ingress traffic for Kubernetes clusters, users cobble together multiple solutions from different providers such as ingress controllers or gateways and load balancers for routing traffic, as well as Web Application Firewalls (WAFs) for enhanced security. Despite the challenges it brings, deploying disparate technologies has been a “necessary evil” for organizations to get all the capabilities needed for holistic Kubernetes networking. Here, we’ll explore challenges this proliferation of tooling introduces, and provide actionable tips for today’s platform and security teams to overcome these issues. The fragmented approach to networking and network security in Kubernetes leads to challenges and inefficiencies, including: Operational overhead: Each technology comes with its own learning curve, setup, configuration, integration, and maintenance requirements. This leads to a challenging user experience. Increased costs: Licensing and operational costs accumulate as more tools are deployed. Scaling challenges: As clusters grow or spread across diverse environments, ensuring consistent and secure networking becomes harder. Security gaps: Disjointed solutions impair visibility and may create security gaps. Troubleshooting issues: Without a single pane of glass, troubleshooting and understanding latency issues across clusters is a common problem operators face. Take managing ingress traffic, and everything that goes with it.</description></item><item><title>What’s New in Calico: Spring 2025</title><link>https://kubermates.org/docs/2025-05-21-what-s-new-in-calico-spring-2025/</link><pubDate>Wed, 21 May 2025 21:52:07 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-21-what-s-new-in-calico-spring-2025/</guid><description>Introducing Calico Cloud Free Tier Why Calico Cloud Free Tier? How Calico Cloud Free Tier Enhances Calico Open Source Improved Accuracy and Encryption for Commercial Editions Improved accuracy of Calico policy reporting in flow logs WireGuard: Provide support for encryption between federated endpoints and services in different clusters Summary Calico provides a unified platform for all your Kubernetes networking, network security, and observability requirements. From ingress/egress management and east-west policy enforcement to multi-cluster connectivity, Calico delivers comprehensive capabilities. It is distribution-agnostic, preventing vendor lock-in and offering a consistent experience across popular Kubernetes distributions and managed services. Calico eliminates silos, providing seamless networking and observability for containers, VMs, and bare metal servers, and extends effortlessly to multi-cluster environments, in the cloud, on-premises, and at the edge. With the recent release of Calico Open Source 3.30 , we added: Improved observability to visualize and troubleshoot workload communication with Calico Whisker and the Goldmane API. Kubernetes Network Policies are critical for preventing ransomware, achieving microsegmentation to isolate sensitive assets for compliance, and thwarting attacks from malicious actors. However, implementing them effectively can be challenging due to the complexity of identifying, testing, and rapidly updating policies to meet evolving threats. Calico Open Source 3.30 introduces staged policies to enable teams to audit and validate policies before they are enforced, reducing the risk of misconfigured policies and improving security and compliance. The ability to manage Kubernetes ingress traffic with Calico Ingress Gateway, a 100% upstream, enterprise-ready implementation of the Kubernetes Gateway API. And to expand on our robust Calico Open Source 3.30 offering, we’re excited to introduce Calico Cloud Free Tier. This new product edition further expands our open source offerings by providing enhanced observability and policy management capabilities to help visualize and troubleshoot workload communication, and simplify network security enforcement and microsegmentation. Already using Calico Open Source 3.30 or higher? Get started in less than five minutes.</description></item><item><title>Agentic Cloud: Reinventing the Cloud with AI Agents</title><link>https://kubermates.org/docs/2025-05-19-agentic-cloud-reinventing-the-cloud-with-ai-agents/</link><pubDate>Mon, 19 May 2025 20:22:04 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-19-agentic-cloud-reinventing-the-cloud-with-ai-agents/</guid><description>Agentic Cloud: Reinventing the Cloud with AI Agents What are AI agents? AI agents will transform businesses Agentic Cloud - AI agents transforming the cloud About the author Try DigitalOcean for free Related Articles Introducing langchain-gradient: Seamless LangChain Integration with DigitalOcean Gradientâ¢ AI Platform How to optimize your cloud architecture for business growth Choosing the Right DigitalOcean Offering for Your AI/ML Workload By Bratin Saha, Chief Product &amp;amp; Technology Officer Published: May 19, 2025 5 min read AI is shaping up to be one of the most transformational technologies of our lifetimes and will change how we live and work just like the PC, the internet, and the smartphone did. The launch of ChatGPT was a seminal moment in the evolution of AI, and since then, AIâs adoption has accelerated. According to â The rapid adoption of Generative AI. Bick, Blandin, &amp;amp; Deming (2024 )â, AI is being adopted at twice the rate of the PC and internet at a similar point in their evolution. Not just adoption, the rate of innovation in AI has been faster than anything we have seen before. So where is AI headed? To answer this question, let us start by looking at how other transformational technologies have evolved throughout history. Take the Internet for example â it started with the infrastructure - with routers and network switches. But the true inflection point came with the emergence of platform capabilities, such as web browsers and HTML, which allowed people to build and use applications like search engines, e-commerce, and social media. If you look at the PC, microprocessors and memory and other infrastructure components came first. But what triggered the massive adoption was the creation of a platform â WinTel, which provided a standardized environment for developing and running PC applications. Same with the smartphones; the infrastructure started first with modems and cell towers, but it really took off when a platform enabled the creation and use of applications. AI will follow a similar IPA ( I nfrastructure -&amp;gt; P latform -&amp;gt; A pplication) model, with the innovation moving from Infrastructure (e.</description></item><item><title>Kubernetes v1.33: In-Place Pod Resize Graduated to Beta</title><link>https://kubermates.org/docs/2025-05-16-kubernetes-v1-33-in-place-pod-resize-graduated-to-beta/</link><pubDate>Fri, 16 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-16-kubernetes-v1-33-in-place-pod-resize-graduated-to-beta/</guid><description>Kubernetes v1.33: In-Place Pod Resize Graduated to Beta What is in-place Pod resize? Why does in-place Pod resize matter? What&amp;rsquo;s changed between Alpha and Beta? Notable user-facing changes Stability and reliability enhancements What&amp;rsquo;s next? Getting started and providing feedback On behalf of the Kubernetes project, I am excited to announce that the in-place Pod resize feature (also known as In-Place Pod Vertical Scaling), first introduced as alpha in Kubernetes v1.27, has graduated to Beta and will be enabled by default in the Kubernetes v1.33 release! This marks a significant milestone in making resource management for Kubernetes workloads more flexible and less disruptive. Traditionally, changing the CPU or memory resources allocated to a container required restarting the Pod. While acceptable for many stateless applications, this could be disruptive for stateful services, batch jobs, or any workloads sensitive to restarts. In-place Pod resizing allows you to change the CPU and memory requests and limits assigned to containers within a running Pod, often without requiring a container restart. Here&amp;rsquo;s the core idea: The spec. containers[&lt;em&gt;]. resources field in a Pod specification now represents the desired resources and is mutable for CPU and memory. spec. containers[&lt;/em&gt;]. resources The status. containerStatuses[*]. resources field reflects the actual resources currently configured on a running container.</description></item><item><title>Announcing etcd v3.6.0</title><link>https://kubermates.org/docs/2025-05-15-announcing-etcd-v3-6-0/</link><pubDate>Thu, 15 May 2025 16:00:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-15-announcing-etcd-v3-6-0/</guid><description>Announcing etcd v3.6.0 Security Features Migration to v3store Downgrade Feature gates livez / readyz checks v3discovery Performance Memory Throughput Breaking changes Peer endpoints no longer serve client requests Clear boundary between etcdctl and etcdutl Critical bug fixes Upgrade issue Testing Platforms Dependencies Dependency bumping guide Core Dependency Updates grpc-gateway@v2 grpc-ecosystem/go-grpc-middleware/providers/prometheus Community etcd Becomes a Kubernetes SIG New contributors, maintainers, and reviewers Introducing the etcd Operator Working Group Future Development This announcement originally appeared on the etcd blog. Today, we are releasing etcd v3.6.0 , the first minor release since etcd v3.5.0 on June 15, 2021. This release introduces several new features, makes significant progress on long-standing efforts like downgrade support and migration to v3store, and addresses numerous critical &amp;amp; major issues. It also includes major optimizations in memory usage, improving efficiency and performance. In addition to the features of v3.6.0, etcd has joined Kubernetes as a SIG (sig-etcd), enabling us to improve project sustainability. We&amp;rsquo;ve introduced systematic robustness testing to ensure correctness and reliability. Through the etcd-operator Working Group, we plan to improve usability as well. What follows are the most significant changes introduced in etcd v3.6.0, along with the discussion of the roadmap for future development. For a detailed list of changes, please refer to the CHANGELOG-3.6. A heartfelt thank you to all the contributors who made this release possible! etcd takes security seriously. To enhance software security in v3.6.0, we have improved our workflow checks by integrating govulncheck to scan the source code and trivy to scan container images. These improvements have also been backported to supported stable releases.</description></item><item><title>Kubernetes 1.33: Job's SuccessPolicy Goes GA</title><link>https://kubermates.org/docs/2025-05-15-kubernetes-1-33-job-s-successpolicy-goes-ga/</link><pubDate>Thu, 15 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-15-kubernetes-1-33-job-s-successpolicy-goes-ga/</guid><description>Kubernetes 1.33: Job&amp;rsquo;s SuccessPolicy Goes GA About Job&amp;rsquo;s Success Policy How it works Learn more Get involved On behalf of the Kubernetes project, I&amp;rsquo;m pleased to announce that Job success policy has graduated to General Availability (GA) as part of the v1.33 release. In batch workloads, you might want to use leader-follower patterns like MPI , in which the leader controls the execution, including the followers&amp;rsquo; lifecycle. In this case, you might want to mark it as succeeded even if some of the indexes failed. Unfortunately, a leader-follower Kubernetes Job that didn&amp;rsquo;t use a success policy, in most cases, would have to require all Pods to finish successfully for that Job to reach an overall succeeded state. For Kubernetes Jobs, the API allows you to specify the early exit criteria using the. spec. successPolicy field (you can only use the. spec. successPolicy field for an indexed Job ). Which describes a set of rules either using a list of succeeded indexes for a job, or defining a minimal required size of succeeded indexes. spec. successPolicy.</description></item><item><title>Kubernetes v1.33: Updates to Container Lifecycle</title><link>https://kubermates.org/docs/2025-05-14-kubernetes-v1-33-updates-to-container-lifecycle/</link><pubDate>Wed, 14 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-14-kubernetes-v1-33-updates-to-container-lifecycle/</guid><description>Kubernetes v1.33: Updates to Container Lifecycle Zero value for Sleep action Container stop signals Default behaviour Version skew Using container stop signals How do I get involved? Kubernetes v1.33 introduces a few updates to the lifecycle of containers. The Sleep action for container lifecycle hooks now supports a zero sleep duration (feature enabled by default). There is also alpha support for customizing the stop signal sent to containers when they are being terminated. This blog post goes into the details of these new aspects of the container lifecycle, and how you can use them. Kubernetes v1.29 introduced the Sleep action for container PreStop and PostStart Lifecycle hooks. The Sleep action lets your containers pause for a specified duration after the container is started or before it is terminated. This was needed to provide a straightforward way to manage graceful shutdowns. Before the Sleep action, folks used to run the sleep command using the exec action in their container lifecycle hooks. If you wanted to do this you&amp;rsquo;d need to have the binary for the sleep command in your container image. This is difficult if you&amp;rsquo;re using third party images. Sleep sleep sleep The sleep action when it was added initially didn&amp;rsquo;t have support for a sleep duration of zero seconds. The time.</description></item><item><title>Automate Policy Violation Tracking with Jira in Nirmata Control Hub</title><link>https://kubermates.org/docs/2025-05-14-automate-policy-violation-tracking-with-jira-in-nirmata-control-hub/</link><pubDate>Wed, 14 May 2025 10:08:17 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-14-automate-policy-violation-tracking-with-jira-in-nirmata-control-hub/</guid><description>Automate Policy Violation Tracking with Jira in Nirmata Control Hub Why This Matters How It Works Manual Ticket Creation Where It Matters Automate What Should Be Automated The Real Value: Moving from Visibility to Action Try It Out Today Keeping track of Kubernetes policy violations across namespaces and clusters can be tedious, especially when teams rely on disconnected tools and manual workflows. With Nirmata Control Hub’s Jira integration, you can automatically or manually create Jira tickets for policy violations directly from the platform. This ensures accountability, reduces MTTR (mean time to resolution), and improves team productivity by embedding policy compliance into your teams’ tools. Every misconfiguration or policy violation is a potential security or compliance risk. However, identifying issues is only the first step. The real value is in closing the loop—from detection to resolution. Traditionally, teams rely on Slack alerts, dashboards, or Excel exports to track violations. But these quickly fall out of sync with what’s happening in the cluster or Git. Hira is where real work happens, and that’s where Nirmata Control Hub (NCH) now meets your team. From any Policy Reports page, users can instantly generate a Jira issue for violations. Simply click “Create Jira Issue” and fill in the ticket details, such as project, title, issue type, priority, and assignee. This creates traceability right at the source of the ossie, whether it;s a misconfigured deployment, a missing securityContext, or a policy deviation.</description></item><item><title>Why Policy as Code is a Game Changer for Platform Engineers</title><link>https://kubermates.org/docs/2025-05-13-why-policy-as-code-is-a-game-changer-for-platform-engineers/</link><pubDate>Tue, 13 May 2025 20:07:21 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-13-why-policy-as-code-is-a-game-changer-for-platform-engineers/</guid><description>Why Policy as Code is a Game Changer for Platform Engineers Empower Developers with Faster Feedback Enable Secure Self-Service Achieve Unprecedented Transparency Boost Efficiency and Scalability Ensure Consistent and Standardized Environments Automate Governance and Security Fortify Your Security Posture Streamline Compliance Effortlessly The Future of Platforms is Policy as Code Explore More About Platform Engineering Trends and Best Practices Platform engineers, let’s talk about a fundamental shift that’s revolutionizing how we build and manage internal developer platforms: Policy as Code (PaC). This isn’t just another buzzword; it’s the key to creating scalable, secure, and efficient operations that empower developers. Let’s dive in! Platform engineers, let’s talk about a fundamental shift that’s revolutionizing how we build and manage internal developer platforms: Policy as Code (PaC). This isn’t just another buzzword; it’s the key to creating scalable, secure, and efficient operations that empower developers. Let’s dive in! Policy as Code drastically improves developer experience. By implementing policy checks early in the development process (“shifting left”), developers get instant feedback on potential issues. This prevents problems from reaching production and reduces mental load, allowing them to focus on innovation, not compliance headaches. Platform engineering is about giving developers self-service capabilities. Infrastructure as Code (IaC) provides automation, but Policy as Code (PaC) provides the critical guardrails. Developers can provision resources and deploy applications within defined “golden paths,” ensuring agility without compromising security or compliance. Policies written as code are stored in version control, creating a clear history of changes. This auditability is essential for troubleshooting and demonstrating compliance.</description></item><item><title>Kubernetes v1.33: Job's Backoff Limit Per Index Goes GA</title><link>https://kubermates.org/docs/2025-05-13-kubernetes-v1-33-job-s-backoff-limit-per-index-goes-ga/</link><pubDate>Tue, 13 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-13-kubernetes-v1-33-job-s-backoff-limit-per-index-goes-ga/</guid><description>Kubernetes v1.33: Job&amp;rsquo;s Backoff Limit Per Index Goes GA About backoff limit per index How backoff limit per index works Example Learn more Get involved In Kubernetes v1.33, the Backoff Limit Per Index feature reaches general availability (GA). This blog describes the Backoff Limit Per Index feature and its benefits. When you run workloads on Kubernetes, you must consider scenarios where Pod failures can affect the completion of your workloads. Ideally, your workload should tolerate transient failures and continue running. To achieve failure tolerance in a Kubernetes Job, you can set the spec. backoffLimit field. This field specifies the total number of tolerated failures. spec. backoffLimit However, for workloads where every index is considered independent, like embarassingly parallel workloads - the spec. backoffLimit field is often not flexible enough. For example, you may choose to run multiple suites of integration tests by representing each suite as an index within an Indexed Job. In that setup, a fast-failing index (test suite) is likely to consume your entire budget for tolerating Pod failures, and you might not be able to run the other indexes.</description></item><item><title>Kubernetes v1.33: Image Pull Policy the way you always thought it worked!</title><link>https://kubermates.org/docs/2025-05-12-kubernetes-v1-33-image-pull-policy-the-way-you-always-thought-it-worked/</link><pubDate>Mon, 12 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-12-kubernetes-v1-33-image-pull-policy-the-way-you-always-thought-it-worked/</guid><description>Kubernetes v1.33: Image Pull Policy the way you always thought it worked! Image Pull Policy the way you always thought it worked! IfNotPresent, even if I&amp;rsquo;m not supposed to have it IfNotPresent, but only if I am supposed to have it Never pull, but use if authorized Always pull, if authorized How it all works Try it out What&amp;rsquo;s next? How to get involved Some things in Kubernetes are surprising, and the way imagePullPolicy behaves might be one of them. Given Kubernetes is all about running pods, it may be peculiar to learn that there has been a caveat to restricting pod access to authenticated images for over 10 years in the form of issue 18787 ! It is an exciting release when you can resolve a ten-year-old issue. imagePullPolicy The gist of the problem is that the imagePullPolicy: IfNotPresent strategy has done precisely what it says, and nothing more. Let&amp;rsquo;s set up a scenario. To begin, Pod A in Namespace X is scheduled to Node 1 and requires image Foo from a private repository. For it&amp;rsquo;s image pull authentication material, the pod references Secret 1 in its imagePullSecrets. Secret 1 contains the necessary credentials to pull from the private repository. The Kubelet will utilize the credentials from Secret 1 as supplied by Pod A and it will pull container image Foo from the registry. This is the intended (and secure) behavior. imagePullPolicy: IfNotPresent imagePullSecrets But now things get curious. If Pod B in Namespace Y happens to also be scheduled to Node 1 , unexpected (and potentially insecure) things happen. Pod B may reference the same private image, specifying the IfNotPresent image pull policy.</description></item><item><title>How to optimize your cloud architecture for business growth</title><link>https://kubermates.org/docs/2025-05-09-how-to-optimize-your-cloud-architecture-for-business-growth/</link><pubDate>Fri, 09 May 2025 23:25:48 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-09-how-to-optimize-your-cloud-architecture-for-business-growth/</guid><description>How to optimize your cloud architecture for business growth How cloud architecture reviews can benefit you Tips to boost cloud performance and security Tips to save money and grow efficiently How DigitalOcean experts help: A real-world example Optimize your cloud to empower your business About the author Try DigitalOcean for free Related Articles Introducing langchain-gradient: Seamless LangChain Integration with DigitalOcean Gradientâ¢ AI Platform Agentic Cloud: Reinventing the Cloud with AI Agents Choosing the Right DigitalOcean Offering for Your AI/ML Workload By Anantha Ramachandran Principal Product Marketing Manager Published: May 9, 2025 5 min read Your cloud setup is like a ship on the ocean. To ensure a safe journey, it needs regular maintenance and care. As your business grows and your tech stack becomes more complex, itâs easy for small inefficiencies to creep into your cloud environment which can drive up costs, waste resources, and make it harder to effectively scale. DigitalOcean is proud to offer hands-on support to our customers to help them optimize their cloud setup, which includes free architecture reviews with expert solutions engineers, and tips to avoid inefficiencies in your cloud setup. If youâre a DigitalOcean customer, you can request your own architecture review now. To explore actionable insights on your own, watch our recent Sail to Success webinar, where we shared practical tips on improving your cloud architecture, including ways to boost performance, enhance security, and save on costs. Regular cloud architecture reviews are beneficial to businesses of all sizes. Here are a few reasons we recommend examining your cloud setup regularly: They support business growth: As your business evolves, your cloud infrastructure must evolve with it. A cloud architecture review helps you align your current environment with your business goals, helping to ensure your cloud is positioned to support where you are today, where youâre heading, and the outcomes you need to achieve. They support business growth: As your business evolves, your cloud infrastructure must evolve with it. A cloud architecture review helps you align your current environment with your business goals, helping to ensure your cloud is positioned to support where you are today, where youâre heading, and the outcomes you need to achieve. They improve performance and security: Regular reviews can find hidden problems that can slow you down or make you less secure.</description></item><item><title>Kubernetes v1.33: Streaming List responses</title><link>https://kubermates.org/docs/2025-05-09-kubernetes-v1-33-streaming-list-responses/</link><pubDate>Fri, 09 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-09-kubernetes-v1-33-streaming-list-responses/</guid><description>Kubernetes v1.33: Streaming List responses The problem: unnecessary memory consumption with large resources Streaming encoder for List responses Performance gains you&amp;rsquo;ll notice Benchmark results Managing Kubernetes cluster stability becomes increasingly critical as your infrastructure grows. One of the most challenging aspects of operating large-scale clusters has been handling List requests that fetch substantial datasets - a common operation that could unexpectedly impact your cluster&amp;rsquo;s stability. Today, the Kubernetes community is excited to announce a significant architectural improvement: streaming encoding for List responses. Current API response encoders just serialize an entire response into a single contiguous memory and perform one ResponseWriter. Write call to transmit data to the client. Despite HTTP/2&amp;rsquo;s capability to split responses into smaller frames for transmission, the underlying HTTP server continues to hold the complete response data as a single buffer. Even as individual frames are transmitted to the client, the memory associated with these frames cannot be freed incrementally. When cluster size grows, the single response body can be substantial - like hundreds of megabytes in size. At large scale, the current approach becomes particularly inefficient, as it prevents incremental memory release during transmission. Imagining that when network congestion occurs, that large response body’s memory block stays active for tens of seconds or even minutes. This limitation leads to unnecessarily high and prolonged memory consumption in the kube-apiserver process. If multiple large List requests occur simultaneously, the cumulative memory consumption can escalate rapidly, potentially leading to an Out-of-Memory (OOM) situation that compromises cluster stability.</description></item><item><title>Kubernetes 1.33: Volume Populators Graduate to GA</title><link>https://kubermates.org/docs/2025-05-08-kubernetes-1-33-volume-populators-graduate-to-ga/</link><pubDate>Thu, 08 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-08-kubernetes-1-33-volume-populators-graduate-to-ga/</guid><description>Kubernetes 1.33: Volume Populators Graduate to GA What is new Populator Pod is optional Mutator functions to modify the Kubernetes resources Flexible metric handling for providers Clean up for temporary resources How to use it Future directions and potential feature requests Kubernetes volume populators are now generally available (GA)! The AnyVolumeDataSource feature gate is treated as always enabled for Kubernetes v1.33, which means that users can specify any appropriate custom resource as the data source of a PersistentVolumeClaim (PVC). AnyVolumeDataSource An example of how to use dataSourceRef in PVC: apiVersion : v1 kind : PersistentVolumeClaim metadata : name : pvc1 spec :. dataSourceRef : apiGroup : provider. example. com kind : Provider name : provider1 apiVersion : v1 kind : PersistentVolumeClaim metadata : name : pvc1 spec :. dataSourceRef : apiGroup : provider. example. com kind : Provider name : provider1 There are four major enhancements from beta. During the beta phase, contributors to Kubernetes identified potential resource leaks with PersistentVolumeClaim (PVC) deletion while volume population was in progress; these leaks happened due to limitations in finalizer handling. Ahead of the graduation to general availability, the Kubernetes project added support to delete temporary resources (PVC prime, etc. ) if the original PVC is deleted. To accommodate this, we&amp;rsquo;ve introduced three new plugin-based functions: PopulateFn() : Executes the provider-specific data population logic.</description></item><item><title>Expanding our GPU Droplet portfolio - NVIDIA RTX 4000 Ada Generation, NVIDIA RTX 6000 Ada Generation, and NVIDIA L40S</title><link>https://kubermates.org/docs/2025-05-08-expanding-our-gpu-droplet-portfolio-nvidia-rtx-4000-ada-generation-nvidia-rtx-60/</link><pubDate>Thu, 08 May 2025 08:05:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-08-expanding-our-gpu-droplet-portfolio-nvidia-rtx-4000-ada-generation-nvidia-rtx-60/</guid><description>Expanding our GPU Droplet portfolio - NVIDIA RTX 4000 Ada Generation, NVIDIA RTX 6000 Ada Generation, and NVIDIA L40S About the author Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 By Waverly Swinton Published: May 8, 2025 2 min read GPU Droplets are now DigitalOcean GradientAI GPU Droplets. Learn more about DigitalOcean GradientAI , our suite of AI products. At DigitalOcean, weâre committed to bringing you the latest technology to power your AI projects. Weâre excited to announce that NVIDIA RTX 4000 Ada Generation, NVIDIA RTX 6000 Ada Generation, and NVIDIA L40S GPUs are now available as DigitalOcean GPU Droplets. NVIDIAâs RTX 4000 Ada Generation, 6000 Ada Generation, and L40S deliver on AI inference, training, and graphical workloads. These GPU Droplets are available in budget-friendly, single-node configurations. Read on for more on how DigitalOcean and NVIDIA can help make your AI journey simple, scalable, and cost-effective. What are NVIDIA RTX 4000 Ada Generation, 6000 Ada Generation and L40S? These Nvidia GPU Droplets combine powerful AI compute with best-in-class graphics and media acceleration and deliver end-to-end acceleration for the next generation of AI-enabled applicationsâfrom generative AI, large-language models (LLM) inference, small-model training and fine-tuning to 3D graphics, rendering, and video applicationsâ¦ Key features of these newly introduced machines include: NVIDIA RTX 4000 Ada Generation GPU is a powerful single-slot GPU. Key use cases include content creation, 3D modeling, rendering, video, and inference workflows with exceptional performance and efficiency. NVIDIA RTX 4000 Ada Generation GPU is a powerful single-slot GPU. Key use cases include content creation, 3D modeling, rendering, video, and inference workflows with exceptional performance and efficiency. NVIDIA RTX 6000 Ada Generation GPU is built on the NVIDIA Ada Lovelace GPU architecture.</description></item><item><title>Powered by DigitalOcean Hatch: Ontra Mobility is Building Smarter Cities</title><link>https://kubermates.org/docs/2025-05-07-powered-by-digitalocean-hatch-ontra-mobility-is-building-smarter-cities/</link><pubDate>Wed, 07 May 2025 21:30:29 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-07-powered-by-digitalocean-hatch-ontra-mobility-is-building-smarter-cities/</guid><description>Powered by DigitalOcean Hatch: Ontra Mobility is Building Smarter Cities Solving real-world problems comes with real-world challenges Simplifying building with DigitalOcean and Hatch Full speed ahead About the author Try DigitalOcean for free Related Articles Stop Building SaaS from Scratch: Meet the SeaNotes Starter Kit Powered by DigitalOcean Hatch: Why Uxifyâs Founders Always Choose DigitalOcean Powered by DigitalOcean Hatch: How Ex-human uses GPU Droplets to Build Empathetic AI that Serves Customers By Martin Nguyen Published: May 7, 2025 4 min read Hatch is DigitalOceanâs global program for startups, which provides selected growing technology companies with credits and discounts on computing resources so they can build and scale with less worry about costs. By keeping infrastructure and cost management simple, the Hatch program helps enable startups to build solutions that move society forward. Ontra Mobility , a Y Combinator startup, transforms communities by developing data-driven tools that help cities and transit agencies plan and operate more efficient, equitable, and sustainable transportation systems, focusing on optimizing networks and improving first-and-last mile access through on-demand ridesharing. We sat down with CTO Connor Riley, who chose DigitalOcean for its simple, managed infrastructure so they can focus on making public transit more accessible. The Hatch program provided credits and resources that helped support their mission. During their PhD programs at Georgia Institute of Technology, Riley and his co-founder Anthony Trasatti worked with their academic advisor, Pascal Van Hentenryck, to optimize public transit, which serves as a vital lifeline for many. Working closely with MARTA in Atlanta, GA, they developed techniques for network design optimization, special events scheduling, and dispatching algorithms for on-demand ridesharing. These projects powered on-demand ridesharing pilots MARTA Reach and CAT SMART in Savannah, Georgia. Getting adoption for a product that serves public agencies is as much of a battle as building the product itself. Since many residents rely on public transportation, establishing trust with transit agencies is essential before theyâll risk adopting new products that could impact municipal residents. Despite these hurdles, theyâre undeterred: theyâve already gained traction with a number of customers, including conducting a microtransit planning project in Washington D. C.</description></item><item><title>Kubernetes v1.33: From Secrets to Service Accounts: Kubernetes Image Pulls Evolved</title><link>https://kubermates.org/docs/2025-05-07-kubernetes-v1-33-from-secrets-to-service-accounts-kubernetes-image-pulls-evolved/</link><pubDate>Wed, 07 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-07-kubernetes-v1-33-from-secrets-to-service-accounts-kubernetes-image-pulls-evolved/</guid><description>Kubernetes v1.33: From Secrets to Service Accounts: Kubernetes Image Pulls Evolved The problem with image pull secrets The solution: Service Account token integration for Kubelet credential providers How it works 1. Service Account tokens for credential providers 2. Image registry authentication flow Benefits of this approach What&amp;rsquo;s next? Try it out How to get involved Kubernetes has steadily evolved to reduce reliance on long-lived credentials stored in the API. A prime example of this shift is the transition of Kubernetes Service Account (KSA) tokens from long-lived, static tokens to ephemeral, automatically rotated tokens with OpenID Connect (OIDC)-compliant semantics. This advancement enables workloads to securely authenticate with external services without needing persistent secrets. However, one major gap remains: image pull authentication. Today, Kubernetes clusters rely on image pull secrets stored in the API, which are long-lived and difficult to rotate, or on node-level kubelet credential providers, which allow any pod running on a node to access the same credentials. This presents security and operational challenges. To address this, Kubernetes is introducing Service Account Token Integration for Kubelet Credential Providers , now available in alpha. This enhancement allows credential providers to use pod-specific service account tokens to obtain registry credentials, which kubelet can then use for image pulls — eliminating the need for long-lived image pull secrets. Currently, Kubernetes administrators have two primary options for handling private container image pulls: Image pull secrets stored in the Kubernetes API These secrets are often long-lived because they are hard to rotate. They must be explicitly attached to a service account or pod.</description></item><item><title>Do You Need a Degree to Be a DevOps Engineer? A 2025 Guide</title><link>https://kubermates.org/docs/2025-05-07-do-you-need-a-degree-to-be-a-devops-engineer-a-2025-guide/</link><pubDate>Wed, 07 May 2025 17:41:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-07-do-you-need-a-degree-to-be-a-devops-engineer-a-2025-guide/</guid><description>The Myth vs. The Reality What Do Employers Really Want? Traditional Degree Pathway The No-Degree Pathway Here’s how you can do the same: Popular Certifications That Replace (or Supplement) a Degree Certs = Proof you know your stuff. The Unspoken DevOps Skills You MUST Have Real Talk: Pros &amp;amp; Cons of Having a Degree Hiring Trends in 2025 and Beyond Pro Tip: Build a DevOps Portfolio Conclusion: Degree or No Degree? DevOps doesn’t care where you came from. It cares what you can deliver. Exploring System Architecture for DevOps Engineers Why KubeCon India 2025 Meant More to KodeKloud Linux: List Disks Linux: &amp;ldquo;cat&amp;rdquo; Command Linux Made Easy for DevOps Beginners From CFP to Stage: Win Your Tech Talk Slot MCP Explained Simply: How AI Can Actually Do Things Now Still Not Job-Ready After Learning DevOps? What Is Kubernetes? Finally, a Simple Explanation! That was the old rulebook. In today’s cloud-powered, open-source-fueled, community-driven tech world… the rules have changed. 👉 The truth? A degree is nice to have, but it’s NOT a mandatory ticket to join the DevOps revolution. 👉 The better question: “Do you have the skills and mindset of a DevOps engineer?” Top companies (even tech giants) now hire for skills, not just degrees. They want someone who can: Solve infrastructure and deployment problems Automate repetitive tasks Improve the development pipeline Collaborate with cross-functional teams In short: They want a problem-solver. A degree shows you can learn. Hands-on skills show you can do. The second matters way more in DevOps.</description></item><item><title>Introducing Role-Based Access Control to DigitalOcean Managed MongoDB with Predefined Roles</title><link>https://kubermates.org/docs/2025-05-07-introducing-role-based-access-control-to-digitalocean-managed-mongodb-with-prede/</link><pubDate>Wed, 07 May 2025 16:46:32 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-07-introducing-role-based-access-control-to-digitalocean-managed-mongodb-with-prede/</guid><description>Introducing Role-Based Access Control to DigitalOcean Managed MongoDB with Predefined Roles Benefits of RBAC for DigitalOcean Managed MongoDB users Best practices for implementing predefined roles/RBAC in your organization Get Started About the author Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 By Nicole Ghalwash Published: May 7, 2025 2 min read We are excited to announce that role-based access control (RBAC) is now available for DigitalOcean Managed MongoDB, starting with predefined roles! These new roles include the pre-defined roles of â readOnly â, â readWrite â and â dbAdmin â. Learn more about it here. This update brings greater security, efficiency, and compliance to your MongoDB clusters. With predefined roles, you can now easily manage access control, reduce security risks, and streamline database administration across your MongoDB clusters on DigitalOcean. Managing database access can be a challenge, especially as teams grow and security requirements become more complex. Without a structured approach, organizations risk unauthorized access, operational inefficiencies, and compliance gaps. With RBAC now applicable to your MongoDB environment, you can enforce clear, predefined access policiesâhelping to ensure secure, efficient, and scalable database management. Hereâs how RBAC can benefit your business: Enhanced security: RBAC protects against unauthorized access by only allowing verified users to interact with sensitive database resources. This reduces the risk of data breaches and strengthens overall security posture. Enhanced security: RBAC protects against unauthorized access by only allowing verified users to interact with sensitive database resources. This reduces the risk of data breaches and strengthens overall security posture. Operational efficiency: With predefined roles, administrators can streamline user provisioning and de-provisioning, minimizing the manual workload and reducing errors.</description></item><item><title>Kubernetes v1.33: Fine-grained SupplementalGroups Control Graduates to Beta</title><link>https://kubermates.org/docs/2025-05-06-kubernetes-v1-33-fine-grained-supplementalgroups-control-graduates-to-beta/</link><pubDate>Tue, 06 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-06-kubernetes-v1-33-fine-grained-supplementalgroups-control-graduates-to-beta/</guid><description>Kubernetes v1.33: Fine-grained SupplementalGroups Control Graduates to Beta Motivation: Implicit group memberships defined in /etc/group in the container image What&amp;rsquo;s wrong with it? Fine-grained supplemental groups control in a Pod: supplementaryGroupsPolicy Attached process identity in Pod status Strict Policy requires newer CRI versions The behavioral changes introduced in beta Upgrade consideration Getting involved How can I learn more? The new field, supplementalGroupsPolicy , was introduced as an opt-in alpha feature for Kubernetes v1.31 and has graduated to beta in v1.33; the corresponding feature gate ( SupplementalGroupsPolicy ) is now enabled by default. This feature enables to implement more precise control over supplemental groups in containers that can strengthen the security posture, particularly in accessing volumes. Moreover, it also enhances the transparency of UID/GID details in containers, offering improved security oversight. supplementalGroupsPolicy SupplementalGroupsPolicy Please be aware that this beta release contains some behavioral breaking change. See The Behavioral Changes Introduced In Beta and Upgrade Considerations sections for details. /etc/group Although the majority of Kubernetes cluster admins/users may not be aware, kubernetes, by default, merges group information from the Pod with information defined in /etc/group in the container image. /etc/group Let&amp;rsquo;s see an example, below Pod manifest specifies runAsUser=1000 , runAsGroup=3000 and supplementalGroups=4000 in the Pod&amp;rsquo;s security context. runAsUser=1000 runAsGroup=3000 supplementalGroups=4000 apiVersion : v1 kind : Pod metadata : name : implicit-groups spec : securityContext : runAsUser : 1000 runAsGroup : 3000 supplementalGroups : [ 4000 ] containers : - name : ctr image : registry. k8s. io/e2e-test-images/agnhost:2.45 command : [ &amp;ldquo;sh&amp;rdquo; , &amp;ldquo;-c&amp;rdquo; , &amp;ldquo;sleep 1h&amp;rdquo; ] securityContext : allowPrivilegeEscalation : false apiVersion : v1 kind : Pod metadata : name : implicit-groups spec : securityContext : runAsUser : 1000 runAsGroup : 3000 supplementalGroups : [ 4000 ] containers : - name : ctr image : registry. k8s. io/e2e-test-images/agnhost:2.45 command : [ &amp;ldquo;sh&amp;rdquo; , &amp;ldquo;-c&amp;rdquo; , &amp;ldquo;sleep 1h&amp;rdquo; ] securityContext : allowPrivilegeEscalation : false What is the result of id command in the ctr container? The output should be similar to this: id ctr uid=1000 gid=3000 groups=3000,4000,50000 uid=1000 gid=3000 groups=3000,4000,50000 Where does group ID 50000 in supplementary groups ( groups field) come from, even though 50000 is not defined in the Pod&amp;rsquo;s manifest at all? The answer is /etc/group file in the container image.</description></item><item><title>Nirmata’s AI-Powered Remediations: A Smarter Way to Fix Policy Violations</title><link>https://kubermates.org/docs/2025-05-06-nirmata-s-ai-powered-remediations-a-smarter-way-to-fix-policy-violations/</link><pubDate>Tue, 06 May 2025 18:25:24 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-06-nirmata-s-ai-powered-remediations-a-smarter-way-to-fix-policy-violations/</guid><description>Nirmata’s AI-Powered Remediations: A Smarter Way to Fix Policy Violations Why AI Remediations for Policy as Code? How It Works Designed for Developer Velocity and Platform Efficiency Where to Find It What’s Next Every modern enterprise strives for faster software delivery without compromising on security and compliance. As cloud-native environments grow in scale and complexity, so does the burden of identifying and fixing misconfigurations across clusters, pipelines, and cloud infrastructure. Today, we’re excited to announce a major leap forward in our mission to simplify cloud governance: AI-Powered Remediations , which is now available in preview in Nirmata Control Hub (NCH). Security and platform teams often face a growing backlog of policy violations—from missing labels to insecure container configurations to overly permissive network rules. Manually investigating each violation, understanding its root cause, and crafting a compliant fix takes time—and that time adds up. With AI Remediations, we’re dramatically reducing the Mean Time to Remediation (MTTR ). Instead of spending hours chasing down documentation or writing YAML from scratch, teams can now review and apply context-aware fixes in minutes. And the benefits go beyond speed: Dev teams are no longer blocked by vague policy errors. Instead, they receive concrete, explainable suggestions to fix issues early. Platform engineers can spend less time triaging violations and more time building scalable, reliable infrastructure. Security teams have peace of mind knowing issues aren’t just detected—they’re getting resolved faster than ever. Whether it’s a Deployment, ConfigMap, NetworkPolicy, or any Kubernetes resource, if it violates a rule enforced by your Kyverno policies in NCH, we can generate a fix.</description></item><item><title>Mainframes Are the New AI Infrastructure. Protect it with Secure AI</title><link>https://kubermates.org/docs/2025-05-06-mainframes-are-the-new-ai-infrastructure-protect-it-with-secure-ai/</link><pubDate>Tue, 06 May 2025 10:13:27 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-06-mainframes-are-the-new-ai-infrastructure-protect-it-with-secure-ai/</guid><description>Mainframes Are the New AI Infrastructure. Protect it with Secure AI Mainframes: AI Infrastructure for Next Generation Applications AI and Containerization on the Mainframe Introducing Aqua Secure AI Bringing Aqua Security to IBM Z and IBM LinuxONE Modernize Without Compromise If your AI workloads run in containers, then securing those containers is the first and most important step in protecting your AI. And as enterprises begin to deploy containerized AI workloads on Red Hat OpenShift for mainframe environments, that priority becomes even more urgent. IBM Z and IBM LinuxONE, long trusted to power the world’s most critical business systems are now evolving into innovation hubs, supporting advanced, containerized applications. With this transformation comes a new challenge: securing the infrastructure behind your most sensitive and high-stakes workloads The rise of generative AI and large language models (LLMs) has changed how organizations build and deliver value. From real-time fraud detection to intelligent customer support, AI is becoming embedded in nearly every business function. According to a recent McKinsey study , “ 78 percent of respondents say their organizations use AI in at least one business function, up from 72 percent in early 2024. ” These AI workloads are built and deployed in containers. Why? Containers offer portability, scalability, and efficiency, which makes them ideal for AI training, inference, and everything in between. IDC projects that 1 billion new logical applications will be created by 2028, resulting in more than 10 billion container instances across enterprise environments. These billions of containers won’t just run in a general purpose cloud. They will be deployed on purpose built hardware.</description></item><item><title>Kubernetes v1.33: Prevent PersistentVolume Leaks When Deleting out of Order graduates to GA</title><link>https://kubermates.org/docs/2025-05-05-kubernetes-v1-33-prevent-persistentvolume-leaks-when-deleting-out-of-order-gradu/</link><pubDate>Mon, 05 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-05-kubernetes-v1-33-prevent-persistentvolume-leaks-when-deleting-out-of-order-gradu/</guid><description>Kubernetes v1.33: Prevent PersistentVolume Leaks When Deleting out of Order graduates to GA How did reclaim work in previous Kubernetes releases? PV reclaim policy with Kubernetes v1.33 How does it work? Important note How to enable new behavior? References How do I get involved? I am thrilled to announce that the feature to prevent PersistentVolume (or PVs for short) leaks when deleting out of order has graduated to General Availability (GA) in Kubernetes v1.33! This improvement, initially introduced as a beta feature in Kubernetes v1.31, ensures that your storage resources are properly reclaimed, preventing unwanted leaks. PersistentVolumeClaim (or PVC for short) is a user&amp;rsquo;s request for storage. A PV and PVC are considered Bound if a newly created PV or a matching PV is found. The PVs themselves are backed by volumes allocated by the storage backend. Normally, if the volume is to be deleted, then the expectation is to delete the PVC for a bound PV-PVC pair. However, there are no restrictions on deleting a PV before deleting a PVC. For a Bound PV-PVC pair, the ordering of PV-PVC deletion determines whether the PV reclaim policy is honored. The reclaim policy is honored if the PVC is deleted first; however, if the PV is deleted prior to deleting the PVC, then the reclaim policy is not exercised. As a result of this behavior, the associated storage asset in the external infrastructure is not removed. Bound With the graduation to GA in Kubernetes v1.33, this issue is now resolved. Kubernetes now reliably honors the configured Delete reclaim policy, even when PVs are deleted before their bound PVCs. This is achieved through the use of finalizers, ensuring that the storage backend releases the allocated storage resource as intended.</description></item><item><title>Nirmata Enterprise for Kyverno (N4K) Now Available on AWS Marketplace!</title><link>https://kubermates.org/docs/2025-05-05-nirmata-enterprise-for-kyverno-n4k-now-available-on-aws-marketplace/</link><pubDate>Mon, 05 May 2025 17:55:11 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-05-nirmata-enterprise-for-kyverno-n4k-now-available-on-aws-marketplace/</guid><description>Nirmata Enterprise for Kyverno (N4K) Now Available on AWS Marketplace! What is Nirmata Enterprise for Kyverno (N4K)? Key Features of Nirmata Enterprise for Kyverno Why Choose Nirmata Enterprise for Kyverno? Why AWS Marketplace? Flexible Payment Options Get Started Today We are thrilled to announce that Nirmata Enterprise for Kyverno (N4K) is now available on AWS Marketplace ! This enterprise-grade distribution of Kyverno brings the power of Kubernetes policy management with enhanced features, security, and support to meet the needs of the most demanding production environments. Nirmata Enterprise for Kyverno (N4K) is the enterprise-grade distribution of the widely-used Kyverno policy engine. N4K offers the same powerful policy enforcement capabilities as Kyverno but with key enhancements that cater to large-scale, security-conscious organizations. This includes zero CVEs , 24×7 support with SLA , and priority fixes and features to ensure your Kubernetes environments are always secure, compliant, and running at peak performance. Zero CVEs : N4K provides a CVE-free experience , ensuring that your Kubernetes clusters remain secure with a distribution that is continuously updated and maintained. Our enterprise offering is fully vetted for vulnerabilities, so you can trust that your workloads are protected. 24×7 Support with SLA : With N4K, you get the peace of mind that comes with round-the-clock, enterprise-grade support. Our SLA ensures fast response times and access to expert assistance whenever you need it, helping your team stay productive and your environment secure. Priority Fixes and Features : As an N4K customer, you gain access to priority bug fixes , security patches , and new features. Your business will always be the first to benefit from updates that improve performance, security, and usability. Scalability at Enterprise Scale : Built to support large enterprises, N4K offers high-performance policy enforcement , enabling organizations to govern hundreds or even thousands of Kubernetes clusters efficiently while maintaining consistency across their entire environment. NCTL – CI Pipeline Integration : With NCTL , users can integrate security scanning directly into their CI pipelines.</description></item><item><title>Kubernetes 1.33: Top 5 Features of “Octarine</title><link>https://kubermates.org/docs/2025-05-05-kubernetes-1-33-top-5-features-of-octarine/</link><pubDate>Mon, 05 May 2025 17:20:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-05-kubernetes-1-33-top-5-features-of-octarine/</guid><description>A Closer Look at the Release Stats What You’ll Learn in This Post 1. Sidecar Containers Graduate to Stable The Problem (Before) What’s New in Kubernetes 1.33 Why It Matters 2. In-Place Pod Vertical Scaling (Beta) The Problem (Before) What’s New in Kubernetes 1.33 How to Use It Why It Matters 3. OCI Artifact &amp;amp; Image Volumes (Beta) The Problem (Before) What’s New in Kubernetes 1.33 The Benefits Why It Matters 4. User Namespaces for Pods (Security Beta) The Problem (Before) What’s New in Kubernetes 1.33 How It Works &amp;amp; Usage Why It Matters 5. kubectl. kuberc Configuration (Alpha) The Problem (Before) What’s New in Kubernetes 1.33 How It Works &amp;amp; Usage Why It Matters Wrapping Up: Kubernetes 1.33 Is Ready for You Bonus: Try Kubernetes 1.33 Right Now at KodeKloud Goodbye… Until Next Time! Exploring System Architecture for DevOps Engineers Why KubeCon India 2025 Meant More to KodeKloud Linux: List Disks Linux: &amp;ldquo;cat&amp;rdquo; Command Linux Made Easy for DevOps Beginners From CFP to Stage: Win Your Tech Talk Slot MCP Explained Simply: How AI Can Actually Do Things Now Still Not Job-Ready After Learning DevOps? What Is Kubernetes? Finally, a Simple Explanation! Code-named “Octarine” , a nod to the mythical “color of magic” from Terry Pratchett’s Discworld novels, this release continues Kubernetes’ steady momentum of delivering a scalable, secure, and developer-friendly container orchestration platform. According to the official Kubernetes announcement , Kubernetes 1.33 is all about pushing boundaries while making life easier for everyone working with Kubernetes in production environments. Whether you’re running massive enterprise workloads or experimenting in a dev cluster, this version has something for you. Kubernetes 1.33 includes a total of 64 enhancements , breaking down as follows: 18 Stable (GA) features 20 Beta features 24 Alpha features This broad set of updates signals ongoing investments across the project’s key pillars: performance, scalability, security, extensibility, and usability. With such a packed release, it’s easy to get lost in the full changelog. That’s why we’ve focused this overview on the five most important and popular features that developers and operators are most excited about.</description></item><item><title>Sharks of DigitalOcean: Jason Dobry, Staff IT Project Specialist</title><link>https://kubermates.org/docs/2025-05-05-sharks-of-digitalocean-jason-dobry-staff-it-project-specialist/</link><pubDate>Mon, 05 May 2025 13:05:52 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-05-sharks-of-digitalocean-jason-dobry-staff-it-project-specialist/</guid><description>Sharks of DigitalOcean: Jason Dobry, Staff IT Project Specialist What makes DigitalOcean stand out as a workplace? What excites you most about DigitalOcean? Can you share a moment when you felt especially proud to work at DO? Which of DOâs values resonates most with you? Dive into the future with DigitalOcean About the author Try DigitalOcean for free Related Articles Sharks of DigitalOcean: Darian Wilkin, Senior Manager, Solutions Engineering Sharks of DigitalOcean: Laura Schaffer, VP, Growth Sharks of DigitalOcean: Ali Munir, Staff Technical Account Manager By Sujatha R Technical Writer Published: May 5, 2025 3 min read For Jason Dobry, joining DigitalOcean was about finding genuine belonging, not just accepting a new position. Today, as a Staff IT Project Specialist, Jason coordinates IT project work across global support teams, helping admins and agents across APAC (Asia-Pacific) and EMEA (Europe, Middle East, and Africa) stay aligned and effective. He leads the Atlassian modernization team and collaborates closely with the IT support teams and system admins. Whether itâs optimizing internal dashboards to track team performance, coordinating with vendors on new initiatives, or uncovering trends in operational data, Jason acts as a multi-tool within the IT organization. Heâs passionate about creating opportunities for his colleagues to grow, learn, and find joy in their work. Read on to discover Jasonâs journey, his passion for purpose-driven work, and how heâs now shaping the very culture he once sought out at DO. When I joined DigitalOcean, I was looking for more than just a jobâI wanted a place that felt like home. DO gave me that from day one. The way the company supported me, especially outside of work, allowed me to show up as my best self every day. Through big transitions like the pandemic, they always put my family first. That support helped me stay grounded and, in turn, empowered me to help others do their best work for our customers. ð¥ Have a look at Jason Dobryâs full conversation â¬ï¸ Our willingness to experiment.</description></item><item><title>Day 7: Your Kubernetes Learning Roadmap — What’s Next After the Basics?</title><link>https://kubermates.org/docs/2025-05-03-day-7-your-kubernetes-learning-roadmap-what-s-next-after-the-basics/</link><pubDate>Sat, 03 May 2025 13:23:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-03-day-7-your-kubernetes-learning-roadmap-what-s-next-after-the-basics/</guid><description>Congratulations! The Kubernetes Learning Roadmap 1️⃣ Beginner Stage: Solidify the Core 2️⃣ Advanced Beginner Stage: Start Real-World Projects 3️⃣ Intermediate Stage: Kubernetes in Production Bonus: Printable Kubernetes Learning Checklist Final Words of Encouragement Where to Go From Here Exploring System Architecture for DevOps Engineers Why KubeCon India 2025 Meant More to KodeKloud Linux: List Disks Linux: &amp;ldquo;cat&amp;rdquo; Command Linux Made Easy for DevOps Beginners From CFP to Stage: Win Your Tech Talk Slot MCP Explained Simply: How AI Can Actually Do Things Now Still Not Job-Ready After Learning DevOps? What Is Kubernetes? Finally, a Simple Explanation! But Kubernetes is big. It’s normal to now think: This post will give you the answer. Here’s a simple path we recommend: What to learn next: Understand Namespaces ( kubectl get ns ) kubectl get ns Practice creating Pods, Deployments, Services Play with ConfigMaps &amp;amp; Secrets Understand CrashLoopBackOff, Pending Pod errors Tools: 👉 KodeKloud Kubernetes Free Labs 👉 Kubernetes Playground Goal: Be confident to deploy small apps on Kubernetes. What to explore: Volumes &amp;amp; Persistent Storage Kubernetes Networking basics Rolling Updates &amp;amp; Rollbacks Horizontal Pod Autoscaling Tools: 👉 KodeKloud Kubernetes Learning Path Goal: Run small microservices apps in Kubernetes with persistence &amp;amp; scaling. What to master: Ingress Controllers Network Policies Pod Security &amp;amp; RBAC Monitoring with Prometheus + Grafana Troubleshooting broken apps in Kubernetes Helm charts for deploying apps Certification Tip: Start preparing for CKA (Certified Kubernetes Administrator). Tools: 👉 KodeKloud CKA Course 👉 Official Kubernetes Docs Goal: Be job-ready for DevOps &amp;amp; Cloud Native roles. We recommend keeping this simple checklist as your personal tracker : [ ] I understand Containers &amp;amp; Docker [ ] I know how to use kubectl [ ] I can create and manage Pods [ ] I know how Deployments &amp;amp; ReplicaSets work [ ] I can expose apps using Services [ ] I can manage ConfigMaps &amp;amp; Secrets [ ] I understand Clusters, Nodes, and the Control Plane [ ] I ’ve tried Kubernetes labs or playgrounds [ ] I’ve deployed a real-world app in Kubernetes [ ] I ’m exploring intermediate concepts (Volumes, Ingress, Monitoring) [ ] I [ ] I know how to [ ] I [ ] I [ ] I [ ] I [ ] I [ ] I [ ] I’ve deployed a [ ] I 👉 Pro tip: Print it or keep it in Notion/GitHub and check items off as you progress! Kubernetes is NOT something you master overnight. The key is consistent practice + real projects. Even learning these basics puts you ahead of most beginner engineers. 🔎 Keep practicing in the KodeKloud Playground 🎥 Watch Kubernetes beginner &amp;amp; intermediate videos on our KodeKloud YouTube channel 💻 Enroll in guided courses &amp;amp; labs on kodekloud. com You’ve started your Cloud Native journey — keep going. We’ll be here to help every step of the way.</description></item><item><title>Kubernetes v1.33: Mutable CSI Node Allocatable Count</title><link>https://kubermates.org/docs/2025-05-02-kubernetes-v1-33-mutable-csi-node-allocatable-count/</link><pubDate>Fri, 02 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-02-kubernetes-v1-33-mutable-csi-node-allocatable-count/</guid><description>Kubernetes v1.33: Mutable CSI Node Allocatable Count Background Dynamically adapting CSI volume limits How it works Enabling the feature Example CSI driver configuration Immediate updates on attachment failures Getting started Next steps Scheduling stateful applications reliably depends heavily on accurate information about resource availability on nodes. Kubernetes v1.33 introduces an alpha feature called mutable CSI node allocatable count , allowing Container Storage Interface (CSI) drivers to dynamically update the reported maximum number of volumes that a node can handle. This capability significantly enhances the accuracy of pod scheduling decisions and reduces scheduling failures caused by outdated volume capacity information. Traditionally, Kubernetes CSI drivers report a static maximum volume attachment limit when initializing. However, actual attachment capacities can change during a node&amp;rsquo;s lifecycle for various reasons, such as: Manual or external operations attaching/detaching volumes outside of Kubernetes control. Dynamically attached network interfaces or specialized hardware (GPUs, NICs, etc. ) consuming available slots. Multi-driver scenarios, where one CSI driver’s operations affect available capacity reported by another. Static reporting can cause Kubernetes to schedule pods onto nodes that appear to have capacity but don&amp;rsquo;t, leading to pods stuck in a ContainerCreating state. ContainerCreating With the new feature gate MutableCSINodeAllocatableCount , Kubernetes enables CSI drivers to dynamically adjust and report node attachment capacities at runtime. This ensures that the scheduler has the most accurate, up-to-date view of node capacity. MutableCSINodeAllocatableCount When this feature is enabled, Kubernetes supports two mechanisms for updating the reported node volume limits: Periodic Updates: CSI drivers specify an interval to periodically refresh the node&amp;rsquo;s allocatable capacity.</description></item><item><title>Kubernetes v1.33: New features in DRA</title><link>https://kubermates.org/docs/2025-05-01-kubernetes-v1-33-new-features-in-dra/</link><pubDate>Thu, 01 May 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-05-01-kubernetes-v1-33-new-features-in-dra/</guid><description>Kubernetes v1.33: New features in DRA Features promoted to beta New alpha features Preparing for general availability What’s next? Getting involved Acknowledgments Kubernetes Dynamic Resource Allocation (DRA) was originally introduced as an alpha feature in the v1.26 release, and then went through a significant redesign for Kubernetes v1.31. The main DRA feature went to beta in v1.32, and the project hopes it will be generally available in Kubernetes v1.34. The basic feature set of DRA provides a far more powerful and flexible API for requesting devices than Device Plugin. And while DRA remains a beta feature for v1.33, the DRA team has been hard at work implementing a number of new features and UX improvements. One feature has been promoted to beta, while a number of new features have been added in alpha. The team has also made progress towards getting DRA ready for GA. Driver-owned Resource Claim Status was promoted to beta. This allows the driver to report driver-specific device status data for each allocated device in a resource claim, which is particularly useful for supporting network devices. Partitionable Devices lets a driver advertise several overlapping logical devices (“partitions”), and the driver can reconfigure the physical device dynamically based on the actual devices allocated. This makes it possible to partition devices on-demand to meet the needs of the workloads and therefore increase the utilization. Device Taints and Tolerations allow devices to be tainted and for workloads to tolerate those taints. This makes it possible for drivers or cluster administrators to mark devices as unavailable.</description></item><item><title>Day 6: ConfigMaps &amp; Secrets — Managing App Settings and Sensitive Data in Kubernetes</title><link>https://kubermates.org/docs/2025-05-01-day-6-configmaps-secrets-managing-app-settings-and-sensitive-data-in-kubernetes/</link><pubDate>Thu, 01 May 2025 18:11:50 +0000</pubDate><guid>https://kubermates.org/docs/2025-05-01-day-6-configmaps-secrets-managing-app-settings-and-sensitive-data-in-kubernetes/</guid><description>Let’s Begin With What You Might Know What’s a ConfigMap? What’s a Secret? Why Use Them? How It Works Example: Creating a ConfigMap Try It Yourself Real-World Analogy Quick Summary Coming Up. Exploring System Architecture for DevOps Engineers Why KubeCon India 2025 Meant More to KodeKloud Linux: List Disks Linux: &amp;ldquo;cat&amp;rdquo; Command Linux Made Easy for DevOps Beginners From CFP to Stage: Win Your Tech Talk Slot MCP Explained Simply: How AI Can Actually Do Things Now Still Not Job-Ready After Learning DevOps? What Is Kubernetes? Finally, a Simple Explanation! 😬 But in Kubernetes, there’s a better and safer way to do this. Enter: ConfigMaps and Secrets Environment variables App settings File paths Feature flags It keeps your app configs separate from your app code and container image — which is great for flexibility and security. Passwords API tokens TLS certificates Private keys Secrets are base64-encoded and can be managed with tighter access controls in Kubernetes. You: Create a ConfigMap or Secret Attach it to your Pod using: Environment variables Mounted volumes Environment variables Mounted volumes Your app reads the values from the injected location kubectl create configmap app-config \ &amp;ndash;from-literal=APP_ENV=production \ &amp;ndash;from-literal=FEATURE_X=true kubectl create configmap app-config \ &amp;ndash;from-literal=APP_ENV=production \ &amp;ndash;from-literal=FEATURE_X=true And a Secret: kubectl create secret generic db-secret \ &amp;ndash;from-literal=DB_USER=admin \ &amp;ndash;from-literal=DB_PASS=1234 kubectl create secret generic db-secret \ &amp;ndash;from-literal=DB_USER=admin \ &amp;ndash;from-literal=DB_PASS=1234 Then, in your Pod spec (simplified YAML): env: - name: APP_ENV valueFrom: configMapKeyRef: name: app-config key: APP_ENV - name: DB_USER valueFrom: secretKeyRef: name: db-secret key: DB_USER env: - name: APP_ENV valueFrom: configMapKeyRef: name: app-config key: APP_ENV - name: DB_USER valueFrom: secretKeyRef: name: db-secret key: DB_USER 👉 Use the KodeKloud Kubernetes Playground 1 - Create a Secret: kubectl create secret generic mysecret \ &amp;ndash;from-literal=password=mypass123 kubectl create secret generic mysecret \ &amp;ndash;from-literal=password=mypass123 2 - Create a Pod using that secret: apiVersion: v1 kind: Pod metadata: name: secret-demo spec: containers: - name: busybox image: busybox command: [&amp;ldquo;sleep&amp;rdquo;, &amp;ldquo;3600&amp;rdquo;] env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysecret key: password apiVersion: v1 kind: Pod metadata: name: secret-demo spec: containers: - name: busybox image: busybox command: [&amp;ldquo;sleep&amp;rdquo;, &amp;ldquo;3600&amp;rdquo;] env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: mysecret key: password Apply it with: kubectl apply -f pod. yaml kubectl exec -it secret-demo &amp;ndash; printenv DB_PASSWORD kubectl apply -f pod. yaml kubectl exec -it secret-demo &amp;ndash; printenv DB_PASSWORD You’ll see the password securely injected. Imagine you’re deploying an app on a shared team server. Hardcoding passwords in the app = leaving your house key in plain sight Using ConfigMaps and Secrets = storing your keys in a locked drawer with access logs ConfigMaps store general config data (non-sensitive) Secrets store sensitive info (passwords, tokens) Both can be injected into Pods via env vars or mounted files They help decouple config from code , enable reuse, and improve security 📅 Day 7: Your Kubernetes Learning Roadmap — What’s Next After the Basics? You’ll get: A printable roadmap for beginners → advanced Tips on real-world practice Recommended projects, labs, and courses to continue your journey New here? Start from Day 1 and catch up on the series: Day 1: What Is Kubernetes &amp;amp; Why Should You Care? Discover why Kubernetes matters and how it changes the game. Day 2: What Are Pods in Kubernetes? Understand the smallest deployable unit in Kubernetes. Day 3: Understanding Nodes, Clusters &amp;amp; the Kubernetes Control Plane See how all the pieces connect behind the scenes. Day 4: Deployments &amp;amp; ReplicaSets — How Kubernetes Runs and Manages Your App ⚙Learn how Kubernetes keeps your apps running smoothly.</description></item><item><title>Kubernetes v1.33: Storage Capacity Scoring of Nodes for Dynamic Provisioning (alpha)</title><link>https://kubermates.org/docs/2025-04-30-kubernetes-v1-33-storage-capacity-scoring-of-nodes-for-dynamic-provisioning-alph/</link><pubDate>Wed, 30 Apr 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-04-30-kubernetes-v1-33-storage-capacity-scoring-of-nodes-for-dynamic-provisioning-alph/</guid><description>Kubernetes v1.33: Storage Capacity Scoring of Nodes for Dynamic Provisioning (alpha) About this feature How to use Enabling the feature Configuration changes Further reading Additional note: Relationship with VolumeCapacityPriority Kubernetes v1.33 introduces a new alpha feature called StorageCapacityScoring. This feature adds a scoring method for pod scheduling with the topology-aware volume provisioning. This feature eases to schedule pods on nodes with either the most or least available storage capacity. StorageCapacityScoring This feature extends the kube-scheduler&amp;rsquo;s VolumeBinding plugin to perform scoring using node storage capacity information obtained from Storage Capacity. Currently, you can only filter out nodes with insufficient storage capacity. So, you have to use a scheduler extender to achieve storage-capacity-based pod scheduling. This feature is useful for provisioning node-local PVs, which have size limits based on the node&amp;rsquo;s storage capacity. By using this feature, you can assign the PVs to the nodes with the most available storage space so that you can expand the PVs later as much as possible. In another use case, you might want to reduce the number of nodes as much as possible for low operation costs in cloud environments by choosing the least storage capacity node. This feature helps maximize resource utilization by filling up nodes more sequentially, starting with the most utilized nodes first that still have enough storage capacity for the requested volume size. In the alpha phase, StorageCapacityScoring is disabled by default. To use this feature, add StorageCapacityScoring=true to the kube-scheduler command line option &amp;ndash;feature-gates.</description></item><item><title>AI agent development just got easier on GenAI Platform</title><link>https://kubermates.org/docs/2025-04-30-ai-agent-development-just-got-easier-on-genai-platform/</link><pubDate>Wed, 30 Apr 2025 15:30:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-30-ai-agent-development-just-got-easier-on-genai-platform/</guid><description>AI agent development just got easier on GenAI Platform Enhancing your AI agents Build smarter, more transparent AI agents About the author Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 By Grace Morgan Published: April 30, 2025 2 min read DigitalOceanâs GenAI Platform is now DigitalOcean Gradient Platform. Learn more about the GA release and features. As AI agents become increasingly embedded in business operations, user experiences, and regulated workflows, the need for greater transparency, control, and performance visibility is more important than ever. Thatâs why weâre introducing a set of major updates to GenAI Platform, now in public preview, including enhanced agent development features and improved data sourcing through knowledge base citations. These new capabilities make it easier than ever to build, deploy, and refine your AI agents. Weâre rolling out powerful new features that give teams more control, transparency, and efficiency when working with AI agents. Knowledge base citations for better transparency Understanding the source of AI-generated responses is critical for trust and reliability. With the new knowledge base citations feature , AI agents can now provide clear source attribution for retrieved information, helping users trace responses back to their original context. Source attribution displays file names, URLs, PDF pages, and table references. Improved auditability enables verification for compliance and transparency. Enhanced trust reinforces confidence in AI-generated responses. This feature is especially valuable for business intelligence applications and educational platforms, where accurate and verifiable information is paramount.</description></item><item><title>Kubernetes v1.33: Image Volumes graduate to beta!</title><link>https://kubermates.org/docs/2025-04-29-kubernetes-v1-33-image-volumes-graduate-to-beta/</link><pubDate>Tue, 29 Apr 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-04-29-kubernetes-v1-33-image-volumes-graduate-to-beta/</guid><description>Kubernetes v1.33: Image Volumes graduate to beta! What&amp;rsquo;s new Further reading Image Volumes were introduced as an Alpha feature with the Kubernetes v1.31 release as part of KEP-4639. In Kubernetes v1.33, this feature graduates to beta. Please note that the feature is still disabled by default, because not all container runtimes have full support for it. CRI-O supports the initial feature since version v1.31 and will add support for Image Volumes as beta in v1.33. containerd merged support for the alpha feature which will be part of the v2.1.0 release and is working on beta support as part of PR #11578. The major change for the beta graduation of Image Volumes is the support for subPath and subPathExpr mounts for containers via spec. containers[*]. volumeMounts. [subPath,subPathExpr]. This allows end-users to mount a certain subdirectory of an image volume, which is still mounted as readonly ( noexec ). This means that non-existing subdirectories cannot be mounted by default. As for other subPath and subPathExpr values, Kubernetes will ensure that there are no absolute path or relative path components part of the specified sub path.</description></item><item><title>Day 5: Kubernetes Services — How Your App Gets a Stable IP or URL</title><link>https://kubermates.org/docs/2025-04-29-day-5-kubernetes-services-how-your-app-gets-a-stable-ip-or-url/</link><pubDate>Tue, 29 Apr 2025 17:26:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-29-day-5-kubernetes-services-how-your-app-gets-a-stable-ip-or-url/</guid><description>Let’s Start With What You Might Know What Is a Kubernetes Service? How It Works Example Types of Services (Simplified) Real-World Analogy Try It Out Quick Summary Coming Up. Exploring System Architecture for DevOps Engineers Why KubeCon India 2025 Meant More to KodeKloud Linux: List Disks Linux: &amp;ldquo;cat&amp;rdquo; Command Linux Made Easy for DevOps Beginners From CFP to Stage: Win Your Tech Talk Slot MCP Explained Simply: How AI Can Actually Do Things Now Still Not Job-Ready After Learning DevOps? What Is Kubernetes? Finally, a Simple Explanation! But here’s the next problem: You try running: kubectl get pods -o wide kubectl get pods -o wide And you see something like: NAME READY STATUS IP NODE myapp-xyz 1/1 Running 10.244.1.5 worker-node-1 NAME READY STATUS IP NODE myapp-xyz 1/1 Running 10.244.1.5 worker-node-1 Great — but: That Pod IP is internal It changes if the Pod dies and restarts And if you have 3 replicas… which IP do you even hit? That’s where Services come in. You can think of it as: A permanent IP address or DNS name inside the cluster A load balancer that routes traffic to the right Pods A gateway between your app and the outside world (if needed) You attach a Service to a set of Pods using labels The Service tracks the matching Pods — even as Pods come and go Kubernetes uses something called kube-proxy to route traffic to the correct Pod behind the scenes So even if Pods restart, the Service endpoint never changes. Let’s say you run 3 Pods of a web app with the label: app: myapp app: myapp You create a Service like this: selector: app: myapp selector: app: myapp Now, when someone accesses the Service: Kubernetes forwards the request to any of the healthy Pods It’s automatic, balanced, and reliable Imagine your app’s Pods are like food trucks moving around a festival ground. A Service is like the signpost that says: You don’t care where the trucks are parked — you just follow the sign. Kubernetes routes you to the right place, even if trucks come and go. Use the Kubernetes Playground: 👉 KodeKloud Kubernetes Playground Create a deployment: kubectl create deployment myapp &amp;ndash;image=nginx kubectl expose deployment myapp &amp;ndash;port=80 &amp;ndash;type=NodePort kubectl get service kubectl create deployment myapp &amp;ndash;image=nginx kubectl expose deployment myapp &amp;ndash;port=80 &amp;ndash;type=NodePort kubectl get service Now run: kubectl get pods -o wide kubectl describe service myapp kubectl get pods -o wide kubectl describe service myapp Look for the NodePort (e. g. , 30123 ) and try opening: 30123 http://&lt;worker-node-ip&gt;:30123 http://&lt;worker-node-ip&gt;:30123 You’re now accessing your app through a Kubernetes Service! Pods have dynamic IPs and can restart anytime A Service gives your app a stable endpoint Services load-balance traffic to healthy Pods You can expose your app internally or externally with different service types 📅 Day 6: ConfigMaps &amp;amp; Secrets — Managing App Settings and Sensitive Data You’ll learn: Why you should never hardcode passwords or configs in containers How Kubernetes separates config data and secrets How to use them in real applications New here? Start from Day 1 and catch up on the series: Day 1: What Is Kubernetes &amp;amp; Why Should You Care? Discover why Kubernetes matters and how it changes the game. Day 2: What Are Pods in Kubernetes? Understand the smallest deployable unit in Kubernetes. Day 3: Understanding Nodes, Clusters &amp;amp; the Kubernetes Control Plane See how all the pieces connect behind the scenes. Day 4: Deployments &amp;amp; ReplicaSets — How Kubernetes Runs and Manages Your App Learn how Kubernetes keeps your apps running smoothly.</description></item><item><title>Shadow Roles: AWS Defaults Can Open the Door to Service Takeover</title><link>https://kubermates.org/docs/2025-04-29-shadow-roles-aws-defaults-can-open-the-door-to-service-takeover/</link><pubDate>Tue, 29 Apr 2025 16:16:07 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-29-shadow-roles-aws-defaults-can-open-the-door-to-service-takeover/</guid><description>Our research uncovered security concerns in the deployment of resources within a few AWS services, specifically in the default AWS service roles. These roles, often created automatically or recommended during setup, grant overly broad permissions, such as full S3 access. These default roles silently introduce attack paths that allow privilege escalation, cross-service access, and even potential account compromise. Security Threats (120) Container Security (118) Kubernetes Security (97) Cloud Native Security (84) Aqua Open Source (49) Image Vulnerability Scanning (49) AWS Security (38) Runtime Security (38) Aqua Security (37) Vulnerability Management (36) Docker Security (35) Software Supply Chain Security (29) CSPM (28) Cloud compliance (25) DevSecOps (25) Container Vulnerability (24) CI/CD (17) CNAPP (17) Supply Chain Attacks (13) Secrets (12) Application Security (11) Serverless-Security (11) Kubernetes (10) ebpf (10) Cloud security (9) Host Security (9) Advanced malware protection (8) Cloud security conferences (8) Fargate (8) Hybrid Cloud Security (8) Malware Attacks (8) Cloud Workload Protection Platform CWPP (7) Attack Vector (6) Container platforms (6) Google cloud security (6) OpenShift (6) SBOMs (6) Secure VM (6) Security Policy (6) Infrastructure-as-Code (IaC) (5) Security Automation (5) Windows Containers (5) Azure security (4) Docker containers (4) Kubernetes RBAC (4) Service Mesh (4) Container Deployment (3) IBM Cloud (3) Microservices (3) Nano-Segmentation (3) Agentless Security (2) FaaS (2) Network Firewall (2) VMware Tanzu (2) code security (2) Advanced Threat Mitigation (1) Cloud VM (1) Customer Support (1) Drift Prevention (1) Kubernetes Authorization (1) Network (1) shift Left security (1) Aqua Security is the largest pure-play cloud native security company, providing customers the freedom to innovate and accelerate their digital transformations. The Aqua Platform is the leading Cloud Native Application Protection Platform (CNAPP) and provides prevention, detection, and response automation across the entire application lifecycle to secure the supply chain, secure cloud infrastructure and secure running workloads wherever they are deployed. Aqua customers are among the world’s largest enterprises in financial services, software, media, manufacturing and retail, with implementations across a broad range of cloud providers and modern technology stacks spanning containers, serverless functions and cloud VMs. Automate DevSecOps Modernize Security Compliance and Auditing Serverless Containers &amp;amp; Functions Hybrid and Multi Cloud Kubernetes Security OpenShift Security Docker Security AWS Cloud Security Azure Cloud Security Google Cloud Security VMware PKS Security Contact Us Contact Support Aqua Cloud native security Open Source Container Security Platform Integrations Live Webinars O’Reilly Book: Kubernetes Security Cloud native Wiki About Aqua Newsroom Careers.</description></item><item><title>Kubernetes v1.33: HorizontalPodAutoscaler Configurable Tolerance</title><link>https://kubermates.org/docs/2025-04-28-kubernetes-v1-33-horizontalpodautoscaler-configurable-tolerance/</link><pubDate>Mon, 28 Apr 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-04-28-kubernetes-v1-33-horizontalpodautoscaler-configurable-tolerance/</guid><description>Kubernetes v1.33: HorizontalPodAutoscaler Configurable Tolerance What is it? How do I use it? I want all the details! This post describes configurable tolerance for horizontal Pod autoscaling , a new alpha feature first available in Kubernetes 1.33. Horizontal Pod Autoscaling is a well-known Kubernetes feature that allows your workload to automatically resize by adding or removing replicas based on resource utilization. Let&amp;rsquo;s say you have a web application running in a Kubernetes cluster with 50 replicas. You configure the HorizontalPodAutoscaler (HPA) to scale based on CPU utilization, with a target of 75% utilization. Now, imagine that the current CPU utilization across all replicas is 90%, which is higher than the desired 75%. The HPA will calculate the required number of replicas using the formula: In this example: So, the HPA will increase the number of replicas from 50 to 60 to reduce the load on each pod. Similarly, if the CPU utilization were to drop below 75%, the HPA would scale down the number of replicas accordingly. The Kubernetes documentation provides a detailed description of the scaling algorithm. In order to avoid replicas being created or deleted whenever a small metric fluctuation occurs, Kubernetes applies a form of hysteresis: it only changes the number of replicas when the current and desired metric values differ by more than 10%. In the example above, since the ratio between the current and desired metric values is (90/75), or 20% above target, exceeding the 10% tolerance, the scale-up action will proceed. This default tolerance of 10% is cluster-wide; in older Kubernetes releases, it could not be fine-tuned. It&amp;rsquo;s a suitable value for most usage, but too coarse for large deployments, where a 10% tolerance represents tens of pods.</description></item><item><title>Kubernetes v1.33: User Namespaces enabled by default!</title><link>https://kubermates.org/docs/2025-04-25-kubernetes-v1-33-user-namespaces-enabled-by-default/</link><pubDate>Fri, 25 Apr 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-04-25-kubernetes-v1-33-user-namespaces-enabled-by-default/</guid><description>Kubernetes v1.33: User Namespaces enabled by default! What is a user namespace? Demos Everything you wanted to know about user namespaces in Kubernetes Conclusions How do I get involved? In Kubernetes v1.33 support for user namespaces is enabled by default. This means that, when the stack requirements are met, pods can opt-in to use user namespaces. To use the feature there is no need to enable any Kubernetes feature flag anymore! In this blog post we answer some common questions about user namespaces. But, before we dive into that, let&amp;rsquo;s recap what user namespaces are and why they are important. Note: Linux user namespaces are a different concept from Kubernetes namespaces. The former is a Linux kernel feature; the latter is a Kubernetes feature. Linux provides different namespaces to isolate processes from each other. For example, a typical Kubernetes pod runs within a network namespace to isolate the network identity and a PID namespace to isolate the processes. One Linux namespace that was left behind is the user namespace. It isolates the UIDs and GIDs of the containers from the ones on the host. The identifiers in a container can be mapped to identifiers on the host in a way where host and container(s) never end up in overlapping UID/GIDs. Furthermore, the identifiers can be mapped to unprivileged, non-overlapping UIDs and GIDs on the host.</description></item><item><title>Introducing DigitalOcean Managed Caching for Valkey, The New Evolution of Managed Caching</title><link>https://kubermates.org/docs/2025-04-24-introducing-digitalocean-managed-caching-for-valkey-the-new-evolution-of-managed/</link><pubDate>Thu, 24 Apr 2025 20:16:49 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-24-introducing-digitalocean-managed-caching-for-valkey-the-new-evolution-of-managed/</guid><description>Introducing DigitalOcean Managed Caching for Valkey, The New Evolution of Managed Caching Learn more about our new database offering: DigitalOcean Managed Caching for Valkey Benefits and features of Managed Caching for Valkey Where is Managed Caching going? Simple and predictable pricing Get started with DigitalOcean Managed Caching for Valkey About the author Try DigitalOcean for free Related Articles Stop Building SaaS from Scratch: Meet the SeaNotes Starter Kit Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Introducing langchain-gradient: Seamless LangChain Integration with DigitalOcean Gradientâ¢ AI Platform By Nicole Ghalwash Published: April 24, 2025 4 min read Today, weâre excited to announce the launch of DigitalOceanâs Managed Caching for Valkey, our new Managed Database service that seamlessly replaces Managed Caching (previously Managed RedisÂ®). Managed Caching for Valkey builds on the capabilities youâve come to rely on while also offering enhanced tools to support your development needs. DigitalOcean Managed Caching for Valkey is a fully managed, high-performance, in-memory key-value datastore designed for caching, message queues, and primary database use. Fully compatible with Valkey 8.0 and Redis 7.2. 4Â® as a database, it serves as a drop-in replacement for our Managed Caching database service while offering enhanced functionality for fast and efficient data storage. Built for developers and growing businesses, Managed Caching for Valkey helps reduce database load, improve response times, and optimize resource usageâdelivering an easy-to-use, cost-effective alternative to self-managed caching solutions. As for our current customers with Managed Caching environments, Managed Caching will have an end-of-availability (EOA) date of April 30, meaning you cannot create new clusters after this date, and April 29th is when new Redis creates will be disabled. However, existing clusters will remain operational, but we encourage you to explore our product documentation to learn how to convert your Caching workloads to Valkey, step-by-step. DigitalOcean Managed Caching for Valkey offers a wealth of benefits to the modern-day developer. Amongst them all, these are the ones we think will be of the most value to you: A RedisÂ®-compatible database: Valkey is a one-in replacement for Managed Caching, our previous RedisÂ® Managed Database service. You will be able to seamlessly transfer your caching clusters to Valkey with our extensive product documentation. A RedisÂ®-compatible database: Valkey is a one-in replacement for Managed Caching, our previous RedisÂ® Managed Database service.</description></item><item><title>Kubernetes v1.33: Continuing the transition from Endpoints to EndpointSlices</title><link>https://kubermates.org/docs/2025-04-24-kubernetes-v1-33-continuing-the-transition-from-endpoints-to-endpointslices/</link><pubDate>Thu, 24 Apr 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-04-24-kubernetes-v1-33-continuing-the-transition-from-endpoints-to-endpointslices/</guid><description>Kubernetes v1.33: Continuing the transition from Endpoints to EndpointSlices Notes on migrating from Endpoints to EndpointSlices Consuming EndpointSlices rather than Endpoints Generating EndpointSlices rather than Endpoints Since the addition of EndpointSlices ( KEP-752 ) as alpha in v1.15 and later GA in v1.21, the Endpoints API in Kubernetes has been gathering dust. New Service features like dual-stack networking and traffic distribution are only supported via the EndpointSlice API, so all service proxies, Gateway API implementations, and similar controllers have had to be ported from using Endpoints to using EndpointSlices. At this point, the Endpoints API is really only there to avoid breaking end user workloads and scripts that still make use of it. As of Kubernetes 1.33, the Endpoints API is now officially deprecated, and the API server will return warnings to users who read or write Endpoints resources rather than using EndpointSlices. Eventually, the plan (as documented in KEP-4974 ) is to change the Kubernetes Conformance criteria to no longer require that clusters run the Endpoints controller (which generates Endpoints objects based on Services and Pods), to avoid doing work that is unneeded in most modern-day clusters. Thus, while the Kubernetes deprecation policy means that the Endpoints type itself will probably never completely go away, users who still have workloads or scripts that use the Endpoints API should start migrating them to EndpointSlices. For end users, the biggest change between the Endpoints API and the EndpointSlice API is that while every Service with a selector has exactly 1 Endpoints object (with the same name as the Service), a Service may have any number of EndpointSlices associated with it: selector $ kubectl get endpoints myservice Warning: v1 Endpoints is deprecated in v1.33+; use discovery. k8s. io/v1 EndpointSlice NAME ENDPOINTS AGE myservice 10.180.3.17:443 1h $ kubectl get endpointslice -l kubernetes. io/service-name = myservice NAME ADDRESSTYPE PORTS ENDPOINTS AGE myservice-7vzhx IPv4 443 10.180.3.17 21s myservice-jcv8s IPv6 443 2001:db8:0123::5 21s $ kubectl get endpoints myservice Warning: v1 Endpoints is deprecated in v1.33+; use discovery. k8s. io/v1 EndpointSlice NAME ENDPOINTS AGE myservice 10.180.3.17:443 1h $ kubectl get endpointslice -l kubernetes.</description></item><item><title>DigitalOcean Customers Eligible to Process DORA Workloads</title><link>https://kubermates.org/docs/2025-04-24-digitalocean-customers-eligible-to-process-dora-workloads/</link><pubDate>Thu, 24 Apr 2025 12:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-24-digitalocean-customers-eligible-to-process-dora-workloads/</guid><description>DigitalOcean Customers Eligible to Process DORA Workloads What is DORA? DigitalOcean, DORA, and You DORA Addendums Voluntary Audit About the author Try DigitalOcean for free Related Articles Stop Building SaaS from Scratch: Meet the SeaNotes Starter Kit Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Introducing langchain-gradient: Seamless LangChain Integration with DigitalOcean Gradientâ¢ AI Platform By David Lopez Staff Trust &amp;amp; Governance Advisor, Trust and Governance Published: April 24, 2025 1 min read Today, weâre excited to announce that DigitalOcean will begin signing Digital Operational Resilience Act (DORA) Addendums with eligible customers. Additionally, we engaged Schellman &amp;amp; Company to facilitate an audit of our environment, so that customers can deploy on DigitalOcean with trust in the organizationâs risk management, security controls, operational resilience, incident management and reporting, and outsourcing and third party risk management. Effective January 17, 2025, financial entities operating in the European Union (EU) and Information and Communications Technology (ICT) third-party service providers are subject to the EUâs DORA. The regulation standardizes how financial entities report major ICT-related incidents, test their digital operational resilience, and manage ICT third-party risk across the financial services sector and EU member states. While DigitalOcean has not yet been designated a critical Information and Communications Technology (ICT) third-party service provider by European Supervisory Authorities, we recognize the need to enable select customers to address their DORA obligations relative to ICT third-party service providers, which can include; but, are not limited to, cloud service providers. Customers who require the DORA Addendum can request the document by contacting Sales. As part of our continued commitment to you, we engaged Schellman Compliance, LLC to facilitate an audit of our environment against DORAâs core regulatory framework, relevant Regulatory Technical Standards (RTS) and Implementation Technical Standards (ITS). The assessment returned zero findings. A summary letter can be viewed within our Trust Platform. Share August 21, 2025 3 min read Read more August 20, 2025 2 min read Read more Narasimha Badrinath August 19, 2025 2 min read Read more.</description></item><item><title>Recap: KubeCon + CloudNativeCon Europe 2025</title><link>https://kubermates.org/docs/2025-04-23-recap-kubecon-cloudnativecon-europe-2025/</link><pubDate>Wed, 23 Apr 2025 20:32:36 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-23-recap-kubecon-cloudnativecon-europe-2025/</guid><description>CalicoCon 2025 What’s New in Calico v3.30 Enhanced Observability with Whisker Migrating to nftables and Calico eBPF Calico APIs and Integration AMA with Calico Engineers Party with Calico Cool Cats KubeCon Impressions In Summary When I got the assignment to attend KubeCon 1st of April I thought it was an April prank, but as the date got closer I realized—this is for real and I’ll be on the ground in London at the tenth anniversary of cloud native computing. I’ve seen a lot of tech events during my years in the industry while trying not to get replaced by AI and I have to say this one stands out! Image source: CNCF YouTube Channel Here is my recap of KubeCon + CloudNativeCon Europe 2025. CalicoCon is an event that happens twice every year, as a co-located event during KubeCon NA and EU. It’s a free event that allows you to learn about Tigera’s vision for the future of networking and security in the cloud. There’s also an after-party to celebrate our community and people like you who are on this journey with us! This year our main focus was on Calico v3.30 , our upcoming release that will add a lot of anticipated features to Calico, unlocking things like observability, staged network policy, and gateway api. CalicoCon brought together cloud-native enthusiasts to explore the latest advancements in Calico and Kubernetes networking. The following is a brief summary of this year’s CalicoCon sessions. Note: The CalicoCon playlist with session recordings is now available on YouTube. Peter Kelly, VP of Engineering at Tigera, highlighted the new features in Calico v3.30. A key focus was on the new observability features, including the “Whisker” dashboard, designed to provide deeper insights into network behavior. Reza, our developer advocate at TIgera, led a session dedicated to the new observability features in Calico Open Source v3.30, centered around the Whisker dashboard. The session emphasized the importance of network observability in dynamic Kubernetes environments and included a hands-on demo.</description></item><item><title>Kubernetes v1.33: Octarine</title><link>https://kubermates.org/docs/2025-04-23-kubernetes-v1-33-octarine/</link><pubDate>Wed, 23 Apr 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-04-23-kubernetes-v1-33-octarine/</guid><description>Kubernetes v1.33: Octarine Release theme and logo Spotlight on key updates Stable: Sidecar containers Beta: In-place resource resize for vertical scaling of Pods Alpha: New configuration option for kubectl with. kuberc for user preferences Features graduating to Stable Backoff limits per index for indexed Jobs Job success policy Bound ServiceAccount token security improvements Subresource support in kubectl Multiple Service CIDRs nftables backend for kube-proxy Topology aware routing with trafficDistribution: PreferClose Options to reject non SMT-aligned workload Defining Pod affinity or anti-affinity using matchLabelKeys and mismatchLabelKeys Considering taints and tolerations when calculating Pod topology spread skew Volume populators Always honor PersistentVolume reclaim policy New features in Beta Support for Direct Service Return (DSR) in Windows kube-proxy Structured parameter support Dynamic Resource Allocation (DRA) for network interfaces Handle unscheduled pods early when scheduler does not have any pod on activeQ Asynchronous preemption in the Kubernetes Scheduler ClusterTrustBundles Fine-grained SupplementalGroups control Support for mounting images as volumes Support for user namespaces within Linux Pods Pod procMount option CPUManager policy to distribute CPUs across NUMA nodes Zero-second sleeps for container PreStop hooks Internal tooling for declarative validation of Kubernetes-native types New features in Alpha Configurable tolerance for HorizontalPodAutoscalers Configurable container restart delay Custom container stop signals DRA enhancements galore! Robust image pull policy to authenticate images for IfNotPresent and Never Node topology labels are available via downward API Better pod status with generation and observed generation Support for split level 3 cache architecture with kubelet’s CPU Manager PSI (Pressure Stall Information) metrics for scheduling improvements Secret-less image pulls with kubelet Graduations, deprecations, and removals in v1.33 Graduations to stable Deprecations and removals Release notes Availability Release Team Project velocity Event update Upcoming release webinar Get involved Editors: Agustina Barbetta, Aakanksha Bhende, Udi Hofesh, Ryota Sawada, Sneha Yadav Similar to previous releases, the release of Kubernetes v1.33 introduces new stable, beta, and alpha features. The consistent delivery of high-quality releases underscores the strength of our development cycle and the vibrant support from our community. This release consists of 64 enhancements. Of those enhancements, 18 have graduated to Stable, 20 are entering Beta, 24 have entered Alpha, and 2 are deprecated or withdrawn. There are also several notable deprecations and removals in this release; make sure to read about those if you already run an older version of Kubernetes. The theme for Kubernetes v1.33 is Octarine: The Color of Magic 1 , inspired by Terry Pratchett’s Discworld series. This release highlights the open source magic 2 that Kubernetes enables across the ecosystem. If you’re familiar with the world of Discworld, you might recognize a small swamp dragon perched atop the tower of the Unseen University, gazing up at the Kubernetes moon above the city of Ankh-Morpork with 64 stars 3 in the background. As Kubernetes moves into its second decade, we celebrate both the wizardry of its maintainers, the curiosity of new contributors, and the collaborative spirit that fuels the project. The v1.33 release is a reminder that, as Pratchett wrote, “It’s still magic even if you know how it’s done. ” Even if you know the ins and outs of the Kubernetes code base, stepping back at the end of the release cycle, you’ll realize that Kubernetes remains magical.</description></item><item><title>What’s Really Happening in Your Containers? Aqua’s Risk Assessment Has the Answer</title><link>https://kubermates.org/docs/2025-04-23-what-s-really-happening-in-your-containers-aqua-s-risk-assessment-has-the-answer/</link><pubDate>Wed, 23 Apr 2025 12:12:36 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-23-what-s-really-happening-in-your-containers-aqua-s-risk-assessment-has-the-answer/</guid><description>What’s Really Happening in Your Containers? Aqua’s Risk Assessment Has the Answer Beyond the Static: Why Runtime Context Matters Cutting Through the Noise What You’ll Gain from CSRA Built on a Decade of Threat Research A Smarter First Step Get Started Today Containers may be mainstream, but securing them in production remains a moving target. As AI adoption scales and environments grow more complex, so too do the risks, especially at runtime, where traditional tools struggle to provide meaningful visibility. These are not legacy exploits like port scans or brute force attempts. Attackers are targeting what happens inside your environment, at runtime, where misconfigurations, unexpected behaviors, and subtle anomalies can quietly introduce business risk. Yet most security teams are flying blind in production. They have tools that check for vulnerabilities before deployment, scan code for secrets, or assess infrastructure misconfigurations. What they don’t have is visibility into what’s actually happening right now, in the workloads already running in their environment. That’s the gap Aqua is closing with the launch of the Container Security Risk Assessment (CSRA). Security tooling has largely focused on pre-production controls like image scanning, CI/CD pipeline checks, and IaC validation. It is logical, and even easier than runtime. But once containers are running, things change. Configurations drift.</description></item><item><title>Kubernetes Multicontainer Pods: An Overview</title><link>https://kubermates.org/docs/2025-04-22-kubernetes-multicontainer-pods-an-overview/</link><pubDate>Tue, 22 Apr 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-22-kubernetes-multicontainer-pods-an-overview/</guid><description>Kubernetes Multicontainer Pods: An Overview The origins of the sidecar pattern Kubernetes implementation When to embrace (or avoid) sidecars Four essential multi-container patterns Init container pattern Ambassador pattern Configuration helper Adapter pattern Wrap-up As cloud-native architectures continue to evolve, Kubernetes has become the go-to platform for deploying complex, distributed systems. One of the most powerful yet nuanced design patterns in this ecosystem is the sidecar pattern—a technique that allows developers to extend application functionality without diving deep into source code. Think of a sidecar like a trusty companion motorcycle attachment. Historically, IT infrastructures have always used auxiliary services to handle critical tasks. Before containers, we relied on background processes and helper daemons to manage logging, monitoring, and networking. The microservices revolution transformed this approach, making sidecars a structured and intentional architectural choice. With the rise of microservices, the sidecar pattern became more clearly defined, allowing developers to offload specific responsibilities from the main service without altering its code. Service meshes like Istio and Linkerd have popularized sidecar proxies, demonstrating how these companion containers can elegantly handle observability, security, and traffic management in distributed systems. In Kubernetes, sidecar containers operate within the same Pod as the main application, enabling communication and resource sharing. Does this sound just like defining multiple containers along each other inside the Pod? It actually does, and this is how sidecar containers had to be implemented before Kubernetes v1.29.0, which introduced native support for sidecars. Sidecar containers can now be defined within a Pod manifest using the spec. initContainers field.</description></item><item><title>Smarter Knowledge Bases for Smarter AI Agents</title><link>https://kubermates.org/docs/2025-04-16-smarter-knowledge-bases-for-smarter-ai-agents/</link><pubDate>Wed, 16 Apr 2025 22:09:43 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-16-smarter-knowledge-bases-for-smarter-ai-agents/</guid><description>Smarter Knowledge Bases for Smarter AI Agents Web crawling for knowledge bases RAG Improvements Bonus: new Anthropic model availableâClaude 3.7 Unlocking more possibilities for AI developers About the author Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 By Grace Morgan Published: April 16, 2025 3 min read Weâre rolling out new features to the GenAI Platform that make it easier to build, manage, and improve the knowledge bases behind your AI agents. With web crawling, custom crawling rules, and one-click reindexing, you can keep your agents up to date with relevant, real-world information, without manual data collection or external storage. Combined with recent enhancements to our Retrieval-Augmented Generation (RAG) system, these updates can help your AI agents deliver faster, more accurate, and more context-aware responses from richer, better-organized data sources. The quality of your AI agent output depends on the data it can access. With the new web crawling feature, you can crawl publicly available websites and index content into your knowledge base , reducing the need for manual data collection. This is especially valuable for AI agents that rely on public web content to drive insights and actions. Custom crawling rules let you target specific pages or entire domains, ensuring your agent pulls data from the sites you tell it to. One-click reindexing keeps your knowledge base fresh and up to date, making it ideal for use cases like financial analysis and competitive research. No need for external storage , since crawled content is directly integrated into your knowledge base, simplifying setup and reducing overhead. With web crawling, your AI agent can stay informed by pulling in real-time information. To help your agents make the most of that knowledge, weâve significantly upgraded our Retrieval-Augmented Generation (RAG) system. Accuracy on text-based questions has nearly doubled, reaching up to 95 percent accuracy*.</description></item><item><title>Meet our new AI-powered product documentation chatbot</title><link>https://kubermates.org/docs/2025-04-16-meet-our-new-ai-powered-product-documentation-chatbot/</link><pubDate>Wed, 16 Apr 2025 11:45:01 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-16-meet-our-new-ai-powered-product-documentation-chatbot/</guid><description>Meet our new AI-powered product documentation chatbot Making docs search easier and faster An AI assistant to help keep you in flow Try it out today About the author Try DigitalOcean for free Related Articles Stop Building SaaS from Scratch: Meet the SeaNotes Starter Kit Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Introducing langchain-gradient: Seamless LangChain Integration with DigitalOcean Gradientâ¢ AI Platform By Grace Morgan Published: April 16, 2025 2 min read Searching for the right information in product documentation can be time consuming, especially when youâre deep in the middle of development. Thatâs why weâre excited to introduce DigitalOceanâs new product documentation chatbot , a new tool designed to help you quickly find accurate answers to your pressing product questions sourced directly from DigitalOceanâs official documentation. Letâs be honest, while documentation is essential, finding the exact answer you need often means clicking through multiple pages, refining search terms, or scrolling through long guides. Our new product documentation chatbot changes that by allowing you to ask natural-language questions and get instant, documentation-backed responses, helping you stay focused on building instead of searching. Using the chatbot is as simple as asking a question. Instead of digging through pages of documentation, just type what you need, like you would with a teammate, and get instant, sourced answers. No guesswork, no wasted time. Hereâs why it makes a difference: Instant responses pulled directly from our official docs Use natural language to interact in an intuitive way Code snippets and examples at your fingertips Trusted information, no guessing, no unreliable sources Seamless integration within the DigitalOcean documentation site This isnât just another AI tool, itâs built specifically to make your workflow smoother and your development process faster. Whether youâre troubleshooting, configuring a Droplet, or exploring new features, the chatbot helps you get back to coding with confidence. The new product documentation chatbot is now live on docs. digitalocean. com.</description></item><item><title>How to get started with Calico Observability features</title><link>https://kubermates.org/docs/2025-04-15-how-to-get-started-with-calico-observability-features/</link><pubDate>Tue, 15 Apr 2025 15:37:28 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-15-how-to-get-started-with-calico-observability-features/</guid><description>The Need for a Zero Trust Model Simplifying Network Flow Visibility with Calico Whisker Using Calico Whisker to Secure the ANP Demo APP What if you missed something? Conclusion Kubernetes, by default, adopts a permissive networking model where all pods can freely communicate unless explicitly restricted using network policies. While this simplifies application deployment, it introduces significant security risks. Unrestricted network traffic allows workloads to interact with unauthorized destinations, increasing the potential for cyberattacks such as Remote Code Execution (RCE), DNS spoofing, and privilege escalation. To better understand these problems, let’s examine a sample Kubernetes application: ANP Demo App. This application comprises a deployment that spawns pods and a service that exposes them to external users in a similar situation like any real word workload which you will encounter in your environment. If you open the application service before implementing any policies, the application reports the following messages: Container can reach the Internet – Without network policies, an attacker can use our container as an entry point by exploiting it with a vulnerability. This could allow them to exfiltrate data or establish remote control over the workload by leveraging its Internet access. Container can reach CoreDNS Pods – Kubernetes relies heavily on DNS, with records served using CoreDNS Pods. While communication between your Pods and CoreDNS is essential and not inherently a vulnerability, pairing it with unrestricted access to external DNS servers creates a significant security risk such as cluster wide DNS poisoning from a vulnerable pod or a pod with access to NET_RAW capabilities. Container can reach external DNS servers – Without restricting network policies, attackers can leverage techniques such as DNS poisoning, where they manipulate DNS responses to redirect traffic to malicious destinations. Container can reach the Kubernetes API Server – Often overlooked but without network policies all workloads can access the host via local networking addresses, or host sockets. While some applications require these communications, unrestricted access can serve as an escalation entry point for attackers to exploit internal services which are not managed by Kubernetes (e.</description></item><item><title>What's New on the GenAI Platform</title><link>https://kubermates.org/docs/2025-04-14-what-s-new-on-the-genai-platform/</link><pubDate>Mon, 14 Apr 2025 19:02:28 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-14-what-s-new-on-the-genai-platform/</guid><description>What&amp;rsquo;s New on the GenAI Platform About the author Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 By Grace Morgan Published: April 14, 2025 1 min read DigitalOceanâs GenAI Platform is now DigitalOcean Gradient Platform. To learn more about whatâs new, please check out our new blog - Whatâs New on DigitalOcean Gradient Platform. Share Product Updates August 20, 2025 2 min read Read more August 14, 2025 2 min read Read more August 14, 2025 2 min read Read more.</description></item><item><title>Expanding the GenAI Platform: Now supporting OpenAI models</title><link>https://kubermates.org/docs/2025-04-11-expanding-the-genai-platform-now-supporting-openai-models/</link><pubDate>Fri, 11 Apr 2025 14:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-11-expanding-the-genai-platform-now-supporting-openai-models/</guid><description>Expanding the GenAI Platform: Now supporting OpenAI models Bring your own OpenAI API key to DigitalOcean Why use OpenAI models on DigitalOcean? Try before you build in the model playground Get started About the author Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 By Grace Morgan Published: April 11, 2025 2 min read DigitalOceanâs GenAI Platform is now DigitalOcean Gradient Platform. Learn more about the GA release and features. Weâre excited to announce a major update to DigitalOceanâs GenAI Platform: support for OpenAI models is now live ! You can now bring your own OpenAI API key to use models like GPT-4o, o1, and o3-mini with GenAI agents on DigitalOcean. Whether youâre testing ideas in the model playground or deploying an agent into production, this gives you more flexibility when choosing the right model for your use case. GenAI Platform now supports OpenAIâs industry-leading models, allowing you to bring your own OpenAI API key to integrate GPT-4o and reasoning models like o1 and o3-mini into your GenAI agents. Users choose OpenAIâs models for their quality and comparatively advanced capabilities, making them a strong option for a wide range of AI applications. OpenAI models available: GPT-4o is a generalist model, designed for conversational AI, content generation, and code assistance. GPT-4o-mini is designed for fast, efficient task completion across chat, content, and code, with lower latency and cost. o3-mini is a lightweight reasoning model that excels in root cause analysis, due diligence, debugging, and troubleshooting. By supporting these models, DigitalOcean provides more flexibility to build AI-powered applications using OpenAIâs technology while seamlessly integrating into our GenAI Platform. OpenAI on DigitalOceanâs GenAI Platform gives developers a powerful, streamlined path to production-ready AI, no infrastructure headaches, no re-architecting. OpenAI is one of the most popular names in AI today, with models known for their performance, reliability, and capabilities.</description></item><item><title>Announcing enhancements to per-bucket access keys and public preview of Spaces access logs</title><link>https://kubermates.org/docs/2025-04-08-announcing-enhancements-to-per-bucket-access-keys-and-public-preview-of-spaces-a/</link><pubDate>Tue, 08 Apr 2025 17:40:20 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-08-announcing-enhancements-to-per-bucket-access-keys-and-public-preview-of-spaces-a/</guid><description>Announcing enhancements to per-bucket access keys and public preview of Spaces access logs Enhancements to per-bucket access keys provide API support and more granular controls Gain better visibility into your storage usage with Spaces access logs Capabilities of Spaces access logs Per-bucket access key and Spaces access log enhancements are essential Try these new enhancements today About the author(s) Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 By Anantha Ramachandran and Keshav Attrey Published: April 8, 2025 3 min read At DigitalOcean, we continuously enhance our cloud storage solutions to empower developers and growing businesses. Today, weâre excited to announce the general availability of DO API support for per-bucket access keys, mixed permissions support for per-bucket access keys, and the public preview of Spaces access logs, delivering greater automation, visibility, and security to DigitalOcean Spaces object storage. Building on the success of per-bucket access keys, weâre introducing two major upgrades that are now generally available to all customers to streamline storage access management: DO API support for managing access keys â Manage Spaces access keys programmatically using the DigitalOcean API, enabling automation through the DigitalOcean Terraform Provider, doctl CLI, DigitalOcean Go API Client (godo), and DigitalOceanâs Python library (PyDo). DO API support for managing access keys â Manage Spaces access keys programmatically using the DigitalOcean API, enabling automation through the DigitalOcean Terraform Provider, doctl CLI, DigitalOcean Go API Client (godo), and DigitalOceanâs Python library (PyDo). More granular access control â A single access key can now be configured with permissions that vary by bucket. This lets you grant read-only permissions for some buckets and read-write permissions for other buckets to a single person or application. More granular access control â A single access key can now be configured with permissions that vary by bucket. This lets you grant read-only permissions for some buckets and read-write permissions for other buckets to a single person or application. These enhancements simplify storage management for customers handling large-scale deployments, automated backups, and security-driven workflows. Explore the documentation to use DO API and manage mixed-permissions access keys. You can start using these new features right now. Spaces access logs are now available in public preview to provide detailed records of read and write requests to your Spaces buckets, helping you to better understand usage and enhance security.</description></item><item><title>Aqua Security Achieves FedRAMP® High Authorization</title><link>https://kubermates.org/docs/2025-04-08-aqua-security-achieves-fedramp-high-authorization/</link><pubDate>Tue, 08 Apr 2025 12:35:09 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-08-aqua-security-achieves-fedramp-high-authorization/</guid><description>Aqua Security Achieves FedRAMP® High Authorization What Is FedRAMP High and Why Does It Matter? Aqua U. S. Gov: Built for the Public Sector Helping Public and Private Organizations Move Forward A More Secure Cloud Native Future Starts Here Aqua Security’s Cloud Native Application Protection Platform (CNAPP) has achieved FedRAMP® High Impact Authorization, making Aqua one of the few CNAPP providers authorized at the highest level of federal cloud security compliance. This milestone opens the door for U. S. federal agencies, commercial organizations that require FedRAMP High, and cloud service providers operating in FedRAMP-authorized environments to confidently use Aqua’s platform for securing their cloud native applications. We met more than 400 rigorous security controls, giving federal agencies and commercial organizations the peace of mind that they can adopt cloud native technologies while meeting the highest security standards. FedRAMP (Federal Risk and Authorization Management Program) is a United States government-wide program that standardizes security assessments for cloud products and services. FedRAMP High is the most stringent level, designed for cloud services that manage highly sensitive data related to national security, public health, and other sensitive government functions. Achieving this level of authorization involves an extensive review of security controls, practices, and monitoring processes. The result: confidence that the platform is built to withstand today’s complex cyber threats. However, FedRAMP isn’t just about security; it also plays a critical role in compliance.</description></item><item><title>Introducing kube-scheduler-simulator</title><link>https://kubermates.org/docs/2025-04-07-introducing-kube-scheduler-simulator/</link><pubDate>Mon, 07 Apr 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-07-introducing-kube-scheduler-simulator/</guid><description>Introducing kube-scheduler-simulator Motivation Features of the kube-scheduler-simulator The simulator as a better dev cluster What are the use cases? Getting started Getting involved Acknowledgments The Kubernetes Scheduler is a crucial control plane component that determines which node a Pod will run on. Thus, anyone utilizing Kubernetes relies on a scheduler. kube-scheduler-simulator is a simulator for the Kubernetes scheduler, that started as a Google Summer of Code 2021 project developed by me (Kensei Nakada) and later received a lot of contributions. This tool allows users to closely examine the scheduler’s behavior and decisions. It is useful for casual users who employ scheduling constraints (for example, inter-Pod affinity ) and experts who extend the scheduler with custom plugins. The scheduler often appears as a black box, composed of many plugins that each contribute to the scheduling decision-making process from their unique perspectives. Understanding its behavior can be challenging due to the multitude of factors it considers. Even if a Pod appears to be scheduled correctly in a simple test cluster, it might have been scheduled based on different calculations than expected. This discrepancy could lead to unexpected scheduling outcomes when deployed in a large production environment. Also, testing a scheduler is a complex challenge. There are countless patterns of operations executed within a real cluster, making it unfeasible to anticipate every scenario with a finite number of tests. More often than not, bugs are discovered only when the scheduler is deployed in an actual cluster.</description></item><item><title>Sharks of DigitalOcean: Makeissah Robinson, Senior Director, Customer Support</title><link>https://kubermates.org/docs/2025-04-03-sharks-of-digitalocean-makeissah-robinson-senior-director-customer-support/</link><pubDate>Thu, 03 Apr 2025 04:15:54 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-03-sharks-of-digitalocean-makeissah-robinson-senior-director-customer-support/</guid><description>Sharks of DigitalOcean: Makeissah Robinson, Senior Director, Customer Support How does your team contribute to DigitalOceanâs mission? What makes DigitalOcean stand out to you? What excites you most about DigitalOcean? Dive into the future with DigitalOcean About the author Try DigitalOcean for free Related Articles Sharks of DigitalOcean: Darian Wilkin, Senior Manager, Solutions Engineering Sharks of DigitalOcean: Laura Schaffer, VP, Growth Sharks of DigitalOcean: Ali Munir, Staff Technical Account Manager By Sujatha R Technical Writer Published: April 3, 2025 2 min read At DigitalOcean, customers are at the heart of what we do, and Makeissah Robinson, our Senior Director of Customer Support, embodies this commitment. She leads her team with a focus on customer success and continuous improvement. Letâs dive into Makeissahâs leadership journeyâone that blends innovation, cultural inclusivity, and hands-on customer advocacy. I lead the global customer support and service team. We wake up and go to bed thinking about the customer. Whether youâre a student new to the cloud, a developer creating the next great application or a growing tech business looking to build on a platform that provides white-glove service, weâre here to make sure our customers and developers are successful on the DigitalOcean platform. Weâre also here to listen to and echo their needs within the organization. I love the diversity in our working environment and the diversity in our employees. Itâs really great to have different perspectives, and, as a global company, I get to learn a lot about other peopleâs cultures, which is also awesome. I also appreciate that our values promote creativity and innovationâthinking big and bold. Thereâs a strong focus on continuous learning and investing in yourself and your growth. Iâm always learning something new, whether itâs meeting new people, learning new technology, or discovering how our customers are using our products and services.</description></item><item><title>Introducing DigitalOcean Partner Network Connect: Secure, High-Performance Multi-Cloud Connectivity</title><link>https://kubermates.org/docs/2025-04-02-introducing-digitalocean-partner-network-connect-secure-high-performance-multi-c/</link><pubDate>Wed, 02 Apr 2025 12:30:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-02-introducing-digitalocean-partner-network-connect-secure-high-performance-multi-c/</guid><description>Introducing DigitalOcean Partner Network Connect: Secure, High-Performance Multi-Cloud Connectivity Why DigitalOcean Partner Network Connect? Getting started with DigitalOcean Partner Network Connect Simple, transparent pricing with no surprises Connect your multi-cloud workloads today More good news: Network Load Balancer is now generally available About the author(s) Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 By Anantha Ramachandran and Udhay Ravindran Published: April 2, 2025 4 min read Growing businesses increasingly rely on multiple cloud providers and on-premise infrastructure to scale their applications. However, connecting different cloud environments securely and efficiently is a major headache. Many businesses struggle with unpredictable latency and complex VPN setups to keep their distributed workloads running smoothly. Without a seamless solution, managing cloud connectivity becomes a costly and time-consuming burden. At DigitalOcean, weâre committed to helping to eliminate these challenges by making cloud networking simpler, more scalable, and more accessible. Today, weâre excited to introduce the general availability of DigitalOcean Partner Network Connect, a cross-cloud connectivity solution that enables more secure, high-performance networking between DigitalOcean and other cloud providers or on-premise data centers. With Partner Network Connect, growing businesses can establish direct, private connections between DigitalOcean and other cloud providers or on-premise data centers or co-location facilitiesâbypassing the public internet to help improve security, minimize latency, and increase network performance. By integrating with Megaport, a leading Network-as-a-Service (NaaS) provider, weâre helping our customers connect their distributed workloads across multiple clouds easily, reliably, and affordably. As businesses scale, they often deploy workloads across multiple cloud providers or maintain hybrid cloud environments. Until now, cloud customers had to rely on VPNs or self-managed solutions to connect these environments, which can be complex to set up, costly, and difficult to maintain. DigitalOcean Partner Network Connect helps eliminate these challenges for cloud-to-cloud and on-premise connectivity. Key benefits include: Improved latency: Low latency, high-bandwidth private connectivity (1 Gbps to 10 Gbps) for faster, more reliable performance across cloud and on-premise environments.</description></item><item><title>Tomcat in the Crosshairs: New Research Reveals Ongoing Attacks</title><link>https://kubermates.org/docs/2025-04-02-tomcat-in-the-crosshairs-new-research-reveals-ongoing-attacks/</link><pubDate>Wed, 02 Apr 2025 12:00:49 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-02-tomcat-in-the-crosshairs-new-research-reveals-ongoing-attacks/</guid><description>Tomcat in the Crosshairs: New Research Reveals Ongoing Attacks Tomcat Campaign 25′: Attack Flow Detecting This Malware and Protecting your Environments Mitigation of Tomcat environments Indications of Compromise (IOCs) News headlines reported that it took just 30 hours for attackers to exploit a newly discovered vulnerability in Apache Tomcat servers. But what does this mean for workloads relying on Tomcat? Aqua Nautilus researchers discovered a new attack campaign targeting Apache Tomcat. In this blog, we shed light on newly discovered malware that targets Tomcat servers to hijack resources. After gaining initial access, the attackers uploads encrypted and encoded payloads that establish backdoors and persistence mechanisms. They then deploy two binaries disguised as kernel processes to exploit the server. The attack infrastructure appears to be relatively new, and code snippets suggest possible links to a Chinese-speaking threat actor. In this blog, we break down how the attack works—and how to stop it. The campaign targets Apache Tomcat servers and deploys encrypted payloads designed to run on both Windows and Linux systems. Once executed, the attack disguises itself, steals SSH credentials to spread laterally, and ultimately hijacks resources for cryptocurrency mining. It all starts with a brute-force attempt from a remote server using a Python script, which tests commonly used usernames and weak passwords on the Tomcat management console (e. g. , username “Tomcat” and password “123456”).</description></item><item><title>Introducing AMD Instinct MI300X GPUs to the DigitalOcean Bare Metal fleet</title><link>https://kubermates.org/docs/2025-04-01-introducing-amd-instinct-mi300x-gpus-to-the-digitalocean-bare-metal-fleet/</link><pubDate>Tue, 01 Apr 2025 21:42:26 +0000</pubDate><guid>https://kubermates.org/docs/2025-04-01-introducing-amd-instinct-mi300x-gpus-to-the-digitalocean-bare-metal-fleet/</guid><description>Introducing AMD Instinct MI300X GPUs to the DigitalOcean Bare Metal fleet What are AMD Instinct MI300X GPUs? Whatâs next? About the author Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 By Waverly Swinton Published: April 1, 2025 2 min read Bare Metal GPUs are now DigitalOcean GradientAI Bare Metal GPUs. Learn more about DigitalOcean GradientAI , our suite of AI products. Weâre excited to announce that DigitalOcean customers now have access to AMD Instinct MI300X GPUs with ROCm Software to power their AI workloads. At DigitalOcean, weâre committed to bringing you even more options to power your projects. One of the highest-bandwith GPUs (5.3 TB/s of HBM3 memory), AMD Instinct MI300X offers significant benefits like faster deployments and much more for AI, machine learning, and high-performance computing (HPC) workloads. Along with expanding access to GPUs, we continue to improve on our existing offerings for developers like you. Whether youâre deploying popular AI models with our zero-configuration setup that reduces deployment time from weeks to minutes or managing your AI budget with our simple pricing structure , DigitalOcean can help make your AI journey simple, scalable, and cost-effective. AMD Instinct MI300X GPUs are designed to boost AI and HPC capabilities in a new compact, efficient package. These next-generation GPUs are now available on DigitalOcean in single-tenant Bare Metal configurations for customers seeking increased control and computing power. MI300X offers raw acceleration power with eight GPUs per node, leading compute unit counts, and HBM3 capacity. With MI300X, you can accelerate your deployment efforts with ROCm developer resources that support multiple AI and HPC frameworks, leading software platforms and models, and networking infrastructure within an open, proven software ecosystem. By enhancing computational throughput and simplifying programming and deployment, MI300X and DigitalOcean give you the power to improve data center efficiencies, tackle budget and sustainability concerns, and unleash a highly programmable GPU software platform to quickly build AI projects.</description></item><item><title>The Next Evolution of DigitalOcean Kubernetes: Introducing Features that Unlock Superior Scalability for Growing Businesses</title><link>https://kubermates.org/docs/2025-03-31-the-next-evolution-of-digitalocean-kubernetes-introducing-features-that-unlock-s/</link><pubDate>Mon, 31 Mar 2025 19:59:14 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-31-the-next-evolution-of-digitalocean-kubernetes-introducing-features-that-unlock-s/</guid><description>The Next Evolution of DigitalOcean Kubernetes: Introducing Features that Unlock Superior Scalability for Growing Businesses The Next Evolution of DigitalOcean Kubernetes: Introducing Features that Unlock Superior Scalability for Growing Businesses An overview of each new feature, and what it means for your business Increased cluster capacity Optimized networking with VPC-native Kubernetes Improved performance with eBPF-powered networking Managed Cilium for high-performance networking What this means for your business Get started today About the author Try DigitalOcean for free Related Articles Stop Building SaaS from Scratch: Meet the SeaNotes Starter Kit Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Introducing langchain-gradient: Seamless LangChain Integration with DigitalOcean Gradientâ¢ AI Platform By Nicole Ghalwash Published: March 31, 2025 4 min read Kubernetes is the foundation of many modern applications, providing the scalability and resilience needed for todayâs dynamic workloads. However, as applications grow, so do their infrastructure and scaling requirements. Managing multiple clusters to handle large-scale workloads introduces operational complexity, increased network management, and challenges for the DevOps teams. These challenges can include greater resource fragmentation, increased latency due to inter-cluster communication, and the manual burden of maintaining security and compliance across multiple clusters. These challenges are particularly pronounced for data-intensive workloads like video streaming, large-scale data analytics, and security operations. Thatâs why weâre excited to announce a major evolution of DigitalOcean Kubernetes Service (DOKS), which consists of four features: Increased cluster capacity to 1,000 worker nodes per cluster Increased cluster capacity to 1,000 worker nodes per cluster Optimized networking with VPC-native Kubernetes Optimized networking with VPC-native Kubernetes Improved performance with eBPF-powered networking Improved performance with eBPF-powered networking Managed Cilium for high-performance networking Managed Cilium for high-performance networking These updates help to empower businesses to run larger workloads within a single DOKS cluster, reducing the need for complex multi-cluster management and unlocking new levels of performance, reliability, and simplicity. NoBid, a Kubernetes customer who joined us at Deploy 2025, has seen significant improvements in scalability and security when deploying DOKS. âDigitalOcean Kubernetes provides NoBid with a powerful platform for our containerized workloads,&amp;quot; said Shawn Petersen, CIO at NoBid. âAdditionally, the ability to rapidly scale based on business demand, handle 1.3 PB/month in data egress, along with better security controls were all key differentiators for us to partner with DigitalOcean. &amp;quot; If you want to learn more about NoBidâs use case, check out their case study. Also, you can learn more about their migration from AWS to DigitalOcean and their subsequent 30% cost-savings by watching the testimonial they gave at Deploy 2025. This announcement consists of four features that are new and coming to DOKS.</description></item><item><title>Cut Through Alert Noise and Fix Toxic Combinations First</title><link>https://kubermates.org/docs/2025-03-27-cut-through-alert-noise-and-fix-toxic-combinations-first/</link><pubDate>Thu, 27 Mar 2025 17:13:45 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-27-cut-through-alert-noise-and-fix-toxic-combinations-first/</guid><description>Cut Through Alert Noise and Fix Toxic Combinations First See the Full Picture: Prioritizing Risk from Code Commit to Runtime Making Risk-Based Vulnerability Management Actionable with the New Issues Page Real-World Toxic Combinations That Put You at Risk Redefining Cloud Native Vulnerability Management Not every security alert is a threat, but the right combination can bring down your cloud native and containerized applications. Security incidents rarely happen because of a single weak point. Instead, they stem from toxic combinations. A misconfigured workload might seem harmless on its own, but add exposed credentials and an unpatched vulnerability, and attackers have a direct path to exploitation. Traditional vulnerability scanners surface thousands of issues, yet many tools treat vulnerabilities, misconfigurations, malware, and exposed credentials as isolated problems rather than recognizing how they can combine to create real attack scenarios Without understanding the full attack surface and how risks interact, security teams end up chasing alerts instead of preventing breaches. Instead of drowning in alerts, you need context, a way to connect security findings across the entire cloud native application lifecycle. Aqua provides that visibility, correlating risks from the first line of code to runtime so security teams can focus on what is actually exploitable, not just what is flagged. But risk prioritization is not just about what exists, it is about what an attacker can actually access and exploit. A vulnerability might seem critical, but is it isolated in a test environment, restricted within an internal network, or exposed in a production system accessible from the internet? For example, a banking application with an unpatched critical vulnerability might seem like an urgent issue, but if it is running in a segmented test environment, the risk is far lower than if the same vulnerability exists in a publicly accessible production system. Aqua assesses network exposure, identifying whether a CVE is just a theoretical risk or if it is publicly accessible and exploitable. By linking misconfigurations, exposed credentials, and network exposure with known vulnerabilities, Aqua surfaces toxic combinations that create real world attack paths. Aqua also connects these risks across hybrid and multi-cloud environments, ensuring teams have a comprehensive view of their attack surface, no matter where workloads are running.</description></item><item><title>Kubernetes v1.33 sneak peek</title><link>https://kubermates.org/docs/2025-03-26-kubernetes-v1-33-sneak-peek/</link><pubDate>Wed, 26 Mar 2025 10:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-03-26-kubernetes-v1-33-sneak-peek/</guid><description>Kubernetes v1.33 sneak peek The Kubernetes API removal and deprecation process Deprecations and removals for Kubernetes v1.33 Deprecation of the stable Endpoints API Removal of kube-proxy version information in node status Removal of host network support for Windows pods Featured improvement of Kubernetes v1.33 Support for user namespaces within Linux Pods Selected other Kubernetes v1.33 improvements In-place resource resize for vertical scaling of Pods DRA’s ResourceClaim Device Status graduates to beta Ordered namespace deletion Enhancements for indexed job management Want to know more? Get involved As the release of Kubernetes v1.33 approaches, the Kubernetes project continues to evolve. Features may be deprecated, removed, or replaced to improve the overall health of the project. This blog post outlines some planned changes for the v1.33 release, which the release team believes you should be aware of to ensure the continued smooth operation of your Kubernetes environment and to keep you up-to-date with the latest developments. The information below is based on the current status of the v1.33 release and is subject to change before the final release date. The Kubernetes project has a well-documented deprecation policy for features. This policy states that stable APIs may only be deprecated when a newer, stable version of that same API is available and that APIs have a minimum lifetime for each stability level. A deprecated API has been marked for removal in a future Kubernetes release. It will continue to function until removal (at least one year from the deprecation), but usage will result in a warning being displayed. Removed APIs are no longer available in the current version, at which point you must migrate to using the replacement. Generally available (GA) or stable API versions may be marked as deprecated but must not be removed within a major version of Kubernetes. Beta or pre-release API versions must be supported for 3 releases after the deprecation. Alpha or experimental API versions may be removed in any release without prior deprecation notice; this process can become a withdrawal in cases where a different implementation for the same feature is already in place.</description></item><item><title>IngressNightmare Vulnerabilities: All You Need to Know</title><link>https://kubermates.org/docs/2025-03-26-ingressnightmare-vulnerabilities-all-you-need-to-know/</link><pubDate>Wed, 26 Mar 2025 13:15:57 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-26-ingressnightmare-vulnerabilities-all-you-need-to-know/</guid><description>IngressNightmare Vulnerabilities: All You Need to Know What is the Ingress Controller Understanding the IngressNightmare vulnerabilities Vulnerabilities Details and Impact Are you Affected? Detection and Mitigation Mitigating Vulnerabilities with Aqua Detecting and Mitigating with Aqua Trivy Vulnerability Impact Across Managed Kubernetes Platforms CVE-2025-1974 * CVE-2025-24514 * CVE-2025-1097 * CVE-2025-1098 * CVE-2025-24513 On March 24, 2025, a set of critical vulnerabilities (CVE-2025-1097, CVE-2025-1098, CVE-2025-24514, and CVE-2025-1974 — collectively referred to as IngressNightmare was disclosed in the ingress-nginx Controller for Kubernetes. These vulnerabilities could lead to a complete cluster takeover by granting attackers unauthorized access to all secrets stored across all namespaces in the Kubernetes cluster. In Kubernetes, an Ingress Controller manages external access to services within a cluster, typically via HTTP or HTTPS. The ingress NGINX Controller, built on the NGINX web server , is widely used to route incoming traffic to the appropriate backend services based on defined rules. HTTP HTTPS NGINX web server The disclosed vulnerabilities include: CVE-2025-1974 (CVSS Score 9.8 Critical) Allows unauthenticated attackers with pod network access to execute arbitrary code in the ingress-nginx controller, potentially leading to full cluster takeover. CVE-2025-24514, CVE-2025-1097, CVE-2025-1098 (CVSS Score 8.8 High) Involve improper handling of Ingress annotations that can lead to code execution or unauthorized data access. CVE-2025-24513 (CVSS Score 4.8 Medium) Involves directory traversal that can lead to DoS or limited secret disclosure. The researchers who found these vulnerabilities indicated that over 40% of cloud environments were vulnerable to these remote code execution (RCE) risks. Their analysis discovered over 6,500 clusters, including those of Fortune 500 companies, that publicly expose the admission controllers of vulnerable Kubernetes ingress controllers to the public internet, placing them at immediate critical risk. The most critical issue, CVE-2025-1974, affects the admission controller component of the Ingress NGINX Controller. It allows remote code execution through a malicious Ingress object, potentially exposing secrets and compromising the entire cluster. An exploit was published on GitHub.</description></item><item><title>Kubeflow 1.10 Release Announcement</title><link>https://kubermates.org/docs/2025-03-26-kubeflow-1-10-release-announcement/</link><pubDate>Wed, 26 Mar 2025 00:00:00 -0500</pubDate><guid>https://kubermates.org/docs/2025-03-26-kubeflow-1-10-release-announcement/</guid><description>Highlight features Kubeflow Platform (Manifests &amp;amp; Security) Manifests: Security: Pipelines Support for Placeholders in Resource Limits Support for Loop Parallelism Implement SubDAG Output Resolution Model Registry Model Registry UI Custom Storage Initializer Training Operator (Trainer) &amp;amp; Katib Hyperparameter Optimization API for LLMs Support for Various Parameter Distributions Push-Based Metrics Collection Dashboard &amp;amp; Notebooks Prometheus Metrics for Notebooks More Descriptive Error Messages Spark Operator KServe New Python SDK OCI Storage for Models Model Cache Feature Hugging Face Integration What comes next? How to get started with 1.10 Join the Community Want to help? Kubeflow 1.10.0 delivers essential updates that enhance the flexibility, efficiency, and scalability of machine learning workflows. The new features span across several components, improving both user experience and system performance. Trainer 2.0 New UI for Model Registry Spark Operator as a core Kubeflow component Kubernetes and container security (CISO compatibility) Hyperparameter Optimization for LLMs Fine-Tuning Loop parallelism in Pipelines New parameter distributions for Katib Deeper Model Registry integrations with KServe New Python SDK, OCI storage, and model caching for KServe New security contexts and rootless Istio-CNI integrations for Spark Operator The Kubeflow Platform Working Group focuses on simplifying Kubeflow installation, operations, and security. Spark Operator 2.1.0 included in Kubeflow platform, although not installed yet by default Documentation updates that make it easier to install, extend and upgrade Kubeflow For more details and future plans please consult the 1.10.0 and 1.10.1/1.11.0 milestones CVE reductions - regular scanning with trivy Kubernetes and container security best practices: Rootless containers / PodSecurityStandards restricted for: Istio-CNI, Knative, Dex, Oauth2-proxy, Spark 50 % done : KFP, Notebooks / Workspaces, Katib, Trainer, Kserve, … Istio-CNI as default for rootless Kubeflow postponed to 1.10.1 Rootless containers / PodSecurityStandards restricted for: Istio-CNI, Knative, Dex, Oauth2-proxy, Spark 50 % done : KFP, Notebooks / Workspaces, Katib, Trainer, Kserve, … Istio-CNI as default for rootless Kubeflow postponed to 1.10.1 OIDC-authservice has been replaced by oauth2-proxy Oauth2-proxy and Dex documentation for external OIDC authentication (Keycloak, and OIDC providers such as Azure, Google etc. ) Trivy CVE scans March 25 2025: Kubeflow Pipelines 2.4.1 introduces support for placeholders in resource limits , enhancing flexibility in pipeline execution. This update allows users to define dynamic resource limits using parameterized values, enabling more adaptable and reusable pipeline definitions. Kubeflow Pipelines 2.4.1 introduces a new Parallelism Limit for ParallelFor tasks , giving users the ability to run massively parallel inference pipelines, with more control over parallel execution in their workflows. This feature allows users to specify the maximum number of parallel iterations, preventing resource overutilization and improving system stability. When running large pipelines with GPUs, proper use of this feature could save your team thousands of dollars in compute expenses. ParallelFor Kubeflow 1.10 ensures that pipelines using nested DAGs work correctly and reliably when treated as components. Outputs from deeply nested DAGs will now resolve properly, avoiding broken dependencies. Model Registry introduces a new user interface and enhanced model management capabilities.</description></item><item><title>Fresh Swap Features for Linux Users in Kubernetes 1.32</title><link>https://kubermates.org/docs/2025-03-25-fresh-swap-features-for-linux-users-in-kubernetes-1-32/</link><pubDate>Tue, 25 Mar 2025 10:00:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-03-25-fresh-swap-features-for-linux-users-in-kubernetes-1-32/</guid><description>Fresh Swap Features for Linux Users in Kubernetes 1.32 How do I use it? Install a swap-enabled cluster with kubeadm Before you begin Create a swap file and turn swap on Set up a Kubernetes cluster that uses swap-enabled nodes How is the swap limit being determined with LimitedSwap? How does it work? How can I monitor swap? Node and container level metric statistics Node Feature Discovery (NFD) Caveats Memory-backed volumes Good practice for using swap in a Kubernetes cluster Disable swap for system-critical daemons Protect system-critical daemons for I/O latency Swap and control plane nodes Use of a dedicated disk for swap Looking ahead How can I learn more? How do I get involved? Swap is a fundamental and an invaluable Linux feature. It offers numerous benefits, such as effectively increasing a node’s memory by swapping out unused data, shielding nodes from system-level memory spikes, preventing Pods from crashing when they hit their memory limits, and much more. As a result, the node special interest group within the Kubernetes project has invested significant effort into supporting swap on Linux nodes. The 1.22 release introduced Alpha support for configuring swap memory usage for Kubernetes workloads running on Linux on a per-node basis. Later, in release 1.28, support for swap on Linux nodes has graduated to Beta, along with many new improvements. In the following Kubernetes releases more improvements were made, paving the way to GA in the near future. Prior to version 1.22, Kubernetes did not provide support for swap memory on Linux systems. This was due to the inherent difficulty in guaranteeing and accounting for pod memory utilization when swap memory was involved. As a result, swap support was deemed out of scope in the initial design of Kubernetes, and the default behavior of a kubelet was to fail to start if swap memory was detected on a node. In version 1.22, the swap feature for Linux was initially introduced in its Alpha stage. This provided Linux users the opportunity to experiment with the swap feature for the first time. However, as an Alpha version, it was not fully developed and only partially worked on limited environments.</description></item><item><title>Ingress-nginx CVE-2025-1974: What You Need to Know</title><link>https://kubermates.org/docs/2025-03-24-ingress-nginx-cve-2025-1974-what-you-need-to-know/</link><pubDate>Mon, 24 Mar 2025 12:00:00 -0800</pubDate><guid>https://kubermates.org/docs/2025-03-24-ingress-nginx-cve-2025-1974-what-you-need-to-know/</guid><description>Ingress-nginx CVE-2025-1974: What You Need to Know Background Vulnerabilities Patched Today Your next steps Conclusion, thanks, and further reading Today, the ingress-nginx maintainers have released patches for a batch of critical vulnerabilities that could make it easy for attackers to take over your Kubernetes cluster: ingress-nginx v1.12.1 and ingress-nginx v1.11.5. If you are among the over 40% of Kubernetes administrators using ingress-nginx , you should take action immediately to protect your users and data. Ingress is the traditional Kubernetes feature for exposing your workload Pods to the world so that they can be useful. In an implementation-agnostic way, Kubernetes users can define how their applications should be made available on the network. Then, an ingress controller uses that definition to set up local or cloud resources as required for the user’s particular situation and needs. Many different ingress controllers are available, to suit users of different cloud providers or brands of load balancers. Ingress-nginx is a software-only ingress controller provided by the Kubernetes project. Because of its versatility and ease of use, ingress-nginx is quite popular: it is deployed in over 40% of Kubernetes clusters! Ingress-nginx translates the requirements from Ingress objects into configuration for nginx, a powerful open source webserver daemon. Then, nginx uses that configuration to accept and route requests to the various applications running within a Kubernetes cluster. Proper handling of these nginx configuration parameters is crucial, because ingress-nginx needs to allow users significant flexibility while preventing them from accidentally or intentionally tricking nginx into doing things it shouldn’t. Four of today’s ingress-nginx vulnerabilities are improvements to how ingress-nginx handles particular bits of nginx config. Without these fixes, a specially-crafted Ingress object can cause nginx to misbehave in various ways, including revealing the values of Secrets that are accessible to ingress-nginx.</description></item><item><title>How the Google-Wiz acquisition redefines cloud security</title><link>https://kubermates.org/docs/2025-03-24-how-the-google-wiz-acquisition-redefines-cloud-security/</link><pubDate>Mon, 24 Mar 2025 16:36:45 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-24-how-the-google-wiz-acquisition-redefines-cloud-security/</guid><description>How the Google-Wiz acquisition redefines cloud security CNAPP is Dead! Long Live CNAPP! What does the future hold for our customers? Google’s acquisition of Wiz, announced last week, is a pivotal moment as it marks a strategic shift in how cyber security will evolve over the next few years. It instantly turns Google into a major player in security, adding Wiz to other building blocks Google has racked up in the past couple of years, most notably Mandiant and Google Chronicle. Google will be a new gorilla in the security market, just as Microsoft created a thriving, multi-billion dollar business out of the Defender product line. But while Microsoft has a long history of “owning” operating systems, servers, and Office applications, and is leveraging that to expand its footprint, Google is new to the enterprise security business. In the first phase of public cloud adoption, the “lift and shift” phase, CSPM (cloud security posture management) emerged to ensure proper configuration of cloud services, and CWPP (cloud workload protection platforms) to monitor and protect workloads running in the cloud. The second phase of cloud adoption, the cloud native phase, driven by technologies such as containers, serverless functions, CI/CD, and orchestration (Kubernetes), imparted an even more dramatic change in security – the integration of multiple silos of application related security information. It introduced integrated shift left capabilities , providing a broader risk-based approach to vulnerability management, hardening, and incident management. Thus CNAPP (cloud native application protection platforms) was born. In the early days of cloud services, the cloud providers adopted the shared responsibility model in which some responsibilities were borne by them (such as infrastructure and physical security), others were clearly on the shoulders of customers (configuring of services, their data, their users, their applications), and there’s been a non-negligible area of “shared responsibility” where the answer is often “it depends”. I believe that with this move, Google is shifting the borders within this model and will be offering more of the posture management and visibility to customers as part of its infrastructure. After all this is what Wiz has become famous for – agentless scanning of the cloud estate, providing broad visibility and risk assessment. And this is something a cloud provider can easily integrate into cloud operations.</description></item><item><title>Introducing JobSet</title><link>https://kubermates.org/docs/2025-03-23-introducing-jobset/</link><pubDate>Sun, 23 Mar 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-23-introducing-jobset/</guid><description>Introducing JobSet Why JobSet? How JobSet Works Example use case Distributed ML training on multiple TPU slices with Jax Future work and getting involved Authors : Daniel Vega-Myhre (Google), Abdullah Gharaibeh (Google), Kevin Hannon (Red Hat) In this article, we introduce JobSet , an open source API for representing distributed jobs. The goal of JobSet is to provide a unified API for distributed ML training and HPC workloads on Kubernetes. The Kubernetes community’s recent enhancements to the batch ecosystem on Kubernetes has attracted ML engineers who have found it to be a natural fit for the requirements of running distributed training workloads. Large ML models (particularly LLMs) which cannot fit into the memory of the GPU or TPU chips on a single host are often distributed across tens of thousands of accelerator chips, which in turn may span thousands of hosts. As such, the model training code is often containerized and executed simultaneously on all these hosts, performing distributed computations which often shard both the model parameters and/or the training dataset across the target accelerator chips, using communication collective primitives like all-gather and all-reduce to perform distributed computations and synchronize gradients between hosts. These workload characteristics make Kubernetes a great fit for this type of workload, as efficiently scheduling and managing the lifecycle of containerized applications across a cluster of compute resources is an area where it shines. It is also very extensible, allowing developers to define their own Kubernetes APIs, objects, and controllers which manage the behavior and life cycle of these objects, allowing engineers to develop custom distributed training orchestration solutions to fit their needs. However, as distributed ML training techniques continue to evolve, existing Kubernetes primitives do not adequately model them alone anymore. Furthermore, the landscape of Kubernetes distributed training orchestration APIs has become fragmented, and each of the existing solutions in this fragmented landscape has certain limitations that make it non-optimal for distributed ML training. For example, the KubeFlow training operator defines custom APIs for different frameworks (e. g. PyTorchJob, TFJob, MPIJob, etc.</description></item><item><title>Powering AI Innovation: DigitalOcean Bare Metal GPUs in EU Data Center</title><link>https://kubermates.org/docs/2025-03-20-powering-ai-innovation-digitalocean-bare-metal-gpus-in-eu-data-center/</link><pubDate>Thu, 20 Mar 2025 14:59:45 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-20-powering-ai-innovation-digitalocean-bare-metal-gpus-in-eu-data-center/</guid><description>Powering AI Innovation: DigitalOcean Bare Metal GPUs in EU Data Center Lower latency for inferencing in the EU Privacy at the core The benefits of bare metal for AI/ML workloads Powering European AI: DigitalOcean Bare Metal GPUs in EU data center About the author Try DigitalOcean for free Related Articles Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Build smarter AI agents: new tools now available for the DigitalOcean Gradientâ¢ AI Platform Introducing GPU Droplets accelerated by NVIDIA HGX H200 By DigitalOcean Published: March 20, 2025 5 min read Bare Metal GPUs are now DigitalOcean GradientAI Bare Metal GPUs. Learn more about DigitalOcean GradientAI , our suite of AI products. Whether youâre building a generative video platform like Moonvalley or an advanced coding assistant like Supermaven , GPU computing is now a non-negotiable for engineering AI applications. Whether itâs for training LLMs or powering real-time inference, these processors have become essential infrastructure for startups working on AI products. DigitalOcean is powering this development by providing a number of AI/ML offerings, including Bare Metal GPUs âdedicated machines with NVIDIA Hopper architecture for the most demanding AI workloads. These computing resources are available in our European data center based in Amsterdam, Netherlands, giving EU-based companies (or international businesses with EU customers) direct access to high-performance AI infrastructure right where they need it. Build your AI projects with DigitalOceanâs Bare Metal GPUs in Amsterdam. Reserve capacity to experience the power of NVIDIA HGX H100 with up to 640 GB of GPU RAM and enhanced NVLink for multi-GPU scaling. Contact our experts to talk through your workload needs and join innovative companies who are already making use of our high-performance, dedicated infrastructure for their AI/ML applications. When it comes to AI applications, milliseconds matter. Placing your inference workloads physically closer to your end users reduces the time it takes for requests to travel to your servers and for responses to return. With DigitalOceanâs Bare Metal GPUs in Amsterdam, companies serving European markets can cut latency compared to running the same workloads in data centers outside of Europe.</description></item><item><title>Scale into the stratosphere: Managed MySQL &amp; PostgreSQL now support up to 20TB and 30TB</title><link>https://kubermates.org/docs/2025-03-19-scale-into-the-stratosphere-managed-mysql-postgresql-now-support-up-to-20tb-and-/</link><pubDate>Wed, 19 Mar 2025 17:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-19-scale-into-the-stratosphere-managed-mysql-postgresql-now-support-up-to-20tb-and-/</guid><description>Scale into the stratosphere: Managed MySQL &amp;amp; PostgreSQL now support up to 20TB and 30TB Why larger plan sizes matter Who benefits from this update? Why choose DigitalOcean Managed MySQL and/or PostgreSQL? Determining which database engine is right for your workload Get started today About the author Try DigitalOcean for free Related Articles Stop Building SaaS from Scratch: Meet the SeaNotes Starter Kit Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Introducing langchain-gradient: Seamless LangChain Integration with DigitalOcean Gradientâ¢ AI Platform By Nicole Ghalwash Published: March 19, 2025 3 min read Good news for growing businesses and folks with data-intensive applications: weâre excited to introduce 100% larger storage plans for both Managed MySQL and PostgreSQL in select regions. Starting today, Managed MySQL storage has doubled to 20TB, and Managed PostgreSQL now supports up to 30TB, up from 15TB. These expanded storage options are available in Singapore (SGP1), San Francisco (SFO2), and Frankfurt (FRA1), with plans to roll out to more regions later this year. Now users have even more flexibility to scale seamlessly, whether growing existing workloads or migrating from self-managed databases. With DigitalOceanâs fully managed service, you get the performance, security, and simplicity you needâwithout the operational burden. Scaling your databases is now easier than ever, helping to ensure your infrastructure keeps pace with your business growth. As your businesses grow, so do data storage requirements. Our expanded, scalable storage options allow you to: Big databases, no worries: Support large databases with up to 20TB and 30TB of scalable storage. Big databases, no worries: Support large databases with up to 20TB and 30TB of scalable storage. Control both performance and costs: Scale storage independently of CPU and memory, giving you the flexibility to adapt as your workload evolves. This ensures you pay only for what you need while avoiding over- or under-provisioning resources. Control both performance and costs: Scale storage independently of CPU and memory, giving you the flexibility to adapt as your workload evolves.</description></item><item><title>GPU Droplets Achieve SOC 2 Compliance</title><link>https://kubermates.org/docs/2025-03-17-gpu-droplets-achieve-soc-2-compliance/</link><pubDate>Mon, 17 Mar 2025 22:08:32 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-17-gpu-droplets-achieve-soc-2-compliance/</guid><description>GPU Droplets Achieve SOC 2 Compliance Why This Matters What is SOC 2 Compliance? Accessing the Report Get Started with GPU Droplets About the author Try DigitalOcean for free Related Articles Stop Building SaaS from Scratch: Meet the SeaNotes Starter Kit Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Introducing langchain-gradient: Seamless LangChain Integration with DigitalOcean Gradientâ¢ AI Platform By David Lopez Staff Trust &amp;amp; Governance Advisor, Trust and Governance Published: March 17, 2025 2 min read GPU Droplets are now DigitalOcean GradientAI GPU Droplets. Learn more about DigitalOcean GradientAI , our suite of AI products. Today, weâre excited to announce that GPU Droplets join the suite of DigitalOceanâs Service Organization Control (SOC) 2 Type II compliant products. With this achievement, customers can experiment, train, and scale AI projects on GPU Droplets with trust in DigitalOceanâs ongoing commitment to protect customersâ sensitive information. Adhering to SOC 2 remains a critical part of DigitalOceanâs operations given our commitment to security and simplicity. This means that when you build on DigitalOcean, you build on a foundation of proven security practices and controls. As an example, consider the following: Security: Based on the cloud product in use, customers inherit a portion of DigitalOceanâs security posture under the Shared Responsibility Model. By using a SOC 2 compliant product, customers can be more confident that they can manage their risk footprint to the requirements of that standard. Security: Based on the cloud product in use, customers inherit a portion of DigitalOceanâs security posture under the Shared Responsibility Model. By using a SOC 2 compliant product, customers can be more confident that they can manage their risk footprint to the requirements of that standard. Simplicity: Many regulatory regimes have overlapping compliance requirements as the SOC 2, therefore helping you further enhance your own compliance objectives. SOC 2 is one of three SOC standards ( SOC 1, SOC 2, and SOC 3 ) developed by the American Institute of Certified Public Accountants (AICPA) which assess an organizationâs controls against the applicable Trust Services Criteria, which include some combination of the following: Security, availability, processing integrity, confidentiality, and privacy.</description></item><item><title>Supply Chain Security Risk: GitHub Action tj-actions/changed-files Compromised</title><link>https://kubermates.org/docs/2025-03-16-supply-chain-security-risk-github-action-tj-actions-changed-files-compromised/</link><pubDate>Sun, 16 Mar 2025 15:56:32 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-16-supply-chain-security-risk-github-action-tj-actions-changed-files-compromised/</guid><description>Supply Chain Security Risk: GitHub Action tj-actions/changed-files Compromised Background: Understanding tj-actions/changed-files Technical Details on GitHub Action tj-actions How to Check if You’re Impacted Immediate Next Steps How Aqua Security Helps CVE-2025-30066 On March 14th, 2025, security researchers discovered a critical software supply chain vulnerability in the widely-used GitHub Action tj-actions/changed-files ( CVE-2025-30066 ). This vulnerability allows remote attackers to expose CI/CD secrets via the action’s build logs. The issue affects users who rely on the tj-actions/changed-files action in GitHub workflows to track changed files within a pull request. tj-actions/changed-files tj-actions/changed-files Due to the compromised action, sensitive CI/CD secrets are being inadvertently logged in the GitHub Actions build logs. If these logs are publicly accessible, such as in public repositories, unauthorized users could access and retrieve the clear text secrets. However, there is no evidence suggesting that the exposed secrets were transmitted to any external network. The tj-actions/changed-files action is widely used in GitHub CI/CD workflows to efficiently detect file changes within pull requests, streamlining development processes by conditionally triggering actions based on modified files. With over 23,000 active repositories and more than 1 million monthly downloads, its widespread adoption makes this compromise particularly impactful, exposing numerous organizations to potential supply chain attacks. tj-actions/changed-files According to the initial report by StepSecurity this incident was first discovered at 4PM UTC on March 14th, 2025, and isolated by 10:30 AM UTC on March 15th, 2025. But, we still warmly advise to avoid using this action until this matter is fully resolved. Initial investigation implies on a malicious commit ( hash:0e58ed8671d6b60d0890c21b07f8835ace038e67 ), and a retroactive compromise of multiple versions, possibly all versions. hash:0e58ed8671d6b60d0890c21b07f8835ace038e67 The attackers introduced malicious JavaScript code directly into the dist/index.</description></item><item><title>🚀 Announcing the Kubeflow Spark Operator Benchmarking Results</title><link>https://kubermates.org/docs/2025-03-15-announcing-the-kubeflow-spark-operator-benchmarking-results/</link><pubDate>Sat, 15 Mar 2025 00:00:00 -0500</pubDate><guid>https://kubermates.org/docs/2025-03-15-announcing-the-kubeflow-spark-operator-benchmarking-results/</guid><description>🔍 What’s Included? ❌ The Challenges: Why Benchmarking Matters 🛠 Tuning Best Practices for Spark Operator Deploy Multiple Spark Operator Instances Disable Webhooks for Faster Job Starts Increase Controller Workers Enable a Batch Scheduler (Volcano / YuniKorn) Optimize API Server Scaling Distribute Spark Jobs Across Multiple Namespaces Monitor &amp;amp; Tune Using the Open-Source Grafana Dashboard 📖 Learn More &amp;amp; Get Started Kubernetes has become the go-to platform for running large-scale Apache Spark workloads. But as workloads scale, how do you ensure your Spark jobs run efficiently without hitting bottlenecks? Managing thousands of concurrent Spark jobs can introduce severe performance challenges —from CPU saturation in the Spark Operator to Kubernetes API slowdowns and job scheduling inefficiencies. To address these challenges, we are excited to introduce the Kubeflow Spark Operator Benchmarking Results and Toolkit —a comprehensive framework to analyze performance, pinpoint bottlenecks, and optimize your Spark on Kubernetes deployments. This benchmarking effort provides three key outcomes to help you take full control of your Spark on Kubernetes deployment: ✅ Benchmarking Results – A detailed evaluation of performance insights and tuning recommendations for large-scale Spark workloads. 🛠 Benchmarking Test Toolkit – A fully reproducible test suite to help users evaluate their own Spark Operator performance and validate improvements. 📊 Open-Sourced Grafana Dashboard – A battle-tested visualization tool designed specifically to track large-scale Spark Operator deployments, providing real-time monitoring of job processing efficiency, API latencies, and system health. Running thousands of Spark jobs on Kubernetes at scale uncovers several performance roadblocks that can cripple efficiency if left unresolved: 🚦 Spark Operator Becomes CPU-Bound : When handling thousands of Spark jobs, the controller pod maxes out CPU resources, limiting job submission rates. 🐢 High API Server Latency : As workloads scale, Kubernetes API responsiveness degrades—job status updates slow down, affecting observability and scheduling efficiency. 🕒 Webhook Overhead Slows Job Starts : Using webhooks adds ~60 seconds of extra latency per job, reducing throughput in high-concurrency environments. 💥 Namespace Overload Causes Failures : Running 6,000+ SparkApplications in a single namespace resulted in pod failures due to excessive environment variables and service object overload. 💡 So, how do you fix these issues and optimize your Spark Operator deployment? That’s where our benchmarking results and toolkit come in. Based on our benchmarking findings, we provide clear, actionable recommendations for improving Spark Operator performance at scale.</description></item><item><title>DigitalOcean Managed MongoDB now supports MongoDB 8.0</title><link>https://kubermates.org/docs/2025-03-13-digitalocean-managed-mongodb-now-supports-mongodb-8-0/</link><pubDate>Thu, 13 Mar 2025 17:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-13-digitalocean-managed-mongodb-now-supports-mongodb-8-0/</guid><description>DigitalOcean Managed MongoDB now supports MongoDB 8.0 Whatâs new in MongoDB 8.0? Why you should upgrade to MongoDB 8.0 The benefits of DigitalOcean Managed MongoDB Get started today About the author Try DigitalOcean for free Related Articles Stop Building SaaS from Scratch: Meet the SeaNotes Starter Kit Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Introducing langchain-gradient: Seamless LangChain Integration with DigitalOcean Gradientâ¢ AI Platform By Nicole Ghalwash Published: March 13, 2025 3 min read MongoDB 8.0 is here, bringing significant performance, scalability, and security enhancements to DigitalOcean Managed MongoDB. One of the most popular database engines available, MongoDB continues to evolve to meet the demands of cutting-edge applications. With MongoDB 8.0, Managed Database customers running MongoDB get improved query efficiency, expanded encryption capabilities, and optimizations that make scaling large workloads easier than ever. MongoDB 8.0 features several upgrades designed to enhance performance, security, and ease of use. Whether youâre managing high-throughput applications or looking for better query optimization, these improvements make DigitalOcean Managed MongoDB even more powerful. Higher throughput and improved replication performance : MongoDB 8.0 enhances concurrent writes during data replication, reducing bottlenecks and increasing overall update speeds. Better time-series handling : Store and manage time-series data more efficiently, enabling smoother analytics and reporting for applications that rely on time-sensitive information. Expanded client-side encryption : MongoDB 8.0 now supports range queries, enabling more flexible and secure data operations. With encrypted searches that donât expose sensitive data, it enhances both privacy and compliance. Expanded client-side encryption : MongoDB 8.0 now supports range queries, enabling more flexible and secure data operations. With encrypted searches that donât expose sensitive data, it enhances both privacy and compliance. Greater performance control : Set default maximum execution times for queries and persist query settings after restarts, providing more predictable database performance.</description></item><item><title>Blog: Spotlight on SIG Apps</title><link>https://kubermates.org/docs/2025-03-12-blog-spotlight-on-sig-apps/</link><pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-12-blog-spotlight-on-sig-apps/</guid><description>Spotlight on SIG Apps Introductions About SIG Apps Best practices and challenges Contributing to SIG Apps Looking ahead In our ongoing SIG Spotlight series, we dive into the heart of the Kubernetes project by talking to the leaders of its various Special Interest Groups (SIGs). This time, we focus on SIG Apps , the group responsible for everything related to developing, deploying, and operating applications on Kubernetes. Sandipan Panda ( DevZero ) had the opportunity to interview Maciej Szulik ( Defense Unicorns ) and Janet Kuo ( Google ), the chairs and tech leads of SIG Apps. They shared their experiences, challenges, and visions for the future of application management within the Kubernetes ecosystem. Sandipan: Hello, could you start by telling us a bit about yourself, your role, and your journey within the Kubernetes community that led to your current roles in SIG Apps? Maciej : Hey, my name is Maciej, and I’m one of the leads for SIG Apps. Aside from this role, you can also find me helping SIG CLI and also being one of the Steering Committee members. I’ve been contributing to Kubernetes since late 2014 in various areas, including controllers, apiserver, and kubectl. Janet : Certainly! I’m Janet, a Staff Software Engineer at Google, and I’ve been deeply involved with the Kubernetes project since its early days, even before the 1.0 launch in 2015. It’s been an amazing journey! My current role within the Kubernetes community is one of the chairs and tech leads of SIG Apps. My journey with SIG Apps started organically. I started with building the Deployment API and adding rolling update functionalities. I naturally gravitated towards SIG Apps and became increasingly involved.</description></item><item><title>Spotlight on SIG Apps</title><link>https://kubermates.org/docs/2025-03-12-spotlight-on-sig-apps/</link><pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-12-spotlight-on-sig-apps/</guid><description>Spotlight on SIG Apps Introductions About SIG Apps Best practices and challenges Contributing to SIG Apps Looking ahead In our ongoing SIG Spotlight series, we dive into the heart of the Kubernetes project by talking to the leaders of its various Special Interest Groups (SIGs). This time, we focus on SIG Apps , the group responsible for everything related to developing, deploying, and operating applications on Kubernetes. Sandipan Panda ( DevZero ) had the opportunity to interview Maciej Szulik ( Defense Unicorns ) and Janet Kuo ( Google ), the chairs and tech leads of SIG Apps. They shared their experiences, challenges, and visions for the future of application management within the Kubernetes ecosystem. Sandipan: Hello, could you start by telling us a bit about yourself, your role, and your journey within the Kubernetes community that led to your current roles in SIG Apps? Maciej : Hey, my name is Maciej, and I’m one of the leads for SIG Apps. Aside from this role, you can also find me helping SIG CLI and also being one of the Steering Committee members. I’ve been contributing to Kubernetes since late 2014 in various areas, including controllers, apiserver, and kubectl. Janet : Certainly! I&amp;rsquo;m Janet, a Staff Software Engineer at Google, and I&amp;rsquo;ve been deeply involved with the Kubernetes project since its early days, even before the 1.0 launch in 2015. It&amp;rsquo;s been an amazing journey! My current role within the Kubernetes community is one of the chairs and tech leads of SIG Apps. My journey with SIG Apps started organically. I started with building the Deployment API and adding rolling update functionalities. I naturally gravitated towards SIG Apps and became increasingly involved.</description></item><item><title>Stopping Sobolan Malware with Aqua Runtime Protection</title><link>https://kubermates.org/docs/2025-03-11-stopping-sobolan-malware-with-aqua-runtime-protection/</link><pubDate>Tue, 11 Mar 2025 14:01:42 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-11-stopping-sobolan-malware-with-aqua-runtime-protection/</guid><description>Stopping Sobolan Malware with Aqua Runtime Protection The Attacked Workload Mapping the Attack Flow Detailed Summary of the Blocked Events Summary Indications of Compromise (IOCs) Aqua Nautilus researchers have discovered a new attack campaign targeting interactive computing environments such as Jupyter Notebooks. The attack consists of multiple stages, beginning with the download of a compressed file from a remote server. Once executed, the attacker deploys several malicious tools to exploit the server and establish persistence. This campaign poses a significant risk to cloud-native environments, as it enables unauthorized access and long-term control over compromised systems. Jupyter Notebooks In this blog, we will outline the attack stages, discuss the potential risks, and provide recommendations to strengthen security and prevent exploitation. Interactive Computing Environments or Notebook Interfaces are platforms designed for data scientists and programmers to write, execute, and analyze code interactively. There are many products available, including Jupyter Notebook , JupyterLab , Apache Zeppelin , Google Colab , Databricks Notebooks , and others. These environments are often connected to the internet and require authentication to access data or execute code. However, a simple misconfiguration can sometimes expose the server to malicious activity by hackers. Jupyter Notebook JupyterLab Apache Zeppelin Google Colab Databricks Notebooks Figure 1: Sobolan campaign attack flow The attackers gained initial access through an unauthenticated JupyterLab instance, allowing them to deploy malware and cryptominers. They first downloaded and extracted a compressed archive containing 13 malicious files, consisting of both binaries and shell scripts. Once executed, these scripts initiated multiple processes to establish persistence, hijack system resources for cryptomining, and evade detection (as shown in Figure 1).</description></item><item><title>Introducing a Managed Component for Maintaining Host Routes in Kubernetes</title><link>https://kubermates.org/docs/2025-03-10-introducing-a-managed-component-for-maintaining-host-routes-in-kubernetes/</link><pubDate>Mon, 10 Mar 2025 19:39:28 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-10-introducing-a-managed-component-for-maintaining-host-routes-in-kubernetes/</guid><description>Introducing a Managed Component for Maintaining Host Routes in Kubernetes Key Features of the DOKS Routing Agent 1. Static Route Management via Custom Resources 2. Support for Multiple Gateways and ECMP 3. Overriding Default Routes 4. Node Selection for Routes Enabling the DOKS Routing Agent Example Commands: Usage for Static Egress IP Why This Matters: Coming Soon: Fully Managed NAT Gateway Simplify Static Route Management Get started today About the author Try DigitalOcean for free Related Articles Stop Building SaaS from Scratch: Meet the SeaNotes Starter Kit Announcing OpenAI gpt-oss Models on the DigitalOcean Gradientâ¢ AI Platform Introducing langchain-gradient: Seamless LangChain Integration with DigitalOcean Gradientâ¢ AI Platform By Marco Jantke Published: March 10, 2025 3 min read Our new DOKS routing agent is a managed component for configuring static routes on Kubernetes worker nodes. It is a direct response to user feedback on its predecessor, the static route operator, and introduces new features to enhance routing flexibility. Despite being a managed component, the DOKS routing agent is included at no additional cost for users. The DOKS routing agent enables users to configure IP routes on their Kubernetes worker nodes using a dedicated Kubernetes Custom Resource. This is particularly useful for VPN setups or tunneling egress traffic through specific gateway nodes. apiVersion : networking. doks. digitalocean.</description></item><item><title>Blog: Spotlight on SIG etcd</title><link>https://kubermates.org/docs/2025-03-04-blog-spotlight-on-sig-etcd/</link><pubDate>Tue, 04 Mar 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-04-blog-spotlight-on-sig-etcd/</guid><description>Spotlight on SIG etcd Introducing SIG etcd Becoming a Kubernetes Special Interest Group (SIG) The road ahead Getting involved In this SIG etcd spotlight we talked with James Blair , Marek Siarkowicz , Wenjia Zhang , and Benjamin Wang to learn a bit more about this Kubernetes Special Interest Group. Frederico: Hello, thank you for the time! Let’s start with some introductions, could you tell us a bit about yourself, your role and how you got involved in Kubernetes. Benjamin: Hello, I am Benjamin. I am a SIG etcd Tech Lead and one of the etcd maintainers. I work for VMware, which is part of the Broadcom group. I got involved in Kubernetes &amp;amp; etcd &amp;amp; CSI ( Container Storage Interface ) because of work and also a big passion for open source. I have been working on Kubernetes &amp;amp; etcd (and also CSI) since 2020. James: Hey team, I’m James, a co-chair for SIG etcd and etcd maintainer. I work at Red Hat as a Specialist Architect helping people adopt cloud native technology. I got involved with the Kubernetes ecosystem in 2019. Around the end of 2022 I noticed how the etcd community and project needed help so started contributing as often as I could. There is a saying in our community that “you come for the technology, and stay for the people”: for me this is absolutely real, it’s been a wonderful journey so far and I’m excited to support our community moving forward.</description></item><item><title>Spotlight on SIG etcd</title><link>https://kubermates.org/docs/2025-03-04-spotlight-on-sig-etcd/</link><pubDate>Tue, 04 Mar 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-03-04-spotlight-on-sig-etcd/</guid><description>Spotlight on SIG etcd Introducing SIG etcd Becoming a Kubernetes Special Interest Group (SIG) The road ahead Getting involved In this SIG etcd spotlight we talked with James Blair , Marek Siarkowicz , Wenjia Zhang , and Benjamin Wang to learn a bit more about this Kubernetes Special Interest Group. Frederico: Hello, thank you for the time! Let’s start with some introductions, could you tell us a bit about yourself, your role and how you got involved in Kubernetes. Benjamin: Hello, I am Benjamin. I am a SIG etcd Tech Lead and one of the etcd maintainers. I work for VMware, which is part of the Broadcom group. I got involved in Kubernetes &amp;amp; etcd &amp;amp; CSI ( Container Storage Interface ) because of work and also a big passion for open source. I have been working on Kubernetes &amp;amp; etcd (and also CSI) since 2020. James: Hey team, I’m James, a co-chair for SIG etcd and etcd maintainer. I work at Red Hat as a Specialist Architect helping people adopt cloud native technology. I got involved with the Kubernetes ecosystem in 2019. Around the end of 2022 I noticed how the etcd community and project needed help so started contributing as often as I could. There is a saying in our community that &amp;ldquo;you come for the technology, and stay for the people&amp;rdquo;: for me this is absolutely real, it’s been a wonderful journey so far and I’m excited to support our community moving forward.</description></item><item><title>NFTables mode for kube-proxy</title><link>https://kubermates.org/docs/2025-02-28-nftables-mode-for-kube-proxy/</link><pubDate>Fri, 28 Feb 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-02-28-nftables-mode-for-kube-proxy/</guid><description>NFTables mode for kube-proxy Why nftables? Part 1: data plane latency Why nftables? Part 2: control plane latency Why not nftables? Trying out nftables mode Future plans Learn more A new nftables mode for kube-proxy was introduced as an alpha feature in Kubernetes 1.29. Currently in beta, it is expected to be GA as of 1.33. The new mode fixes long-standing performance problems with the iptables mode and all users running on systems with reasonably-recent kernels are encouraged to try it out. (For compatibility reasons, even once nftables becomes GA, iptables will still be the default. ) The iptables API was designed for implementing simple firewalls, and has problems scaling up to support Service proxying in a large Kubernetes cluster with tens of thousands of Services. In general, the ruleset generated by kube-proxy in iptables mode has a number of iptables rules proportional to the sum of the number of Services and the total number of endpoints. In particular, at the top level of the ruleset, there is one rule to test each possible Service IP (and port) that a packet might be addressed to: # If the packet is addressed to 172.30.0.41:80, then jump to the chain # KUBE-SVC-XPGD46QRK7WJZT7O for further processing -A KUBE-SERVICES -m comment &amp;ndash;comment &amp;ldquo;namespace1/service1:p80 cluster IP&amp;rdquo; -m tcp -p tcp -d 172.30.0.41 &amp;ndash;dport 80 -j KUBE-SVC-XPGD46QRK7WJZT7O # If the packet is addressed to 172.30.0.42:443, then. -A KUBE-SERVICES -m comment &amp;ndash;comment &amp;ldquo;namespace2/service2:p443 cluster IP&amp;rdquo; -m tcp -p tcp -d 172.30.0.42 &amp;ndash;dport 443 -j KUBE-SVC-GNZBNJ2PO5MGZ6GT # etc. -A KUBE-SERVICES -m comment &amp;ndash;comment &amp;ldquo;namespace3/service3:p80 cluster IP&amp;rdquo; -m tcp -p tcp -d 172.30.0.43 &amp;ndash;dport 80 -j KUBE-SVC-X27LE4BHSL4DOUIK # If the packet is addressed to 172.30.0.41:80, then jump to the chain # KUBE-SVC-XPGD46QRK7WJZT7O for further processing -A KUBE-SERVICES -m comment &amp;ndash;comment &amp;ldquo;namespace1/service1:p80 cluster IP&amp;rdquo; -m tcp -p tcp -d 172.30.0.41 &amp;ndash;dport 80 -j KUBE-SVC-XPGD46QRK7WJZT7O # If the packet is addressed to 172.30.0.42:443, then. -A KUBE-SERVICES -m comment &amp;ndash;comment &amp;ldquo;namespace2/service2:p443 cluster IP&amp;rdquo; -m tcp -p tcp -d 172.30.0.42 &amp;ndash;dport 443 -j KUBE-SVC-GNZBNJ2PO5MGZ6GT # etc. -A KUBE-SERVICES -m comment &amp;ndash;comment &amp;ldquo;namespace3/service3:p80 cluster IP&amp;rdquo; -m tcp -p tcp -d 172.30.0.43 &amp;ndash;dport 80 -j KUBE-SVC-X27LE4BHSL4DOUIK This means that when a packet comes in, the time it takes the kernel to check it against all of the Service rules is O(n) in the number of Services. As the number of Services increases, both the average and the worst-case latency for the first packet of a new connection increases (with the difference between best-case, average, and worst-case being mostly determined by whether a given Service IP address appears earlier or later in the KUBE-SERVICES chain).</description></item><item><title>Optimizing RAG Pipelines with Katib: Hyperparameter Tuning for Better Retrieval &amp;amp; Generation</title><link>https://kubermates.org/docs/2025-02-21-optimizing-rag-pipelines-with-katib-hyperparameter-tuning-for-better-retrieval-a/</link><pubDate>Fri, 21 Feb 2025 00:00:00 -0600</pubDate><guid>https://kubermates.org/docs/2025-02-21-optimizing-rag-pipelines-with-katib-hyperparameter-tuning-for-better-retrieval-a/</guid><description>Introduction Let’s Get Started! STEP 1: Setup STEP 2: Implementing RAG pipeline Implementation Details: STEP 3: Run a Katib Experiment Define hyperparameter search space Conclusion Introduction Let’s Get Started! STEP 1: Setup STEP 2: Implementing RAG pipeline Implementation Details: STEP 3: Run a Katib Experiment STEP 1: Setup STEP 2: Implementing RAG pipeline Implementation Details: Implementation Details: STEP 3: Run a Katib Experiment Define hyperparameter search space Conclusion As artificial intelligence and machine learning models become more sophisticated, optimising their performance remains a critical challenge. Kubeflow provides a robust component, Katib , designed for hyperparameter optimization and neural architecture search. As a part of the Kubeflow ecosystem, Katib enables scalable, automated tuning of underlying machine learning models, reducing the manual effort required for parameter selection while improving model performance across diverse ML workflows. With Retrieval-Augmented Generation ( RAG ) becoming an increasingly popular approach for improving search and retrieval quality, optimizing its parameters is essential to achieving high-quality results. RAG pipelines involve multiple hyperparameters that influence retrieval accuracy, hallucination reduction, and language generation quality. In this blog, we will explore how Katib can be leveraged to fine-tune a RAG pipeline, ensuring optimal performance by systematically adjusting key hyperparameters. Since compute resources are scarcer than a perfectly labeled dataset :), we’ll use a lightweight Kind cluster (Kubernetes in Docker) cluster to run this example locally. Rest assured, this setup can seamlessly scale to larger clusters by increasing the dataset size and the number of hyperparameters to tune. To get started, we’ll first install the Katib control plane in our cluster by following the steps outlined in the documentation. In this implementation, we use a retriever model , which encodes queries and documents into vector representations to find the most relevant matches, to fetch relevant documents based on a query and a generator model to produce coherent text responses. Retriever: Sentence Transformer &amp;amp; FAISS (Facebook AI Similarity Search) Index A SentenceTransformer model (paraphrase-MiniLM-L6-v2) encodes predefined documents into vector representations. FAISS is used to index these document embeddings and perform efficient similarity searches to retrieve the most relevant documents.</description></item><item><title>Synthetic Data Generation with Kubeflow Pipelines</title><link>https://kubermates.org/docs/2025-02-16-synthetic-data-generation-with-kubeflow-pipelines/</link><pubDate>Sun, 16 Feb 2025 00:00:00 -0600</pubDate><guid>https://kubermates.org/docs/2025-02-16-synthetic-data-generation-with-kubeflow-pipelines/</guid><description>Synthetic Data Generation - Why and How? Key Benefits of Using Synthetic Data Frameworks for Creating Synthetic Data The Synthetic Data Vault (SDV) Evaluation Criteria for Synthetic Data Our On-Premise Analytics Platform: ARCUS Needed environment to create synthetic data Exploring the Creation and Usefulness of Synthetic Data Using Synthetic Data Generators to Enable Multiple Environments without Data Transfer Summary Synthetic Data Generation - Why and How? Key Benefits of Using Synthetic Data Frameworks for Creating Synthetic Data The Synthetic Data Vault (SDV) Evaluation Criteria for Synthetic Data Our On-Premise Analytics Platform: ARCUS Needed environment to create synthetic data Parallelism needed Parallelism needed Exploring the Creation and Usefulness of Synthetic Data Using Synthetic Data Generators to Enable Multiple Environments without Data Transfer On-premise Cloud On-premise On-premise Cloud On-premise Summary When creating insights, decisions, and actions from data, the best results come from real data. But accessing real data often requires lengthy security and legal processes. The data may also be incomplete, biased, or too small, and during early exploration, we may not even know if it’s worth pursuing. While real data is essential for proper evaluation, gaps or limited access frequently hinder progress until the formal process is complete. To address these challenges, synthetic data provides an alternative. It mimics real data’s statistical properties while preserving privacy and accessibility. Synthetic data generators (synthesizers) are models trained on real data to generate new datasets that follow the same statistical distributions and relationships but do not contain real records. This allows for accelerated development, improved data availability, and enhanced privacy. Depending on the technique used, synthetic data not only mirrors statistical base properties of real data but also preserves correlations between features. These synthesizers — such as those based on Gaussian Copulas, Generative Adversarial Networks (GANs), and Variational Autoencoders (VAEs) — enable the creation of high-fidelity synthetic datasets. See more description of these techniques below. While the above focuses on speed of development in general, and augmentation of data to improve performance of analytical modes, there are more motivations for creating (synthetic) data: Enhanced Privacy and Security Mimics real datasets without containing sensitive or personally identifiable information, mitigating privacy risks and ensuring compliance with regulations like GDPR.</description></item><item><title>The Cloud Controller Manager Chicken and Egg Problem</title><link>https://kubermates.org/docs/2025-02-14-the-cloud-controller-manager-chicken-and-egg-problem/</link><pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-02-14-the-cloud-controller-manager-chicken-and-egg-problem/</guid><description>The Cloud Controller Manager Chicken and Egg Problem Examples of the dependency problem Example: Cloud controller manager not scheduling due to uninitialized taint Example: Cloud controller manager not scheduling due to not-ready taint Our Recommendations Example Kubernetes 1.31 completed the largest migration in Kubernetes history , removing the in-tree cloud provider. While the component migration is now done, this leaves some additional complexity for users and installer projects (for example, kOps or Cluster API). We will go over those additional steps and failure points and make recommendations for cluster owners. This migration was complex and some logic had to be extracted from the core components, building four new subsystems. Cloud controller manager ( KEP-2392 ) API server network proxy ( KEP-1281 ) kubelet credential provider plugins ( KEP-2133 ) Storage migration to use CSI ( KEP-625 ) The cloud controller manager is part of the control plane. It is a critical component that replaces some functionality that existed previously in the kube-controller-manager and the kubelet. Components of Kubernetes One of the most critical functionalities of the cloud controller manager is the node controller, which is responsible for the initialization of the nodes. As you can see in the following diagram, when the kubelet starts, it registers the Node object with the apiserver, Tainting the node so it can be processed first by the cloud-controller-manager. The initial Node is missing the cloud-provider specific information, like the Node Addresses and the Labels with the cloud provider specific information like the Node, Region and Instance type information. Chicken and egg problem sequence diagram This new initialization process adds some latency to the node readiness. Previously, the kubelet was able to initialize the node at the same time it created the node. Since the logic has moved to the cloud-controller-manager, this can cause a chicken and egg problem during the cluster bootstrapping for those Kubernetes architectures that do not deploy the controller manager as the other components of the control plane, commonly as static pods, standalone binaries or daemonsets/deployments with tolerations to the taints and using hostNetwork (more on this below) hostNetwork As noted above, it is possible during bootstrapping for the cloud-controller-manager to be unschedulable and as such the cluster will not initialize properly.</description></item><item><title>Blog: Spotlight on SIG Architecture: Enhancements</title><link>https://kubermates.org/docs/2025-01-21-blog-spotlight-on-sig-architecture-enhancements/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-01-21-blog-spotlight-on-sig-architecture-enhancements/</guid><description>Spotlight on SIG Architecture: Enhancements The Enhancements subproject The KEP and its impact Current areas of focus Getting involved This is the fourth interview of a SIG Architecture Spotlight series that will cover the different subprojects, and we will be covering SIG Architecture: Enhancements. In this SIG Architecture spotlight we talked with Kirsten Garrison , lead of the Enhancements subproject. Frederico (FSM): Hi Kirsten, very happy to have the opportunity to talk about the Enhancements subproject. Let’s start with some quick information about yourself and your role. Kirsten Garrison (KG) : I’m a lead of the Enhancements subproject of SIG-Architecture and currently work at Google. I first got involved by contributing to the service-catalog project with the help of Carolyn Van Slyck. With time, I joined the Release team , eventually becoming the Enhancements Lead and a Release Lead shadow. While on the release team, I worked on some ideas to make the process better for the SIGs and Enhancements team (the opt-in process) based on my team’s experiences. Eventually, I started attending Subproject meetings and contributing to the Subproject’s work. FSM: You mentioned the Enhancements subproject: how would you describe its main goals and areas of intervention? KG : The Enhancements Subproject primarily concerns itself with the Kubernetes Enhancement Proposal ( KEP for short)—the “design” documents required for all features and significant changes to the Kubernetes project. FSM: The improvement of the KEP process was (and is) one in which SIG Architecture was heavily involved. Could you explain the process to those that aren’t aware of it? KG : Every release , the SIGs let the Release Team know which features they intend to work on to be put into the release.</description></item><item><title>Demystifying the OpenTelemetry Operator: Observing Kubernetes applications without writing code</title><link>https://kubermates.org/docs/2025-01-21-demystifying-the-opentelemetry-operator-observing-kubernetes-applications-withou/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-01-21-demystifying-the-opentelemetry-operator-observing-kubernetes-applications-withou/</guid><description>How to monitor your homelab with Beyla, eBPF, and OpenTelemetry Colin Steele Â· 22 Aug 2025 Â· 5 min read Learn how Beyla, eBPF, and OpenTelemetry combine to make homelab observability easy with this recap of a recent GrafanaCON 2025 session.</description></item><item><title>Spotlight on SIG Architecture: Enhancements</title><link>https://kubermates.org/docs/2025-01-21-spotlight-on-sig-architecture-enhancements/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2025-01-21-spotlight-on-sig-architecture-enhancements/</guid><description>Spotlight on SIG Architecture: Enhancements The Enhancements subproject The KEP and its impact Current areas of focus Getting involved This is the fourth interview of a SIG Architecture Spotlight series that will cover the different subprojects, and we will be covering SIG Architecture: Enhancements. In this SIG Architecture spotlight we talked with Kirsten Garrison , lead of the Enhancements subproject. Frederico (FSM): Hi Kirsten, very happy to have the opportunity to talk about the Enhancements subproject. Let&amp;rsquo;s start with some quick information about yourself and your role. Kirsten Garrison (KG) : I’m a lead of the Enhancements subproject of SIG-Architecture and currently work at Google. I first got involved by contributing to the service-catalog project with the help of Carolyn Van Slyck. With time, I joined the Release team , eventually becoming the Enhancements Lead and a Release Lead shadow. While on the release team, I worked on some ideas to make the process better for the SIGs and Enhancements team (the opt-in process) based on my team’s experiences. Eventually, I started attending Subproject meetings and contributing to the Subproject’s work. FSM: You mentioned the Enhancements subproject: how would you describe its main goals and areas of intervention? KG : The Enhancements Subproject primarily concerns itself with the Kubernetes Enhancement Proposal ( KEP for short)—the &amp;ldquo;design&amp;rdquo; documents required for all features and significant changes to the Kubernetes project. FSM: The improvement of the KEP process was (and is) one in which SIG Architecture was heavily involved. Could you explain the process to those that aren’t aware of it? KG : Every release , the SIGs let the Release Team know which features they intend to work on to be put into the release.</description></item><item><title>Kubernetes 1.32: Moving Volume Group Snapshots to Beta</title><link>https://kubermates.org/docs/2024-12-18-kubernetes-1-32-moving-volume-group-snapshots-to-beta/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-12-18-kubernetes-1-32-moving-volume-group-snapshots-to-beta/</guid><description>Kubernetes 1.32: Moving Volume Group Snapshots to Beta An overview of volume group snapshots Why add volume group snapshots to Kubernetes? Kubernetes APIs for volume group snapshots What components are needed to support volume group snapshots What&amp;rsquo;s new in Beta? How do I use Kubernetes volume group snapshots Creating a new group snapshot with Kubernetes How to use group snapshot for restore in Kubernetes As a storage vendor, how do I add support for group snapshots to my CSI driver? What are the limitations? What’s next? How do I get involved? Volume group snapshots were introduced as an Alpha feature with the Kubernetes 1.27 release. The recent release of Kubernetes v1.32 moved that support to beta. The support for volume group snapshots relies on a set of extension APIs for group snapshots. These APIs allow users to take crash consistent snapshots for a set of volumes. Behind the scenes, Kubernetes uses a label selector to group multiple PersistentVolumeClaims for snapshotting. A key aim is to allow you restore that set of snapshots to new volumes and recover your workload based on a crash consistent recovery point. This new feature is only supported for CSI volume drivers. Some storage systems provide the ability to create a crash consistent snapshot of multiple volumes. A group snapshot represents copies made from multiple volumes, that are taken at the same point-in-time. A group snapshot can be used either to rehydrate new volumes (pre-populated with the snapshot data) or to restore existing volumes to a previous state (represented by the snapshots). The Kubernetes volume plugin system already provides a powerful abstraction that automates the provisioning, attaching, mounting, resizing, and snapshotting of block and file storage. Underpinning all these features is the Kubernetes goal of workload portability: Kubernetes aims to create an abstraction layer between distributed applications and underlying clusters so that applications can be agnostic to the specifics of the cluster they run on and application deployment requires no cluster specific knowledge.</description></item><item><title>Enhancing Kubernetes API Server Efficiency with API Streaming</title><link>https://kubermates.org/docs/2024-12-17-enhancing-kubernetes-api-server-efficiency-with-api-streaming/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-12-17-enhancing-kubernetes-api-server-efficiency-with-api-streaming/</guid><description>Enhancing Kubernetes API Server Efficiency with API Streaming Why does kube-apiserver allocate so much memory for list requests? Streaming list requests Enabling API Streaming for your component What&amp;rsquo;s next? The synthetic test Kubernetes 1.33 update Managing Kubernetes clusters efficiently is critical, especially as their size is growing. A significant challenge with large clusters is the memory overhead caused by list requests. In the existing implementation, the kube-apiserver processes list requests by assembling the entire response in-memory before transmitting any data to the client. But what if the response body is substantial, say hundreds of megabytes? Additionally, imagine a scenario where multiple list requests flood in simultaneously, perhaps after a brief network outage. While API Priority and Fairness has proven to reasonably protect kube-apiserver from CPU overload, its impact is visibly smaller for memory protection. This can be explained by the differing nature of resource consumption by a single API request - the CPU usage at any given time is capped by a constant, whereas memory, being uncompressible, can grow proportionally with the number of processed objects and is unbounded. This situation poses a genuine risk, potentially overwhelming and crashing any kube-apiserver within seconds due to out-of-memory (OOM) conditions. To better visualize the issue, let&amp;rsquo;s consider the below graph. The graph shows the memory usage of a kube-apiserver during a synthetic test. (see the synthetic test section for more details). The results clearly show that increasing the number of informers significantly boosts the server&amp;rsquo;s memory consumption. Notably, at approximately 16:40, the server crashed when serving only 16 informers.</description></item><item><title>Kubernetes v1.32 Adds A New CPU Manager Static Policy Option For Strict CPU Reservation</title><link>https://kubermates.org/docs/2024-12-16-kubernetes-v1-32-adds-a-new-cpu-manager-static-policy-option-for-strict-cpu-rese/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-12-16-kubernetes-v1-32-adds-a-new-cpu-manager-static-policy-option-for-strict-cpu-rese/</guid><description>Kubernetes v1.32 Adds A New CPU Manager Static Policy Option For Strict CPU Reservation Understanding the feature Enabling the feature Monitoring the feature Conclusion Further reading Getting involved In Kubernetes v1.32, after years of community discussion, we are excited to introduce a strict-cpu-reservation option for the CPU Manager static policy. This feature is currently in alpha, with the associated policy hidden by default. You can only use the policy if you explicitly enable the alpha behavior in your cluster. strict-cpu-reservation The CPU Manager static policy is used to reduce latency or improve performance. The reservedSystemCPUs defines an explicit CPU set for OS system daemons and kubernetes system daemons. This option is designed for Telco/NFV type use cases where uncontrolled interrupts/timers may impact the workload performance. you can use this option to define the explicit cpuset for the system/kubernetes daemons as well as the interrupts/timers, so the rest CPUs on the system can be used exclusively for workloads, with less impact from uncontrolled interrupts/timers. More details of this parameter can be found on the Explicitly Reserved CPU List page. reservedSystemCPUs If you want to protect your system daemons and interrupt processing, the obvious way is to use the reservedSystemCPUs option. reservedSystemCPUs However, until the Kubernetes v1.32 release, this isolation was only implemented for guaranteed pods that made requests for a whole number of CPUs. At pod admission time, the kubelet only compares the CPU requests against the allocatable CPUs. In Kubernetes, limits can be higher than the requests; the previous implementation allowed burstable and best-effort pods to use up the capacity of reservedSystemCPUs , which could then starve host OS services of CPU - and we know that people saw this in real life deployments.</description></item><item><title>Kubernetes v1.32: Memory Manager Goes GA</title><link>https://kubermates.org/docs/2024-12-13-kubernetes-v1-32-memory-manager-goes-ga/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-12-13-kubernetes-v1-32-memory-manager-goes-ga/</guid><description>Kubernetes v1.32: Memory Manager Goes GA Observability improvements Improving memory manager reliability and consistency Future development Getting involved With Kubernetes 1.32, the memory manager has officially graduated to General Availability (GA), marking a significant milestone in the journey toward efficient and predictable memory allocation for containerized applications. Since Kubernetes v1.22, where it graduated to beta, the memory manager has proved itself reliable, stable and a good complementary feature for the CPU Manager. As part of kubelet&amp;rsquo;s workload admission process, the memory manager provides topology hints to optimize memory allocation and alignment. This enables users to allocate exclusive memory for Pods in the Guaranteed QoS class. More details about the process can be found in the memory manager goes to beta blog. Most of the changes introduced since the Beta are bug fixes, internal refactoring and observability improvements, such as metrics and better logging. As part of the effort to increase the observability of memory manager, new metrics have been added to provide some statistics on memory allocation patterns. memory_manager_pinning_requests_total - tracks the number of times the pod spec required the memory manager to pin memory pages. memory_manager_pinning_errors_total - tracks the number of times the pod spec required the memory manager to pin memory pages, but the allocation failed. The kubelet does not guarantee pod ordering when admitting pods after a restart or reboot. In certain edge cases, this behavior could cause the memory manager to reject some pods, and in more extreme cases, it may cause kubelet to fail upon restart. Previously, the beta implementation lacked certain checks and logic to prevent these issues.</description></item><item><title>Kubernetes v1.32: QueueingHint Brings a New Possibility to Optimize Pod Scheduling</title><link>https://kubermates.org/docs/2024-12-12-kubernetes-v1-32-queueinghint-brings-a-new-possibility-to-optimize-pod-schedulin/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-12-12-kubernetes-v1-32-queueinghint-brings-a-new-possibility-to-optimize-pod-schedulin/</guid><description>Kubernetes v1.32: QueueingHint Brings a New Possibility to Optimize Pod Scheduling Scheduling queue Scheduling framework and plugins Improvements to retrying Pod scheduling with QueuingHint QueueingHint&amp;rsquo;s history and what&amp;rsquo;s new in v1.32 Getting involved How can I learn more? The Kubernetes scheduler is the core component that selects the nodes on which new Pods run. The scheduler processes these new Pods one by one. Therefore, the larger your clusters, the more important the throughput of the scheduler becomes. Over the years, Kubernetes SIG Scheduling has improved the throughput of the scheduler in multiple enhancements. This blog post describes a major improvement to the scheduler in Kubernetes v1.32: a scheduling context element named QueueingHint. This page provides background knowledge of the scheduler and explains how QueueingHint improves scheduling throughput. The scheduler stores all unscheduled Pods in an internal component called the scheduling queue. The scheduling queue consists of the following data structures: ActiveQ : holds newly created Pods or Pods that are ready to be retried for scheduling. BackoffQ : holds Pods that are ready to be retried but are waiting for a backoff period to end. The backoff period depends on the number of unsuccessful scheduling attempts performed by the scheduler on that Pod. Unschedulable Pod Pool : holds Pods that the scheduler won&amp;rsquo;t attempt to schedule for one of the following reasons: The scheduler previously attempted and was unable to schedule the Pods. Since that attempt, the cluster hasn&amp;rsquo;t changed in a way that could make those Pods schedulable.</description></item><item><title>Kubernetes v1.32: Penelope</title><link>https://kubermates.org/docs/2024-12-11-kubernetes-v1-32-penelope/</link><pubDate>Wed, 11 Dec 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-12-11-kubernetes-v1-32-penelope/</guid><description>Kubernetes v1.32: Penelope Release theme and logo Updates to recent key features A note on DRA enhancements Quality of life improvements on nodes and sidecar containers update Highlights of features graduating to Stable Custom Resource field selectors Support to size memory backed volumes Bound service account token improvement Structured authorization configuration Auto remove PVCs created by StatefulSet Highlights of features graduating to Beta Job API managed-by mechanism Only allow anonymous auth for configured endpoints Per-plugin callback functions for accurate requeueing in kube-scheduler enhancements Recover from volume expansion failure Volume group snapshot Structured parameter support Label and field selector authorization Highlights of new features in Alpha Asynchronous preemption in the Kubernetes Scheduler Mutating admission policies using CEL expressions Pod-level resource specifications Allow zero value for sleep action of PreStop hook DRA: Standardized network interface data for resource claim status New statusz and flagz endpoints for core components Windows strikes back! Graduations, deprecations, and removals in 1.32 Graduations to Stable Deprecations and removals Release notes and upgrade actions required Availability Release team Project velocity Event updates Upcoming release webinar Get involved Editors: Matteo Bianchi, Edith Puclla, William Rizzo, Ryota Sawada, Rashan Smith Announcing the release of Kubernetes v1.32: Penelope! In line with previous releases, the release of Kubernetes v1.32 introduces new stable, beta, and alpha features. The consistent delivery of high-quality releases underscores the strength of our development cycle and the vibrant support from our community. This release consists of 44 enhancements in total. Of those enhancements, 13 have graduated to Stable, 12 are entering Beta, and 19 have entered in Alpha. The Kubernetes v1.32 Release Theme is &amp;ldquo;Penelope&amp;rdquo;. If Kubernetes is Ancient Greek for &amp;ldquo;pilot&amp;rdquo;, in this release we start from that origin and reflect on the last 10 years of Kubernetes and our accomplishments: each release cycle is a journey, and just like Penelope, in &amp;ldquo;The Odyssey&amp;rdquo;, weaved for 10 years &amp;ndash; each night removing parts of what she had done during the day &amp;ndash; so does each release add new features and removes others, albeit here with a much clearer purpose of constantly improving Kubernetes. With v1.32 being the last release in the year Kubernetes marks its first decade anniversary, we wanted to honour all of those that have been part of the global Kubernetes crew that roams the cloud-native seas through perils and challanges: may we continue to weave the future of Kubernetes together. In this release, like the previous one, the Kubernetes project continues proposing a number of enhancements to the Dynamic Resource Allocation (DRA), a key component of the Kubernetes resource management system. These enhancements aim to improve the flexibility and efficiency of resource allocation for workloads that require specialized hardware, such as GPUs, FPGAs and network adapters. These features are particularly useful for use-cases such as machine learning or high-performance computing applications. The core part enabling DRA Structured parameter support got promoted to beta. SIG Node has the following highlights that go beyond KEPs: The systemd watchdog capability is now used to restart the kubelet when its health check fails, while also limiting the maximum number of restarts within a given time period.</description></item><item><title>Gateway API v1.2: WebSockets, Timeouts, Retries, and More</title><link>https://kubermates.org/docs/2024-11-21-gateway-api-v1-2-websockets-timeouts-retries-and-more/</link><pubDate>Thu, 21 Nov 2024 09:00:00 -0800</pubDate><guid>https://kubermates.org/docs/2024-11-21-gateway-api-v1-2-websockets-timeouts-retries-and-more/</guid><description>Gateway API v1.2: WebSockets, Timeouts, Retries, and More Breaking changes GRPCRoute and ReferenceGrant v1alpha2 removal Change to. status. supportedFeatures (experimental) Graduations to the standard channel HTTPRoute timeouts Gateway infrastructure labels and annotations Backend protocol support New additions to experimental channel Named rules for *Route resources HTTPRoute retry support HTTPRoute percentage-based mirroring Additional backend TLS configuration More changes Project updates Release process improvements gwctl moves out Maintainer changes Try it out Get involved Related Kubernetes blog articles Kubernetes SIG Network is delighted to announce the general availability of Gateway API v1.2! This version of the API was released on October 3, and we&amp;rsquo;re delighted to report that we now have a number of conformant implementations of it for you to try out. Gateway API v1.2 brings a number of new features to the Standard channel (Gateway API&amp;rsquo;s GA release channel), introduces some new experimental features, and inaugurates our new release process — but it also brings two breaking changes that you&amp;rsquo;ll want to be careful of. v1alpha2 Now that the v1 versions of GRPCRoute and ReferenceGrant have graduated to Standard, the old v1alpha2 versions have been removed from both the Standard and Experimental channels, in order to ease the maintenance burden that perpetually supporting the old versions would place on the Gateway API community. v1 v1alpha2 Before upgrading to Gateway API v1.2, you&amp;rsquo;ll want to confirm that any implementations of Gateway API have been upgraded to support the v1 API version of these resources instead of the v1alpha2 API version. Note that even if you&amp;rsquo;ve been using v1 in your YAML manifests, a controller may still be using v1alpha2 which would cause it to fail during this upgrade. Additionally, Kubernetes itself goes to some effort to stop you from removing a CRD version that it thinks you&amp;rsquo;re using: check out the release notes for more information about what you need to do to safely upgrade. status. supportedFeatures A much smaller breaking change:. status. supportedFeatures in a Gateway is now a list of objects instead of a list of strings.</description></item><item><title>How we built a dynamic Kubernetes API Server for the API Aggregation Layer in Cozystack</title><link>https://kubermates.org/docs/2024-11-21-how-we-built-a-dynamic-kubernetes-api-server-for-the-api-aggregation-layer-in-co/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-11-21-how-we-built-a-dynamic-kubernetes-api-server-for-the-api-aggregation-layer-in-co/</guid><description>How we built a dynamic Kubernetes API Server for the API Aggregation Layer in Cozystack What Is the API Aggregation Layer? When to use the API Aggregation Layer Imperative Logic and Subresources You&amp;rsquo;re not tied to use etcd One-Time resources Full control over conversion, validation, and output formatting Dynamic resource registration When not to use the API Aggregation Layer Unstable backend Slow requests Why we needed it in Cozystack Limitations of the RBAC model Need for a public API Two-Way conversion Implementation Disable etcd support Generate a common resource kind Configure configuration loading Implement our own registry What did we achieve? Next Steps Conclusion Hi there! I&amp;rsquo;m Andrei Kvapil, but you might know me as @kvaps in communities dedicated to Kubernetes and cloud-native tools. In this article, I want to share how we implemented our own extension api-server in the open-source PaaS platform, Cozystack. Kubernetes truly amazes me with its powerful extensibility features. You&amp;rsquo;re probably already familiar with the controller concept and frameworks like kubebuilder and operator-sdk that help you implement it. In a nutshell, they allow you to extend your Kubernetes cluster by defining custom resources (CRDs) and writing additional controllers that handle your business logic for reconciling and managing these kinds of resources. This approach is well-documented, with a wealth of information available online on how to develop your own operators. However, this is not the only way to extend the Kubernetes API. For more complex scenarios such as implementing imperative logic, managing subresources, and dynamically generating responses—the Kubernetes API aggregation layer provides an effective alternative. Through the aggregation layer, you can develop a custom extension API server and seamlessly integrate it within the broader Kubernetes API framework. In this article, I will explore the API aggregation layer, the types of challenges it is well-suited to address, cases where it may be less appropriate, and how we utilized this model to implement our own extension API server in Cozystack. First, let&amp;rsquo;s get definitions straight to avoid any confusion down the road. The API aggregation layer is a feature in Kubernetes, while an extension api-server is a specific implementation of an API server for the aggregation layer.</description></item><item><title>Kubernetes v1.32 sneak peek</title><link>https://kubermates.org/docs/2024-11-08-kubernetes-v1-32-sneak-peek/</link><pubDate>Fri, 08 Nov 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-11-08-kubernetes-v1-32-sneak-peek/</guid><description>As we get closer to the release date for Kubernetes v1. 32, the project develops and matures. Features may be deprecated, removed, or replaced with better ones for the project&amp;rsquo;s overall health. This blog outlines some of the planned changes for the Kubernetes v1. 32 release, that the release team feels you should be aware of, for the continued maintenance of your Kubernetes environment and keeping up to date with the latest changes. Information listed below is based on the current status of the v1. 32 release and may change before the actual release date. The Kubernetes project has a well-documented deprecation policy for features. This policy states that stable APIs may only be deprecated when a newer, stable version of that API is available and that APIs have a minimum lifetime for each stability level. A deprecated API has been marked for removal in a future Kubernetes release will continue to function until removal (at least one year from the deprecation). Its usage will result in a warning being displayed. Removed APIs are no longer available in the current version, so you must migrate to use the replacement instead.</description></item><item><title>Creating alerts from panels in Kubernetes Monitoring: an overlooked, powerhouse feature</title><link>https://kubermates.org/docs/2024-11-04-creating-alerts-from-panels-in-kubernetes-monitoring-an-overlooked-powerhouse-fe/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-11-04-creating-alerts-from-panels-in-kubernetes-monitoring-an-overlooked-powerhouse-fe/</guid><description>New in Grafana Alerting: a faster, more scalable way to manage your alerts in Grafana Alejandro Fraenkel Â· 5 Aug 2025 Â· 9 min read We built a brand new alert rules list page. Find out how this new design will make your on-call life a little easier and get insights into how we.</description></item><item><title>Blog: Spotlight on Kubernetes Upstream Training in Japan</title><link>https://kubermates.org/docs/2024-10-28-blog-spotlight-on-kubernetes-upstream-training-in-japan/</link><pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-10-28-blog-spotlight-on-kubernetes-upstream-training-in-japan/</guid><description>Spotlight on Kubernetes Upstream Training in Japan What is Kubernetes upstream training in Japan? Interview with participants Keita Mochizuki ( NTT DATA Group Corporation ) Yoshiki Fujikane ( CyberAgent, Inc. ) Future of Kubernetes upstream training We are organizers of Kubernetes Upstream Training in Japan. Our team is composed of members who actively contribute to Kubernetes, including individuals who hold roles such as member, reviewer, approver, and chair. Our goal is to increase the number of Kubernetes contributors and foster the growth of the community. While Kubernetes community is friendly and collaborative, newcomers may find the first step of contributing to be a bit challenging. Our training program aims to lower that barrier and create an environment where even beginners can participate smoothly. Our training started in 2019 and is held 1 to 2 times a year. Initially, Kubernetes Upstream Training was conducted as a co-located event of KubeCon (Kubernetes Contributor Summit), but we launched Kubernetes Upstream Training in Japan with the aim of increasing Japanese contributors by hosting a similar event in Japan. Before the pandemic, the training was held in person, but since 2020, it has been conducted online. The training offers the following content for those who have not yet contributed to Kubernetes: Introduction to Kubernetes community Overview of Kubernetes codebase and how to create your first PR Tips and encouragement to lower participation barriers, such as language How to set up the development environment Hands-on session using kubernetes-sigs/contributor-playground At the beginning of the program, we explain why contributing to Kubernetes is important and who can contribute. We emphasize that contributing to Kubernetes allows you to make a global impact and that Kubernetes community is looking forward to your contributions! We also explain Kubernetes community, SIGs, and Working Groups. Next, we explain the roles and responsibilities of Member, Reviewer, Approver, Tech Lead, and Chair.</description></item><item><title>Monitoring Kubernetes: Why traditional techniques aren't enough</title><link>https://kubermates.org/docs/2024-10-18-monitoring-kubernetes-why-traditional-techniques-aren-t-enough/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-10-18-monitoring-kubernetes-why-traditional-techniques-aren-t-enough/</guid><description>Kubernetes Monitoring backend 2.2: better cluster observability through new alert and recording. Serena Kei Â· 15 Jul 2025 Â· 4 min read The latest backend update to our Kubernetes Monitoring app in Grafana Cloud features significant improvements to alert rules and recording rules that.</description></item><item><title>Blog: Announcing the 2024 Steering Committee Election Results</title><link>https://kubermates.org/docs/2024-10-02-blog-announcing-the-2024-steering-committee-election-results/</link><pubDate>Wed, 02 Oct 2024 15:10:00 -0500</pubDate><guid>https://kubermates.org/docs/2024-10-02-blog-announcing-the-2024-steering-committee-election-results/</guid><description>Announcing the 2024 Steering Committee Election Results Results Big thanks! Get involved with the Steering Committee The 2024 Steering Committee Election is now complete. The Kubernetes Steering Committee consists of 7 seats, 3 of which were up for election in 2024. Incoming committee members serve a term of 2 years, and all members are elected by the Kubernetes Community. This community body is significant since it oversees the governance of the entire Kubernetes project. With that great power comes great responsibility. You can learn more about the steering committee’s role in their charter. Thank you to everyone who voted in the election; your participation helps support the community’s continued health and success. Congratulations to the elected committee members whose two year terms begin immediately (listed in alphabetical order by GitHub handle): Antonio Ojea ( @aojea ), Google Benjamin Elder ( @BenTheElder ), Google Sascha Grunert ( @saschagrunert ), Red Hat They join continuing members: Stephen Augustus ( @justaugustus ), Cisco Paco Xu 徐俊杰 ( @pacoxu ), DaoCloud Patrick Ohly ( @pohly ), Intel Maciej Szulik ( @soltysh ), Defense Unicorns Benjamin Elder is a returning Steering Committee Member. Thank you and congratulations on a successful election to this round’s election officers: Bridget Kromhout ( @bridgetkromhout ) Christoph Blecker ( @cblecker ) Priyanka Saggu ( @Priyankasaggu11929 ) Thanks to the Emeritus Steering Committee Members. Your service is appreciated by the community: Bob Killen ( @mrbobbytables ) Nabarun Pal ( @palnabarun ) And thank you to all the candidates who came forward to run for election. This governing body, like all of Kubernetes, is open to all. You can follow along with Steering Committee meeting notes and weigh in by filing an issue or creating a PR against their repo.</description></item><item><title>Container monitoring with Grafana: Helpful resources to get started</title><link>https://kubermates.org/docs/2024-10-02-container-monitoring-with-grafana-helpful-resources-to-get-started/</link><pubDate>Wed, 02 Oct 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-10-02-container-monitoring-with-grafana-helpful-resources-to-get-started/</guid><description>Observability trends in Brazil: insights from our localized survey Trevor Jones Â· 12 Aug 2025 Â· 9 min read Check out the highlights from our first survey about observability practices in Brazil, including analysis on adoption, maturity, challenges, and.</description></item><item><title>Blog: Spotlight on CNCF Deaf and Hard-of-hearing Working Group (DHHWG)</title><link>https://kubermates.org/docs/2024-09-30-blog-spotlight-on-cncf-deaf-and-hard-of-hearing-working-group-dhhwg/</link><pubDate>Mon, 30 Sep 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-09-30-blog-spotlight-on-cncf-deaf-and-hard-of-hearing-working-group-dhhwg/</guid><description>Spotlight on CNCF Deaf and Hard-of-hearing Working Group (DHHWG) Introduction Motivation and early milestones Overcoming challenges and addressing misconceptions Impact and the role of allies Expanding DEI efforts and future vision Call to action Wrapping up In recognition of Deaf Awareness Month and the importance of inclusivity in the tech community, we are spotlighting Catherine Paganini , facilitator and one of the founding members of CNCF Deaf and Hard-of-Hearing Working Group (DHHWG). In this interview, Sandeep Kanabar , a deaf member of the DHHWG and part of the Kubernetes SIG ContribEx Communications team , sits down with Catherine to explore the impact of the DHHWG on cloud native projects like Kubernetes. Sandeep’s journey is a testament to the power of inclusion. Through his involvement in the DHHWG, he connected with members of the Kubernetes community who encouraged him to join SIG ContribEx - the group responsible for sustaining the Kubernetes contributor experience. In an ecosystem where open-source projects are actively seeking contributors and maintainers, this story highlights how important it is to create pathways for underrepresented groups, including those with disabilities, to contribute their unique perspectives and skills. In this interview, we delve into Catherine’s journey, the challenges and triumphs of establishing the DHHWG, and the vision for a more inclusive future in cloud native. We invite Kubernetes contributors, maintainers, and community members to reflect on the significance of empathy, advocacy, and community in fostering a truly inclusive environment for all, and to think about how they can support efforts to increase diversity and accessibility within their own projects. Sandeep Kanabar (SK): Hello Catherine, could you please introduce yourself, share your professional background, and explain your connection to the Kubernetes ecosystem? Catherine Paganini (CP) : I’m the Head of Marketing at Buoyant , the creator of Linkerd , the CNCF-graduated service mesh, and 5th CNCF project. Four years ago, I started contributing to open source. The initial motivation was to make cloud native concepts more accessible to newbies and non-technical people. Without a technical background, it was hard for me to understand what Kubernetes, containers, service meshes, etc. mean.</description></item><item><title>Kubeflow and Me: A Story Started with Push-based Metrics Collection</title><link>https://kubermates.org/docs/2024-09-28-kubeflow-and-me-a-story-started-with-push-based-metrics-collection/</link><pubDate>Sat, 28 Sep 2024 00:00:00 -0500</pubDate><guid>https://kubermates.org/docs/2024-09-28-kubeflow-and-me-a-story-started-with-push-based-metrics-collection/</guid><description>Problem Solution My Contributions during the GSoC Lessons Learned In the End Links This summer, I gained a precious opportunity to participate in the Google Summer of Code(GSoC), in which I would contribute to Katib and fulfill a project named “Push-based Metrics Collection in Katib” within 12 weeks. Firstly, I got to know about GSoC and Kubeflow with the recommendation from the former active maintainer Ce Gao(gaocegege)’s personal blog. And I was deeply impressed by the idea of cloud native AI toolkits, I decided to dive into this area and learn some skills to enhance my career and future. In the blog, I’ll provide my personal insight into Katib, for those who are interested in cloud native, AI, and hyperparameters tuning. The project aims to provide a Python SDK API interface for users to push metrics to Katib DB directly. The current implementation of Metrics Collector is pull-based, raising design problems such as determining the frequency at which we scrape the metrics, performance issues like the overhead caused by too many sidecar containers, and restrictions on developing environments that must support sidecar containers and admission webhooks. And also, for data scientists, they need to pay attention to the format of metrics printed in the training scripts, which is error prone and may be hard to recognize. We decided to implement a new API for Katib Python SDK to offer users a push-based way to store metrics directly into the Kaitb DB and resolve those issues raised by pull-based metrics collection. In the new design, users just need to set metrics_collector_config={&amp;ldquo;kind&amp;rdquo;: &amp;ldquo;Push&amp;rdquo;} in the tune() function and call the report_metrics() API in their objective function to push metrics to Katib DB directly. There are no sidecar containers and restricted metric log formats any more. After that, Trial Controller will continuously collect metrics from Katib DB and update the status of Trial, which is the same as pull-based metrics collection. metrics_collector_config={&amp;ldquo;kind&amp;rdquo;: &amp;ldquo;Push&amp;rdquo;} tune() report_metrics() If you are interested in it, please refer to this doc and example for more details.</description></item><item><title>Blog: Spotlight on SIG Scheduling</title><link>https://kubermates.org/docs/2024-09-24-blog-spotlight-on-sig-scheduling/</link><pubDate>Tue, 24 Sep 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-09-24-blog-spotlight-on-sig-scheduling/</guid><description>Spotlight on SIG Scheduling Introductions About SIG Scheduling Contributing to SIG Scheduling Future Directions Closing Remarks In this SIG Scheduling spotlight we talked with Kensei Nakada , an approver in SIG Scheduling. Arvind: Hello, thank you for the opportunity to learn more about SIG Scheduling! Would you like to introduce yourself and tell us a bit about your role, and how you got involved with Kubernetes? Kensei : Hi, thanks for the opportunity! I’m Kensei Nakada ( @sanposhiho ), a software engineer at Tetrate. io. I have been contributing to Kubernetes in my free time for more than 3 years, and now I’m an approver of SIG Scheduling in Kubernetes. Also, I’m a founder/owner of two SIG subprojects, kube-scheduler-simulator and kube-scheduler-wasm-extension. AP: That’s awesome! You’ve been involved with the project since a long time. Can you provide a brief overview of SIG Scheduling and explain its role within the Kubernetes ecosystem? KN : As the name implies, our responsibility is to enhance scheduling within Kubernetes. Specifically, we develop the components that determine which Node is the best place for each Pod. In Kubernetes, our main focus is on maintaining the kube-scheduler , along with other scheduling-related components as part of our SIG subprojects. That makes me curious–what recent innovations or developments has SIG Scheduling introduced to Kubernetes scheduling? KN : From a feature perspective, there have been several enhancements to PodTopologySpread recently. PodTopologySpread is a relatively new feature in the scheduler, and we are still in the process of gathering feedback and making improvements. PodTopologySpread PodTopologySpread Most recently, we have been focusing on a new internal enhancement called QueueingHint which aims to enhance scheduling throughput.</description></item><item><title>LLM Hyperparameter Optimization API: My Google Summer of Code Journey with Kubeflow</title><link>https://kubermates.org/docs/2024-09-19-llm-hyperparameter-optimization-api-my-google-summer-of-code-journey-with-kubefl/</link><pubDate>Thu, 19 Sep 2024 00:00:00 -0500</pubDate><guid>https://kubermates.org/docs/2024-09-19-llm-hyperparameter-optimization-api-my-google-summer-of-code-journey-with-kubefl/</guid><description>Motivation Goal My Contributions to the GSoC Project Lessons Learned Think from the User’s Perspective Don’t Fear Bugs Communication is Important Every Contribution Counts In The End This summer, I had the opportunity to participate in the Google Summer of Code (GSoC) program, where I contributed to Kubeflow, an open-source machine learning toolkit. My project focused on developing a high-level API for optimizing hyperparameters in Large Language Models (LLMs) within Katib, Kubeflow’s automated hyperparameter tuning system. I’d like to share insights from this experience with others interested in Kubeflow, GSoC, or optimizing LLMs. The rapid advancements and rising popularity of LLMs, such as GPT and BERT, have created a growing demand for efficient LLMOps in Kubernetes. To address this, we have developed a train API within the Training Python SDK, simplifying the process of fine-tuning LLMs using distributed PyTorchJob workers. However, hyperparameter optimization remains a crucial yet labor-intensive task for enhancing model performance. Hyperparameter optimization is essential but time-consuming, especially for LLMs with billions of parameters. This API simplifies the process by handling Kubernetes infrastructure, allowing data scientists to focus on model performance rather than system configuration. With this API, users can import pretrained models and datasets from Hugging Face and Amazon S3, define parameters including the hyperparameter search space, optimization objective, and resource configuration. The API then automates the creation of Experiment, which contains multiple Trials with different hyperparameter settings using PyTorch distributed training. It then collects and analyzes the metrics from each Trial to identify the optimal hyperparameter configuration. For detailed instruction on using the API, please refer to this guide My work on the project can be broadly divided into four stages: Stage 1 : Designing the API, drafting the project proposal, and refining it into a Kubeflow Enhancement Proposal (KEP).</description></item><item><title>Observe deleted Kubernetes components in Grafana Cloud to boost troubleshooting and resource management</title><link>https://kubermates.org/docs/2024-08-08-observe-deleted-kubernetes-components-in-grafana-cloud-to-boost-troubleshooting-/</link><pubDate>Thu, 08 Aug 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-08-08-observe-deleted-kubernetes-components-in-grafana-cloud-to-boost-troubleshooting-/</guid><description>Inside Grafana Labsâ Voice of Customer program: whatâs new and whatâs next Elizabeth Burkly Â· 26 Aug 2025 Â· 9 min read With our Voice of Customer program, we can tighten the loop between what our users ask for and what we prioritize in R&amp;amp;D. Hereâs a look at some recent.</description></item><item><title>Blog: Spotlight on SIG API Machinery</title><link>https://kubermates.org/docs/2024-08-07-blog-spotlight-on-sig-api-machinery/</link><pubDate>Wed, 07 Aug 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-08-07-blog-spotlight-on-sig-api-machinery/</guid><description>Spotlight on SIG API Machinery Introductions SIG Machinery’s scope An evolving collaboration model The challenges of Kubernetes popularity The road ahead Joining the fun We recently talked with Federico Bongiovanni (Google) and David Eads (Red Hat), Chairs of SIG API Machinery, to know a bit more about this Kubernetes Special Interest Group. Frederico (FSM): Hello, and thank your for your time. To start with, could you tell us about yourselves and how you got involved in Kubernetes? David : I started working on OpenShift (the Red Hat distribution of Kubernetes) in the fall of 2014 and got involved pretty quickly in API Machinery. My first PRs were fixing kube-apiserver error messages and from there I branched out to kubectl ( kubeconfigs are my fault!), auth ( RBAC and *Review APIs are ports from OpenShift), apps ( workqueues and sharedinformers for example). Don’t tell the others, but API Machinery is still my favorite :) kubectl auth *Review apps Federico : I was not as early in Kubernetes as David, but now it’s been more than six years. At my previous company we were starting to use Kubernetes for our own products, and when I came across the opportunity to work directly with Kubernetes I left everything and boarded the ship (no pun intended). I joined Google and Kubernetes in early 2018, and have been involved since. FSM: It only takes a quick look at the SIG API Machinery charter to see that it has quite a significant scope, nothing less than the Kubernetes control plane. Could you describe this scope in your own words? David : We own the kube-apiserver and how to efficiently use it. On the backend, that includes its contract with backend storage and how it allows API schema evolution over time. On the frontend, that includes schema best practices, serialization, client patterns, and controller patterns on top of all of it. kube-apiserver Federico : Kubernetes has a lot of different components, but the control plane has a really critical mission: it’s your communication layer with the cluster and also owns all the extensibility mechanisms that make Kubernetes so powerful.</description></item><item><title>Monitor these Kubernetes signals to help rightsize your fleet</title><link>https://kubermates.org/docs/2024-07-29-monitor-these-kubernetes-signals-to-help-rightsize-your-fleet/</link><pubDate>Mon, 29 Jul 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-07-29-monitor-these-kubernetes-signals-to-help-rightsize-your-fleet/</guid><description>Kubernetes Monitoring backend 2.2: better cluster observability through new alert and recording. Serena Kei Â· 15 Jul 2025 Â· 4 min read The latest backend update to our Kubernetes Monitoring app in Grafana Cloud features significant improvements to alert rules and recording rules that.</description></item><item><title>Kubeflow 1.9: New Tools for Model Management and Training Optimization</title><link>https://kubermates.org/docs/2024-07-22-kubeflow-1-9-new-tools-for-model-management-and-training-optimization/</link><pubDate>Mon, 22 Jul 2024 00:00:00 -0500</pubDate><guid>https://kubermates.org/docs/2024-07-22-kubeflow-1-9-new-tools-for-model-management-and-training-optimization/</guid><description>Model Registry Fine-Tune APIs for LLMs Pipelines v1 Feature Parity Argo Workflows and Tekton Backends Consolidation Argo Workflows Upgrade Katib Central Dashboard Notebooks Kubeflow Platform (Security and Manifests) Security Manifests KServe Documentation Honorable Mentions Google Spark Operator migration to Kubeflow Google Summer of Code What’s next How to get started with 1.9 Join the Community Want to help? Kubeflow 1.9 significantly simplifies the development, tuning and management of secure machine learning models and LLMs. Highlights include: Model Registry : Centralized management for ML models, versions, and artifacts. Fine-Tune APIs for LLMs : Simplifies fine-tuning of LLMs with custom datasets. Pipelines : Consolidation of Tekton and Argo Workflows backends for improved flexibility. Security Enhancements : Network policies, Oauth2-proxy, and CVE scanning. Integration Upgrades : Improved integrations with Ray, Seldon, BentoML, and KServe for LLM GPU optimizations. Installation and Documentation : Streamlined installation, updated platform dependencies, and enhanced documentation. These updates aim to simplify workflows, improve integration dependencies, and provide Kubernetes-native operational efficiencies for enterprise scale, security, and isolation. A model registry provides a central catalog for ML model developers to index and manage models, versions, and ML artifacts metadata. It fills a gap between model experimentation and production activities. It provides a central interface for all stakeholders in the ML lifecycle to collaborate on ML models. Model registry has been asked by the community for a long time and we are delighted to introduce it to the Kubeflow ecosystem.</description></item><item><title>Blog: Spotlight on SIG Node</title><link>https://kubermates.org/docs/2024-06-20-blog-spotlight-on-sig-node/</link><pubDate>Thu, 20 Jun 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-06-20-blog-spotlight-on-sig-node/</guid><description>Spotlight on SIG Node Introductions Understanding SIG Node Challenges and Opportunities Sidecar containers Contributing to SIG Node Conclusion In the world of container orchestration, Kubernetes reigns supreme, powering some of the most complex and dynamic applications across the globe. Behind the scenes, a network of Special Interest Groups (SIGs) drives Kubernetes’ innovation and stability. Today, I have the privilege of speaking with Matthias Bertschy , Gunju Kim , and Sergey Kanzhelev , members of SIG Node , who will shed some light on their roles, challenges, and the exciting developments within SIG Node. Answers given collectively by all interviewees will be marked by their initials. Arpit: Thank you for joining us today. Could you please introduce yourselves and provide a brief overview of your roles within SIG Node? Matthias: My name is Matthias Bertschy, I am French and live next to Lake Geneva, near the French Alps. I have been a Kubernetes contributor since 2017, a reviewer for SIG Node and a maintainer of Prow. I work as a Senior Kubernetes Developer for a security startup named ARMO , which donated Kubescape to the CNCF. Gunju: My name is Gunju Kim. I am a software engineer at NAVER , where I focus on developing a cloud platform for search services. I have been contributing to the Kubernetes project in my free time since 2021. Sergey: My name is Sergey Kanzhelev.</description></item><item><title>Blog: Introducing Hydrophone</title><link>https://kubermates.org/docs/2024-05-23-blog-introducing-hydrophone/</link><pubDate>Thu, 23 May 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-05-23-blog-introducing-hydrophone/</guid><description>Introducing Hydrophone Simplified Kubernetes testing with Hydrophone Key features of Hydrophone Streamlining Kubernetes conformance with Hydrophone Getting started with Hydrophone Community and contributions Join us in simplifying Kubernetes testing In the ever-changing landscape of Kubernetes, ensuring that clusters operate as intended is essential. This is where conformance testing becomes crucial, verifying that a Kubernetes cluster meets the required standards set by the community. Today, we’re thrilled to introduce Hydrophone , a lightweight runner designed to streamline Kubernetes tests using the official conformance images released by the Kubernetes release team. Hydrophone’s design philosophy centers around ease of use. By starting the conformance image as a pod within the conformance namespace, Hydrophone waits for the tests to conclude, then prints and exports the results. This approach offers a hassle-free method for running either individual tests or the entire Conformance Test Suite. Ease of Use : Designed with simplicity in mind, Hydrophone provides an easy-to-use tool for conducting Kubernetes conformance tests. Official Conformance Images : It leverages the official conformance images from the Kubernetes Release Team, ensuring that you’re using the most up-to-date and reliable resources for testing. Flexible Test Execution : Whether you need to run a single test, the entire Conformance Test Suite, or anything in between. In the Kubernetes world, where providers like EKS, Rancher, and k3s offer diverse environments, ensuring consistent experiences is vital. This consistency is anchored in conformance testing, which validates whether these environments adhere to Kubernetes community standards. Historically, this validation has either been cumbersome or requires third-party tools.</description></item><item><title>Blog: Migrate to Google Artifact Registry</title><link>https://kubermates.org/docs/2024-05-06-blog-migrate-to-google-artifact-registry/</link><pubDate>Mon, 06 May 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-05-06-blog-migrate-to-google-artifact-registry/</guid><description>Migrate to Google Artifact Registry Google has announced that container registry will be shut down some time after March 18, 2025. For GKE clusters created with version 1.12.0 or later of terraform-google-jx it’s unlikely that anything needs to be done, but for older clusters you should upgrade your cluster while considering our advice regarding migration from container registry to artifact registry. If you are using a Google Service Account to run terraform you need to add the role requirement roles/artifactregistry. admin. See our guide regarding Google Service Account for details. ← Previous.</description></item><item><title>How to use the Grafana Operator: Managing a Grafana Cloud stack in Kubernetes</title><link>https://kubermates.org/docs/2024-04-24-how-to-use-the-grafana-operator-managing-a-grafana-cloud-stack-in-kubernetes/</link><pubDate>Wed, 24 Apr 2024 10:22:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-04-24-how-to-use-the-grafana-operator-managing-a-grafana-cloud-stack-in-kubernetes/</guid><description>Inside Grafana Labsâ Voice of Customer program: whatâs new and whatâs next Elizabeth Burkly Â· 26 Aug 2025 Â· 9 min read With our Voice of Customer program, we can tighten the loop between what our users ask for and what we prioritize in R&amp;amp;D. Hereâs a look at some recent.</description></item><item><title>Announcing the Kubeflow Spark Operator: Building a Stronger Spark on Kubernetes Community</title><link>https://kubermates.org/docs/2024-04-15-announcing-the-kubeflow-spark-operator-building-a-stronger-spark-on-kubernetes-c/</link><pubDate>Mon, 15 Apr 2024 00:00:00 -0500</pubDate><guid>https://kubermates.org/docs/2024-04-15-announcing-the-kubeflow-spark-operator-building-a-stronger-spark-on-kubernetes-c/</guid><description>The Journey to Kubeflow Spark Operator Why Kubeflow? What’s Next? Join the Movement We’re excited to announce the migration of Google’s Spark Operator to the Kubeflow Spark Operator , marking the launch of a significant addition to the Kubeflow ecosystem. The Kubeflow Spark Operator simplifies the deployment and management of Apache Spark applications on Kubernetes. This announcement isn’t just about a new piece of technology, it’s about building a stronger, open-governed, and more collaborative community around Spark on Kubernetes. The journey of the Kubeflow Spark Operator began with Google Cloud Platform’s Spark on Kubernetes Operator (https://cloud. google. com/blog/products/data-analytics/data-analytics-meet-containers-kubernetes-operator-for-apache-spark-now-in-beta). With over 2. 3k stars and 1. 3k forks on GitHub, this project laid the foundation for a robust Spark on Kubernetes experience, enabling users to deploy Spark workloads seamlessly across Kubernetes clusters. Growth and innovation require not just code but also community. Acknowledging the resource and time limitations faced by Google Cloud’s original maintainers, Kubeflow has taken up the mantle. This transition is not merely administrative but a strategic move towards fostering a vibrant, diverse, and more actively engaged community.</description></item><item><title>Blog: Spotlight on SIG Architecture: Code Organization</title><link>https://kubermates.org/docs/2024-04-11-blog-spotlight-on-sig-architecture-code-organization/</link><pubDate>Thu, 11 Apr 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-04-11-blog-spotlight-on-sig-architecture-code-organization/</guid><description>Spotlight on SIG Architecture: Code Organization Introducing the Code Organization subproject Code organization and Kubernetes Release cycle and current priorities Wrapping up This is the third interview of a SIG Architecture Spotlight series that will cover the different subprojects. We will cover SIG Architecture: Code Organization. In this SIG Architecture spotlight I talked with Madhav Jivrajani (VMware), a member of the Code Organization subproject. Frederico (FSM) : Hello Madhav, thank you for your availability. Could you start by telling us a bit about yourself, your role and how you got involved in Kubernetes? Madhav Jivrajani (MJ) : Hello! My name is Madhav Jivrajani, I serve as a technical lead for SIG Contributor Experience and a GitHub Admin for the Kubernetes project. Apart from that I also contribute to SIG API Machinery and SIG Etcd, but more recently, I’ve been helping out with the work that is needed to help Kubernetes stay on supported versions of Go , and it is through this that I am involved with the Code Organization subproject of SIG Architecture. FSM : A project the size of Kubernetes must have unique challenges in terms of code organization – is this a fair assumption? If so, what would you pick as some of the main challenges that are specific to Kubernetes? MJ : That’s a fair assumption! The first interesting challenge comes from the sheer size of the Kubernetes codebase. We have ≅2.2 million lines of Go code (which is steadily decreasing thanks to dims and other folks in this sub-project!), and a little over 240 dependencies that we rely on either directly or indirectly, which is why having a sub-project dedicated to helping out with dependency management is crucial: we need to know what dependencies we’re pulling in, what versions these dependencies are at, and tooling to help make sure we are managing these dependencies across different parts of the codebase in a consistent manner. Another interesting challenge with Kubernetes is that we publish a lot of Go modules as part of the Kubernetes release cycles, one example of this is client-go. However, we as a project would also like the benefits of having everything in one repository to get the advantages of using a monorepo, like atomic commits… so, because of this, code organization works with other SIGs (like SIG Release) to automate the process of publishing code from the monorepo to downstream individual repositories which are much easier to consume, and this way you won’t have to import the entire Kubernetes codebase! client-go FSM : For someone just starting contributing to Kubernetes code-wise, what are the main things they should consider in terms of code organization? How would you sum up the key concepts? MJ : I think one of the key things to keep in mind at least as you’re starting off is the concept of staging directories. In the kubernetes/kubernetes repository, you will come across a directory called staging/. The sub-folders in this directory serve as a bunch of pseudo-repositories.</description></item><item><title>Blog: Using Go workspaces in Kubernetes</title><link>https://kubermates.org/docs/2024-03-19-blog-using-go-workspaces-in-kubernetes/</link><pubDate>Tue, 19 Mar 2024 08:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2024-03-19-blog-using-go-workspaces-in-kubernetes/</guid><description>Using Go workspaces in Kubernetes GOPATH and Go modules The problems Enter workspaces The work Results Thanks The Go programming language has played a huge role in the success of Kubernetes. As Kubernetes has grown, matured, and pushed the bounds of what “regular” projects do, the Go project team has also grown and evolved the language and tools. In recent releases, Go introduced a feature called “workspaces” which was aimed at making projects like Kubernetes easier to manage. We’ve just completed a major effort to adopt workspaces in Kubernetes, and the results are great. Our codebase is simpler and less error-prone, and we’re no longer off on our own technology island. Kubernetes is one of the most visible open source projects written in Go. The earliest versions of Kubernetes, dating back to 2014, were built with Go 1.3. Today, 10 years later, Go is up to version 1.22 — and let’s just say that a whole lot has changed. In 2014, Go development was entirely based on GOPATH. As a Go project, Kubernetes lived by the rules of GOPATH. In the buildup to Kubernetes 1.4 (mid 2016), we introduced a directory tree called staging. This allowed us to pretend to be multiple projects, but still exist within one git repository (which had advantages for development velocity).</description></item><item><title>Grafana Labs at KubeCon: eBPF, sustainability, Prometheus, and more</title><link>https://kubermates.org/docs/2024-03-15-grafana-labs-at-kubecon-ebpf-sustainability-prometheus-and-more/</link><pubDate>Fri, 15 Mar 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-03-15-grafana-labs-at-kubecon-ebpf-sustainability-prometheus-and-more/</guid><description>How to monitor your homelab with Beyla, eBPF, and OpenTelemetry Colin Steele Â· 22 Aug 2025 Â· 5 min read Learn how Beyla, eBPF, and OpenTelemetry combine to make homelab observability easy with this recap of a recent GrafanaCON 2025 session.</description></item><item><title>Blog: Spotlight on SIG Cloud Provider</title><link>https://kubermates.org/docs/2024-03-01-blog-spotlight-on-sig-cloud-provider/</link><pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-03-01-blog-spotlight-on-sig-cloud-provider/</guid><description>Spotlight on SIG Cloud Provider Introduction Functioning and working Important subprojects Accomplishments Advice for new contributors One of the most popular ways developers use Kubernetes-related services is via cloud providers, but have you ever wondered how cloud providers can do that? How does this whole process of integration of Kubernetes to various cloud providers happen? To answer that, let’s put the spotlight on SIG Cloud Provider. SIG Cloud Provider works to create seamless integrations between Kubernetes and various cloud providers. Their mission? Keeping the Kubernetes ecosystem fair and open for all. By setting clear standards and requirements, they ensure every cloud provider plays nicely with Kubernetes. It is their responsibility to configure cluster components to enable cloud provider integrations. In this blog of the SIG Spotlight series, Arujjwal Negi interviews Michael McCune (Red Hat), also known as elmiko , co-chair of SIG Cloud Provider, to give us an insight into the workings of this group. Arujjwal : Let’s start by getting to know you. Can you give us a small intro about yourself and how you got into Kubernetes? Michael : Hi, I’m Michael McCune, most people around the community call me by my handle, elmiko. I’ve been a software developer for a long time now (Windows 3.1 was popular when I started!), and I’ve been involved with open-source software for most of my career. I first got involved with Kubernetes as a developer of machine learning and data science applications; the team I was on at the time was creating tutorials and examples to demonstrate the use of technologies like Apache Spark on Kubernetes. That said, I’ve been interested in distributed systems for many years and when an opportunity arose to join a team working directly on Kubernetes, I jumped at it! Arujjwal : Can you give us an insight into what SIG Cloud Provider does and how it functions? Michael : SIG Cloud Provider was formed to help ensure that Kubernetes provides a neutral integration point for all infrastructure providers. Our largest task to date has been the extraction and migration of in-tree cloud controllers to out-of-tree components.</description></item><item><title>Blog: A look into the Kubernetes Book Club</title><link>https://kubermates.org/docs/2024-02-22-blog-a-look-into-the-kubernetes-book-club/</link><pubDate>Thu, 22 Feb 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-02-22-blog-a-look-into-the-kubernetes-book-club/</guid><description>A look into the Kubernetes Book Club Creating the Book Club Topics covered in the Book Club Joining the Book Club Learning Kubernetes and the entire ecosystem of technologies around it is not without its challenges. In this interview, we will talk with Carlos Santana (AWS) to learn a bit more about how he created the Kubernetes Book Club , how it works, and how anyone can join in to take advantage of a community-based learning experience. Frederico Muñoz (FSM) : Hello Carlos, thank you so much for your availability. To start with, could you tell us a bit about yourself? Carlos Santana (CS) : Of course. My experience in deploying Kubernetes in production six years ago opened the door for me to join Knative and then contribute to Kubernetes through the Release Team. Working on upstream Kubernetes has been one of the best experiences I’ve had in open-source. Over the past two years, in my role as a Senior Specialist Solutions Architect at AWS, I have been assisting large enterprises build their internal developer platforms (IDP) on top of Kubernetes. Going forward, my open source contributions are directed towards CNOE and CNCF projects like Argo , Crossplane , and Backstage. FSM : So your path led you to Kubernetes, and at that point what was the motivating factor for starting the Book Club? CS : The idea for the Kubernetes Book Club sprang from a casual suggestion during a TGIK livestream. For me, it was more than just about reading a book; it was about creating a learning community. This platform has not only been a source of knowledge but also a support system, especially during the challenging times of the pandemic. It’s gratifying to see how this initiative has helped members cope and grow.</description></item><item><title>Kubernetes alerting: Simplify anomaly detection in Kubernetes clusters with Grafana Cloud</title><link>https://kubermates.org/docs/2024-02-15-kubernetes-alerting-simplify-anomaly-detection-in-kubernetes-clusters-with-grafa/</link><pubDate>Thu, 15 Feb 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-02-15-kubernetes-alerting-simplify-anomaly-detection-in-kubernetes-clusters-with-grafa/</guid><description>New in Grafana Alerting: a faster, more scalable way to manage your alerts in Grafana Alejandro Fraenkel Â· 5 Aug 2025 Â· 9 min read We built a brand new alert rules list page. Find out how this new design will make your on-call life a little easier and get insights into how we.</description></item><item><title>Blog: Spotlight on SIG Release (Release Team Subproject)</title><link>https://kubermates.org/docs/2024-01-15-blog-spotlight-on-sig-release-release-team-subproject/</link><pubDate>Mon, 15 Jan 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-01-15-blog-spotlight-on-sig-release-release-team-subproject/</guid><description>Spotlight on SIG Release (Release Team Subproject) Final thoughts The Release Special Interest Group (SIG Release), where Kubernetes sharpens its blade with cutting-edge features and bug fixes every 4 months. Have you ever considered how such a big project like Kubernetes manages its timeline so efficiently to release its new version, or how the internal workings of the Release Team look like? If you’re curious about these questions or want to know more and get involved with the work SIG Release does, read on! SIG Release plays a crucial role in the development and evolution of Kubernetes. Its primary responsibility is to manage the release process of new versions of Kubernetes. It operates on a regular release cycle, typically every three to four months. During this cycle, the Kubernetes Release Team works closely with other SIGs and contributors to ensure a smooth and well-coordinated release. This includes planning the release schedule, setting deadlines for code freeze and testing phases, as well as creating release artefacts like binaries, documentation, and release notes. Before you read further, it is important to note that there are two subprojects under SIG Release - Release Engineering and Release Team. In this blog post, Nitish Kumar interviews Verónica López (PlanetScale), Technical Lead of SIG Release, with the spotlight on the Release Team subproject, how the release process looks like, and ways to get involved. What is the typical release process for a new version of Kubernetes, from initial planning to the final release? Are there any specific methodologies and tools that you use to ensure a smooth release? The release process for a new Kubernetes version is a well-structured and community-driven effort. There are no specific methodologies or tools as such that we follow, except a calendar with a series of steps to keep things organised. The complete release process looks like this: What is the typical release process for a new version of Kubernetes, from initial planning to the final release? Are there any specific methodologies and tools that you use to ensure a smooth release? The release process for a new Kubernetes version is a well-structured and community-driven effort. There are no specific methodologies or tools as such that we follow, except a calendar with a series of steps to keep things organised.</description></item><item><title>Blog: Blixt - A load-balancer written in Rust, using eBPF, born from Gateway API</title><link>https://kubermates.org/docs/2024-01-08-blog-blixt-a-load-balancer-written-in-rust-using-ebpf-born-from-gateway-api/</link><pubDate>Mon, 08 Jan 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-01-08-blog-blixt-a-load-balancer-written-in-rust-using-ebpf-born-from-gateway-api/</guid><description>Blixt - A load-balancer written in Rust, using eBPF, born from Gateway API History Goals Getting involved In SIG Network we now have a layer 4 (“L4”) load balancer named Blixt. This project started as a fun experiment using emerging technologies and is intended to become a utility for CI and testing to help facilitate the continued development of Gateway API. Are you interested in developing networking tools in Rust and eBPF ? Or perhaps you’re specifically interested in Gateway API? We’ll tell you a bit about the project and how it might benefit you. Blixt originated at Kong as an experiment to test load-balancing ingress traffic for Kubernetes clusters using eBPF for the dataplane. Around the time of Kubecon Detroit (2022) we (the Gateway API maintainers) realized it had significant potential to help us move our TCPRoute and UDPRoute support forward, which had been sort of “stuck in alpha” at the time due to a lack of conformance tests being developed for them. At the same time, various others in the SIG Network community developed an interest in the project due to the rapid growth of eBPFs use on Kubernetes. Given the potential for benefit to the Kubernetes ecosystem and the growing interest, Kong decided it would be helpful to donate the project to Kubernetes SIGs to benefit upstream Kubernetes. Over several months we rewrote the project in Rust (from C), due to a strong contingency of Rust knowledge (and interest) between us developing the project and an active interest in the burgeoning Aya project (a Rust framework for developing eBPF programs). We did eventually move the control plane (specifically) to Golang however, so that we could take advantage of the Kubebuilder and controller-runtime ecosystems. Additionally, we augmented our custom program loader (in eBPF, you generally write loaders that load your BPF byte code into the kernel) with bpfman : a project adjacent to us in the Rust + eBPF ecosystem, which helps solve several security and ergonomic problems with managing BPF programs on Linux systems. After the recently completed license review process , which provided a blanket exception for the use of dual licensed eBPF in CNCF code, the project became officially part of Kubernetes and interest has been growing. We have several goals for the project which revolve around the continued development of Gateway API, with a specific focus on helping mature Layer 4 support (e.</description></item><item><title>Blog: Kubernetes supports running kube-proxy in an unprivileged container</title><link>https://kubermates.org/docs/2024-01-05-blog-kubernetes-supports-running-kube-proxy-in-an-unprivileged-container/</link><pubDate>Fri, 05 Jan 2024 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2024-01-05-blog-kubernetes-supports-running-kube-proxy-in-an-unprivileged-container/</guid><description>Kubernetes supports running kube-proxy in an unprivileged container Background Initializing kube-proxy in an init container Summary This post describes how the &amp;ndash;init-only flag to kube-proxy can be used to run the main kube-proxy container in a stricter securityContext , by performing the configuration that requires privileged mode in a separate init container. Since Windows doesn’t have the equivalent of capabilities , this only works on Linux. &amp;ndash;init-only kube-proxy securityContext capabilities The kube-proxy Pod still only meets the privileged Pod Security Standard , but there is still an improvement because the running container doesn’t need to run privileged. kube-proxy Please note that kube-proxy can be installed in different ways. The examples below assume that kube-proxy is run from a pod, but similar changes could be made in clusters where it is run as a system service. kube-proxy It is undesirable to run a server container like kube-proxy in privileged mode. Security aware users wants to use capabilities instead. kube-proxy If kube-proxy is installed as a POD, the initialization requires “privileged” mode, mostly for setting sysctl’s. However, kube-proxy only tries to set the sysctl’s if they don’t already have the right values. In theory, then, if a privileged init container set the sysctls to the right values, then kube-proxy could run unprivileged. kube-proxy kube-proxy kube-proxy The problem is to know what to setup. Until now the only option has been to read the source to see what changes kube-proxy would have made, but with &amp;ndash;init-only you can have kube-proxy itself do the setup exactly as on a normal start, and then exit.</description></item><item><title>Blog: Contextual logging in Kubernetes 1.29: Better troubleshooting and enhanced logging</title><link>https://kubermates.org/docs/2023-12-20-blog-contextual-logging-in-kubernetes-1-29-better-troubleshooting-and-enhanced-l/</link><pubDate>Wed, 20 Dec 2023 09:30:00 -0800</pubDate><guid>https://kubermates.org/docs/2023-12-20-blog-contextual-logging-in-kubernetes-1-29-better-troubleshooting-and-enhanced-l/</guid><description>Contextual logging in Kubernetes 1.29: Better troubleshooting and enhanced logging What is contextual logging? How to use it Performance impact Impact on downstream users Further reading Get involved On behalf of the Structured Logging Working Group and SIG Instrumentation , we are pleased to announce that the contextual logging feature introduced in Kubernetes v1.24 has now been successfully migrated to two components (kube-scheduler and kube-controller-manager) as well as some directories. This feature aims to provide more useful logs for better troubleshooting of Kubernetes and to empower developers to enhance Kubernetes. Contextual logging is based on the go-logr API. The key idea is that libraries are passed a logger instance by their caller and use that for logging instead of accessing a global logger. The binary decides the logging implementation, not the libraries. The go-logr API is designed around structured logging and supports attaching additional information to a logger. This enables additional use cases: The caller can attach additional information to a logger: WithName adds a “logger” key with the names concatenated by a dot as value WithValues adds key/value pairs When passing this extended logger into a function, and the function uses it instead of the global logger, the additional information is then included in all log entries, without having to modify the code that generates the log entries. This is useful in highly parallel applications where it can become hard to identify all log entries for a certain operation, because the output from different operations gets interleaved. The caller can attach additional information to a logger: WithName adds a “logger” key with the names concatenated by a dot as value WithValues adds key/value pairs When passing this extended logger into a function, and the function uses it instead of the global logger, the additional information is then included in all log entries, without having to modify the code that generates the log entries. This is useful in highly parallel applications where it can become hard to identify all log entries for a certain operation, because the output from different operations gets interleaved. When running unit tests, log output can be associated with the current test. Then, when a test fails, only the log output of the failed test gets shown by go test.</description></item><item><title>Blog: Spotlight on SIG Testing</title><link>https://kubermates.org/docs/2023-11-24-blog-spotlight-on-sig-testing/</link><pubDate>Fri, 24 Nov 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-11-24-blog-spotlight-on-sig-testing/</guid><description>Spotlight on SIG Testing Meet the contributors Testing practices and tools Subprojects owned by SIG Testing Key challenges and accomplishments The people and the scope Looking ahead Welcome to another edition of the SIG spotlight blog series, where we highlight the incredible work being done by various Special Interest Groups (SIGs) within the Kubernetes project. In this edition, we turn our attention to SIG Testing , a group interested in effective testing of Kubernetes and automating away project toil. SIG Testing focus on creating and running tools and infrastructure that make it easier for the community to write and run tests, and to contribute, analyze and act upon test results. To gain some insights into SIG Testing, Sandipan Panda spoke with Michelle Shepardson , a senior software engineer at Google and a chair of SIG Testing, and Patrick Ohly , a software engineer and architect at Intel and a SIG Testing Tech Lead. Sandipan: Could you tell us a bit about yourself, your role, and how you got involved in the Kubernetes project and SIG Testing? Michelle: Hi! I’m Michelle, a senior software engineer at Google. I first got involved in Kubernetes through working on tooling for SIG Testing, like the external instance of TestGrid. I’m part of oncall for TestGrid and Prow, and am now a chair for the SIG. Patrick: Hello! I work as a software engineer and architect in a team at Intel which focuses on open source Cloud Native projects. When I ramped up on Kubernetes to develop a storage driver, my very first question was “how do I test it in a cluster and how do I log information?” That interest led to various enhancement proposals until I had (re)written enough code that also took over official roles as SIG Testing Tech Lead (for the E2E framework ) and structured logging WG lead. Sandipan: Testing is a field in which multiple approaches and tools exist; how did you arrive at the existing practices? Patrick: I can’t speak about the early days because I wasn’t around yet 😆, but looking back at some of the commit history it’s pretty obvious that developers just took what was available and started using it. For E2E testing, that was Ginkgo+Gomega. Some hacks were necessary, for example around cleanup after a test run and for categorising tests.</description></item><item><title>Blog: Kubernetes Contributor Summit: Behind-the-scenes</title><link>https://kubermates.org/docs/2023-11-03-blog-kubernetes-contributor-summit-behind-the-scenes/</link><pubDate>Fri, 03 Nov 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-11-03-blog-kubernetes-contributor-summit-behind-the-scenes/</guid><description>Kubernetes Contributor Summit: Behind-the-scenes The Contributor Summit What makes it special Organizing the Summit A look ahead Every year, just before the official start of KubeCon+CloudNativeCon, there’s a special event that has a very special place in the hearts of those organizing and participating in it: the Kubernetes Contributor Summit. To find out why, and to provide a behind-the-scenes perspective, we interview Noah Abrahams, whom amongst other roles was the co-lead for the Kubernetes Contributor Summit in 2023. Frederico Muñoz (FSM) : Hello Noah, and welcome. Could you start by introducing yourself and telling us how you got involved in Kubernetes? Noah Abrahams (NA) : I’ve been in this space for quite a while. I got started in IT in the mid 90’s, and I’ve been working in the “Cloud” space for about 15 years. It was, frankly, through a combination of sheer luck (being in the right place at the right time) and having good mentors to pull me into those places (thanks, Tim!), that I ended up at a startup called Apprenda in 2016. While I was there, they pivoted into Kubernetes, and it was the best thing that could have happened to my career. It was around v1.2 and someone asked me if I could give a presentation on Kubernetes concepts at “my local meetup” in Las Vegas. The meetup didn’t exist yet, so I created it, and got involved in the wider community. One thing led to another, and soon I was involved in ContribEx, joined the release team, was doing booth duty for the CNCF, became an ambassador, and here we are today. FM : Before leading the organisation of the KCSEU 2023, how many other Contributor Summits were you a part of? NA : I was involved in four or five before taking the lead. If I’m recalling correctly, I attended the summit in Copenhagen, then sometime in 2018 I joined the wrong meeting, because the summit staff meeting was listed on the ContribEx calendar.</description></item><item><title>Blog: Spotlight on SIG Architecture: Production Readiness</title><link>https://kubermates.org/docs/2023-11-02-blog-spotlight-on-sig-architecture-production-readiness/</link><pubDate>Thu, 02 Nov 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-11-02-blog-spotlight-on-sig-architecture-production-readiness/</guid><description>Spotlight on SIG Architecture: Production Readiness About SIG Architecture and the Production Readiness subproject Production readiness and the Kubernetes project Helping with Production Readiness This is the second interview of a SIG Architecture Spotlight series that will cover the different subprojects. In this blog, we will cover the SIG Architecture: Production Readiness subproject. In this SIG Architecture spotlight, we talked with Wojciech Tyczynski (Google), lead of the Production Readiness subproject. Frederico (FSM) : Hello Wojciech, could you tell us a bit about yourself, your role and how you got involved in Kubernetes? Wojciech Tyczynski (WT) : I started contributing to Kubernetes in January 2015. At that time, Google (where I was and still am working) decided to start a Kubernetes team in the Warsaw office (in addition to already existing teams in California and Seattle). I was lucky enough to be one of the seeding engineers for that team. After two months of onboarding and helping with different tasks across the project towards 1.0 launch, I took ownership of the scalability area and I was leading Kubernetes to support clusters with 5000 nodes. I’m still involved in SIG Scalability as its Technical Lead. That was the start of a journey since scalability is such a cross-cutting topic, and I started contributing to many other areas including, over time, to SIG Architecture. FSM : In SIG Architecture, why specifically the Production Readiness subproject? Was it something you had in mind from the start, or was it an unexpected consequence of your initial involvement in scalability? WT : After reaching that milestone of Kubernetes supporting 5000-node clusters , one of the goals was to ensure that Kubernetes would not degrade its scalability properties over time. While non-scalable implementation is always fixable, designing non-scalable APIs or contracts is problematic. I was looking for a way to ensure that people are thinking about scalability when they create new features and capabilities without introducing too much overhead.</description></item><item><title>Blog: A Quick Recap of 2023 China Kubernetes Contributor Summit</title><link>https://kubermates.org/docs/2023-10-20-blog-a-quick-recap-of-2023-china-kubernetes-contributor-summit/</link><pubDate>Fri, 20 Oct 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-10-20-blog-a-quick-recap-of-2023-china-kubernetes-contributor-summit/</guid><description>A Quick Recap of 2023 China Kubernetes Contributor Summit A joyful meetup Technical sharing and discussions China’s contributor statistics Acknowledgments On September 26, 2023, the first day of KubeCon + CloudNativeCon + Open Source Summit China 2023 , nearly 50 contributors gathered in Shanghai for the Kubernetes Contributor Summit. All participants in the 2023 Kubernetes Contributor Summit This marked the first in-person offline gathering held in China after three years of the pandemic. The event began with welcome speeches from Kevin Wang from Huawei Cloud, one of the co-chairs of KubeCon, and Puja from Giant Swarm. Following the opening remarks, the contributors introduced themselves briefly. Most attendees were from China, while some contributors had made the journey from Europe and the United States specifically for the conference. Technical experts from companies such as Microsoft, Intel, Huawei, as well as emerging forces like DaoCloud, were present. Laughter and cheerful voices filled the room, regardless of whether English was spoken with European or American accents or if conversations were carried out in authentic Chinese language. This created an atmosphere of comfort, joy, respect, and anticipation. Past contributions brought everyone closer, and mutual recognition and accomplishments made this offline gathering possible. Face to face meeting in Shanghai The attending contributors were no longer just GitHub IDs; they transformed into vivid faces. From sitting together and capturing group photos to attempting to identify “Who is who,” a loosely connected collective emerged. This team structure, although loosely knit and free-spirited, was established to pursue shared dreams.</description></item><item><title>Blog: Spotlight on SIG Architecture: Conformance</title><link>https://kubermates.org/docs/2023-10-05-blog-spotlight-on-sig-architecture-conformance/</link><pubDate>Thu, 05 Oct 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-10-05-blog-spotlight-on-sig-architecture-conformance/</guid><description>Spotlight on SIG Architecture: Conformance About SIG Architecture and the Conformance subproject More on the Conformance Test Suite This is the first interview of a SIG Architecture Spotlight series that will cover the different subprojects. We start with the SIG Architecture: Conformance subproject In this SIG Architecture spotlight, we talked with Riaan Kleinhans (ii-Team), Lead for the Conformance sub-project. Frederico (FSM) : Hello Riaan, and welcome! For starters, tell us a bit about yourself, your role and how you got involved in Kubernetes. Riaan Kleinhans (RK) : Hi! My name is Riaan Kleinhans and I live in South Africa. I am the Project manager for the ii-Team in New Zealand. When I joined ii the plan was to move to New Zealand in April 2020 and then Covid happened. Fortunately, being a flexible and dynamic team we were able to make it work remotely and in very different time zones. The ii. nz team have been tasked with managing the Kubernetes Conformance testing technical debt and writing tests to clear the technical debt. I stepped into the role of project manager to be the link between monitoring, test writing and the community. Through that work I had the privilege of meeting the late Dan Kohn in those first months, his enthusiasm about the work we were doing was a great inspiration. FSM : Thank you - so, your involvement in SIG Architecture started because of the conformance work? RK : SIG Architecture is the home for the Kubernetes Conformance subproject.</description></item><item><title>Blog: Announcing the 2023 Steering Committee Election Results</title><link>https://kubermates.org/docs/2023-10-02-blog-announcing-the-2023-steering-committee-election-results/</link><pubDate>Mon, 02 Oct 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-10-02-blog-announcing-the-2023-steering-committee-election-results/</guid><description>Announcing the 2023 Steering Committee Election Results Results Big Thanks! Get Involved with the Steering Committee The 2023 Steering Committee Election is now complete. The Kubernetes Steering Committee consists of 7 seats, 4 of which were up for election in 2023. Incoming committee members serve a term of 2 years, and all members are elected by the Kubernetes Community. This community body is significant since it oversees the governance of the entire Kubernetes project. With that great power comes great responsibility. You can learn more about the steering committee’s role in their charter. Thank you to everyone who voted in the election; your participation helps support the community’s continued health and success. Congratulations to the elected committee members whose two year terms begin immediately (listed in alphabetical order by GitHub handle): Stephen Augustus ( @justaugustus ), Cisco Paco Xu 徐俊杰 ( @pacoxu ), DaoCloud Patrick Ohly ( @pohly ), Intel Maciej Szulik ( @soltysh ), Red Hat They join continuing members: Benjamin Elder ( @bentheelder ), Google Bob Killen ( @mrbobbytables ), Google Nabarun Pal ( @palnabarun , VMware Stephen Augustus is a returning Steering Committee Member. Thank you and congratulations on a successful election to this round’s election officers: Bridget Kromhout ( @bridgetkromhout ) Davanum Srinavas ( @dims ) Kaslin Fields ( @kaslin ) Thanks to the Emeritus Steering Committee Members. Your service is appreciated by the community: Christoph Blecker ( @cblecker ) Carlos Tadeu Panato Jr. ( @cpanato ) Tim Pepper ( @tpepper ) And thank you to all the candidates who came forward to run for election. This governing body, like all of Kubernetes, is open to all.</description></item><item><title>Blog: Spotlight on SIG ContribEx</title><link>https://kubermates.org/docs/2023-08-14-blog-spotlight-on-sig-contribex/</link><pubDate>Mon, 14 Aug 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-08-14-blog-spotlight-on-sig-contribex/</guid><description>Spotlight on SIG ContribEx Introductions Primary goals and scope Beginner’s guide! Sub-projects under SIG ContribEx Accomplishments Upcoming initiatives Final thoughts What next? Author : Fyka Ansari Welcome to the world of Kubernetes and its vibrant contributor community! In this blog post, we’ll be shining a spotlight on the Special Interest Group for Contributor Experience (SIG ContribEx), an essential component of the Kubernetes project. SIG ContribEx in Kubernetes is responsible for developing and maintaining a healthy and productive community of contributors to the project. This involves identifying and addressing bottlenecks that may hinder the project’s growth and feature velocity, such as pull request latency and the number of open pull requests and issues. SIG ContribEx works to improve the overall contributor experience by creating and maintaining guidelines, tools, and processes that facilitate collaboration and communication among contributors. They also focus on community building and support, including outreach programs and mentorship initiatives to onboard and retain new contributors. Ultimately, the role of SIG ContribEx is to foster a welcoming and inclusive environment that encourages contribution and supports the long-term sustainability of the Kubernetes project. In this blog post, Fyka Ansari interviews Kaslin Fields , a DevRel Engineer at Google, who is a chair of SIG ContribEx, and Madhav Jivrajani , a Software Engineer at VMWare who serves as a SIG ContribEx Tech Lead. This interview covers various aspects of SIG ContribEx, including current initiatives, exciting developments, and how interested individuals can get involved and contribute to the group. It provides valuable insights into the workings of SIG ContribEx and highlights the importance of its role in the Kubernetes ecosystem. Fyka: Let’s start by diving into your background and how you got involved in the Kubernetes ecosystem. Can you tell us more about that journey? Kaslin: I first got involved in the Kubernetes ecosystem through my mentor, Jonathan Rippy, who introduced me to containers during my early days in tech. Eventually, I transitioned to a team working with containers, which sparked my interest in Kubernetes when it was announced.</description></item><item><title>Blog: Spotlight on SIG CLI</title><link>https://kubermates.org/docs/2023-07-20-blog-spotlight-on-sig-cli/</link><pubDate>Thu, 20 Jul 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-07-20-blog-spotlight-on-sig-cli/</guid><description>Spotlight on SIG CLI Introductions About SIG CLI Current projects and challenges Future plans and contribution In the world of Kubernetes, managing containerized applications at scale requires powerful and efficient tools. The command-line interface (CLI) is an integral part of any developer or operator’s toolkit, offering a convenient and flexible way to interact with a Kubernetes cluster. SIG CLI plays a crucial role in improving the Kubernetes CLI experience by focusing on the development and enhancement of kubectl , the primary command-line tool for Kubernetes. kubectl In this SIG CLI Spotlight, Arpit Agrawal, SIG ContribEx-Comms team member, talked with Katrina Verey , Tech Lead &amp;amp; Chair of SIG CLI,and Maciej Szulik , SIG CLI Batch Lead, about SIG CLI, current projects, challenges and how anyone can get involved. So, whether you are a seasoned Kubernetes enthusiast or just getting started, understanding the significance of SIG CLI will undoubtedly enhance your Kubernetes journey. Arpit : Could you tell us a bit about yourself, your role, and how you got involved in SIG CLI? Maciej : I’m one of the technical leads for SIG-CLI. I was working on Kubernetes in multiple areas since 2014, and in 2018 I got appointed a lead. Katrina : I’ve been working with Kubernetes as an end-user since 2016, but it was only in late 2019 that I discovered how well SIG CLI aligned with my experience from internal projects. I started regularly attending meetings and made a few small PRs, and by 2021 I was working more deeply with the Kustomize team specifically. Later that year, I was appointed to my current roles as subproject owner for Kustomize and KRM Functions, and as SIG CLI Tech Lead and Chair. Arpit : Thank you! Could you share with us the purpose and goals of SIG CLI? Maciej : Our charter has the most detailed description, but in few words, we handle all CLI tooling that helps you manage your Kubernetes manifests and interact with your Kubernetes clusters. And how does SIG CLI work to promote best-practices for CLI development and usage in the cloud native ecosystem? Maciej : Within kubectl , we have several on-going efforts that try to encourage new contributors to align existing commands to new standards.</description></item><item><title>Blog: Improve your changelogs</title><link>https://kubermates.org/docs/2023-05-24-blog-improve-your-changelogs/</link><pubDate>Wed, 24 May 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-05-24-blog-improve-your-changelogs/</guid><description>Improve your changelogs Background Overview of major improvements Example How to write commit messages Manually edit changelog Configuration Changelog for cluster repository Reuse pull requests Custom pipelines Jira as issue tracker More customizations References A standard part of the Jenkins X pipelines since a long time is the execution of jx changelog create that takes the commit messages between the release currently being created and the previous one and creates a change log from these. The change log is then stored as a release note in GitHub or other git provider. jx changelog create During the last year some improvements have landed in various Jenkins X components to improve the changelogs and their usefulness. So I’ll take this opportunity to describe these improvements and also in general give hints to how to get useful changelogs. Changelogs haven’t been very informative with regard to upgrades, ie those applied with jx promote or jx updatebot. One example of this is the release notes of jx after the split out of most functionality to plugins. Lately these have improved due to new functionality to propagate changelogs via pull requests. jx promote jx updatebot One place where changelogs have been completely lacking is in cluster repositories. But using the functionality for propagation of changelogs and some changes in jx boot job you can now a get a changelog for every successful application of changes in a cluster. An example of what this functionality achieves can be seen in a release of jx: https://github. com/jenkins-x/jx/releases/tag/v3.10.81 If you scroll past the boilerplate installation instructions you first see the changelog of jx itself generated from commit messages: https://github. com/jenkins-x/jx/compare/v3.10.80.</description></item><item><title>Blog: Spotlight on SIG Network</title><link>https://kubermates.org/docs/2023-05-09-blog-spotlight-on-sig-network/</link><pubDate>Tue, 09 May 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-05-09-blog-spotlight-on-sig-network/</guid><description>Spotlight on SIG Network Networking is one of the core pillars of Kubernetes, and the Special Interest Group for Networking (SIG Network) is responsible for developing and maintaining the networking features of Kubernetes. It covers all aspects to ensure Kubernetes provides a reliable and scalable network infrastructure for containerized applications. In this SIG Network spotlight, Sujay Dey talked with Shane Utt , Software Engineer at Kong, chair of SIG Network and maintainer of Gateway API, on different aspects of the SIG, what are the exciting things going on and how anyone can get involved and contribute here. Sujay : Hello, and first of all, thanks for the opportunity of learning more about SIG Network. I would love to hear your story, so could you please tell us a bit about yourself, your role, and how you got involved in Kubernetes, especially in SIG Network? Shane : Hello! Thank you for reaching out. My Kubernetes journey started while I was working for a small data centre: we were early adopters of Kubernetes and focused on using Kubernetes to provide SaaS products. That experience led to my next position developing a distribution of Kubernetes with a focus on networking. During this period in my career, I was active in SIG Network (predominantly as a consumer). When I joined Kong my role in the community changed significantly, as Kong actively encourages upstream participation. I greatly increased my engagement and contributions to the Gateway API project during those years, and eventually became a maintainer. I care deeply about this community and the future of our technology, so when a chair position for the SIG became available, I volunteered my time immediately. I’ve enjoyed working on Kubernetes over the better part of a decade and I want to continue to do my part to ensure our community and technology continues to flourish.</description></item><item><title>Blog: E2E Testing Best Practices, Reloaded</title><link>https://kubermates.org/docs/2023-04-12-blog-e2e-testing-best-practices-reloaded/</link><pubDate>Wed, 12 Apr 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-04-12-blog-e2e-testing-best-practices-reloaded/</guid><description>E2E Testing Best Practices, Reloaded Overall architecture Debuggability Recovering from test failures Interrupting tests Polling and timeouts Tips for writing and debugging long-running tests Next steps End-to-end (E2E) testing in Kubernetes is how the project validates functionality with real clusters. Contributors sooner or later encounter it when asked to write E2E tests for new features or to help with debugging test failures. Cluster admins or vendors might run the conformance tests, a subset of all tests in the E2E test suite. The underlying E2E framework for writing these E2E tests has been around for a long time. Functionality was added to it as needed, leading to code that became hard to maintain and use. The testing commons WG started cleaning it up, but dissolved before completely achieving their goals. After the migration to Gingko v2 in Kubernetes 1.25, I picked up several of the loose ends and started untangling them. This blog post is a summary of those changes. Some of this content is also found in the Kubernetes contributor document about writing good E2E tests and gets reproduced here to raise awareness that the document has been updated. At the moment, the framework is used in-tree for testing against a cluster ( test/e2e ), testing kubeadm ( test/e2e_kubeadm ) and kubelet ( test/e2e_node ). The goal is to make the core test/e2e/framework a package that has no dependencies on internal code and that can be used in different E2E suites without polluting them with features or options that make no sense for them. This is currently only a technical goal.</description></item><item><title>Blog: From Zero to Kubernets Subproject Lead</title><link>https://kubermates.org/docs/2023-03-29-blog-from-zero-to-kubernets-subproject-lead/</link><pubDate>Wed, 29 Mar 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-03-29-blog-from-zero-to-kubernets-subproject-lead/</guid><description>From Zero to Kubernets Subproject Lead Starting up Always learning Why you - a beginner - are important to the project Getting started in any open-source community can be daunting, especially if it’s a big one like Kubernetes. I wrote this post to share my experience and encourage others to join up. All it takes is some curiosity and a willingness to show up! Here’s how my journey unfolded at a high level: What am I interested in? Is there a SIG (Special Interest Group) or a WG (Working Group) that is dedicated to that topic, or something similar? Sign up for their mailing list and start hopping on meetings. When (never if!) there are opportunities to help out and it aligns with your skills and desired growth areas, raise your hand. Ask for lots of help and don’t be shy about not knowing everything (or anything!) Keep plugging along, even if progress isn’t as fast as you would like it to be. First things first. What are you interested in learning more about? There are so many wonderful SIGs and working groups in the Kubernetes community: there’s something for everyone. And continuing to show up and participate will be so much easier if you think what you are doing is interesting. Likewise, continued participation is what keeps the community thriving, so that interest will drive you to have more of an impact. Also: it’s ok to show up knowing nothing! I remember showing up knowing very little about Kubernetes or how the community itself worked. And while I know more about how the community functions today, I am still learning all the time about it and the project. Fortunately, the community is full of friendly people who want to help you learn.</description></item><item><title>Blog: Reconcile with kpt live apply</title><link>https://kubermates.org/docs/2023-03-09-blog-reconcile-with-kpt-live-apply/</link><pubDate>Thu, 09 Mar 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-03-09-blog-reconcile-with-kpt-live-apply/</guid><description>Reconcile with kpt live apply Configuration Waiting for resources to be reconciled Tagging and release notes kpt live apply Since the dawn of Jenkins X 3 the default last step of reconciling the state of the files in your cluster repository to your cluster has been to execute kubectl apply. You can find more details about this here. kubectl apply There are some drawbacks with kubectl apply though. The one that made me start looking for alternatives was that if you remove a resource from your cluster repository it may not be removed from your cluster. The way deletion works with kubectl apply is that it is handed the option &amp;ndash;prune which will remove resources that are not in the manifests. Except that it doesn’t always work as expected. It will only remove certain kinds of resources defined in kubectl. In my case I removed an HorizontalPodAutoscaler from my cluster repository, but it wasn’t removed from my cluster. kubectl apply kubectl apply &amp;ndash;prune When trying to find a solution to this I first tried to override this default list in kubectl of things to prune, but this turned out to be difficult in the general case. I also tried the already existing alternative of using kapp to apply the manifests, but I couldn’t get that to work. Looking for other options I settled for kpt live apply. kapp kpt live apply You enable the use of kpt live apply by adding kpt live apply KUBEAPPLY = kpt-apply KUBEAPPLY = kpt-apply to the Makefile of your cluster repository anywhere before include versionStream/src/Makefile.</description></item><item><title>Blog: Introducing KWOK: Kubernetes WithOut Kubelet</title><link>https://kubermates.org/docs/2023-03-01-blog-introducing-kwok-kubernetes-without-kubelet/</link><pubDate>Wed, 01 Mar 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-03-01-blog-introducing-kwok-kubernetes-without-kubelet/</guid><description>Introducing KWOK: Kubernetes WithOut Kubelet What is KWOK? Why use KWOK? What are the use cases? What are the limitations? Getting started Getting Involved Author: Shiming Zhang (DaoCloud), Wei Huang (Apple), Yibo Zhuang (Apple) Have you ever wondered how to set up a cluster of thousands of nodes just in seconds, how to simulate real nodes with a low resource footprint, and how to test your Kubernetes controller at scale without spending much on infrastructure? If you answered “yes” to any of these questions, then you might be interested in KWOK, a toolkit that enables you to create a cluster of thousands of nodes in seconds. KWOK stands for Kubernetes WithOut Kubelet. So far, it provides two tools: kwok kwok kwokctl kwokctl kwok KWOK has several advantages: Speed : You can create and delete clusters and nodes almost instantly, without waiting for boot or provisioning. Compatibility : KWOK works with any tools or clients that are compliant with Kubernetes APIs, such as kubectl, helm, kui, etc. Portability : KWOK has no specific hardware or software requirements. You can run it using pre-built images, once Docker or Nerdctl is installed. Alternatively, binaries are also available for all platforms and can be easily installed. Flexibility : You can configure different node types, labels, taints, capacities, conditions, etc. , and you can configure different pod behaviors, status, etc. to test different scenarios and edge cases. Performance : You can simulate thousands of nodes on your laptop without significant consumption of CPU or memory resources. KWOK can be used for various purposes: Learning : You can use KWOK to learn about Kubernetes concepts and features without worrying about resource waste or other consequences.</description></item><item><title>Blog: Foreign aliases</title><link>https://kubermates.org/docs/2023-02-09-blog-foreign-aliases/</link><pubDate>Thu, 09 Feb 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-02-09-blog-foreign-aliases/</guid><description>Foreign aliases Background Foreign aliases OWNERS and OWNERS_ALIASES in new repositories In an organisation with many repositories and developers that are frequently shifting the maintenance of OWNERS and OWNERS_ALIASES files can be tedious. In the passing year a couple of functionalities has been added to help with this. To avoid maintaining the OWNERS_ALIASES file in many repositories you can now refer to the OWNERS_ALIASES file in another repository. In the Jenkins X project we have the main OWNERS_ALIASES file in the jx-community repository. So in the jx repository the OWNERS_ALIASES file only looks like this: foreignAliases : - name : jx-community foreignAliases : - name : jx-community The organisation defaults to be the same as for the repository, but can specify as well. So in the jx-project repository the OWNERS_ALIASES file looks like this: foreignAliases : - name : jx-community org : jenkins-x foreignAliases : - name : jx-community org : jenkins-x Using the filed ref you can also specify a branch or tag to use instead of the default one of the repository. ref When creating or importing a repository using jx project the default content of OWNERS and OWNERS_ALIASES isn’t that useful since only the current user are put in the files. jx project If you create your own quickstarts you place the OWNERS and / or OWNERS_ALIASES files with the content of your liking in those. A recent new functionality is that you can put OWNERS and / or OWNERS_ALIASES files in the extensions directory of your cluster repository. These files will then be used as the default content of the files in new repositories. extensions ← Previous.</description></item><item><title>Blog: Project ideas for Google Summer of Code 2023 ☀️</title><link>https://kubermates.org/docs/2023-02-06-blog-project-ideas-for-google-summer-of-code-2023/</link><pubDate>Mon, 06 Feb 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-02-06-blog-project-ideas-for-google-summer-of-code-2023/</guid><description>Project ideas for Google Summer of Code 2023 ☀️ 1. CD events integration with Jenkins X 2. Implement drift detection (gitops) 3. RBAC (Role Based Access Control) in Jenkins X UI 4. Simplifying Jenkins X pipeline syntax 5. Multi-tenancy in Jenkins X Next Steps We have put together some project ideas as part of our application to participate in the Google Summer of Code 2023 program. The cdEvents project standardises the way systems talk to each other, which enables Interoperability between systems so they speak a common language through the cdEvents spec in the event. Creating a capability in Jenkins X that can receive and sent a cdEvent would benefit the project and the DevOps ecosystem in general, by stopping glue code used to integrate systems and power innovation by letting end users swap out tools with no effort. Ability to receive cdEvents Ability to parse cdEvents so Jenkins X understands them Ability to send cdEvents Jenkins X, Kubernetes, golang, cdEvents Brad McCoy https://cloudevents. io/ https://www. youtube. com/watch?v=yg7RuDWHwV8 https://www.</description></item><item><title>Blog: Spotlight on SIG Instrumentation</title><link>https://kubermates.org/docs/2023-02-03-blog-spotlight-on-sig-instrumentation/</link><pubDate>Fri, 03 Feb 2023 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2023-02-03-blog-spotlight-on-sig-instrumentation/</guid><description>Spotlight on SIG Instrumentation About SIG Instrumentation Current status and ongoing challenges Community and contribution Observability requires the right data at the right time for the right consumer (human or piece of software) to make the right decision. In the context of Kubernetes, having best practices for cluster observability across all Kubernetes components is crucial. SIG Instrumentation helps to address this issue by providing best practices and tools that all other SIGs use to instrument Kubernetes components-like the API server , scheduler , kubelet and kube-controller-manager. In this SIG Instrumentation spotlight, Imran Noor Mohamed , SIG ContribEx-Comms tech lead talked with Elana Hashman , and Han Kang , chairs of SIG Instrumentation, on how the SIG is organized, what are the current challenges and how anyone can get involved and contribute. Imran (INM) : Hello, thank you for the opportunity of learning more about SIG Instrumentation. Could you tell us a bit about yourself, your role, and how you got involved in SIG Instrumentation? Han (HK) : I started in SIG Instrumentation in 2018, and became a chair in 2020. I primarily got involved with SIG instrumentation due to a number of upstream issues with metrics which ended up affecting GKE in bad ways. As a result, we ended up launching an initiative to stabilize our metrics and make metrics a proper API. Elana (EH) : I also joined SIG Instrumentation in 2018 and became a chair at the same time as Han. I was working as a site reliability engineer (SRE) on bare metal Kubernetes clusters and was working to build out our observability stack. I encountered some issues with label joins where Kubernetes metrics didn’t match kube-state-metrics ( KSM ) and started participating in SIG meetings to improve things. I helped test performance improvements to kube-state-metrics and ultimately coauthored a KEP for overhauling metrics in the 1.14 release to improve usability.</description></item><item><title>Blog: Prow and Tide for Kubernetes Contributors</title><link>https://kubermates.org/docs/2022-12-12-blog-prow-and-tide-for-kubernetes-contributors/</link><pubDate>Mon, 12 Dec 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-12-12-blog-prow-and-tide-for-kubernetes-contributors/</guid><description>Prow and Tide for Kubernetes Contributors Why is this helpful Back to squashing Assignment, review, approval. More about Prow and Tide More advanced usage Authors: Chris Short , Frederico Muñoz In my work in the Kubernetes world, I look up a label or Prow command often. The systems behind the scenes ( Prow and Tide ) are here to help Kubernetes Contributors get stuff done. Labeling which SIG, WG, or subproject is as important as the issue or PR having someone assigned. To quote the docs , “Tide is a Prow component for managing a pool of GitHub PRs that match a given set of criteria. It will automatically retest PRs that meet the criteria (’tide comes in’) and automatically merge them when they have up-to-date passing test results (’tide goes out’). ” What actually prompted this article is the awesomely amazing folks on the Contributor Comms team saying, “I need to squash my commits and push that. ” Which immediately made me remember the wonder of the Tide label: tide/merge-method-squash. tide/merge-method-squash Contributing to Kubernetes will, most of the time, involve some kind of git-based action, specifically on the Kubernetes GitHub. This can be an obstacle to those less exposed to git and/or GitHub, and is especially noticeable when we’re dealing with non-code contributions (documentation, blog posts, etc. ). git When a contributor submits something, it will generally be through a pull request.</description></item><item><title>Blog: GSoC 2022 Final Report: Building Jenkins X UI</title><link>https://kubermates.org/docs/2022-11-13-blog-gsoc-2022-final-report-building-jenkins-x-ui/</link><pubDate>Sun, 13 Nov 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-11-13-blog-gsoc-2022-final-report-building-jenkins-x-ui/</guid><description>GSoC 2022 Final Report: Building Jenkins X UI Jenkins X New UI Why need a new UI? How to use it? Work Done Stop a running or pending PipelineActivity from UI Show message with status of the PipelineActivity Implement a DAG for PipelineActivity What’s next? Acknowledgements It is a web application built with Golang for the backend and Sveltekit for the frontend, both of which are built together and used in the same container. To function properly, it must be installed as a helm chart with Jenkins X CRDs. 🌟 It has light and dark themes. A good UI is essential for a CI/CD tool, as not everyone is familiar with the CLI. The current UI (jx-pipeline-visualizer) is a read-only UI, the user can view the logs of PipelineActivity but neither can start nor stop the pipeline. Features that the UI will provide: Start and Stop a PipelineActivity. Have an audit trail. A graphical representation of PipelineActivity. RBAC to limit access to certain functionalities. New Jenkins X UI focus on Simplicity, Security and a Superb User Experience. This is NOT GA (General Availability) yet. Visit the project repo here to try it.</description></item><item><title>Blog: GSoC 2022 Final Report: Improving Supply Chain Security</title><link>https://kubermates.org/docs/2022-11-08-blog-gsoc-2022-final-report-improving-supply-chain-security/</link><pubDate>Tue, 08 Nov 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-11-08-blog-gsoc-2022-final-report-improving-supply-chain-security/</guid><description>GSoC 2022 Final Report: Improving Supply Chain Security Project Description Work Done Enhancing the jx version output Integrating with Tekton Chains to sign TaskRuns and PipelineRuns Software Bill of Materials (SBOM) Signing Jenkins X artifacts What’s next? Acknowledgements Supply chain security is a rising concern in the current software era. Securing the software supply chain encompasses vulnerability remediation and the implementation of controls throughout the software development process. Due to massive increase in attacks on software supply chain and the diversity of its types , Jenkins X has to make efforts to ensure that the build process is secure. As part of securing Jenkins X installation by default I worked on both securing our own components and enabling our users to use these features in their build and release steps. The work done so far covers these four sections. Enhancing the jx version output jx version Integrating with Tekton Chains to sign TaskRuns and PipelineRuns Software Bill of Materials (SBOM) Signing Jenkins X artifacts jx version Description: A first step towards securing Jenkins X supply chain is to increase the amount of information gained from running jx version command. jx version Implementation The issue created for this task is here. The PR to fix it is here. Description: As Jenkins X uses tekton as its pipeline execution engine, TaskRun and PipelineRun are considered the key components of Jenkins X pipeline activities and steps Tekton Chains monitors the execution of all TaskRun and PipelineRun inside the cluster and takes a snapshot upon completion of each of them to sign with user-provided cryptographic keys and store them on the backend storage. The payload and signature cn be verified later using cosign verify-blob. TaskRun PipelineRun activities steps TaskRun PipelineRun cosign verify-blob Implementation I used the helm chart developed by Chainguard for integrating Chains with Jenkins X. To integrate the chart and added support for it on jx3-versions to make installation of helm chart easy for our users.</description></item><item><title>Blog: Hacktoberfest 2022</title><link>https://kubermates.org/docs/2022-10-03-blog-hacktoberfest-2022/</link><pubDate>Mon, 03 Oct 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-10-03-blog-hacktoberfest-2022/</guid><description>Hacktoberfest 2022 Contribute to Jenkins X Ask us questions We are excited to announce that Jenkins X will be participating in Hacktoberfest again this year! Hacktoberfest is a month-long global celebration of open source software. All backgrounds and skill levels are encouraged to participate in Hacktoberfest and join a global community of open source contributors. Learn more about Hacktoberfest and sign up here. We welcome your contributions to the Jenkins X project ! Issues labelled “hacktoberfest” generally indicate good first issues. However, all pull requests will count towards your Hacktoberfest challenge. Refer to the contribution guides for making code and documentation changes. Refer to this document to get an idea about the location of the different Jenkins X source code repositories. Normally the labels assigned to the ticket will help you in deciding which part of the Jenkins X codebase to look at. The maintainers will also try to link the relevant repository for hacktoberfest issues in the issue comment. Once you are done with the contribution, request a review from the maintainers by adding the comment in your pull request. /cc @ankitm123 @babadofar @msvticket @osamamagdy @rajatgupta24 @tomhobson /cc @ankitm123 @babadofar @msvticket @osamamagdy @rajatgupta24 @tomhobson We’re happy to help if you have any questions. Talk to us on our slack channels, which are part of the Kubernetes slack.</description></item><item><title>Blog: Implementing the Auto-refreshing Official Kubernetes CVE Feed</title><link>https://kubermates.org/docs/2022-09-12-blog-implementing-the-auto-refreshing-official-kubernetes-cve-feed/</link><pubDate>Mon, 12 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-09-12-blog-implementing-the-auto-refreshing-official-kubernetes-cve-feed/</guid><description>Implementing the Auto-refreshing Official Kubernetes CVE Feed Implementation Details Pre-requisites Building on existing tooling Design Considerations Integrity and Access Control Protections Freshness Guarantees What’s Next? Author : Pushkar Joglekar (VMware) Accompanying the release of Kubernetes v1.25, we announced availability of an official CVE feed as an alpha feature. This blog will cover how we implemented this feature. alpha An auto-refreshing CVE feed allows users and implementers to programmatically fetch the list of CVEs announced by the Kubernetes SRC (Security Response Committee). To ensure freshness and minimal maintainer overhead, the feed updates automatically by fetching the CVE related information from the CVE announcement GitHub Issues. Creating these issues is already part of the existing Security Response Committee (SRC) workflow. Until December 2021, it was not possible to filter for issues or PRs that are tied to CVEs announced by Kubernetes SRC. We added a new label, official-cve-feed to address that, and SIG-Security labelled relevant issues with it. The in-scope issues are closed issues for which there is a CVE ID(s) and is officially announced as a Kubernetes security vulnerability by SRC. You can now filter on all of these issues and find them here. official-cve-feed closed For future security vulnerabilities, we added the label to the SRC playbook so that all the future in-scope issues will automatically have this label. For the next step, we created a prow job in order to periodically query the GitHub REST API and pull the relevant issues. The job runs every two hours and pushes the CVE related information fetched from GitHub into a Google Cloud Bucket.</description></item><item><title>Blog: Enhancements Opt-in Process Change for v1.26</title><link>https://kubermates.org/docs/2022-09-09-blog-enhancements-opt-in-process-change-for-v1-26/</link><pubDate>Fri, 09 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-09-09-blog-enhancements-opt-in-process-change-for-v1-26/</guid><description>Enhancements Opt-in Process Change for v1.26 Context and Motivations How does the Github Project Board work? What does this mean for the community? Author: Grace Nguyen Since the inception of the Kubernetes release team, we have used a spreadsheet to keep track of enhancements for the release. The project has scaled massively in the past few years, with almost a hundred enhancements collected for the 1.24 release. This process has become error-prone and time consuming. A lot of manual work is required from the release team and the SIG leads to populate KEPs data in the sheet. We have received continuous feedback from our contributors to streamline the process. Starting with the 1.26 release, we are replacing the enhancements tracking spreadsheet with an automated GitHub project board. The board is populated with a script gathering all KEP issues in the kubernetes/enhancements repo that have the label lead-opted-in. The enhancements’ stage and SIG information will also be automatically pulled from the KEP issue. kubernetes/enhancements lead-opted-in After the KEP is populated on the Github Project Board, the Enhancements team will manually update the KEP with the label tracked/yes , tracked/no and on occasions, tracked/out-of-tree. The tracked label signifies qualification for the closest approaching milestone. For example, at the beginning of the release, tracked/yes means that the KEP has satisfied all Enhancements Freeze requirements and similarly for Code Freeze, tracked/yes means that all code related to the KEP has been merged. The tracked label is reserved for the Enhancements team use only.</description></item><item><title>Blog: Spotlight on SIG Storage</title><link>https://kubermates.org/docs/2022-08-22-blog-spotlight-on-sig-storage/</link><pubDate>Mon, 22 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-08-22-blog-spotlight-on-sig-storage/</guid><description>Spotlight on SIG Storage About SIG Storage Storage and Kubernetes Ongoing challenges Community involvement Since the very beginning of Kubernetes, the topic of persistent data and how to address the requirement of stateful applications has been an important topic. Support for stateless deployments was natural, present from the start, and garnered attention, becoming very well-known. Work on better support for stateful applications was also present from early on, with each release increasing the scope of what could be run on Kubernetes. Message queues, databases, clustered filesystems: these are some examples of the solutions that have different storage requirements and that are, today, increasingly deployed in Kubernetes. Dealing with ephemeral and persistent storage, local or remote, file or block, from many different vendors, while considering how to provide the needed resiliency and data consistency that users expect, all of this is under SIG Storage’s umbrella. In this SIG Storage spotlight, Frederico Muñoz (Cloud &amp;amp; Architecture Lead at SAS) talked with Xing Yang , Tech Lead at VMware and co-chair of SIG Storage, on how the SIG is organized, what are the current challenges and how anyone can get involved and contribute. Frederico (FSM) : Hello, thank you for the opportunity of learning more about SIG Storage. Could you tell us a bit about yourself, your role, and how you got involved in SIG Storage. Xing Yang (XY) : I am a Tech Lead at VMware, working on Cloud Native Storage. I am also a Co-Chair of SIG Storage. I started to get involved in K8s SIG Storage at the end of 2017, starting with contributing to the VolumeSnapshot project. At that time, the VolumeSnapshot project was still in an experimental, pre-alpha stage.</description></item><item><title>Blog: Meet Our Contributors - APAC (China region)</title><link>https://kubermates.org/docs/2022-08-15-blog-meet-our-contributors-apac-china-region/</link><pubDate>Mon, 15 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-08-15-blog-meet-our-contributors-apac-china-region/</guid><description>Meet Our Contributors - APAC (China region) Andy Zhang Shiming Zhang Paco Xu Jintao Zhang Authors &amp;amp; Interviewers: Avinesh Tripathi , Debabrata Panigrahi , Jayesh Srivastava , Priyanka Saggu , Purneswar Prasad , Vedant Kakde Hello, everyone 👋 Welcome back to the third edition of the “Meet Our Contributors” blog post series for APAC. This post features four outstanding contributors from China, who have played diverse leadership and community roles in the upstream Kubernetes project. So, without further ado, let’s get straight to the article. Andy Zhang currently works for Microsoft China at the Shanghai site. His main focus is on Kubernetes storage drivers. Andy started contributing to Kubernetes about 5 years ago. He states that as he is working in Azure Kubernetes Service team and spends most of his time contributing to the Kubernetes community project. Now he is the main contributor of quite a lot Kubernetes subprojects such as Kubernetes cloud provider code. His open source contributions are mainly self-motivated. In the last two years he has mentored a few students contributing to Kubernetes through the LFX Mentorship program, some of whom got jobs due to their expertise and contributions on Kubernetes projects. Andy is an active member of the China Kubernetes community. He adds that the Kubernetes community has a good guide about how to become members, code reviewers, approvers and finally when he found out that some open source projects are in the very early stage, he actively contributed to those projects and became the project maintainer.</description></item><item><title>Blog: Enhancing Kubernetes one KEP at a Time</title><link>https://kubermates.org/docs/2022-08-11-blog-enhancing-kubernetes-one-kep-at-a-time/</link><pubDate>Thu, 11 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-08-11-blog-enhancing-kubernetes-one-kep-at-a-time/</guid><description>Enhancing Kubernetes one KEP at a Time What’s the enhancements subteam? What does the enhancements subteam do? How can I get involved with the release team? How can I find out more? Author: Ryler Hockenbury (Mastercard) Did you know that Kubernetes v1.24 has 46 enhancements ? That’s a lot of new functionality packed into a 4-month release cycle. The Kubernetes release team coordinates the logistics of the release, from remediating test flakes to publishing updated docs. It’s a ton of work, but they always deliver. The release team comprises around 30 people across six subteams - Bug Triage, CI Signal, Enhancements, Release Notes, Communications, and Docs. Each of these subteams manages a component of the release. This post will focus on the role of the enhancements subteam and how you can get involved. Great question. We’ll get to that in a second but first, let’s talk about how features are managed in Kubernetes. Each new feature requires a Kubernetes Enhancement Proposal - KEP for short. KEPs are small structured design documents that provide a way to propose and coordinate new features. The KEP author describes the motivation, design (and alternatives), risks, and tests - then community members provide feedback to build consensus. KEPs are submitted and updated through a pull request (PR) workflow on the k/enhancements repo.</description></item><item><title>Blog: Spotlight on SIG Docs</title><link>https://kubermates.org/docs/2022-08-02-blog-spotlight-on-sig-docs/</link><pubDate>Tue, 02 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-08-02-blog-spotlight-on-sig-docs/</guid><description>Spotlight on SIG Docs Introduction A summary of the conversation Could you tell us a little bit about what SIG Docs does? There are 2 subprojects under Docs: blogs and localization. How has the community benefited from it and are there some interesting contributions by those teams you want to highlight? Recently there has been a lot of buzz around the Kubernetes ecosystem as well as the industry regarding the removal of dockershim in the latest 1.24 release. How has SIG Docs helped the project to ensure a smooth change among the end-users? Why should new and existing contributors consider joining this SIG? How do you help new contributors get started? Are there any prerequisites to join? Any SIG related accomplishment that you’re really proud of? Is there something exciting coming up for the future of SIG Docs that you want the community to know? Wrap Up Author: Purneswar Prasad The official documentation is the go-to source for any open source project. For Kubernetes, it’s an ever-evolving Special Interest Group (SIG) with people constantly putting in their efforts to make details about the project easier to consume for new contributors and users. SIG Docs publishes the official documentation on kubernetes. io which includes, but is not limited to, documentation of the core APIs, core architectural details, and CLI tools shipped with the Kubernetes release. To learn more about the work of SIG Docs and its future ahead in shaping the community, I have summarised my conversation with the co-chairs, Divya Mohan (DM), Rey Lejano (RL) and Natali Vlatko (NV), who ran through the SIG’s goals and how fellow contributors can help. SIG Docs is the special interest group for documentation for the Kubernetes project on kubernetes. io, generating reference guides for the Kubernetes API, kubeadm and kubectl as well as maintaining the official website’s infrastructure and analytics. The remit of their work also extends to docs releases, translation of docs, improvement and adding new features to existing documentation, pushing and reviewing content for the official Kubernetes blog and engaging with the Release Team for each cycle to get docs and blogs reviewed. Blogs : This subproject highlights new or graduated Kubernetes enhancements, community reports, SIG updates or any relevant news to the Kubernetes community such as thought leadership, tutorials and project updates, such as the Dockershim removal and removal of PodSecurityPolicy, which is upcoming in the 1.25 release. Tim Bannister, one of the SIG Docs tech leads, does awesome work and is a major force when pushing contributions through to the docs and blogs.</description></item><item><title>Blog: Introduction to Software Bill Of Materials</title><link>https://kubermates.org/docs/2022-07-24-blog-introduction-to-software-bill-of-materials/</link><pubDate>Sun, 24 Jul 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-07-24-blog-introduction-to-software-bill-of-materials/</guid><description>Introduction to Software Bill Of Materials Introduction Definition: What is SBOM? Use cases in Supply chain security Before going through Software Bill Of Materials (SBOMs), we need to set the ground for a rising concern in the software industry which is Software Supply Chain Security. Like traditional industries, deploying a piece of a software artifact goes through multiple stages composed of collecting source code components, libraries, tools, and processes used in those stages. Fig. 1 https://blog. convisoappsec. com/en/is-your-software-supply-chain-secure/ A supply chain attack can occur along the chain from submitting unauthorized malicious code in your source, unauthorized injection of harmful dependencies, and even replacing packages after being built with other compromised artifacts. A more detailed explanation about those types of attacks is here Due to its importance and being a critical issue, generating SBOM for your software adds another layer of protection to this threat. As far as we know, developers around the world are building web applications using hundreds of third-party open-source libraries and packages. You can confidently tell that 90% of the software products around the world are built over open-source components. With that in mind, we need to keep track of using these dependencies while building our applications. What if there are vulnerabilities in the libraries we use? How to efficiently protect ourselves against it?. Software Bill Of Materials (SBOM) is a complete formally structured list of the materials (components, packages, libraries, SDK) used to build (i.</description></item><item><title>Blog: Software Bill Of Materials Formats</title><link>https://kubermates.org/docs/2022-07-24-blog-software-bill-of-materials-formats/</link><pubDate>Sun, 24 Jul 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-07-24-blog-software-bill-of-materials-formats/</guid><description>Software Bill Of Materials Formats Prerequisite Different SBOM formats comparison 1 - The Software Package Data Exchange (SPDX) 2 - Software Identification (SWID) Tags 3 - CycloneDX Generate SBOMs manually? definitely not If you don’t understand what is Software Bill of Materials (SBOM), please read this blog post first. The National Telecommunications and Information Administration (NTIA) in the U. S. defined minimum requirements for SBOM formats : Identifying the supplier of the software component. Identifying the details about the version of the component. Including unique identifiers for the component like cryptographic hash functions. Including the relationships between all dependencies inside the component. Including a timestamp of when and by whom the SBOM report was created or last modified In this section, we discuss different kinds and formats for SBOM standards and make a brief comparison between them. Three commonly used standards achieved the NTIA minimum requirements for SBOM generation and each one results in a different final SBOM document. History: SPDX is an open-source machine-readable format adopted by the Linux Foundation as an industry standard. The specifications are implemented as a file format that identifies the software components within a larger piece of computer software and fulfilling the requirements of NTIA. The SPDX project started in 2010 and was initially dedicated to solving the issues around open source licensing compliance.</description></item><item><title>Blog: Software Bill Of Materials generation tools</title><link>https://kubermates.org/docs/2022-07-24-blog-software-bill-of-materials-generation-tools/</link><pubDate>Sun, 24 Jul 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-07-24-blog-software-bill-of-materials-generation-tools/</guid><description>Software Bill Of Materials generation tools Prerequisite Different SBOM generation tools comparison 1 - Anchore Syft 2- Opensbom’s Spdx-Sbom-Generator 3- Kubernetes BOM 4- Microsoft SBOM tool 5- Tern Before you read this, you have to understand what are SBOMs and what are different formats of SBOMs If you got this far, you already realize the importance of SBOM generation, and also it should meet certain requirements to achieve its purpose. Due to various requirements depending on what standard you’re following, there has to be a way to automatically generate different output formats for different standards. Also, it has to be suited for ci/cd solutions to keep up with the increasing number of releases for each organization. Note: Here we’re only considering open source tools Introduction: Anchore is a platform that implements sbom-powered supply chain security solutions for developers and enterprises. For generating SBOMs, a CLI tool and library named Syft was developed by Anchore that could be injected into your ci/cd pipeline to generate SBOMs from container images and filesystems at each step. Integration and Support: Syft is supported on Linux, Mac, and Windows and it can run as a docker container which makes it a great suit for CI systems. Other than the 3 SBOM standards, Syft can generate its JSON standard format to be input for other Anchore tools like Grype which is a vulnerability scanner for container images and filesystems. It supports projects based on the following package managers: Alpine (apk) C (conan) C++ (conan) Dart (pubs) Debian (dpkg) Dotnet (deps. json) Objective-C (cocoapods) Go (go. mod, Go binaries) Haskell (cabal, stack) Java (jar, ear, war, par, sar) JavaScript (npm, yarn) Jenkins Plugins (jpi, hpi) PHP (composer) Python (wheel, egg, poetry, requirements. txt) Red Hat (rpm) Ruby (gem) Rust (cargo. lock) Swift (cocoapods) Features and Specs: Easy to use Syft can generate a simple basic sbom by just running syft &lt;image&gt; this will only include the softwares included in the image’s final layer.</description></item><item><title>Blog: GSoC 2022 Community Bonding Period with Jenkins X</title><link>https://kubermates.org/docs/2022-07-12-blog-gsoc-2022-community-bonding-period-with-jenkins-x/</link><pubDate>Tue, 12 Jul 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-07-12-blog-gsoc-2022-community-bonding-period-with-jenkins-x/</guid><description>GSoC 2022 Community Bonding Period with Jenkins X Introduction How I started What I learned during Community Bonding Period Highlights Hello everyone, I am Rajat Gupta, pursuing my bachelor’s in Information Technology. In 2022, I have been selected as a student developer in Google Summer of Code under Jenkins X. We will be building a new UI for Jenkins X. I got this news on May 20th, as I received an email from google. The technologies needed were Golang , Kubernetes , and GitOps. I used golang only once before, while linting Jenkins X codebase, I only used Kubernetes once before while setting up a k3s cluster to run Jenkins X pipelines. These tasks were necessary to do for all GSoC participants. Apart from that, I was a total beginner. So, when I got selected, I had a lot to learn, my mentors gave me a 30 Day plan. They also suggested some resources and conference talks which made it simple for me to start. 30 Day plan was: Learn golang Getting started with Kubernetes Learn what is a CRD and how they are used? Complete a tutorial on Kubebuilder Getting started with Tekton pipelines Learn Jenkins X We also had some amazing pair programming sessions, where I used to share my screen and my mentor guided me through, which is not common because mentors have a very busy schedule. But my mentors helped me a lot.</description></item><item><title>Blog: Contextual Logging in Kubernetes 1.24</title><link>https://kubermates.org/docs/2022-05-25-blog-contextual-logging-in-kubernetes-1-24/</link><pubDate>Wed, 25 May 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-05-25-blog-contextual-logging-in-kubernetes-1-24/</guid><description>Contextual Logging in Kubernetes 1.24 Structured logging Contextual logging klog enhancements Contextual logging API ktesting logger klogr Reusable output test Output flushing Various other changes logcheck Next steps Authors: Patrick Ohly (Intel) The Structured Logging Working Group has added new capabilities to the logging infrastructure in Kubernetes 1.24. This blog post explains how developers can take advantage of those to make log output more useful and how they can get involved with improving Kubernetes. The goal of structured logging is to replace C-style formatting and the resulting opaque log strings with log entries that have a well-defined syntax for storing message and parameters separately, for example as a JSON struct. When using the traditional klog text output format for structured log calls, strings were originally printed with \n escape sequences, except when embedded inside a struct. For structs, log entries could still span multiple lines, with no clean way to split the log stream into individual entries: \n I1112 14:06:35.783529 328441 structured_logging. go:51] &amp;ldquo;using InfoS&amp;rdquo; longData={Name:long Data:Multiple lines with quite a bit of text. internal:0} I1112 14:06:35.783549 328441 structured_logging. go:52] &amp;ldquo;using InfoS with\nthe message across multiple lines&amp;rdquo; int=1 stringData=&amp;ldquo;long: Multiple\nlines\nwith quite a bit\nof text. &amp;quot; str=&amp;ldquo;another value&amp;rdquo; I1112 14:06:35.783529 328441 structured_logging. go:51] &amp;ldquo;using InfoS&amp;rdquo; longData={Name:long Data:Multiple lines with quite a bit of text. internal:0} I1112 14:06:35.783549 328441 structured_logging. go:52] &amp;ldquo;using InfoS with\nthe message across multiple lines&amp;rdquo; int=1 stringData=&amp;ldquo;long: Multiple\nlines\nwith quite a bit\nof text.</description></item><item><title>Blog: February 2022 Community Meeting Highlights</title><link>https://kubermates.org/docs/2022-05-05-blog-february-2022-community-meeting-highlights/</link><pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-05-05-blog-february-2022-community-meeting-highlights/</guid><description>February 2022 Community Meeting Highlights Author: Nigel Brown (VMware) We just had our first contributor community meeting this year, and it was awesome to be back with you in that format. These meetings will be happening on Zoom once per month, on the third Thursday of the month - that should be available in your calendar if you’re subscribed to the k-dev mailing list. Community meetings are an opportunity for you to meet synchronously with other members of the Kubernetes community to talk about issues of general appeal. This meeting kicked off with an update on the 1.24 release with Xander Grzywinski, who is one of the shadows for the release team leads. This release is scheduled for April 19, 2022 with a code freeze scheduled for March 30th. At the time of the meeting there were 66 individual enhancements included, as well as bug fixes. You can join the conversation on Slack in #sig-release. Update: Kubernetes 1.24 was delayed and released on May 3, 2022. From there, the discussion moved to the dockershim removal and the docs updates we need to make around that, with the discussion led by Kat Cosgrove. The main takeaway was that if you have a platform, talk to folks about this change. To most interacting with Kubernetes, it is probably not as impactful as it sounds. We have a helpful FAQ if you need.</description></item><item><title>Blog: Kubernetes 1.22 - Breaking change!</title><link>https://kubermates.org/docs/2022-04-22-blog-kubernetes-1-22-breaking-change/</link><pubDate>Fri, 22 Apr 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-04-22-blog-kubernetes-1-22-breaking-change/</guid><description>Kubernetes 1.22 - Breaking change! To allow Jenkins X to support Kubernetes 1.22, we had to update our version of Tekton. This updated version of Tekton contains breaking changes that has consequences if you made your own custom Jenkins X pipelines. To make sure that your custom pipelines continue to work after this upgrade, you must edit the resource settings in your pipelines. Otherwise your pipelines will most likely not be able to start at all, or if they do, consume a lot of resources. Tekton made changes in how to calculate the resources needed to run a pipeline, in order to support the concept of LimitRange in Kubernetes (introduced in Kubernetes version 1.10). Previously, Tekton simply used the maximum requested cpu and memory of any single step, and set that as limits for the all steps in the pipeline. For more details, please read the Tekton documentation on LimitRange. In the Tekton pipeline files, the StepTemplate needs to be changed to not specify resource requests , but only setting an empty resource limits : requests limits stepTemplate : image : uses:jenkins-x/jx3-pipeline-catalog/tasks/go/release. yaml@a5ab19ebc5a074e0402c5016b11bc11b32cc5c83 name : &amp;quot;&amp;quot; resources : # override limits for all containers here limits : {} stepTemplate : image : uses:jenkins-x/jx3-pipeline-catalog/tasks/go/release. yaml@a5ab19ebc5a074e0402c5016b11bc11b32cc5c83 name : &amp;quot;&amp;quot; resources : # override limits for all containers here limits : {} The resource requests should be set on only one individual step: requests steps : - image : uses:Mentor-Medier/jx3-pipeline-catalog/tasks/git-clone/git-clone-pr. yaml@versionStream name : &amp;quot;&amp;quot; resources : {} - name : jx-variables resources : requests : cpu : 400m memory : 512Mi steps : - image : uses:Mentor-Medier/jx3-pipeline-catalog/tasks/git-clone/git-clone-pr. yaml@versionStream name : &amp;quot;&amp;quot; resources : {} - name : jx-variables resources : requests : cpu : 400m memory : 512Mi To see examples of what changes you need to apply to your custom pipelines you may investigate this PR on The Jenkins X pipeline catalog.</description></item><item><title>Blog: Google Summer of Code 2022 project proposal template ☀️</title><link>https://kubermates.org/docs/2022-04-05-blog-google-summer-of-code-2022-project-proposal-template/</link><pubDate>Tue, 05 Apr 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-04-05-blog-google-summer-of-code-2022-project-proposal-template/</guid><description>Google Summer of Code 2022 project proposal template ☀️ Proposal template Full name: Country: Time zone: Email: Github ID: Personal blog (optional): Twitter/LinkedIn/others: Have you participated in the Google Summer of Code previously? Please describe your experience. Are you applying to any other organizations this year? If so, please list them. How many hours will you devote to your GSoC project each week? Do you have any other commitments during the summer? Have you ever contributed code to Jenkins X? If you have, post the Pull Request (PR) links (It’s ok if they have not been merged/approved yet) Do you plan on contributing to the Jenkins X project after GSoC is finished? Were you able to install Jenkins X locally on your laptop? If no, please explain why (Unsupported architecture, missing documentation, time constraints etc)? If yes, how was the installation experience (missing documentation, complexity etc)? If no, please explain why (Unsupported architecture, missing documentation, time constraints etc)? If yes, how was the installation experience (missing documentation, complexity etc)? Have you used kubernetes in the past? Please provide a brief description and if possible links to the work. Have you used golang in the past? Please provide a brief description and if possible some code samples. (If you are interested in the UI project) Have you worked with a frontend framework/library in the past? Please provide a brief description and if possible some code samples. Repeat this section for as many project ideas as you want (Go in the order most likely to least likely). Title of the idea that you are interested in Brief description of the idea. Have you worked on a similar project in the past? If yes, please provide a brief description (with links if possible) How will the project benefit Jenkins X and the community Will it help in driving adoption of Jenkins X? If yes, then how/why? Will it help in driving adoption of Jenkins X? If yes, then how/why? Deliverables Include any milestones and deadlines Include time for investigation, coding and documentation Include any milestones and deadlines Include time for investigation, coding and documentation ← Previous.</description></item><item><title>Blog: Project proposal for Google Season of Docs 2022 📄</title><link>https://kubermates.org/docs/2022-03-23-blog-project-proposal-for-google-season-of-docs-2022/</link><pubDate>Wed, 23 Mar 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-03-23-blog-project-proposal-for-google-season-of-docs-2022/</guid><description>Project proposal for Google Season of Docs 2022 📄 Project proposal Timeline Budget Additional information Other ideas We have put together a project proposal as part of our application to participate in the Google Season of Docs 2022 program. Jenkins X provides automated CI/CD for Kubernetes with Preview Environments on Pull Requests using Cloud Native pipelines from Tekton. End users of Jenkins X are unable to find information on how to use Jenkins X for real world use cases which includes: Production best practices Scanning images in Jenkins X pipelines How to make your app use different configuration/secrets for each environment Observability for your apps, logging, metrics, tracing Tests (Integration, e2e) Artifact management Multi cluster Integration with other tools in the cloud native sector Production best practices Scanning images in Jenkins X pipelines How to make your app use different configuration/secrets for each environment Observability for your apps, logging, metrics, tracing Tests (Integration, e2e) Artifact management Multi cluster Integration with other tools in the cloud native sector Audit the current Jenkins X documentation and create a friction log Use the friction log as a guide for understanding the gaps in the documentation Interact with the Jenkins X community to create a list of top use cases Create a survey Look at the github issues and slack messages Create a survey Look at the github issues and slack messages Work with the volunteers to incorporate those changes into the documentation Establish a feedback loop (the target customers are the end users of Jenkins X) to improve the documentation Decrease in the number of github issues raised by end users on documentation related to common real world use cases Short 1-2 minute videos on common tasks that end users of Jenkins X would like to perform. Must have: Experience with github Communication skills to work with the Jenkins X community Creating online surveys Experience with github Communication skills to work with the Jenkins X community Creating online surveys Nice to have: Basic experience with docker compose (local development scripts use compose) Good video editing skills. Some knowledge working with the documentation of other tools in the kubernetes ecosystem Basic experience with docker compose (local development scripts use compose) Good video editing skills. Some knowledge working with the documentation of other tools in the kubernetes ecosystem Ankit D Mohapatra Christoffer Vig Mårten Svantesson Tom Hobson The project will take roughly 6 months to complete. Once the technical writer is selected, we will do an orientation to bring him up to speed. This is the timeline we have in mind, but we are flexible. The cost of the t-shirt was taken from here. The t-shirts are for the technical writer, volunteers and any member of the jenkins X community (not officially listed as a volunteer) who helps out by answering the technical writer’s questions. Previous participation in Season of Docs, Google Summer of Code or others Jenkins X participated in the 2020 Season of docs. The maturity level matrix created by the technical writer is still used by the community to evaluate if Jenkins X will work for their use case, as well as used by the Jenkins X maintainers to plan the roadmap for improving Jenkins X.</description></item><item><title>Blog: K8s CI Bot Helper Job: automating "make update"</title><link>https://kubermates.org/docs/2022-03-15-blog-k8s-ci-bot-helper-job-automating-make-update/</link><pubDate>Tue, 15 Mar 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-03-15-blog-k8s-ci-bot-helper-job-automating-make-update/</guid><description>K8s CI Bot Helper Job: automating &amp;ldquo;make update&amp;rdquo; Why is this needed? Potential Workaround Possible solutions to counter the above errors The current situation Solutions Implementing the Short Term Workaround Conclusion Authors: Subhasmita Swain , Davanum Srinivas If you are contributing to the Kubernetes project and are developing on a Windows PC, it is conceivable that you will encounter certain issues that will cause your pull request to get held up by test failures. This article describes a workaround for a similar issue I encountered when attempting to have my modifications approved and merged into the master branch. While contributing to kubernetes/kubernetes for some minor documentation changes, the pushed changes needed to be updated with other verified contents of the entire documentation. So, in order for the change to take effect, a single command must be performed to ensure that all tests on the CI pipeline pass. The single command make update runs all presubmission verification tests. For some reason on the “Windows Subsystem for Linux” environment the tests, specifically the update-openapi-spec. sh script, failed (in my case, take a look at the conversation here ), eventually failing the pull-kubernetes-verify tests. make update pull-kubernetes-verify You might encounter the following on your PR The tests failing the particular issue: Consecutively, Additionally one can check the failed test via the link provided under details in the above image. link Run the failing. sh scripts individually known from the CI job output, to generate the expected files to fix up the failures. The. sh scripts can be found residing under the hack/ directory at the root of the kubernetes/kubernetes code base.</description></item><item><title>Blog: Meet Our Contributors - APAC (Aus-NZ region)</title><link>https://kubermates.org/docs/2022-03-14-blog-meet-our-contributors-apac-aus-nz-region/</link><pubDate>Mon, 14 Mar 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-03-14-blog-meet-our-contributors-apac-aus-nz-region/</guid><description>Meet Our Contributors - APAC (Aus-NZ region) Caleb Woodbine Dylan Graham Hippie Hacker Nick Young Authors &amp;amp; Interviewers: Anubhav Vardhan , Atharva Shinde , Avinesh Tripathi , Brad McCoy , Debabrata Panigrahi , Jayesh Srivastava , Kunal Verma , Pranshu Srivastava , Priyanka Saggu , Purneswar Prasad , Vedant Kakde Good day, everyone 👋 Welcome back to the second episode of the “Meet Our Contributors” blog post series for APAC. This post will feature four outstanding contributors from the Australia and New Zealand regions, who have played diverse leadership and community roles in the Upstream Kubernetes project. So, without further ado, let’s get straight to the article. Caleb Woodbine is currently a member of the ii. nz organisation. He began contributing to the Kubernetes project in 2018 as a member of the Kubernetes Conformance working group. His experience was positive, and he benefited from early guidance from Hippie Hacker , a fellow contributor from New Zealand. He has made major contributions to Kubernetes project since then through SIG k8s-infra and k8s-conformance working group. SIG k8s-infra k8s-conformance Caleb is also a co-organizer of the CloudNative NZ community events, which aim to expand the reach of Kubernetes project throughout New Zealand in order to encourage technical education and improved employment opportunities. There need to be more outreach in APAC and the educators and universities must pick up Kubernetes, as they are very slow and about 8+ years out of date. NZ tends to rather pay overseas than educate locals on the latest cloud tech Locally. Dylan Graham is a cloud engineer from Adeliade, Australia.</description></item><item><title>Blog: Google Summer of Code 2022 ☀️</title><link>https://kubermates.org/docs/2022-03-12-blog-google-summer-of-code-2022/</link><pubDate>Sat, 12 Mar 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-03-12-blog-google-summer-of-code-2022/</guid><description>Google Summer of Code 2022 ☀️ How many contributors are we looking for? What are we looking for in an ideal Jenkins X contributor? What can prospective contributors do? (March 7 - April 19) Interview process and selection criteria (April 19 and May 12) We are very happy to announce that Jenkins X has been selected to participate in the Google Summer of Code (GSoC) 2022! If you are new to GSoC and want to learn more about it, check out their new site. You can find the list of project ideas from Jenkins X here. If you are interested in applying to Jenkins X, please check the timeline. Apply as early as possible. This post aims to give GSoC contributors insight into the selection process and how to get started. We have 3 mentors signed up at the moment and are looking to select a maximum of 2 applicants. We want you to have a great experience contributing to Jenkins X and learn lots of new things! Willingness to learn Jenkins X and engage with the community Independence and a drive to investigate and propose solutions to complex issues that might occur during implementation (Optional) Eventually become a Jenkins X maintainer (and some day mentor new GSoC contributors)! You do not have to complete all of these tasks. This is presented here so that you have some clarity around where to start and which resources to follow. Recommended: Join the jenkins X community slack channels and say hello :) We prefer you post messages in the jenkins-x-dev channel Attend the office hours We prefer you post messages in the jenkins-x-dev channel Attend the office hours Recommended: Interact with potential mentors to understand/review the requirements of the projects. Recommended: Start working on your proposal early on. Optional: Learn about docker Optional: Learn about kubernetes Optional: Learn the basics of golang Optional: Understand Jenkins X concepts and fundamentals by going over the documentation Optional: Set up Jenkins X locally and give it a go (Please don’t spend money running Jenkins X in the cloud). We prefer you use k3s for this, as it’s less resource intensive and can be installed locally on your workstation (linux).</description></item><item><title>Blog: Project ideas for Google Summer of Code 2022 ☀️</title><link>https://kubermates.org/docs/2022-02-20-blog-project-ideas-for-google-summer-of-code-2022/</link><pubDate>Sun, 20 Feb 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-02-20-blog-project-ideas-for-google-summer-of-code-2022/</guid><description>Project ideas for Google Summer of Code 2022 ☀️ 1. Cloud events integration with Jenkins X 2. Supply chain security: Improve integration with sigstore and look at tekton chains 3. New Jenkins X UI 4. Quickstart Improvements 5. Implement drift detection (gitops) 6. Multi-tenancy in Jenkins X Next Steps We have put together some project ideas as part of our application to participate in the Google Summer of Code 2022 program. The only way to trigger jobs/workflows in Jenkins X at the moment is by listening to events from Source Control Management (SCM) providers like github, gitlab, bitbucket, however it would be nice to listen to other event sources and trigger jobs/pipelines in Jenkins X. One interesting application would be to trigger some Jenkins X job in response to some alerting event (pagerduty, opsgenie). As a start we should focus on (emitting and listening to) cloudevents which define a common format for events produced from different sources. This will also help make Jenkins X compatible with other platforms. Jenkins X should be able to emit cloud events Jenkins X should be able to listen to cloud events, and run pipelines Updated documentation Golang, kubernetes, cloudevents, familiarity with lighthouse would be great, but not required Ankit D Mohapatra Christopher Vig Tom Hobson https://cloudevents.</description></item><item><title>Blog: SIG Node CI Subproject Celebrates Two Years of Test Improvements</title><link>https://kubermates.org/docs/2022-02-16-blog-sig-node-ci-subproject-celebrates-two-years-of-test-improvements/</link><pubDate>Wed, 16 Feb 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-02-16-blog-sig-node-ci-subproject-celebrates-two-years-of-test-improvements/</guid><description>SIG Node CI Subproject Celebrates Two Years of Test Improvements Timeline Statistics Future Authors: Sergey Kanzhelev (Google), Elana Hashman (Red Hat) Ensuring the reliability of SIG Node upstream code is a continuous effort that takes a lot of behind-the-scenes effort from many contributors. There are frequent releases of Kubernetes, base operating systems, container runtimes, and test infrastructure that result in a complex matrix that requires attention and steady investment to “keep the lights on. ” In May 2020, the Kubernetes node special interest group (“SIG Node”) organized a new subproject for continuous integration (CI) for node-related code and tests. Since its inauguration, the SIG Node CI subproject has run a weekly meeting, and even the full hour is often not enough to complete triage of all bugs, test-related PRs and issues, and discuss all related ongoing work within the subgroup. Over the past two years, we’ve fixed merge-blocking and release-blocking tests, reducing time to merge Kubernetes contributors’ pull requests thanks to reduced test flakes. When we started, Node test jobs only passed 42% of the time, and through our efforts, we now ensure a consistent &amp;gt;90% job pass rate. We’ve closed 144 test failure issues and merged 176 pull requests just in kubernetes/kubernetes. And we’ve helped subproject participants ascend the Kubernetes contributor ladder, with 3 new org members, 6 new reviewers, and 2 new approvers. The Node CI subproject is an approachable first stop to help new contributors get started with SIG Node. There is a low barrier to entry for new contributors to address high-impact bugs and test fixes, although there is a long road before contributors can climb the entire contributor ladder: it took over a year to establish two new approvers for the group. The complexity of all the different components that power Kubernetes nodes and its test infrastructure requires a sustained investment over a long period for developers to deeply understand the entire system, both at high and low levels of detail. We have several regular contributors at our meetings, however; our reviewers and approvers pool is still small.</description></item><item><title>Blog: Jenkins X Survey Result Details</title><link>https://kubermates.org/docs/2022-02-15-blog-jenkins-x-survey-result-details/</link><pubDate>Tue, 15 Feb 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-02-15-blog-jenkins-x-survey-result-details/</guid><description>Jenkins X Survey Result Details What do you enjoy the most with Jenkins X? What do you believe should be improved with Jenkins X? For more information on the Jenkins X survey see Survey results Some highlights from the free text answers. Cloud-native feel, “you know Kubernetes, you know how to use JX”, plugins-based architecture Complete CI/CD platform in a k8s cluster Configuration as code, auto generation of k8s files, flexibility easy to create preview/dev/staging/prod environments. modularity and ease to extend/change Everything works out of the box. Very intuitive Lightweight and easy to manage. The community is incredible. I really like how everything works together. love the community. scalability and how fast it is! Integration with different secret backends is easy. Also love the community. It’s opinionated No vendor lock in run it against a local running cluster to test changes to Jenkins-X before updated in GitHub. run it against a local running cluster to test changes to Jenkins-X before updated in GitHub. Integration with on-premise (Gitlab) as a lot of organizations are not using public cloud due to security policies, Integration with on-premise (Gitlab) as a lot of organizations are not using public cloud due to security policies, Proper guide on installing into existing cluster without using terraform.</description></item><item><title>Blog: Jenkins X Survey Results</title><link>https://kubermates.org/docs/2022-02-15-blog-jenkins-x-survey-results/</link><pubDate>Tue, 15 Feb 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-02-15-blog-jenkins-x-survey-results/</guid><description>Jenkins X Survey Results The Jenkins X survey was active for four weeks and closed on February 11 2022. We received lots of valuable insights into how people are using Jenkins X. We need more contributors in the Jenkins X community, so if you feel strongly about how Jenkins X should evolve, your best bet is to dive in and get your hands dirty:) Some highlights of the free text responses we got are collected here According to the survey, a typical Jenkinx X user works with Devops and Software Engineering This person is using Jenkins X version 3 on Amazon working in a company of 1-50 people. She finds it somewhat difficult to find documentation, average 2.9 out of 5 The Jenkins X user tries to find information mainly on the main web site, sometimes on slack and less often on github. What he enjoys most about Jenkins X is that it is an easy way to learn, play around and work with Kubernetes. The git(ops(ish)) style of configuration, the preview environments, and the staging /production environments. But one thing is sure, documentation is confusing and shold be improved. The typical Jenkins X user would like to run Jenkins X offline, either to run on a laptop, or behind a (corporate) firewall. Proper support for multi-tenancy would be nice, and who can ignore security in these times. A clear governance of Jenkins X needs to be established, The typical person who answers surveys on Jenkins X is planning to attend Jenkins X Office hours, which is great! See you there next week then! ← Previous.</description></item><item><title>Blog: Spotlight on SIG Multicluster</title><link>https://kubermates.org/docs/2022-02-04-blog-spotlight-on-sig-multicluster/</link><pubDate>Fri, 04 Feb 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-02-04-blog-spotlight-on-sig-multicluster/</guid><description>Spotlight on SIG Multicluster Introduction A summary of their conversation Wrap up Authors: Dewan Ahmed (Aiven) and Chris Short (AWS) SIG Multicluster is the Special Interest Group (SIG) focused on how Kubernetes concepts are expanded and used beyond the cluster boundary. Historically, Kubernetes resources only interacted within that boundary - KRU or Kubernetes Resource Universe (not an actual Kubernetes concept). Kubernetes clusters, even now, don’t really know anything about themselves or, about other clusters. Absence of cluster identifiers is a case in point. With the growing adoption of multicloud and multicluster deployments, the work SIG Multicluster doing is gaining a lot of attention. In this article, Jeremy Olmsted-Thompson, Google and Chris Short, AWS discuss the interesting problems SIG Multicluster is solving and how you can get involved. Their initials JOT and CS will be used for brevity. CS : How long has the SIG Multicluster existed and how was the SIG in its infancy? How long have you been with this SIG? JOT : I’ve been around for almost two years in the SIG Multicluster. All I know about the infancy years is from the lore but even in the early days, it was always about solving this same problem. Early efforts have been things like KubeFed. I think there are still folks using KubeFed but it’s a smaller slice. Back then, I think people out there deploying large numbers of Kubernetes clusters were really not at a point where we had a ton of real concrete use cases.</description></item><item><title>Blog: January 2022 updates from the JX community</title><link>https://kubermates.org/docs/2022-02-02-blog-january-2022-updates-from-the-jx-community/</link><pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-02-02-blog-january-2022-updates-from-the-jx-community/</guid><description>January 2022 updates from the JX community Community effort Features Bug fixes Documentation improvement Plumbing/Quality improvements Happy new year 2022! This monthly blog post series is an attempt to showcase all the incredible work being done by the Jenkins X community to the wider audience. Lot of exciting features, bug fixes and documentation improvements were made. We restarted the office hours this month ( https://jenkins-x. io/community/#office-hours). Drop by to say hello, we are a friendly group! First Jenkins X survey was also created this month ( https://jenkins-x. io/blog/2022/01/21/survey-1-2022/). We have extended the deadline by 2 weeks (Feb 11, 2022 midnight UTC), so fill it out if you have not yet. We will use this for the roadmap moving forward. Monthly blog post update series to keep up with all the amazing progress. jx-plugins/jx-gitops: Cron job to delete old boot jobs Option to Keep n boot jobs older than default age Cron job to delete old boot jobs Option to Keep n boot jobs older than default age jenkins-x/lighthouse: Support for using gitlab nested repositories Allow running lighthouse with cluster scoped permissions Add a flag to keep polling releases until commit status is successful Do not include tekton roles when tekton engine is disabled Support for using gitlab nested repositories Allow running lighthouse with cluster scoped permissions Add a flag to keep polling releases until commit status is successful Do not include tekton roles when tekton engine is disabled jx-plugins/jx-pipeline: Use pager to aid in visualizing long pipeline logs Use pager to aid in visualizing long pipeline logs jenkins-x-terraform/terraform-jx-azure Support provisioning spot instances in the azure terraform jx module Support provisioning spot instances in the azure terraform jx module jx-plugins/jx-verify: Support specifying label for jx verify install Support specifying label for jx verify install jenkins-x-plugins/jx-registry: Support adding ECR registry policy using jx-registry Support adding ECR registry policy using jx-registry jenkins-x-charts/jxboot-helmfile-resources: Make all storage locations available as envrironment variables Make all storage locations available as envrironment variables GoogleContainerTools/kaniko (upstream fix - outside jx codebase): Kaniko in jx pipelines can now push to ACR (Azure Container Registry) Kaniko in jx pipelines can now push to ACR (Azure Container Registry) jenkins-x/terraform-aws-eks-jx: Remove deprecated jx v2 keys from requirements configmap Fix issue with on-demand billing mode of dynamodb Remove deprecated jx v2 keys from requirements configmap Fix issue with on-demand billing mode of dynamodb jenkins-x-plugins/jx-promote: Fix local chart check when using cloud buckets to store helm charts Fix local chart check when using cloud buckets to store helm charts jenkins-x-plugins/jx-project: Fix jx project rendering invalid chart. yaml files on import for custom packs in catalog Fix jx project rendering invalid chart. yaml files on import for custom packs in catalog Guides on configuring Azure service principle and GCP service account for terraform users ( https://jenkins-x.</description></item><item><title>Blog: Jenkins X Survey</title><link>https://kubermates.org/docs/2022-01-21-blog-jenkins-x-survey/</link><pubDate>Fri, 21 Jan 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-01-21-blog-jenkins-x-survey/</guid><description>Jenkins X Survey We have made a short survey where we try to gain insight into how people experience Jenkins X. This is meant to be used as guidelines going forward so we can be more focused on what areas to improve. All contributions are welcome, if you are just browsing or have used it for years, we want to know you all! We have extended the survey period and it will now be open untill Tuesday February 11 2022 midnight UTC. All submissions are anonymous and the results will be published. So don’t hesitate, fill in the Jenkins X survey today ← Previous.</description></item><item><title>Blog: Meet Our Contributors - APAC (India region)</title><link>https://kubermates.org/docs/2022-01-10-blog-meet-our-contributors-apac-india-region/</link><pubDate>Mon, 10 Jan 2022 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2022-01-10-blog-meet-our-contributors-apac-india-region/</guid><description>Meet Our Contributors - APAC (India region) Arsh Sharma Kunal Kushwaha Madhav Jivarajani Rajas Kakodkar Rajula Vineeth Reddy Authors &amp;amp; Interviewers: Anubhav Vardhan , Atharva Shinde , Avinesh Tripathi , Debabrata Panigrahi , Kunal Verma , Pranshu Srivastava , Pritish Samal , Purneswar Prasad , Vedant Kakde Editor: Priyanka Saggu Good day, everyone 👋 Welcome to the first episode of the APAC edition of the “Meet Our Contributors” blog post series. In this post, we’ll introduce you to five amazing folks from the India region who have been actively contributing to the upstream Kubernetes projects in a variety of ways, as well as being the leaders or maintainers of numerous community initiatives. 💫 Let’s get started, so without further ado… Arsh is currently employed with Okteto as a Developer Experience engineer. As a new contributor, he realised that 1:1 mentorship opportunities were quite beneficial in getting him started with the upstream project. He is presently a CI Signal shadow on the Kubernetes 1.23 release team. He is also contributing to the SIG Testing and SIG Docs projects, as well as to the cert-manager tools development work that is being done under the aegis of SIG Architecture. To the newcomers, Arsh helps plan their early contributions sustainably. I would encourage folks to contribute in a way that’s sustainable. What I mean by that is that it’s easy to be very enthusiastic early on and take up more stuff than one can actually handle. This can often lead to burnout in later stages. It’s much more sustainable to work on things iteratively. Kunal Kushwaha is a core member of the Kubernetes marketing council.</description></item><item><title>Blog: Incident: Kaniko and ACR</title><link>https://kubermates.org/docs/2021-12-28-blog-incident-kaniko-and-acr/</link><pubDate>Tue, 28 Dec 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-12-28-blog-incident-kaniko-and-acr/</guid><description>Incident: Kaniko and ACR So what happened? So what are you going to do about it? How can I fix it in the meantime? We’re hoping to make this simpler Help us find and fix things like this in future We’ve recently had an issue with one of our packages come to light. We wanted to talk through the resolution steps we’re going to put into place. Azure users started reporting seeing the following error within the build step: error checking push permissions &amp;ndash; make sure you entered the correct tag name, and that you are authenticated correctly, and try again: checking push permission for &amp;ldquo;xyz. azurecr. io/myorg/myrepo:0.0.1&amp;rdquo;: resolving authorization for xyz. azurecr. io failed: error getting credentials - err: exec: &amp;ldquo;docker-credential-acr-env&amp;rdquo;: executable file not found in $PATH error checking push permissions &amp;ndash; make sure you entered the correct tag name, and that you are authenticated correctly, and try again: checking push permission for &amp;ldquo;xyz. azurecr. io/myorg/myrepo:0.0.1&amp;rdquo;: resolving authorization for xyz. azurecr. io failed: error getting credentials - err: exec: &amp;ldquo;docker-credential-acr-env&amp;rdquo;: executable file not found in $PATH This seemed to be indicating to an authorization issues with the terraform module. Other users were seemingly unaffected by this issue.</description></item><item><title>Blog: Announcing the Kubernetes Contributor Celebration 2021</title><link>https://kubermates.org/docs/2021-12-07-blog-announcing-the-kubernetes-contributor-celebration-2021/</link><pubDate>Tue, 07 Dec 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-12-07-blog-announcing-the-kubernetes-contributor-celebration-2021/</guid><description>Announcing the Kubernetes Contributor Celebration 2021 Contributor Celebration 2021 Registration Events and Activities Schedule See you all at the event By Debabrata Panigrahi It’s that time of the year again, Yayy!! Like last year, this year also we are back with Kubernetes Contributor Celebration, the annual end of the year celebration, to recognize our achievements and have some fun! It’s a time for us to relax, chat and do something fun with your fellow contributors! To register, please fill out the Registration Form. More details on how to join the event. DevOps Party Games “DevOps Party Games takes the idea of “online party games” and tilts it on its head by adding DevOps-inspired content to existing games, and then streams it live via Twitch for a worldwide audience to watch, comment, and hopefully be entertained. In addition, the hosts (Matt Stratton, Jeremy Meiss, and Dan Maher) will provide color commentary, much like a modern day Cotton McKnight and Pepper Brooks (announcers from Dodgeball). ” There will be a Kubernetes slant for this edition of DevOps Party Games. DevOps Party Games “DevOps Party Games takes the idea of “online party games” and tilts it on its head by adding DevOps-inspired content to existing games, and then streams it live via Twitch for a worldwide audience to watch, comment, and hopefully be entertained. In addition, the hosts (Matt Stratton, Jeremy Meiss, and Dan Maher) will provide color commentary, much like a modern day Cotton McKnight and Pepper Brooks (announcers from Dodgeball). ” There will be a Kubernetes slant for this edition of DevOps Party Games. The Great Cloud Native Bakeoff A virtual cooking event styled after the Great British Bake-off will be held during the Kubernetes Celebration Event. Contestants will have 90 minutes to bake something related to to your Cloud Native experience. Anything goes, as long as the majority of preparation is done live. The competition is open to all skill levels, from baking from a cake mix to grinding your own flour as long as it’s somehow Kubernetes-themed.</description></item><item><title>Blog: Improve your documentation with Mermaid.js diagrams</title><link>https://kubermates.org/docs/2021-12-01-blog-improve-your-documentation-with-mermaid-js-diagrams/</link><pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-12-01-blog-improve-your-documentation-with-mermaid-js-diagrams/</guid><description>Improve your documentation with Mermaid. js diagrams Why are diagrams useful for documentation? Mermaid. js What do I need to start using Mermaid? Examples By Chris Metz and Tim Bannister Topics covered in this blog. Live editor link: Topics Covered Friendly landing spot. Greeting readers with a page full of text can be intimidating to those new to Kubernetes, software engineering and tech writing. Friendly landing spot. Greeting readers with a page full of text can be intimidating to those new to Kubernetes, software engineering and tech writing. Faster grasp of concepts. A diagram can serve as a visual roadmap for details covered in the accompanying text. Faster grasp of concepts. A diagram can serve as a visual roadmap for details covered in the accompanying text. Better retention.</description></item><item><title>Blog: Hacktoberfest conclusion 2021</title><link>https://kubermates.org/docs/2021-11-22-blog-hacktoberfest-conclusion-2021/</link><pubDate>Mon, 22 Nov 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-11-22-blog-hacktoberfest-conclusion-2021/</guid><description>Hacktoberfest conclusion 2021 Ask us questions Hacktoberfest 2021 is over, and we got quite a few contributions from the open source community. Contributions included various document improvements, adding jira as an issue tracker for generating changelogs and adding initial support for external vault! The top contributors to Jenkins X in hacktoberfest 2021 were: Marten Svantesson Christopher vig James Rawlings Hays Clark Anatoli Babenia We would also like to thank all the contributors who participated and made it a success. The strength of Jenkins X lies in it’s vast community, and we hope to see many more major contributions from them in the near and far future. Missed this year’s hacktoberfest? No worries, you can always contribute to Jenkins X. We’re happy to help if you have any questions. Talk to us on our slack channels, which are part of the Kubernetes slack. Join Kubernetes slack here and find us on our channels: #jenkins-x-dev for developers of Jenkins X #jenkins-x-user for users of Jenkins X We look forward to participating in the next hacktoberfest! ← Previous.</description></item><item><title>Blog: Hacktoberfest 2021</title><link>https://kubermates.org/docs/2021-10-06-blog-hacktoberfest-2021/</link><pubDate>Wed, 06 Oct 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-10-06-blog-hacktoberfest-2021/</guid><description>Hacktoberfest 2021 Contribute to Jenkins X docs Ask us questions We are excited to announce that Jenkins X will be participating in Hacktoberfest again this year! Hacktoberfest is a month-long global celebration of open source software. All backgrounds and skill levels are encouraged to participate in Hacktoberfest and join a global community of open source contributors. Learn more about Hacktoberfest and sign up here. We welcome your contributions to the Jenkins X documentation project ! Issues labelled “hacktoberfest” generally indicate good first issues. However, all pull requests will count towards your Hacktoberfest challenge. We’re happy to help if you have any questions. Talk to us on our slack channels, which are part of the Kubernetes slack. Join Kubernetes slack here and find us on our channels: #jenkins-x-dev for developers of Jenkins X #jenkins-x-user for users of Jenkins X Find out more about becoming involved in the Jenkins X community here. We look forward to seeing you in open source, fixing all the things! ← Previous.</description></item><item><title>Blog: Moving Jenkins X v2 artifacts</title><link>https://kubermates.org/docs/2021-08-26-blog-moving-jenkins-x-v2-artifacts/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-08-26-blog-moving-jenkins-x-v2-artifacts/</guid><description>Moving Jenkins X v2 artifacts ACTION REQUIRED Why the disruption? What is changing? v2 users v3 users When will all this take place? Why are only the most recent versions v2 images copied to GitHub packages and not all versions? I’m on v2 and use a builder image which is not available on GitHub container registry, how do I build my own version to work with the new helm and image repositories? I’m getting a missing arg &amp;ndash;provider-values-dir and helm repository https://jenkins-x-charts. github. io/v2 does not have an associated prefix in in the ‘charts/repositories. yml’ error TL;DR - Jenkins X specific helm repositories and container registries hosted on GCP have been moved to GitHub. This will mainly affect jx v2 users but there is expected to be a small impact on v3 users too. Below describes the steps we believe are needed to keep Jenkins X installations working as normal but there will be some action needed. When Jenkins X first started we made heavy use of GCP’s services for hosting the cloud infrastructure needed by users to install and run Jenkins X. This was great as we could use the same IAM to push and maintain content from our own hosted build infrastructure and ensure we were validating the same experience of using cloud provider hosted services wherever possible. As Jenkins X grew in popularity the cloud costs began to increase with the pricing model from GCP , specifically the networking costs of cross continent egress. Given this, for jx3 we decided to see if switching to GitHub packages for container images and GitHub pages for helm repositories would be better, the result was it is better. In fact we have made it super easy for users to switch to using GitHub pages for releasing helm charts and using GitHub packages. Now that we have validated GitHub is more cost effective for hosting public images and helm charts for the Jenkins X project, we want to switch to using GitHub for all v2 plus v3 users, then shutdown the GCP services which are causing unnecessary cost.</description></item><item><title>Blog: How to debug your Tekton pipelines</title><link>https://kubermates.org/docs/2021-08-18-blog-how-to-debug-your-tekton-pipelines/</link><pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-08-18-blog-how-to-debug-your-tekton-pipelines/</guid><description>How to debug your Tekton pipelines How to debug Tekton Pipelines Prerequisites Enable a breakpoint Viewing breakpoints Using a breakpoint Opening a shell Continuing after the breakpoint Removing breakpoints Conclusion Tekton recently introduced a debug feature when you create TaskRun resources so that steps can be paused at a breakpoint until told to move forwards so that you can diagnose why pipeline steps fail. TaskRun The latest Tekton release only supports breakpoints on TaskRun resources but there is a Pull Request #4145 to add support also to debugging PipelineRun resources as well. If you are reading this please add your thumbs up emoji feedback to the PR #4145 TaskRun PipelineRun We’ve switched Jenkins X to use a preview image of Tekton with PR #4145 included so that Jenkins X developers can easily debug their pipelines (which typically are PipelineRun resources). PipelineRun Here is a demo which shows how to debug pipelines: Make sure your cluster is upgraded to the latest version stream. If you intend to use the jx in the below examples make sure you upgrade the CLI too jx To enable a breakpoint you can: use the Lens UI as shown in the above video by: right click on a Pipeline action menu select Breakpoint -&amp;gt; Add right click on a Pipeline action menu Pipeline select Breakpoint -&amp;gt; Add Breakpoint -&amp;gt; Add you can use the jx pipeline debug command then select the pipeline to add/remove a breakpoint. You can view breakpoints in the Lens UI in the Breakpoints tab or via: Breakpoints kubectl get lighthousebreakpoints # you can use the short name: kubectl get lhbp kubectl get lighthousebreakpoints # you can use the short name: kubectl get lhbp Once you have set a breakpoint defined for a particular Pipeline you need to trigger the pipeline. e. g. perform a git commit on the git branch to trigger a new pipeline to execute. The pipeline will execute as normal; you’ll be able to view it execute via: Lens UI run jx pipeline grid to watch pipelines run and select the one you wish to view the log run jx pipeline log to watch the log of a specific pipeline Once your breakpoint is reached the pipeline pod will pause, waiting to continue. At this point you can then open a shell inside the container. The easiest way to do this is via the Lens UI , click on the Pipeline action menu then Shell -&amp;gt; latest step and a shell will open.</description></item><item><title>Blog: How to use GitOps and Kubernetes External Secrets for better audit and security</title><link>https://kubermates.org/docs/2021-08-17-blog-how-to-use-gitops-and-kubernetes-external-secrets-for-better-audit-and-secu/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-08-17-blog-how-to-use-gitops-and-kubernetes-external-secrets-for-better-audit-and-secu/</guid><description>How to use GitOps and Kubernetes External Secrets for better audit and security Standardising GitOps layouts Why Jenkins X uses helmfile template Why we use Kubernetes External Secrets How to use this approach to GitOps and Secrets if not using Jenkins X Summary So GitOps is a cool approach to managing kubernetes resources in a cluster, by checking in the source code for: the kubernetes YAMLs details of the helm charts you want to install along with any configuration kustomize scripts. Then everything is versioned and audited; you know who changed what, when and why. If a change breaks things, just revert via git like any other source code change. You can then add pipelines to verify changes in the Pull Requests result in valid kubernetes YAML etc. Then if you merge changes to git then an operator detect the change and do the kubectl apply (or helm install or whatever). kubectl apply helm install There are a number of tools out there for doing this. e. g. Anthos Config Management , argo cd , fleet , flux cd and kapp controller So why did Jenkins X not use these tools and instead created its own git operator ? Over time it would be great to have more standardisation of the Git layout given the different tool. Our current recommended layout that works with many GitOps tools is described here. A number of solutions in the GitOps space define which helm charts to install in git with configuration files; or specify which kustomize templates to apply etc. However Jenkins X defaults to using helmfile to manage installing, upgrading and configuring multiple helm charts.</description></item><item><title>Blog: How to choose a SIG as a non-code Kubernetes contributor</title><link>https://kubermates.org/docs/2021-07-09-blog-how-to-choose-a-sig-as-a-non-code-kubernetes-contributor/</link><pubDate>Fri, 09 Jul 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-07-09-blog-how-to-choose-a-sig-as-a-non-code-kubernetes-contributor/</guid><description>How to choose a SIG as a non-code Kubernetes contributor How I found my SIG How I contribute Hop into the Kubernetes community By Chris Short Kubernetes contributors aren’t people in capes or part of some secret society. How to start committing to the GitHub repos that make up the project is well documented , yet it remains intimidating for many. A few years ago, I spoke at an event and jokingly said, “Kubernetes is just a bunch of APIs and YAML… I’m a contributor; you don’t believe me?” After that talk, someone pulled me aside and asked if I was the Kubernetes contributor. They wanted to get involved in the community. Then came the real question, “I don’t know which special interest group (SIG) I would work in. ” The SIG you work in depends on your skills and what you want to do with your (or your company’s) free time. I’ll start with myself as an example. I do not code as part of my day-to-day work; I never have. While I have made bits of code into programs here and there, I always had a helping hand. It’s also possible my definition of a coder is busted too. But, I consider myself a non-code contributor to Kubernetes. I’m an Ops person that embraced DevOps early.</description></item><item><title>Blog: Jenkins X 3 and Argo CD</title><link>https://kubermates.org/docs/2021-06-28-blog-jenkins-x-3-and-argo-cd/</link><pubDate>Mon, 28 Jun 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-06-28-blog-jenkins-x-3-and-argo-cd/</guid><description>Jenkins X 3 and Argo CD Install Argo CD with Jenkins X Conclusion There have been a number of requests from the Jenkins X community to use Argo CD for the last mile deployment phase of their continuous delivery pipelines. This blog explains some of the advantages of using Jenkins X and Argo CD all together. What’s included? Jenkins X for Cloud Infrastructure using Terraform, core installation management with GitOps, external secret management, ingress controller, quickstarts, automated CI + CD pipelines, ChatOps Tekton for pipeline orchestration Argo CD for end users application deployments (not the main installation) Argo CD provides a declarative GitOps approach to deploying Kubernetes based applications. There is a GUI which helps construct the deployment definition (in the form of an Application custom resource) which offers a number of configuration options, as well and providing insight into your clusters applications. Application You might be wondering why you would want to use BOTH Jenkins X and Argo CD together. Jenkins X aims to embrace OSS, where possible providing a nice UX to integrate with other projects and help provide better solutions for building, developing and running software on Kubernetes. Jenkins X indeed does have a git operator that applies Kubernetes YAML from a Git repository but there are some differences: Jenkins X uses GitOps principles for the entire installation, i. e. the starting point is a Git repository which is used to provision a cluster and manage (automatic if users wish) upgrades whereas today Argo CD uses a manual kubectl apply to manage the Argo installation itself. Jenkins X uses GitOps principles for the entire installation, i. e. the starting point is a Git repository which is used to provision a cluster and manage (automatic if users wish) upgrades whereas today Argo CD uses a manual kubectl apply to manage the Argo installation itself.</description></item><item><title>Blog: Continuous microservices with databases in Jenkins X</title><link>https://kubermates.org/docs/2021-06-25-blog-continuous-microservices-with-databases-in-jenkins-x/</link><pubDate>Fri, 25 Jun 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-06-25-blog-continuous-microservices-with-databases-in-jenkins-x/</guid><description>Continuous microservices with databases in Jenkins X Before you start Create the quickstart How does it work Previews How we can improve More languages and frameworks Cloud databases More modularity options Conclusion A common question we get asked on the Jenkins X project is how to get started creating microservices that use databases with automated CI/CD with GitOps Promotion and Preview Environments. To make things a little easier to get started we’ve created a new node-postgresql quickstart. If you are using the cloud then we prefer cloud over kubernetes for things like databases, storage, ingress and secret managers so please try use your clouds managed databases if you can. So ideally you’d set up your database via your infrastructure as code solution, such as terraform , and then associate your kubernetes Service Account to a cloud IAM role to access the database. However to provide something simple that just works in any kubernetes cluster this quickstart uses the postgres-operator to manage setting up each database cluster in each environment. So to be able to use this quickstart you will need to install this operator into your cluster. You can add charts to your cluster via the CLI. From inside a git clone of your cluster git repository run the following command: jx gitops helmfile add &amp;ndash;chart commonground/postgres-operator &amp;ndash;repository https://charts. commonground. nl/ &amp;ndash;namespace postgres &amp;ndash;version 1.6.2 jx gitops helmfile add &amp;ndash;chart commonground/postgres-operator &amp;ndash;repository https://charts. commonground. nl/ &amp;ndash;namespace postgres &amp;ndash;version 1.6.2 This will modify the helmfile.</description></item><item><title>Blog: Jenkins X at cdCon</title><link>https://kubermates.org/docs/2021-06-22-blog-jenkins-x-at-cdcon/</link><pubDate>Tue, 22 Jun 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-06-22-blog-jenkins-x-at-cdcon/</guid><description>Jenkins X at cdCon Tuesday, June 22 GitOps Summit Wednesday, June 23 Thursday, June 24 cdCon 2021 is about to start with lots of great sessions. Here’ a list of the Jenkins X related sessions: Best Practices for Secret Management with GitOps - Kara de la Marck , CloudBees GitOps uses Git as the “single source of truth” for declarative infrastructure and enables developers to manage infrastructure with the same Git-based workflows they use to manage a codebase. Having all configuration files version-controlled by Git has many advantages, but best practices for securely managing secrets with GitOps remain contested. Join us in this presentation about GitOps and Secret Management. Attendees will learn about different approaches to secret management with GitOps, the issues involved, and the secret management solutions offered by various tools and platforms. We will discuss the pros and cons of Vault, SOPS, offerings by public cloud providers, and more. GitOps uses Git as the “single source of truth” for declarative infrastructure and enables developers to manage infrastructure with the same Git-based workflows they use to manage a codebase. Having all configuration files version-controlled by Git has many advantages, but best practices for securely managing secrets with GitOps remain contested. Join us in this presentation about GitOps and Secret Management. Attendees will learn about different approaches to secret management with GitOps, the issues involved, and the secret management solutions offered by various tools and platforms. We will discuss the pros and cons of Vault, SOPS, offerings by public cloud providers, and more. MLOps with Jenkins-X: Production-ready Machine Learning by Terry Cox Explore ways to treat Machine Learning assets as first class citizens within a DevOps process as Jenkins-X MLOps Lead, Terry Cox demonstrates how to automate your training and release pipeline in Cloud environments, using the library of ML template projects provided with Jenkins-X.</description></item><item><title>Blog: Don't use docker, use kubernetes</title><link>https://kubermates.org/docs/2021-05-17-blog-don-t-use-docker-use-kubernetes/</link><pubDate>Mon, 17 May 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-05-17-blog-don-t-use-docker-use-kubernetes/</guid><description>Don&amp;rsquo;t use docker, use kubernetes Why use kubernetes instead of docker? How to get kubernetes? How do I connect to kubernetes? How do I replace docker with kubernetes? docker run =&amp;gt; kubectl run docker build =&amp;gt; kubectl build compose =&amp;gt; helm testcontainers =&amp;gt; sidecars / kubedock help! we are not even using kubernetes yet other handy kubectl commands inner loop Conclusion Are you developing software that’s intended to run on kubernetes? If so we recommend not to use docker on your laptop. Docker on Windows/MacOS helps you run a VM that can then run linux containers easily. But why bother? We highly recommend just use a development kubernetes cluster - build and run your containers there instead then you’re closer to a production like environment. why test on a completely different VM and container orchestrator than production? It’s better to test on a similar environment to where you are really going to deploy your code test your kubernetes yaml / helm chart and associated configuration at the same time as you run your containers helps you catch mistakes earlier: it’s not just about running the container image; it’s about lots of other things too like networking, configuration, secrets, storage/volumes, cloud infrastructure, service mesh, liveness/readiness/startup probes - so why not test all of those things rather than just the image? it’s not just about running the container image; it’s about lots of other things too like networking, configuration, secrets, storage/volumes, cloud infrastructure, service mesh, liveness/readiness/startup probes - so why not test all of those things rather than just the image? some corporate environments don’t let you run VMs on your laptop anyway so running docker locally isn’t an option First you’ll need a kubernetes cluster. I fully agree with James Ward that developers should not need to run kubernetes. Friends don’t let friends setup and manage kubernetes clusters by hand :). So try ask your infrastructure team for a development cluster or, if you can, use the cloud to set-up a managed kubernetes cluster. All the public clouds have a relatively straightforward way to spin up a fully managed kubernetes cluster for you that will be relatively inexpensive &amp;amp; they are easy to scale down when you don’t need them. e. g. on Google Cloud it’s a couple of clicks and about 5 minutes later you’ll have a fully managed kubernetes cluster ready to use. Its easy to enable auto-scaling too.</description></item><item><title>Blog: Jenkins X 3 - May 2021 LTS</title><link>https://kubermates.org/docs/2021-05-12-blog-jenkins-x-3-may-2021-lts/</link><pubDate>Wed, 12 May 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-05-12-blog-jenkins-x-3-may-2021-lts/</guid><description>Jenkins X 3 - May 2021 LTS May 2001 LTS release is now available! LTS is a slower cadence version stream which contains a verified set of releases and configurations that have been used by teams tracking the bleeding edge Jenkins X. Included in this release: Protect Pipeline Visualiser with OAuth2 how to docs Terraform repositories are now protected by the Jenkins X version stream external-dns helm chart upgrade to v5.0.0 Reduce external secrets polling of cloud services to reduce cloud costs [Jenkins] for users using Jenkins the Tekton Client plugin is now installed by default Stackdriver format logging enabled when using GKE and services that use jx-logging library. If you enable the Stackdriver API in GCP you will get well formatted logs and alerts via Stackdriver. Jenkins X Grafana dashboards updates with Lighthouse telemetry ← Previous.</description></item><item><title>Blog: Jenkins X 3.x GA is here!</title><link>https://kubermates.org/docs/2021-04-15-blog-jenkins-x-3-x-ga-is-here/</link><pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-04-15-blog-jenkins-x-3-x-ga-is-here/</guid><description>Jenkins X 3. x GA is here! Demo Documentation Changes since the 3. x beta User Changes since 2. x Platform Changes Getting started Final thoughts I’m super excited to announce the 3.0 GA (General Availability) release of Jenkins X! Jenkins X automates your CI/CD on kubernetes to help you accelerate : Automated CI/CD pipelines lets you focus on your actually application code while Jenkins X automatically creates battle tested Tekton CI/CD pipelines for your project which are managed via GitOps so that its super easy to keep your pipelines up to date across your repositories or to upgrade or override pipelines or steps for specific repositories. Automatic promotion of versioned artifacts via GitOps through your Environments such as Staging , Pre-production and Production whether they are running in the same kubernetes cluster or you are using multiple clusters for your environments Staging Pre-production Production Preview Environments lets you propose code changes via Pull Requests and have a Preview Environment automatically created, running your code in kubernetes to get fast feedback from your team before agreeing to merge changes to the main branch ChatOps comment on Pull Requests to give feedback, approve/hold changes, trigger optional pipelines for additional testing and other ChatOps commands Here’s a demo of how to develop code with Jenkins X the main documentation of the changes are: the new architecture with modular plugins and improved extension points what has changed since 3. x started how 3. x compares to 2. x DevOps Guides and DevOps Patterns provides an overview of our learnings in the DevOps space here’s a brief summary of the differences: The following improvements have been made since the first beta : Integrated observability and monitoring with Pipeline Tracing Auto scale preview environments with Osiris Enable auto upgrade to keep your cluster up to date As a user the high level UX of Jenkins X is similar: automated Continuous Delivery pipelines for using tekton for your repositories with automatic promotion between your environments pull requests on your repositories create separate Preview Environments where your team can review your changes and give fast feedback before your changes are approved and merged into the main trunk. we now default to vanilla tekton YAML for defining pipelines while accelerating your tekton with tekton catalog we include an open source dashboard for visualising pipelines and logs which you can invoke via: jx dash jx dash we now use helm (3. x) and helmfile along with optionally kustomize in a GitOps style to define and configure both Jenkins X itself, your tools and applications in any namespace support multi cluster out of the box so you can keep Staging and Production in separate clusters to your development cluster where your pipelines run, you create and release immutable container images and other artifacts. Staging Production to setup or upgrade Jenkins X we use terraform to setup your cloud resources on Azure , Amazon or Google while also supporting on-premises, minkube and OpenShift - see the Admin Guides for more detail the actual installation of kubernetes resources takes place using the git operator so it runs reliably inside the cluster itself the actual installation of kubernetes resources takes place using the git operator so it runs reliably inside the cluster itself we default to using Kubernetes External Secrets to manage all secrets for Jenkins X itself, development tools and your applications too. This means we can support various secret backends such as Alibaba Cloud KMS Secret Manager, Amazon Secret Manager, Azure Key Vault, Hashicorp Vault or GCP Secret Manager It also means we can then check in all kubernetes resources and custom resources directly into git (apart from Kubernetes Secrets ) so that it super easy to version, review and reason about your kubernetes resources in a GitOps way.</description></item><item><title>Blog: Jenkins X 3 - April 2021 LTS</title><link>https://kubermates.org/docs/2021-04-12-blog-jenkins-x-3-april-2021-lts/</link><pubDate>Mon, 12 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-04-12-blog-jenkins-x-3-april-2021-lts/</guid><description>Jenkins X 3 - April 2021 LTS This is the second LTS release for Jenkins X 3. x. LTS is a slower cadence version stream which contains a verified set of releases and configurations that have been used by teams tracking the bleeding edge Jenkins X. Initially when we decided to maintain an LTS version stream we thought we’d aim for monthly releases however this second release comes two months after the first. This has given us more chances to run fixes and chart upgrades on Jenkins X own infrastructure to verify stability. Note This LTS release is intended to be the final one before Jenkins X 3 is made Generally Available so stay tuned for the exciting news coming very soon! We will of course continue to develop and release LTS post GA. Included in this release: General beta bug fixes and helm chart upgrades Enable Observability Enable Slack notifications Support for GitLab, Gitea, BitBucket Server and GitHub Enterprise Please be aware of these changes Breaking changes If using Vault move it outside of being managed by Jenkins X GitOps important notes ← Previous.</description></item><item><title>Blog: Traces for your pipelines</title><link>https://kubermates.org/docs/2021-04-08-blog-traces-for-your-pipelines/</link><pubDate>Thu, 08 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-04-08-blog-traces-for-your-pipelines/</guid><description>Traces for your pipelines How can you benefit from it in your own Jenkins X cluster? What’s next? Now that Jenkins X has solid integration with Grafana for its observability , it’s time to start building fun things! And the first one is tracing for all your pipelines : With it, you can easily see the timings of all your pipelines, stages, and steps. This is great to inspect a “slow” pipeline and quickly see the slower steps. We are using OpenTelemetry to generate a “logical” view of the pipeline, with 1 trace per pipeline and 1 span for each stage and step. By default, these traces are ingested by Grafana Tempo. But if you prefer to export them to a different destination, it’s very easy, and thanks to the OpenTelemetry Collector you can export to a lot of different services. You can see the full list here and here. The trace identifier is also stored in the pipeline itself so that the Jenkins X Pipelines Visualizer UI can link directly to the trace. You just need to enable the observability stack, as explained in the observability admin guide. Then, trigger a pipeline, and once it’s finished, go to the web UI, and click on the “Trace” button on the top-right. That’s it! This is only the first step of native tracing support in Jenkins X. Stay tuned for more! ← Previous.</description></item><item><title>Blog: Jenkins X v3: now with built-in observability</title><link>https://kubermates.org/docs/2021-04-01-blog-jenkins-x-v3-now-with-built-in-observability/</link><pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-04-01-blog-jenkins-x-v3-now-with-built-in-observability/</guid><description>Jenkins X v3: now with built-in observability Platform Observability Continuous Delivery Indicators Roadmap As a Continuous Delivery platform, Jenkins X has a central part in your infrastructure. If it becomes unstable or unusable, it will impact the whole software delivery of your organization. This is why observability is a critical topic for Jenkins X, and work has started to get observability built-in for Jenkins X v3: Platform Observability : visualize logs and metrics for everything running in the Kubernetes cluster: Jenkins X’s own components - Tekton, Lighthouse, cert-manager, … - but also your own applications, that will be deployed either in preview environments or in the staging/prod environments. Continuous Delivery Indicators : visualize pull requests, pipelines, releases, and deployments metrics, collected from cluster events and git events. We’re using Grafana as the central visualization component: the main entry point from which you can get a complete overview of both your application’s lifecycle - development, build, tests, releases, deployments, runtime - and your Continuous Delivery platform. Platform observability is not enabled by default for the moment, so the first step is to enable it, as explained in the platform observability admin guide. Once it’s done, you’ll get a running Grafana instance, pre-configured with data sources for applications logs - using Loki - and applications metrics - using Prometheus. But most important, it comes with a set of pre-defined Grafana dashboards for the main platform components: Tekton, Lighthouse, cert-manager, … Here is an example of such a dashboard, using a mix of data sources to display cert-manager metrics collected by Prometheus - including the certificates expiration dates - and logs collected by Loki/Promtail: Continuous Delivery Indicators’ main goal is to give people insights into their workflows/processes so that they can continuously improve them. This is based on the DORA devops metrics and the SPACE framework. The CD Indicators addon is not enabled by default for the moment, so the first step is to enable it, as explained in the continuous delivery indicators admin guide. Once it’s done, you’ll get a running collector, along with a PostgreSQL database. The collector will listen for various events, both from the cluster and the git repositories, and store pull requests, pipelines, releases, and deployments data in the PostgreSQL database.</description></item><item><title>Blog: Scaling Preview Environments with Osiris</title><link>https://kubermates.org/docs/2021-04-01-blog-scaling-preview-environments-with-osiris/</link><pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-04-01-blog-scaling-preview-environments-with-osiris/</guid><description>Scaling Preview Environments with Osiris Osiris How can you benefit from it in your own Jenkins X cluster? How does it work? One of Jenkins X’s core features is the preview environments : temporary environments created automatically for each Pull Requests, to deploy your application and its dependencies. You can then use this preview environment to run integration tests, or manually use/test your application. This is all great until you have more and more applications, each with a few dependencies (postgresql, mongodb, …) and a few opened pull requests at any time. This means that you’ll get more and more pods running in your Kubernetes cluster, in addition to Jenkins X’s own components, your build pipelines, and of course your staging and production applications - unless you are using multi-cluster. The result is that you’ll need more nodes or bigger nodes. Which means more money. But, these preview environments are in fact idle most of the time: they are only used for the integration tests, and sometimes when someone manually uses them. The rest of the time - including all night for example - they are just staying there, idle, and consuming resources. What if we could easily scale them down when they are idle, and automatically bring them up when we need them? So that a Pull Request staying opened for 2 weeks because someone went on vacation won’t consume resources in your cluster. Enter Osiris ! Initially created by the Deislabs team , Osiris is a Kubernetes component that will automatically scale down your “idle” pods, and scale them up when a request comes in. Although the original project has been archived, the Dailymotion team has taken over the maintenance of a fork. And they have been using it with success in their Jenkins X dev cluster for more than 2 years: they regularly have around 50 preview environments active at any time, and… 0 pods from these environments running at night - or on weekends.</description></item><item><title>Blog: cdCon 2021 - Call for Jenkins X Proposals</title><link>https://kubermates.org/docs/2021-02-25-blog-cdcon-2021-call-for-jenkins-x-proposals/</link><pubDate>Thu, 25 Feb 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-02-25-blog-cdcon-2021-call-for-jenkins-x-proposals/</guid><description>cdCon 2021 - Call for Jenkins X Proposals Topics Hear ye! Hear ye! Jenkins X Community, cdCon 2021 (the Continuous Delivery Foundation’s annual flagship event) is happening June 23-24 and its call for papers is open! This is your chance to share what you’ve been doing with Jenkins X. Are you building something cool? Using it to solve real-world problems? Are you making things fast? Secure? Or maybe you’re a contributor and want to share what’s new. In all cases, we want to hear from you! Submit your talk for cdCon 2021 to be part of the conversation driving the future of software delivery for technology teams, enterprise leadership, and open-source communities. Submission Deadline : Friday, March 5 at 11:59 PM PST Here are the suggested tracks: Continuous Delivery Ecosystem – This track spans the entire Continuous Delivery ecosystem, from workflow orchestration, configuration management, testing, security, release automation, deployment strategies, developer experience, and more. Advanced Delivery Techniques – For talks on the very cutting edge of continuous delivery and emerging technology, for example, progressive delivery, observability, and MLOps. GitOps &amp;amp; Cloud-Native CD – Submit to this track for talks related to continuous delivery involving containers, Kubernetes, and cloud*native technologies. This includes GitOps, cloud-native CD pipelines, chatops, best practices, etc. Continuous Delivery in Action – This track is for showcasing real-world continuous delivery addressing challenges in specific domains e. g. fintech, embedded, healthcare, retail, etc. Talks may cover topics such as governance, compliance, security, etc. Leadership Track – Talks for leaders and decision-makers on topics such as measuring DevOps, build vs buy, scaling, culture, security, FinOps, and developer productivity.</description></item><item><title>Blog: GitOps your cloud native pipelines</title><link>https://kubermates.org/docs/2021-02-25-blog-gitops-your-cloud-native-pipelines/</link><pubDate>Thu, 25 Feb 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-02-25-blog-gitops-your-cloud-native-pipelines/</guid><description>GitOps your cloud native pipelines The problem Previous solutions GitOps your pipelines Sharing Tasks and Steps across repositories SourceURI notation Reusing Tasks and Steps from Tekton Catalog How it looks Customizing an inherited step Inlining a pipeline step locally Viewing the effective pipeline Summary Tekton pipelines are cloud native and are designed from the ground up for kubernetes and the cloud: there’s no single point of failure and the pipelines are elastically scalable each pipeline is completely declarative and self defined each pipeline executes independently of any others pipelines are orchestrated via the sophisticated kubernetes scheduler: can use pipeline specific metadata for resource limits and node selectors: memory, CPU, machine type (GPU, windows/macOS/linux etc) can use pipeline specific metadata for resource limits and node selectors: memory, CPU, machine type (GPU, windows/macOS/linux etc) its easy to associate pipelines with Cloud IAM roles to avoid you having to upload cluster admin secrets to your public CI service which really helps security and helps reduce accidental bitcoin mining on your cloud account In a previous blog we talked about how you can accelerate your use of tekton with Jenkins X. We are moving towards a microservice kind of world with many teams writing many bits of software in many repositories. So there are lots and lots of pipelines. These pipelines keep getting more sophisticated over time; doing much more (all kinds of building, analysis, reporting, testing, ChatOps etc) and the software/images/approaches they use change. So how can we manage, configure and maintain them all so that there are many pipelines for many repositories; where each repository can customise anything it needs but we can easily maintain everything continuously and its easy to understand and tool around? We’ve tried to tackle this problem in a number of ways over the years; each has pros and cons. One option is to put all your pipelines in a shared library. You can then reference the pipelines by name in each of your repositories. But what if you want to change a bit of a pipeline for a specific repository? If you change it globally for everyone you can break things. You may just want local customisation for your repository only. You can add parameters into your pipelines. They are quite verbose on Pipelines and PipelineRuns ; but it’s hard to think up front of every parameterisation that may be required by downstream repositories. e.</description></item><item><title>Blog: Jenkins X 3 - February 2021 LTS</title><link>https://kubermates.org/docs/2021-02-01-blog-jenkins-x-3-february-2021-lts/</link><pubDate>Mon, 01 Feb 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-02-01-blog-jenkins-x-3-february-2021-lts/</guid><description>Jenkins X 3 - February 2021 LTS What’s the difference? Anything else to be aware of? What’s next? This is the first LTS release for Jenkins X 3. x. We are still in the Beta release and the leadup to GA includes ensuring the process for LTS monthly releases is validated and working well. This first releases focuses on: community feedback following the Beta release general helm chart upgrades improved developer UX when editing Tekton pipelines here support for in-repo and shared Tekton pipeline libraries including git URI support, e. g. uses:jenkins-x/jx3-pipeline-catalog/packs/javascript/. lighthouse/jenkins-x/pullrequest. &lt;a href="mailto:yaml@v1.2.3"&gt;yaml@v1.2.3&lt;/a&gt; &lt;code&gt;uses:jenkins-x/jx3-pipeline-catalog/packs/javascript/. lighthouse/jenkins-x/pullrequest. yaml@v1.2.3&lt;/code&gt; more documentation and examples can be found here Because Jenkins X uses GitOps we can see the git diff of changes that will be brought in with a cluster upgrade. Here is the Pull Request that has been verified for February LTS release. https://github.</description></item><item><title>Blog: Jenkins X 3.x walkthroughs</title><link>https://kubermates.org/docs/2021-01-26-blog-jenkins-x-3-x-walkthroughs/</link><pubDate>Tue, 26 Jan 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-01-26-blog-jenkins-x-3-x-walkthroughs/</guid><description>Jenkins X 3. x walkthroughs Intro + high level architecture Installation and setup Infrastructure and provisioning TLS and DNS Using Jenkins X GitOps Health Extending pipelines Secrets Upgrades Version Streams Cluster recovery Jenkins X 3. x is now looking ahead towards a GA release, with that we are producing walkthroughs for key areas to help users not only get started but get the most out of Jenkins X. To kick this off we are going to start with 9 videos that we’ll follow up with more dedicated blogs over the coming weeks. The complete playlist can be found here however the blog below gives a more context for each one. There are a few key areas we are focusing on here: Starting off with a very quick introduction including what to expect from the walkthrough series. Jenkins X 3. x has focussed on clearer lines of separation, making the architecture significantly more pluggable, extensible and maintainable. With better tooling including UIs and more reliable guard rails for installations and upgrades. Jenkins X 3 also minimises abstractions and wrapping; so it promotes the direct use of open source projects like Helm, Helmfile and Tekton. Decoupling the management of Cloud infrastructure away from Jenkins X to tools that are better suited for the job. Jenkins X has started with Terraform and this manages all the cloud resources needed by Jenkins X Kubernetes cluster Cloud Service Accounts IAM bindings Storage buckets Over time Jenkins X plans to support other tools (aided by the Kubernetes Cluster API ) users in the Kubernetes ecosystem leverage such as crossplane.</description></item><item><title>Blog: New features in the pipelines visualizer UI</title><link>https://kubermates.org/docs/2021-01-18-blog-new-features-in-the-pipelines-visualizer-ui/</link><pubDate>Mon, 18 Jan 2021 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2021-01-18-blog-new-features-in-the-pipelines-visualizer-ui/</guid><description>New features in the pipelines visualizer UI Pipeline View Homepage Roadmap Contributing The Jenkins X Pipelines Visualizer UI has recently received a number of new features, so let’s do a little tour of these new features! When viewing a pipeline, the biggest new feature is the collapsed logs. No more hundreds - or thousands - of log lines, we now group the logs per-container (step), which are collapsed by default. Along with the status of the step and its duration, so it’s easier to go to the interesting part of the logs. Clicking on a log line will expand the logs for this specific container. You can also use the “Toggle Steps” button to expand/collapse the logs for all the steps at once. While we’re talking about the logs, you can notice the 2 new buttons: View raw logs Download raw logs On top of the logs, we now display some information about the pipeline: the pipeline meta information : name, context, build, and a link to see the raw YAML representation of the pipeline the pipeline status : status, started/finished date/time, and duration the pipeline source : git repository, pull request or branch, commit SHA, author the pipeline stages , with links to see the timeline of the steps in each stage. You can also click on the “Show Timeline” button to view the pipeline timeline with all stages and steps. The pipeline timeline has been improved to include all the steps for all stages, but it is currently hidden by default - to avoid using too much space. Clicking on a stage will bring you to the steps, and clicking on a step will bring you to the logs for this step. Note that for a pipeline which includes a deployment to a Preview Environment, the UI will also display a link to the application’s URL in that specific Preview Environment. The homepage got some love too, with: a few stats about the pipelines: top statuses, repositories, authors and durations - with links to filter the pipelines direct links to the git repositories and pull requests the Jenkins X logo and a favicon We started this project at v0, and we believe that now it has enough features to be a v1! On our roadmap - without any specific order - we have: #73 live refresh of a running pipeline - for now only the logs are updated live, not the meta information of the pipeline (status, stages/steps timings) #42 support local timezone - for now everything is in UTC improve the support for archived pipelines : load pipelines archived in the long-term storage Thanks to all the contributors! All contributions are welcomed, the source code is: github. com/jenkins-x/jx-pipelines-visualizer.</description></item><item><title>Blog: Jenkins X 3.x beta is here!</title><link>https://kubermates.org/docs/2020-12-09-blog-jenkins-x-3-x-beta-is-here/</link><pubDate>Wed, 09 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2020-12-09-blog-jenkins-x-3-x-beta-is-here/</guid><description>Jenkins X 3. x beta is here! User Changes Platform Changes Getting started Final thoughts I’m super excited to announce the 3.0 beta of Jenkins X! Christmas has come early this year! the main documentation of the changes are: the new architecture with modular plugins and improved extension points what has changed since 3. x started how 3. x compares to 2. x but here’s a brief summary of the differences: As a user the high level UX of Jenkins X is similar: automated Continuous Delivery pipelines for using tekton for your repositories with automatic promotion between your environments pull requests on your repositories create separate Preview Environments where your team can review your changes and give fast feedback before your changes are approved and merged into the main trunk. we now default to vanilla tekton YAML for defining pipelines while accelerating your tekton with tekton catalog we include an open source dashboard for visualising pipelines and logs which you can invoke via: jx dash jx dash we now use helm (3. x) and helmfile along with optionally kustomize in a GitOps style to define and configure both Jenkins X itself, your tools and applications in any namespace support multi cluster out of the box so you can keep Staging and Production in separate clusters to your development cluster where your pipelines run, you create and release immutable container images and other artifacts. Staging Production to setup or upgrade Jenkins X we use terraform to setup your cloud resources on Azure , Amazon or Google while also supporting on-premises, minkube and OpenShift - see the Admin Guides for more detail the actual installation of kubernetes resources takes place using the git operator so it runs reliably inside the cluster itself the actual installation of kubernetes resources takes place using the git operator so it runs reliably inside the cluster itself we default to using Kubernetes External Secrets to manage all secrets for Jenkins X itself, development tools and your applications too. This means we can support various secret backends such as Alibaba Cloud KMS Secret Manager, Amazon Secret Manager, Azure Key Vault, Hashicorp Vault or GCP Secret Manager It also means we can then check in all kubernetes resources and custom resources directly into git (apart from Kubernetes Secrets ) so that it super easy to version, review and reason about your kubernetes resources in a GitOps way. Secrets built in TLS and DNS support along with Heath reporting and visualising via kuberhealthy we now have an LTS distribution which lets you switch to a much more slower cadence of releases of Jenkins X We have been using Jenkins X 3. x in production now for many months (for CI/CD of all of the 3. x codebase and continuously upgrading our cluster in the standard way and it’s been much simpler and easier to use, operate and configure.</description></item><item><title>Blog: Jenkins X 3.x - beta is close!</title><link>https://kubermates.org/docs/2020-12-04-blog-jenkins-x-3-x-beta-is-close/</link><pubDate>Fri, 04 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2020-12-04-blog-jenkins-x-3-x-beta-is-close/</guid><description>Jenkins X 3. x - beta is close! It has been ‘all hands on deck’ in recent months with the focus on Jenkins X 3 alpha. First off a huge thankyou to everyone involved. The OSS community spirit has really shone through what has been a very difficult year for everyone. Knowing that people from all over the world come and help each other, share banter and work at all hours of the day to help build out a true open source cloud native continuous delivery solution for developers - it’s quite fantastic to see and amazing to be apart of. As a result of all this hard work the Beta is iminent so this is a good opportunity to thank all involved so far and to outline what to expect in the coming days. While we’ve been in the Alpha phase it has provided us with the opportunity to deprecate and remove APIs, commands and obsolete features that existed in v2. This means we will not have any code dependency on the v2 codebase and so going forward v3 will be easier to maintain without the tech debt. With that, we aim to make a big push and roll out a few last changes in preparation for Beta, here’s a couple you will notice if you are already on the Alpha. We recommend taking time to understand these, and avoid upgrading for a few days so that changes can be handled in one go, as there will be a constant stream of larger updates happening: jx requirements - this is the yaml file used to describe install needs for Jenkins X, until now there have been options available that were unsupported, confusing and in some cases did nothing. These have now all been removed and the structure of the file has changed to be CRD like including an API version. Upon upgrade jx gitops upgrade will migrate your jx-requirements.</description></item><item><title>Blog: Accelerate your Tekton with Jenkins X</title><link>https://kubermates.org/docs/2020-11-11-blog-accelerate-your-tekton-with-jenkins-x/</link><pubDate>Wed, 11 Nov 2020 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2020-11-11-blog-accelerate-your-tekton-with-jenkins-x/</guid><description>Accelerate your Tekton with Jenkins X Version 2. x Vision Version 3. x Using Tekton in your repository Reusing Tekton Catalog Tasks Sharing steps between Tasks Custom Pipeline Catalogs Conclusion One of the goals of Jenkins X has always been to help accelerate and automate Continuous Delivery so that developers can focus on delivering value to their customers; either by creating that new microservice or adding features to an existing project and not writing and managing pipelines. Pipeline engines like Jenkins and Tekton are awesome - they can do anything! But they start as a blank sheet of paper where you have to fill in all the details of how to compile your code, test it, verify it, tag it, release, distribute and delivery it to production. Figuring all that stuff out can take a huge amount of time to create and maintain. This gets even more complex as we are all creating more and more microservices each with their own pipelines making more and more things to create and manage. We want to be able to reuse pipelines and tasks to get work done. But at the same time we want flexibility; not all applications are the same and sometimes things need to be changed on a per team or application basis. In Jenkins X 2. x we went with a jenkins-x. xml approach to pipelines which let you inherit pipelines from reuable pipeline library and then use a composition DSL above Tekton which lets you add/remove/replace steps. jenkins-x.</description></item><item><title>Blog: Hacktoberfest</title><link>https://kubermates.org/docs/2020-09-23-blog-hacktoberfest/</link><pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2020-09-23-blog-hacktoberfest/</guid><description>Hacktoberfest Contribute to Jenkins X Contribute to jx source code Contribute to the docs Ask us questions We are excited to announce that Jenkins X will be participating in Hacktoberfest again this year! Hacktoberfest is a month-long global celebration of open source software. From October 1 to October 31, submit four pull requests to qualify for the limited edition Hacktoberfest shirt. All backgrounds and skill levels are encouraged to participate in Hacktoberfest and join a global community of open source contributors. Learn more about Hacktoberfest and sign up here. We welcome your contributions to the Jenkins X project! Issues labelled “hacktoberfest” generally indicate good first issues. However, all pull requests will count towards your Hacktoberfest challenge. Jenkins X welcomes contributors to both: the Jenkins X jx source code jx the Jenkins X documentation website jx There are plenty of open issues , and we welcome your help in making Jenkins X even more awesome. Jenkins X is written largely in Go, but you don’t need to be an expert to contribute! If you are new to the project, search for issues labelled “good-first-issue”. Our Contributing Guide has advice for getting started with contributing to Jenkins X. We welcome your help in improving the Jenkins X documenation. If you see areas of the documentation that need fixing or augmentation please raise a pull request. Our guide for Contributing to the Documentation has advice for getting started with contributing to the Jenkins X docs.</description></item><item><title>Blog: New UI to visualize your pipelines and logs</title><link>https://kubermates.org/docs/2020-09-23-blog-new-ui-to-visualize-your-pipelines-and-logs/</link><pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2020-09-23-blog-new-ui-to-visualize-your-pipelines-and-logs/</guid><description>New UI to visualize your pipelines and logs Why a new UI? Features Roadmap Demo Next steps Welcome to the Jenkins X Pipelines Visualizer : a new open-source read-only UI for Jenkins X, with a very specific goal and scope: visualize the pipelines and logs. This project was started at Dailymotion and quickly shared with the Jenkins X community. There is already the Octant-based UI , so why a new UI? The main reason is that Octant “is an application and is intended as a single client tool and at this time there are no plans to support hosted versions of Octant” - see this thread on the Octant github repository for more information and details. So while Octant answers to a lot of use-cases, there is one for which it is not suited: quickly printing the build logs on a browser, for a specific pipeline. We want to be able to click on a link from a Pull/Merge Request, and get the pipeline logs. This is the specific use-case covered by the Pipelines Visualizer. We want to keep it small, focused, and fast. It’s a read-only UI, so there won’t be “actions” to trigger a pipeline - because it can already be done using “chatops” commands in the Pull Request for example. But there are a few interesting features already: first, it’s very fast to get the logs. Much faster than the old JXUI. it can retrieve the logs from pipelines that have been garbage-collected - if you configure the URL of the buckets where the logs are stored. it has URLs compatible with the old JXUI - so it’s very easy to replace the old JXUI with this new UI and keep all the links working.</description></item><item><title>Blog: Jenkins X Talks at CDCon</title><link>https://kubermates.org/docs/2020-09-16-blog-jenkins-x-talks-at-cdcon/</link><pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2020-09-16-blog-jenkins-x-talks-at-cdcon/</guid><description>Jenkins X Talks at CDCon Wednesday, October 7 Thursday, October 8 Register for CDCon There will be six Jenkins X talks given by various speakers at the first-ever CDCon on October 7-8. The two-day virtual event, hosted by the Continuous Delivery Foundation, will focus on improving the world’s capacity to deliver software with security and speed. Register to attend the virtual event for only 25 USD and get access to all of the following Jenkins X talks and more. A CI/CD Framework for Production Machine Learning at Massive Scale (using Jenkins X and Seldon Core) Speaker: Alejandro Saucedo, Seldon Time: 1:15 PM PDT Managing production machine learning systems as internal data science infrastructure requirements grow, has uncovered new challenges which have required fundamentally different approaches to that of traditional CI/CD in software engineering. In this talk we will dive into the work we are doing at the SIG-MLOps and the CD Foundation towards developing the methodologies that encompass best practices to continuously integrate and deploy machine learning in production at massive scale. In this talk, we’ll provide key insights on the core MLOps concepts, as well as a hands-on coding example where we take a text classification model through its training, deployment and promotion as canary and shadow deployments, which will also allow us to get deeper and more specific insight on our production environment. Dailymotion’s Continuous Delivery Story Speaker: Vincent Behar, Dailymotion Time: 3:30 PM PDT In this session, Vincent will share Dailymotion’s Continuous Delivery story with Jenkins, Jenkins X and Tekton. He will come back on the initial state and issues faced on the CI/CD topic, and how it was solved. He will insist on the practices that were put in place and the benefits that resulted from switching to Jenkins X. He will conclude with the new challenges brought by improving Dailymotion’s Continuous Delivery platform. If you are wondering if Jenkins X is the right tool for you, and the impact it can have on your team(s), then this is the right session for you! Moving from Jenkins to Jenkins X: Scaling and Accelerating CI/CD Speaker: Dr Michael Garbade, Education Ecosystem Time: 12:00 PM PDT Jenkins has served as a continuous integration (CI) tool long before the emergence of Kubernetes and distributed systems running on cloud-native platforms. Working with Jenkins as a stand-alone open-source tool has proved to be extremely difficult for distributed systems engineers, as it is designed for small projects and not scalable to bigger projects.</description></item><item><title>Blog: Welcome to Jenkins X 3.x alpha!</title><link>https://kubermates.org/docs/2020-09-16-blog-welcome-to-jenkins-x-3-x-alpha/</link><pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2020-09-16-blog-welcome-to-jenkins-x-3-x-alpha/</guid><description>Welcome to Jenkins X 3. x alpha! We are very pleased to announce the alpha release of Jenkins X version 3 You can read more about the overview of the architecture and components here. This release has lots of benefits over version 2 is much easier to use, understand and manage. It is more flexible and simpler to configure for different infrastructures and cloud providers. The new release works well with any combination of helm 3, helmfile, kpt and/or kustomize. For those who’ve used Jenkins X version 2 you can check out a comparison of the two versions Here is a demo video showing how to get started with Jenkins X on Google Cloud with Terraform and Vault : We also did a live demo of getting started and using version 3 at the octant office hours last week. We also included the helmfile based preview environments enhancement via the new jx-preview plugin which makes it much easier to make more sophisticated previews such as using multiple namespaces for you previews using per preview namespaces or deploying previews into shared namespaces (e. g. via Canaries) using helmfile to cleanly remove resources We now have support, via Terraform for 2 of the big 3 public clouds: Amazon Google Azure support is getting really close; if you’d like to help get it ready join us on slack Also when using your laptop or local kubernetes cluster without terraform we support: Minikube so you can run Jenkins X on your laptop On-Premises so you can use any vanilla kubernetes cluster We are working on improving the UX of the installation/upgrade; we’re hoping to soon have a pure terraform (or Terraform Cloud) way to spin up a Jenkins X installation on a public cloud with a minimum of fuss. We’ll hopefully blog about that soon… So please take it for a spin and let us know what you think ! If you can think of any ways we can improve let us know! ← Previous.</description></item><item><title>Blog: Octant: the OSS UI for Jenkins X</title><link>https://kubermates.org/docs/2020-08-06-blog-octant-the-oss-ui-for-jenkins-x/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://kubermates.org/docs/2020-08-06-blog-octant-the-oss-ui-for-jenkins-x/</guid><description>Octant: the OSS UI for Jenkins X Why Octant? Features Demo A common question we have heard in the community over the years is Is there an open source UI for Jenkins X?. Well we now have an answer: its Octant using the octant-jx plugin. We love Octant because: open source and very easy to extend with plugins in Go or TypeScript/JavaScript lets you visualise and work with all kubernetes and custom resources across multiple clusters thanks to octant-jx has awesome integration with Jenkins X components like apps, environments, pipelines, repositories etc. Longer term we’re planning on making most of the developer and operations features of Jenkins X available through the UI via octant-jx. e. g. we hope as part of Jenkins X 3. x you’ll be able to install or upgrade Jenkins X and watch the installation proceed all via Octant. But already right now today you can: view applications, environments, pipelines, repositories for a pipeline quickly navigate to: its Pod, Log, Pull Request or Preview Environment for each step you can view the step detail or log of the step its Pod, Log, Pull Request or Preview Environment for each step you can view the step detail or log of the step see the various jobs and pipelines used to operate Jenkins X itself over time will add management UI capabilities for installing, upgrading and administering Jenkins Find out more about installing and using Octant here. We did a demo of octant-jx at the last office hours. We also presented octant-jx at the octant office hours this week. Here is a demo video showing octant in action with Jenkins X : ← Previous.</description></item></channel></rss>