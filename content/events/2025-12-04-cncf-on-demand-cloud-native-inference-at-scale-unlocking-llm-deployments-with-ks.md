---
title: 'CNCF On-Demand: Cloud Native Inference at Scale - Unlocking LLM Deployments
  with KServe'
date: '2025-12-04T08:00:00+00:00'
endDate: null
location: Online (US)
city: null
country: null
region: null
lat: null
lon: null
organizer: null
mode: online
tags:
- cncf
- event
source: cncf-api
external_url: https://community.cncf.io/events/details/cncf-cncf-online-programs-presents-cncf-on-demand-cloud-native-inference-at-scale-unlocking-llm-deployments-with-kserve/
draft: false
uid: 7894725760c32fa5
provider: cncf
---
LLM inference has unique challenges—long prompts, token-by-token generation, bursty traffic, and the need for high GPU utilization—making routing, scheduling, and autoscaling harder than standard ML serving. This talk covers how KServe enables scalable, cost-efficient, Kubernetes-native LLM inferenc
