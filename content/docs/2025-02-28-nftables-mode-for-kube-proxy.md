---
title: NFTables mode for kube-proxy
date: '2025-02-28T00:00:00+00:00'
tags:
- kubernetes
source: Kubernetes Blog
external_url: https://kubernetes.io/blog/2025/02/28/nftables-kube-proxy/
post_kind: link
draft: false
tldr: A new nftables mode for kube-proxy was introduced as an alpha feature in Kubernetes
  1. 29.
summary: 'A new nftables mode for kube-proxy was introduced as an alpha feature in
  Kubernetes 1. 29. Currently in beta, it is expected to be GA as of 1. 33. The new
  mode fixes long-standing performance problems with the iptables mode and all users
  running on systems with reasonably-recent kernels are encouraged to try it out.
  (For compatibility reasons, even once nftables becomes GA, iptables will still be
  the default. ) The iptables API was designed for implementing simple firewalls,
  and has problems scaling up to support Service proxying in a large Kubernetes cluster
  with tens of thousands of Services. In general, the ruleset generated by kube-proxy
  in iptables mode has a number of iptables rules proportional to the sum of the number
  of Services and the total number of endpoints. In particular, at the top level of
  the ruleset, there is one rule to test each possible Service IP (and port) that
  a packet might be addressed to: This means that when a packet comes in, the time
  it takes the kernel to check it against all of the Service rules is O(n) in the
  number of Services. As the number of Services increases, both the average and the
  worst-case latency for the first packet of a new connection increases (with the
  difference between best-case, average, and worst-case being mostly determined by
  whether a given Service IP address appears earlier or later in the KUBE-SERVICES
  chain). By contrast, with nftables, the normal way to write a ruleset like this
  is to have a single rule, using a "verdict map" to do the dispatch: Since there''s
  only a single rule, with a roughly O(1) map lookup, packet processing time is more
  or less constant regardless of cluster size, and the best/average/worst cases are
  very similar: But note the huge difference in the vertical scale between the iptables
  and nftables graphs! In the clusters with 5000 and 10,000 Services, the p50 (average)
  latency for nftables is about the same as the p01 (approximately best-case) latency
  for iptables. In the 30,000 Service cluster, the p99 (approximately worst-case)
  latency for nftables manages to beat out the p01 latency for iptables by a few microseconds!
  Here''s both sets of data together, but you may have to squint to see the nftables
  results!: While the improvements to data plane latency in large clusters are great,
  there''s another problem with iptables kube-proxy that often keeps users from even
  being able to grow their clusters to that size: the time it takes kube-proxy to
  program new iptables rules when Services and their endpoints change.'
---
Open the original post â†— https://kubernetes.io/blog/2025/02/28/nftables-kube-proxy/
